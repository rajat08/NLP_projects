{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2A3pym-53gl"
   },
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Generating Shakespeare with a Character-Level RNN\n",
    "\n",
    "[In the RNN classification tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) we used a RNN to classify text one character at a time. This time we'll generate text one character at a time.\n",
    "\n",
    "```\n",
    "> python generate.py -n 500\n",
    "\n",
    "PAOLTREDN:\n",
    "Let, yil exter shis owrach we so sain, fleas,\n",
    "Be wast the shall deas, puty sonse my sheete.\n",
    "\n",
    "BAUFIO:\n",
    "Sirh carrow out with the knonuot my comest sifard queences\n",
    "O all a man unterd.\n",
    "\n",
    "PROMENSJO:\n",
    "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
    "An as shalp will of that seal think.\n",
    "\n",
    "NUKINUS:\n",
    "And house it to thee word off hee:\n",
    "And thou charrota the son hange of that shall denthand\n",
    "For the say hor you are of I folles muth me?\n",
    "```\n",
    "\n",
    "This one might make you question the series title &mdash; \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more. See the [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) for more on that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TH9CbJsP53go"
   },
   "source": [
    "# Recommended Reading\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
    "\n",
    "* http://pytorch.org/ For installation instructions\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) to get started with PyTorch in general\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general\n",
    "\n",
    "Also see these related tutorials from the series:\n",
    "\n",
    "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) uses an RNN for classification\n",
    "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) builds on this model to add a category as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYxWZoso53gs"
   },
   "source": [
    "# Prepare data\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EVbsKAQw62f7",
    "outputId": "63aacc34-e1fc-47de-ab4c-67a1fb6bf14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fvE_7uwtG8Iq",
    "outputId": "a32fe5d8-bdac-41aa-91d9-e8dd0c10e26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WPhmbYFQ53gw",
    "outputId": "7c774aab-d2f1-4004-d446-47d574a4f6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 893170\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "\n",
    "file = unidecode.unidecode(open('data/speeches.txt').read())\n",
    "#file = unidecode.unidecode(open('drive/My Drive/Colab Notebooks/speeches.txt').read())\n",
    "file = re.sub(\"SPEECH\\s+\\d+\", \"\", file)  # Remove the speech headers\n",
    "file = re.sub(\"\\n+\", \"\\n\", file)\n",
    "file = re.sub(\"\\d+\\\\\\d+\\\\\\d+\", \"\", file)  # Remove the dates\n",
    "file = re.sub(\" +\", \" \", file)\n",
    "file =' '.join(file.splitlines())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkgF27wp53hA"
   },
   "source": [
    "To make inputs out of this big string of data, we will be splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZHXGvpYs53hD",
    "outputId": "caf6f3c9-3b8a-4b76-fbe0-6993de622808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " said that Brexit's going to happen. I said that they are going to break away. And everybody laughed at me. And the odds were 20 percent. And then when it happened, she took an ad saying, oh, Donald Tr\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vi_pL3StxbSV"
   },
   "source": [
    "Creating similar validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "oBHf1OJvjeVS",
    "outputId": "95ec708c-5750-4493-e7f8-3bdd83b9cdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " why aren't our politicians finding out where those e-mails are? They talk about executive orders and they talk about immigration and they talk about oh well we have to stop the border; that's the end \n"
     ]
    }
   ],
   "source": [
    "similar_val_set = []\n",
    "val_start_index = 5000\n",
    "for i in range(200):\n",
    "    val_end_index = val_start_index + chunk_len + 1\n",
    "    similar_val_set.append(file[val_start_index:val_end_index])\n",
    "    val_start_index = val_end_index\n",
    "print(similar_val_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ5WIf0SxgIZ"
   },
   "source": [
    "Creating different validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "yk3tI1ocxjW7",
    "outputId": "92724a27-5fd2-4675-e15a-8af68477fb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father to be\n",
      "proceeding, his friends to be urging him.  She considered it as an act\n",
      "of indispensable duty to clear away the claims of creditors with all\n",
      "the expedition which the most comprehensive retr\n"
     ]
    }
   ],
   "source": [
    "diff_val_set = []\n",
    "# with open('drive/My Drive/Colab Notebooks/jane_austen.txt',encoding='utf-8',errors='ignore') as f:\n",
    "#     file_2 = f.read()\n",
    "\n",
    "\n",
    "with open('data/jane_austen.txt',encoding='utf-8',errors='ignore') as f:\n",
    "    file_2 = f.read()\n",
    "\n",
    "val_start_index = 20000\n",
    "for i in range(200):\n",
    "    val_end_index = val_start_index + chunk_len + 1\n",
    "    diff_val_set.append(file_2[val_start_index:val_end_index])\n",
    "    val_start_index = val_end_index\n",
    "print(diff_val_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_02Grdqs53hP"
   },
   "source": [
    "# Build the Model\n",
    "\n",
    "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-g-KR8EW53hQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OpoTle2T53hW"
   },
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LTDzsbw53hX"
   },
   "source": [
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ANC7JcKr53hY",
    "outputId": "81f3dc4e-293c-4b01-b57d-639b9a69bbdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1ECpJe053hj"
   },
   "source": [
    "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters *up to the last*, and the target will be all characters *from the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNFqAkST53hl"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3jEU1Ca53ht"
   },
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CH3FCOyW53hy"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZvYPc1Hg53h3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYhJmhuS53h3"
   },
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azkZgceJ53h4"
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHKUPrQY53h-"
   },
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4EyielG53h_"
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    #print(inp,target)\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].unsqueeze(0))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYA6WhDAU9_i"
   },
   "outputs": [],
   "source": [
    "# generally when we calculate perplexity by taking exponent of the loss ,but in this character level rnn ,we are not going to take exponents of every character rather averaging \n",
    "# them out and then averaging out over the chunks\n",
    "def get_perplexity(validation_set):\n",
    "    p = 0\n",
    "    for chunk in validation_set:\n",
    "        hidden = decoder.init_hidden()\n",
    "        chunk_loss = 0\n",
    "        inp = char_tensor(chunk[:-1])\n",
    "        target = char_tensor(chunk[1:])\n",
    "        for c in range(chunk_len):  \n",
    "            output, hidden = decoder(inp[c], hidden)\n",
    "            chunk_loss += criterion(output, target[c].unsqueeze(0))\n",
    "        p = p + np.exp((chunk_loss/chunk_len).data.item())\n",
    "    avg_perplexity = p/len(validation_set)\n",
    "    return avg_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmbIWwk753iG"
   },
   "source": [
    "Then we define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3SzqPIRjynpv",
    "outputId": "67e2a334-439b-4fb8-c5df-aba771d58812",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 13s (100 5%) 2.3425]\n",
      "Perplexity : 11.192438511535299\n",
      "Whe. wedls, rer W. I Tiny gore din goabilc thing thacaithig the ary a ogtianke thaiy sant taut the the \n",
      "\n",
      "[0m 39s (200 10%) 2.2341]\n",
      "Perplexity : 9.550013886515126\n",
      "Whs go menatell. Oon ow, the us hat wat. I they wiveoll to a secthery. Thingid way didaingopent to gor \n",
      "\n",
      "[1m 4s (300 15%) 1.9608]\n",
      "Perplexity : 8.435203606575298\n",
      "Wh}enting zake and betrss. The taon fere then more as sing bl. I gothead thom yol. We coip. We kessino \n",
      "\n",
      "[1m 31s (400 20%) 1.9826]\n",
      "Perplexity : 7.803039320414591\n",
      "Wh. Iv't dighs ther'm to so with evencaus. I And doodd wact wo wit that kat that out koud have ring th \n",
      "\n",
      "[1m 57s (500 25%) 1.7678]\n",
      "Perplexity : 7.385696775531398\n",
      "Wh. We do mesple sore preople becon allit. ne- memould plenisusser of to be conbal. And Amen. So cout  \n",
      "\n",
      "[2m 23s (600 30%) 1.4868]\n",
      "Perplexity : 6.907227423328348\n",
      "Wh don't sorly monery and in the's a people inth we monow say the whorly int hill concry up.S.. I me w \n",
      "\n",
      "[2m 49s (700 35%) 2.0547]\n",
      "Perplexity : 6.77399926778095\n",
      "Wha's gone on I gom ime fortory that wame hat and think I don he ofere the next it in the ging tent an \n",
      "\n",
      "[3m 15s (800 40%) 1.8763]\n",
      "Perplexity : 6.54953090399594\n",
      "Whough wiome's now lot the can't never say wret of the caup all up we're last inastralt dogling this w \n",
      "\n",
      "[3m 41s (900 45%) 1.4050]\n",
      "Perplexity : 6.392931299533355\n",
      "Whank on a wats for or the Creagare the have to serer. They just so it. And you ones I don't whe You e \n",
      "\n",
      "[4m 7s (1000 50%) 1.4265]\n",
      "Perplexity : 6.244691296782982\n",
      "What for trade mean, thieve said, maned a go, that won't a deally at fatilling to gunt it. Don't mone  \n",
      "\n",
      "[4m 33s (1100 55%) 2.3001]\n",
      "Perplexity : 6.059482949211908\n",
      "Wh, the :illo the was the deen the nemerico he not back - theak the wall is can Une's that you cas wit \n",
      "\n",
      "[4m 58s (1200 60%) 2.0927]\n",
      "Perplexity : 6.009189343554361\n",
      "Whe cane ware look and and the talig to be get the silitile a stack talks. We has word. I worbs me. Th \n",
      "\n",
      "[5m 24s (1300 65%) 1.5162]\n",
      "Perplexity : 5.845659204918019\n",
      "Whank I diver he was go make that leind aft me if eacine molst be theer siking the cow. You last it wh \n",
      "\n",
      "[5m 49s (1400 70%) 1.8311]\n",
      "Perplexity : 5.805033825622218\n",
      "Wh? You know, mabe you the are thing of erpay, and of the don't recluve to said. I was no, \"I've some  \n",
      "\n",
      "[6m 14s (1500 75%) 1.3328]\n",
      "Perplexity : 5.749808802542974\n",
      "Whay. Why supleiving Americe I said I don't havangers of we're going to know, that's want to it fan ou \n",
      "\n",
      "[6m 40s (1600 80%) 1.7656]\n",
      "Perplexity : 5.6872671907516725\n",
      "Whe not here and ever. Anght says they're getury, not somelit we ad for peace. I that's ve do ons of p \n",
      "\n",
      "[7m 6s (1700 85%) 1.5919]\n",
      "Perplexity : 5.629845254782266\n",
      "Whe count. I'm ruble to so litesdirans that want to said as but in like lillion. The Right simes (uppi \n",
      "\n",
      "[7m 32s (1800 90%) 1.4621]\n",
      "Perplexity : 5.583912007238138\n",
      "Whable again it what every reath that with it's as to be was our could and Iran look that. It love who \n",
      "\n",
      "[7m 57s (1900 95%) 1.3758]\n",
      "Perplexity : 5.526807202378659\n",
      "Whobsion Sest polieve. I ment he eress. No, on I'm get the going in, they lives its a lot the he wors  \n",
      "\n",
      "[8m 23s (2000 100%) 1.7231]\n",
      "Perplexity : 5.412911452077031\n",
      "Who many our pody. They is a milger the be to my say by and who money. And horete in the better this - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.002\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(\"Perplexity : \" + str(get_perplexity(similar_val_set)))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3h30kU653iO"
   },
   "source": [
    "# Plotting the Training Losses\n",
    "\n",
    "Plotting the historical loss from all_losses shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Yt_uR2tj53iQ",
    "outputId": "808ccc68-6062-48ba-8cd2-169647bb9725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa05382d208>]"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIjTQlVJqitBAV0qpKeL/B8i6eyAzMts4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfqR8SN6yJ3s"
   },
   "source": [
    "# Comparing perplexities for similar and different validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Dfk29yhlySad",
    "outputId": "9796fa85-42b5-43a2-a2c9-6fc36f149ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar dataset perplexity : 5.412911452077031\n",
      "Different dataset perplexity : 11.023044967539768\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar dataset perplexity : \" + str(get_perplexity(similar_val_set)))\n",
    "print(\"Different dataset perplexity : \" + str(get_perplexity(diff_val_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XW2HU4jX53ie"
   },
   "source": [
    "# Evaluating at different \"temperatures\"\n",
    "\n",
    "In the `evaluate` function above, every time a prediction is made the outputs are divided by the \"temperature\" argument passed. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. As we turn the temperature towards zero we are choosing only the most likely outputs.\n",
    "\n",
    "We can see the effects of this by adjusting the `temperature` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "_MIDjskk53ie",
    "outputId": "735b35da-f7e8-4388-abbc-5a7daccaa977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The have to do it to it workes like the have to every note. It's because the protion at like they don't let all of the can't let it for every one of the say they are that big me the done, the would the \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgmJvb5Q53ik"
   },
   "source": [
    "Lower temperatures are less varied, choosing only the more probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "EgwmxNQF53il",
    "outputId": "60b249f9-54b8-48e9-a9af-a2f80b6ca69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The people the said I said \"Well and the come and the some of the come of the come of the for the country it the been the better the can and the better the come it the be the better the so be a lot the \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCEefW_j53in"
   },
   "source": [
    "Higher temperatures more varied, choosing less probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PM0VASdl53io",
    "outputId": "bc647b68-4e77-4d73-f640-018abdf2c4f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thesafleal fight -ore for makevoned, whocle conney's use Pact if bin, the wel rid leat. Ss whose vet a preage's nyid, get's gest a fabuaugh, Maxffof one in a. I veh mirst rime Diggy. Flino, it Mun(ppre \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "ry6B9wIp53ir"
   },
   "source": [
    "# Exercises\n",
    "\n",
    "* Train with your own dataset, e.g.\n",
    "    * Text from another author\n",
    "    * Blog posts\n",
    "    * Code\n",
    "* Increase number of layers and network size to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t59r437l53is"
   },
   "source": [
    "**Next**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "char_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
