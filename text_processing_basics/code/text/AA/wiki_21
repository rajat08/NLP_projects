{"id": "13831", "url": "https://en.wikipedia.org/wiki?curid=13831", "title": "Human rights", "text": "Human rights\n\nHuman rights are moral principles or norms that describe certain standards of human behaviour and are regularly protected as natural and legal rights in municipal and international law. They are commonly understood as inalienable, fundamental rights \"to which a person is inherently entitled simply because she or he is a human being\" and which are \"inherent in all human beings\", regardless of their age, ethnic origin, location, language, religion, ethnicity, or any other status. They are applicable everywhere and at every time in the sense of being universal, and they are egalitarian in the sense of being the same for everyone. They are regarded as requiring empathy and the rule of law and imposing an obligation on persons to respect the human rights of others, and it is generally considered that they should not be taken away except as a result of due process based on specific circumstances; for example, human rights may include freedom from unlawful imprisonment, torture, and execution.\n\nThe doctrine of human rights has been highly influential within international law and global and regional institutions. Actions by states and non-governmental organisations form a basis of public policy worldwide. The idea of human rights suggests that \"if the public discourse of peacetime global society can be said to have a common moral language, it is that of human rights\". The strong claims made by the doctrine of human rights continue to provoke considerable scepticism and debates about the content, nature and justifications of human rights to this day. The precise meaning of the term \"right\" is controversial and is the subject of continued philosophical debate; while there is consensus that human rights encompasses a wide variety of rights such as the right to a fair trial, protection against enslavement, prohibition of genocide, free speech or a right to education, there is disagreement about which of these particular rights should be included within the general framework of human rights; some thinkers suggest that human rights should be a minimum requirement to avoid the worst-case abuses, while others see it as a higher standard.\n\nMany of the basic ideas that animated the human rights movement developed in the aftermath of the Second World War and the events of the Holocaust, culminating in the adoption of the Universal Declaration of Human Rights in Paris by the United Nations General Assembly in 1948. Ancient peoples did not have the same modern-day conception of universal human rights. The true forerunner of human rights discourse was the concept of natural rights which appeared as part of the medieval natural law tradition that became prominent during the European Enlightenment with such philosophers as John Locke, Francis Hutcheson and Jean-Jacques Burlamaqui and which featured prominently in the political discourse of the American Revolution and the French Revolution. From this foundation, the modern human rights arguments emerged over the latter half of the 20th century, possibly as a reaction to slavery, torture, genocide and war crimes, as a realisation of inherent human vulnerability and as being a precondition for the possibility of a just society.\n\nAncient peoples did not have the same modern-day conception of universal human rights. The true forerunner of human-rights discourse was the concept of natural rights which appeared as part of the medieval natural law tradition that became prominent during the European Enlightenment. From this foundation, the modern human rights arguments emerged over the latter half of the 20th century.\n17th-century English philosopher John Locke discussed natural rights in his work, identifying them as being \"life, liberty, and estate (property)\", and argued that such fundamental rights could not be surrendered in the social contract. In Britain in 1689, the English Bill of Rights and the Scottish Claim of Right each made illegal a range of oppressive governmental actions. Two major revolutions occurred during the 18th century, in the United States (1776) and in France (1789), leading to the United States Declaration of Independence and the French Declaration of the Rights of Man and of the Citizen respectively, both of which articulated certain human rights. Additionally, the Virginia Declaration of Rights of 1776 encoded into law a number of fundamental civil rights and civil freedoms.\n\nPhilosophers such as Thomas Paine, John Stuart Mill and Hegel expanded on the theme of universality during the 18th and 19th centuries. In 1831 William Lloyd Garrison wrote in a newspaper called \"The Liberator\" that he was trying to enlist his readers in \"the great cause of human rights\" so the term \"human rights\" probably came into use sometime between Paine's \"The Rights of Man\" and Garrison's publication. In 1849 a contemporary, Henry David Thoreau, wrote about human rights in his treatise \"On the Duty of Civil Disobedience\" which was later influential on human rights and civil rights thinkers. United States Supreme Court Justice David Davis, in his 1867 opinion for Ex Parte Milligan, wrote \"By the protection of the law, human rights are secured; withdraw that protection and they are at the mercy of wicked rulers or the clamor of an excited people.\"\n\nMany groups and movements have managed to achieve profound social changes over the course of the 20th century in the name of human rights. In Western Europe and North America, labour unions brought about laws granting workers the right to strike, establishing minimum work conditions and forbidding or regulating child labour. The women's rights movement succeeded in gaining for many women the right to vote. National liberation movements in many countries succeeded in driving out colonial powers. One of the most influential was Mahatma Gandhi's movement to free his native India from British rule. Movements by long-oppressed racial and religious minorities succeeded in many parts of the world, among them the civil rights movement, and more recent diverse identity politics movements, on behalf of women and minorities in the United States.\n\nThe foundation of the International Committee of the Red Cross, the 1864 Lieber Code and the first of the Geneva Conventions in 1864 laid the foundations of International humanitarian law, to be further developed following the two World Wars.\n\nThe League of Nations was established in 1919 at the negotiations over the Treaty of Versailles following the end of World War I. The League's goals included disarmament, preventing war through collective security, settling disputes between countries through negotiation, diplomacy and improving global welfare. Enshrined in its Charter was a mandate to promote many of the rights which were later included in the Universal Declaration of Human Rights.\n\nThe League of Nations had mandates to support many of the former colonies of the Western European colonial powers during their transition from colony to independent state.\n\nEstablished as an agency of the League of Nations, and now part of United Nations, the International Labour Organization also had a mandate to promote and safeguard certain of the rights later included in the Universal Declaration of Human Rights (UDHR):\n\nOn the issue of \"universal\", the declarations did not apply to domestic discrimination or racism. Henry J. Richardson III has argued:\n\nThe Universal Declaration of Human Rights (UDHR) is a non-binding declaration adopted by the United Nations General Assembly in 1948, partly in response to the barbarism of World War II. The UDHR urges member states to promote a number of human, civil, economic and social rights, asserting these rights are part of the \"foundation of freedom, justice and peace in the world\". The declaration was the first international legal effort to limit the behavior of states and press upon them duties to their citizens following the model of the rights-duty duality.\n\nThe UDHR was framed by members of the Human Rights Commission, with Eleanor Roosevelt as Chair, who began to discuss an \"International Bill of Rights\" in 1947. The members of the Commission did not immediately agree on the form of such a bill of rights, and whether, or how, it should be enforced. The Commission proceeded to frame the UDHR and accompanying treaties, but the UDHR quickly became the priority. Canadian law professor John Humprey and French lawyer Rene Cassin were responsible for much of the cross-national research and the structure of the document respectively, where the articles of the declaration were interpretative of the general principle of the preamble. The document was structured by Cassin to include the basic principles of dignity, liberty, equality and brotherhood in the first two articles, followed successively by rights pertaining to individuals; rights of individuals in relation to each other and to groups; spiritual, public and political rights; and economic, social and cultural rights. The final three articles place, according to Cassin, rights in the context of limits, duties and the social and political order in which they are to be realized. Humphrey and Cassin intended the rights in the UDHR to be legally enforceable through some means, as is reflected in the third clause of the preamble:\n\nSome of the UDHR was researched and written by a committee of international experts on human rights, including representatives from all continents and all major religions, and drawing on consultation with leaders such as Mahatma Gandhi. The inclusion of both civil and political rights and economic, social and cultural rights was predicated on the assumption that basic human rights are indivisible and that the different types of rights listed are inextricably linked. Though this principle was not opposed by any member states at the time of adoption (the declaration was adopted unanimously, with the abstention of the Soviet bloc, Apartheid South Africa and Saudi Arabia), this principle was later subject to significant challenges.\n\nThe onset of the Cold War soon after the UDHR was conceived brought to the fore divisions over the inclusion of both economic and social rights and civil and political rights in the declaration. Capitalist states tended to place strong emphasis on civil and political rights (such as freedom of association and expression), and were reluctant to include economic and social rights (such as the right to work and the right to join a union). Socialist states placed much greater importance on economic and social rights and argued strongly for their inclusion.\n\nBecause of the divisions over which rights to include, and because some states declined to ratify any treaties including certain specific interpretations of human rights, and despite the Soviet bloc and a number of developing countries arguing strongly for the inclusion of all rights in a so-called \"Unity Resolution\", the rights enshrined in the UDHR were split into two separate covenants, allowing states to adopt some rights and derogate others. Though this allowed the covenants to be created, it denied the proposed principle that all rights are linked which was central to some interpretations of the UDHR.\n\nAlthough the UDHR is a non-binding resolution, it is now considered to be a central component of international customary law which may be invoked under appropriate circumstances by state judiciaries and other judiciaries.\n\nIn 1966, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR) were adopted by the United Nations, between them making the rights contained in the UDHR binding on all states. However, they came into force only in 1976, when they were ratified by a sufficient number of countries (despite achieving the ICCPR, a covenant including no economic or social rights, the US only ratified the ICCPR in 1992). The ICESCR commits 155 state parties to work toward the granting of economic, social, and cultural rights (ESCR) to individuals. \nSince then numerous other treaties (pieces of legislation) have been offered at the international level. They are generally known as \"human rights instruments\". Some of the most significant are:\n\n\nThe United Nations (UN) is the only multilateral governmental agency with universally accepted international jurisdiction for universal human rights legislation. All UN organs have advisory roles to the United Nations Security Council and the United Nations Human Rights Council, and there are numerous committees within the UN with responsibilities for safeguarding different human rights treaties. The most senior body of the UN with regard to human rights is the Office of the High Commissioner for Human Rights. The United Nations has an international mandate to:\n\nThe United Nations Human Rights Council, created at the 2005 World Summit to replace the United Nations Commission on Human Rights, has a mandate to investigate violations of human rights. The Human Rights Council is a subsidiary body of the General Assembly and reports directly to it. It ranks below the Security Council, which is the final authority for the interpretation of the United Nations Charter. Forty-seven of the one hundred ninety-one member states sit on the council, elected by simple majority in a secret ballot of the United Nations General Assembly. Members serve a maximum of six years and may have their membership suspended for gross human rights abuses. The Council is based in Geneva, and meets three times a year; with additional meetings to respond to urgent situations.\n\nIndependent experts (\"rapporteurs\") are retained by the Council to investigate alleged human rights abuses and to provide the Council with reports.\n\nThe Human Rights Council may request that the Security Council refer cases to the International Criminal Court (ICC) even if the issue being referred is outside the normal jurisdiction of the ICC.\n\nIn addition to the political bodies whose mandate flows from the UN charter, the UN has set up a number of \"treaty-based\" bodies, comprising committees of independent experts who monitor compliance with human rights standards and norms flowing from the core international human rights treaties. They are supported by and are created by the treaty that they monitor, With the exception of the CESCR, which was established under a resolution of the Economic and Social Council to carry out the monitoring functions originally assigned to that body under the Covenant, they are technically autonomous bodies, established by the treaties that they monitor and accountable to the state parties of those treaties – rather than subsidiary to the United Nations, though in practice they are closely intertwined with the United Nations system and are supported by the UN High Commissioner for Human Rights (UNHCHR) and the UN Centre for Human Rights.\n\nEach treaty body receives secretariat support from the Human Rights Council and Treaties Division of Office of the High Commissioner on Human Rights (OHCHR) in Geneva except CEDAW, which is supported by the Division for the Advancement of Women (DAW). CEDAW formerly held all its sessions at United Nations headquarters in New York but now frequently meets at the United Nations Office in Geneva; the other treaty bodies meet in Geneva. The Human Rights Committee usually holds its March session in New York City.\n\nThere are many regional agreements and organizations promoting and governing human rights.\n\nThe African Union (AU) is a supranational union consisting of fifty-five African states. Established in 2001, the AU's purpose is to help secure Africa's democracy, human rights, and a sustainable economy, especially by bringing an end to intra-African conflict and creating an effective common market.\n\nThe African Commission on Human and Peoples' Rights (ACHPR) is a quasi-judicial organ of the African Union tasked with promoting and protecting human rights and collective (peoples') rights throughout the African continent as well as interpreting the African Charter on Human and Peoples' Rights and considering individual complaints of violations of the Charter. The Commission has three broad areas of responsibility:\n\n\nIn pursuit of these goals, the Commission is mandated to \"collect documents, undertake studies and researches on African problems in the field of human and peoples, rights, organise seminars, symposia and conferences, disseminate information, encourage national and local institutions concerned with human and peoples' rights and, should the case arise, give its views or make recommendations to governments\" (Charter, Art. 45).\n\nWith the creation of the African Court on Human and Peoples' Rights (under a protocol to the Charter which was adopted in 1998 and entered into force in January 2004), the Commission will have the additional task of preparing cases for submission to the Court's jurisdiction. In a July 2004 decision, the AU Assembly resolved that the future Court on Human and Peoples' Rights would be integrated with the African Court of Justice.\n\nThe Court of Justice of the African Union is intended to be the \"principal judicial organ of the Union\" (Protocol of the Court of Justice of the African Union, Article 2.2). Although it has not yet been established, it is intended to take over the duties of the African Commission on Human and Peoples' Rights, as well as act as the supreme court of the African Union, interpreting all necessary laws and treaties. The Protocol establishing the African Court on Human and Peoples' Rights entered into force in January 2004 but its merging with the Court of Justice has delayed its establishment. The Protocol establishing the Court of Justice will come into force when ratified by 15 countries.\n\nThere are many countries in Africa accused of human rights violations by the international community and NGOs.\n\nThe Organization of American States (OAS) is an international organization, headquartered in Washington, D.C., United States. Its members are the thirty-five independent states of the Americas. Over the course of the 1990s, with the end of the Cold War, the return to democracy in Latin America, and the thrust toward globalization, the OAS made major efforts to reinvent itself to fit the new context. Its stated priorities now include the following:\n\n\nThe Inter-American Commission on Human Rights (the IACHR) is an autonomous organ of the Organization of American States, also based in Washington, D.C. Along with the Inter-American Court of Human Rights, based in San José, Costa Rica, it is one of the bodies that comprise the inter-American system for the promotion and protection of human rights. The IACHR is a permanent body which meets in regular and special sessions several times a year to examine allegations of human rights violations in the hemisphere. Its human rights duties stem from three documents:\n\n\nThe Inter-Americal Court of Human Rights was established in 1979 with the purpose of enforcing and interpreting the provisions of the American Convention on Human Rights. Its two main functions are thus adjudicatory and advisory. Under the former, it hears and rules on the specific cases of human rights violations referred to it. Under the latter, it issues opinions on matters of legal interpretation brought to its attention by other OAS bodies or member states.\n\nThere are no Asia-wide organisations or conventions to promote or protect human rights. Countries vary widely in their approach to human rights and their record of human rights protection.\n\nThe Association of Southeast Asian Nations (ASEAN) is a geo-political and economic organization of 10 countries located in Southeast Asia, which was formed in 1967 by Indonesia, Malaysia, the Philippines, Singapore and Thailand. The organisation now also includes Brunei Darussalam, Vietnam, Laos, Myanmar and Cambodia. In October 2009, the ASEAN Intergovernmental Commission on Human Rights was inaugurated, and subsequently, the ASEAN Human Rights Declaration was adopted unanimously by ASEAN members on 18 November 2012.\n\nThe Arab Charter on Human Rights (ACHR) was adopted by the Council of the League of Arab States on 22 May 2004.\n\nThe Council of Europe, founded in 1949, is the oldest organisation working for European integration. It is an international organisation with legal personality recognised under public international law and has observer status with the United Nations. The seat of the Council of Europe is in Strasbourg in France. The Council of Europe is responsible for both the European Convention on Human Rights and the European Court of Human Rights. These institutions bind the Council's members to a code of human rights which, though strict, are more lenient than those of the United Nations charter on human rights. The Council also promotes the European Charter for Regional or Minority Languages and the European Social Charter. Membership is open to all European states which seek European integration, accept the principle of the rule of law and are able and willing to guarantee democracy, fundamental human rights and freedoms.\n\nThe Council of Europe is an organisation that is not part of the European Union, but the latter is expected to accede to the European Convention and potentially the Council itself. The EU has its own human rights document; the Charter of Fundamental Rights of the European Union.\n\nThe European Convention on Human Rights defines and guarantees since 1950 human rights and fundamental freedoms in Europe. All 47 member states of the Council of Europe have signed this Convention and are therefore under the jurisdiction of the European Court of Human Rights in Strasbourg. In order to prevent torture and inhuman or degrading treatment (Article 3 of the Convention), the European Committee for the Prevention of Torture was established.\n\nSeveral theoretical approaches have been advanced to explain how and why human rights become part of social expectations.\n\nOne of the oldest Western philosophies on human rights is that they are a product of a natural law, stemming from different philosophical or religious grounds.\n\nOther theories hold that human rights codify moral behavior which is a human social product developed by a process of biological and social evolution (associated with Hume). Human rights are also described as a sociological pattern of rule setting (as in the sociological theory of law and the work of Weber). These approaches include the notion that individuals in a society accept rules from legitimate authority in exchange for security and economic advantage (as in Rawls) – a social contract.\n\nNatural law theories base human rights on a \"natural\" moral, religious or even biological order which is independent of transitory human laws or traditions.\n\nSocrates and his philosophic heirs, Plato and Aristotle, posited the existence of natural justice or natural right (\"dikaion physikon\", \"δικαιον φυσικον\", Latin \"ius naturale\"). Of these, Aristotle is often said to be the father of natural law, although evidence for this is due largely to the interpretations of his work of Thomas Aquinas.\n\nThe development of this tradition of natural justice into one of natural law is usually attributed to the Stoics.\n\nSome of the early Church fathers sought to incorporate the until then pagan concept of natural law into Christianity. Natural law theories have featured greatly in the philosophies of Thomas Aquinas, Francisco Suárez, Richard Hooker, Thomas Hobbes, Hugo Grotius, Samuel von Pufendorf, and John Locke.\n\nIn the Seventeenth Century Thomas Hobbes founded a contractualist theory of legal positivism on what all men could agree upon: what they sought (happiness) was subject to contention, but a broad consensus could form around what they feared (violent death at the hands of another). The natural law was how a rational human being, seeking to survive and prosper, would act. It was discovered by considering humankind's natural rights, whereas previously it could be said that natural rights were discovered by considering the natural law. In Hobbes' opinion, the only way natural law could prevail was for men to submit to the commands of the sovereign. In this lay the foundations of the theory of a social contract between the governed and the governor.\n\nHugo Grotius based his philosophy of international law on natural law. He wrote that \"even the will of an omnipotent being cannot change or abrogate\" natural law, which \"would maintain its objective validity even if we should assume the impossible, that there is no God or that he does not care for human affairs.\" (\"De iure belli ac pacis\", Prolegomeni XI). This is the famous argument \"etiamsi daremus\" (\"non-esse Deum\"), that made natural law no longer dependent on theology.\n\nJohn Locke incorporated natural law into many of his theories and philosophy, especially in \"Two Treatises of Government\". Locke turned Hobbes' prescription around, saying that if the ruler went against natural law and failed to protect \"life, liberty, and property,\" people could justifiably overthrow the existing state and create a new one.\n\nThe Belgian philosopher of law Frank van Dun is one among those who are elaborating a secular conception of natural law in the liberal tradition. There are also emerging and secular forms of natural law theory that define human rights as derivative of the notion of universal human dignity.\n\nThe term \"human rights\" has replaced the term \"natural rights\" in popularity, because the rights are less and less frequently seen as requiring natural law for their existence.\n\nThe philosopher John Finnis argues that human rights are justifiable on the grounds of their instrumental value in creating the necessary conditions for human well-being. Interest theories highlight the duty to respect the rights of other individuals on grounds of self-interest:\n\nThe biological theory considers the comparative reproductive advantage of human social behavior based on empathy and altruism in the context of natural selection.\n\nThe most common categorization of human rights is to split them into civil and political rights, and economic, social and cultural rights.\n\nCivil and political rights are enshrined in articles 3 to 21 of the Universal Declaration of Human Rights and in the ICCPR. Economic, social and cultural rights are enshrined in articles 22 to 28 of the Universal Declaration of Human Rights and in the ICESCR. The UDHR included both economic, social and cultural rights and civil and political rights because it was based on the principle that the different rights could only successfully exist in combination:\n\nThis is held to be true because without civil and political rights the public cannot assert their economic, social and cultural rights. Similarly, without livelihoods and a working society, the public cannot assert or make use of civil or political rights (known as the \"full belly thesis\")\n\nAlthough accepted by the signaturies to the UDHR, most of them do not in practice give equal weight to the different types of rights. Western cultures have often given priority to civil and political rights, sometimes at the expense of economic and social rights such as the right to work, to education, health and housing. For example, in the United States there is no universal access to healthcare free at the point of use. That is not to say that Western cultures have overlooked these rights entirely (the welfare states that exist in Western Europe are evidence of this). Similarly the ex Soviet bloc countries and Asian countries have tended to give priority to economic, social and cultural rights, but have often failed to provide civil and political rights.\n\nAnother categorization, offered by Karel Vasak, is that there are \"three generations of human rights\": first-generation civil and political rights (right to life and political participation), second-generation economic, social and cultural rights (right to subsistence) and third-generation solidarity rights (right to peace, right to clean environment). Out of these generations, the third generation is the most debated and lacks both legal and political recognition. This categorisation is at odds with the indivisibility of rights, as it implicitly states that some rights can exist without others. Prioritisation of rights for pragmatic reasons is however a widely accepted necessity. Human rights expert Philip Alston argues:\n\nHe, and others, urge caution with prioritisation of rights:\n\nSome human rights are said to be \"inalienable rights.\" The term inalienable rights (or unalienable rights) refers to \"a set of human rights that are fundamental, are not awarded by human power, and cannot be surrendered.\"\n\nThe adherence to the principle of indivisibility by the international community was reaffirmed in 1995:\n\nThis statement was again endorsed at the 2005 World Summit in New York (paragraph 121).\n\nThe UDHR enshrines, by definition, rights that apply to all humans equally, whichever geographical location, state, race or culture they belong to.\n\nProponents of cultural relativism suggest that human rights are not all universal, and indeed conflict with some cultures and threaten their survival.\n\nRights which are most often contested with relativistic arguments are the rights of women. For example, Female genital mutilation occurs in different cultures in Africa, Asia and South America. It is not mandated by any religion, but has become a tradition in many cultures. It is considered a violation of women's and girl's rights by much of the international community, and is outlawed in some countries.\n\nUniversalism has been described by some as cultural, economic or political imperialism. In particular, the concept of human rights is often claimed to be fundamentally rooted in a politically liberal outlook which, although generally accepted in Europe, Japan or North America, is not necessarily taken as standard elsewhere.\n\nFor example, in 1981, the Iranian representative to the United Nations, Said Rajaie-Khorassani, articulated the position of his country regarding the Universal Declaration of Human Rights by saying that the UDHR was \"a secular understanding of the Judeo-Christian tradition\", which could not be implemented by Muslims without trespassing the Islamic law. The former Prime Ministers of Singapore, Lee Kuan Yew, and of Malaysia, Mahathir bin Mohamad both claimed in the 1990s that \"Asian values\" were significantly different from western values and included a sense of loyalty and foregoing personal freedoms for the sake of social stability and prosperity, and therefore authoritarian government is more appropriate in Asia than democracy. This view is countered by Mahathir's former deputy:\n\nand also by Singapore's opposition leader Chee Soon Juan who states that it is racist to assert that Asians do not want human rights.\n\nAn appeal is often made to the fact that influential human rights thinkers, such as John Locke and John Stuart Mill, have all been Western and indeed that some were involved in the running of Empires themselves.\n\nRelativistic arguments tend to neglect the fact that modern human rights are new to all cultures, dating back no further than the UDHR in 1948. They also don't account for the fact that the UDHR was drafted by people from many different cultures and traditions, including a US Roman Catholic, a Chinese Confucian philosopher, a French Zionist and a representative from the Arab League, amongst others, and drew upon advice from thinkers such as Mahatma Gandhi.\n\nMichael Ignatieff has argued that cultural relativism is almost exclusively an argument used by those who wield power in cultures which commit human rights abuses, and that those whose human rights are compromised are the powerless. This reflects the fact that the difficulty in judging universalism versus relativism lies in who is claiming to represent a particular culture.\n\nAlthough the argument between universalism and relativism is far from complete, it is an academic discussion in that all international human rights instruments adhere to the principle that human rights are universally applicable. The 2005 World Summit reaffirmed the international community's adherence to this principle:\n\nCompanies, NGOs, political parties, informal groups, and individuals are known as \"non-State actors\". Non-State actors can also commit human rights abuses, but are not subject to human rights law other than International Humanitarian Law, which applies to individuals.\n\nMulti-national companies play an increasingly large role in the world, and are responsible for a large number of human rights abuses. Although the legal and moral environment surrounding the actions of governments is reasonably well developed, that surrounding multi-national companies is both controversial and ill-defined. Multi-national companies' primary responsibility is to their shareholders, not to those affected by their actions. Such companies are often larger than the economies of the states in which they operate, and can wield significant economic and political power. No international treaties exist to specifically cover the behavior of companies with regard to human rights, and national legislation is very variable. Jean Ziegler, Special Rapporteur of the UN Commission on Human Rights on the right to food stated in a report in 2003:\n\nIn August 2003 the Human Rights Commission's Sub-Commission on the Promotion and Protection of Human Rights produced draft \"Norms on the responsibilities of transnational corporations and other business enterprises with regard to human rights\". These were considered by the Human Rights Commission in 2004, but have no binding status on corporations and are not monitored.\n\nRealism and national loyalties have been described as a destructive influence on the human rights movement because they deny people's innately similar human qualities.\n\nWith the exception of non-derogable human rights (international conventions class the right to life, the right to be free from slavery, the right to be free from torture and the right to be free from retroactive application of penal laws as non-derogable), the UN recognises that human rights can be limited or even pushed aside during times of national emergency – although\n\nRights that cannot be derogated for reasons of national security in any circumstances are known as peremptory norms or \"jus cogens\". Such International law obligations are binding on all states and cannot be modified by treaty.\n\nThe human rights enshrined in the UDHR, the Geneva Conventions and the various enforced treaties of the United Nations are enforceable in law. In practice, many rights are very difficult to legally enforce due to the absence of consensus on the application of certain rights, the lack of relevant national legislation or of bodies empowered to take legal action to enforce them.\n\nThere exist a number of internationally recognized organisations with worldwide mandate or jurisdiction over certain aspects of human rights:\n\n\nThe ICC and other international courts (see Regional human rights above exist to take action where the national legal system of a state is unable to try the case itself. If national law is able to safeguard human rights and punish those who breach human rights legislation, it has primary jurisdiction by complementarity. Only when all \"local remedies\" have been exhausted does international law take effect.\n\nIn over 110 countries National human rights institutions (NHRIs) have been set up to protect, promote or monitor human rights with jurisdiction in a given country. Although not all NHRIs are compliant with the Paris Principles, the number and effect of these institutions is increasing. The Paris Principles were defined at the first International Workshop on National Institutions for the Promotion and Protection of Human Rights in Paris on 7–9 October 1991, and adopted by United Nations Human Rights Commission Resolution 1992/54 of 1992 and the General Assembly Resolution 48/134 of 1993. The Paris Principles list a number of responsibilities for national institutions.\n\nUniversal jurisdiction is a controversial principle in international law whereby states claim criminal jurisdiction over persons whose alleged crimes were committed outside the boundaries of the prosecuting state, regardless of nationality, country of residence, or any other relation with the prosecuting country. The state backs its claim on the grounds that the crime committed is considered a crime against all, which any state is authorized to punish. The concept of universal jurisdiction is therefore closely linked to the idea that certain international norms are erga omnes, or owed to the entire world community, as well as the concept of jus cogens. In 1993 Belgium passed a \"law of universal jurisdiction\" to give its courts jurisdiction over crimes against humanity in other countries, and in 1998 Augusto Pinochet was arrested in London following an indictment by Spanish judge Baltasar Garzon under the universal jurisdiction principle. The principle is supported by Amnesty International and other human rights organisations as they believe certain crimes pose a threat to the international community as a whole and the community has a moral duty to act, but others, including Henry Kissinger (who has himself been accused of war crimes by several commentators), argue that state sovereignty is paramount, because breaches of rights committed in other countries are outside states' sovereign interest and because states could use the principle for political reasons.\n\nHuman rights violations occur when any state or non-state actor breaches any of the terms of the UDHR or other international human rights or humanitarian law. In regard to human rights violations of United Nations laws. Article 39 of the United Nations Charter designates the UN Security Council (or an appointed authority) as the only tribunal that may determine UN human rights violations.\n\nHuman rights abuses are monitored by United Nations committees, national institutions and governments and by many independent non-governmental organizations, such as Amnesty International, Human Rights Watch, World Organisation Against Torture, Freedom House, International Freedom of Expression Exchange and Anti-Slavery International. These organisations collect evidence and documentation of human rights abuses and apply pressure to promote human rights\n\nWars of aggression, war crimes and crimes against humanity, including genocide, are breaches of International humanitarian law.\n\n\n\n"}
{"id": "44664", "url": "https://en.wikipedia.org/wiki?curid=44664", "title": "One-child policy", "text": "One-child policy\n\nChina's one-child policy was part of a birth planning program designed to control the size of its rapidly growing population. Distinct from the family planning policies of most other countries (which focus on providing contraceptive options to help women have the number of children they want), it set a limit on the number of births parents could have, the world's most extreme example of population planning. It was introduced in 1979 (after a decade-long two-child policy), modified beginning in the mid 1980s to allow rural parents a second child if the first was a daughter, and then lasted three more decades before being eliminated at the end of 2015. The policy also allowed exceptions for some other groups, including ethnic minorities. Thus, the term \"one-child policy\" has been called a \"misnomer\", because for nearly 30 of the 36 years that it existed (1979–2015), about half of all parents in China were allowed to have a second child.\n\nTo enforce existing birth limits (of one or two children), provincial governments could, and did, require the use of contraception, sterilizations and abortions to ensure compliance, and imposed enormous fines for violations. Local and national governments created commissions to promote the program and monitor compliance. China also rewarded families with only one child. From 1982 onwards, in accordance with the instructions on further family planning issued by the CPC central committee and the state council in that year, regulations awarded 5 yuan per month for families with one child. Parents who had only one child would also get a \"one-child glory certificate\".\n\nAccording to China's government, 400million births were prevented. Originally, this estimate referred to the full birth program starting from 1970, although more recently the numbers have been attributed to one-child restrictions since 1980. Several scholars have disputed the official claim, contending that the one-child program had little effect on birth rates or the size of the total population when one considers the large drop in fertility in the two-child decade preceding it. China has been compared to countries such as Thailand, along with the Indian states of Kerala and Tamil Nadu, which experienced similar declines of fertility without a one-child policy. Another study takes such arguments even further based on a model which implies that the one-child program, contrary to popular belief and its government's intentions, had a pronatal effect that raised birth rates \"above\" what they otherwise would have been. Yet this latter study has itself been disputed as an implausible \"erasure of the impact of this program from history. Moreover, the comparative models proposed by those dismissing official estimates as exaggerations imply that China's birth planning since 1970 has already averted between 600 and 700 million births, a number projected to grow to one billion or more by 2060 given the averted descendants of the births originally averted by policy. The real dispute concerns what portion of these total averted births (and population) are due to China's tightened one-child program as opposed to the two-child program that preceded it. \n\nAlthough 76% of Chinese people said that they supported the policy in a 2008 survey, it was controversial outside of China.. Effective from January 2016, the national birth planning policy became a universal two-child policy that allowed each couple to have two children.\n\nDuring the period of Mao Zedong's leadership in China, the birth rate fell from 37 per thousand to 20 per thousand. Infant mortality declined from 227 per thousand births in 1949 to 53 per thousand in 1981, and life expectancy dramatically increased from around 35 years in 1948 to 66 years in 1976. Until the 1960s, the government encouraged families to have as many children as possible because of Mao's belief that population growth empowered the country, preventing the emergence of family planning programs earlier in China's development. The population grew from around 540million in 1949 to 940million in 1976. Beginning in 1970, citizens were required to marry at later ages and many were limited to have only two children.\n\nAlthough China's fertility rate plummeted faster than anywhere else in the world during the 1970s under these restrictions, the Chinese government thought that fertility was still too high, influenced by the global debate over a possible overpopulation catastrophe suggested by organizations such as Club of Rome and Sierra Club. It thus began to encourage one-child families in 1978, and then announced in 1979 its intention to advocate for one-child families. In 1980, the central government organized a meeting in Chengdu to discuss the speed and scope of one-child restrictions.\n\nOne participant at the Chengdu meeting had read two influential books about population concerns, \"The Limits to Growth\" and \"A Blueprint for Survival\" while visiting Europe in 1979. That official, Song Jian, along with several associates, determined that the ideal population of China was 700million, and that a universal one-child policy for all would be required to meet that goal. Moreover, Song and his group showed that if fertility rates remained constant at 3 births per woman, China's population would surpass 3 billion by 2060 and 4 billion by 2080. In spite of some criticism inside the party, the plan (also referred to as the Family Planning Policy) was formally implemented as a temporary measure on 18 September 1980. The plan called for families to have one child each in order to curb a then-surging population and alleviate social, economic, and environmental problems in China.\n\nAlthough a recent and often-repeated interpretation by Greenhalgh claims that Song Jian was the central architect of the one-child policy and that he \"hijacked\" the population policymaking process, that claim has been refuted by several leading scholars, including Liang Zhongtang, a leading internal critic of one-child restrictions and an eye-witness at the discussions in Chengdu. In the words of Wang et al., \"the idea of the one-child policy came from leaders within the Party, not from scientists who offered evidence to support it” Central officials had already decided in 1979 to advocate for one-child restrictions before knowing of Song's work and, upon learning of his work in 1980, already seemed sympathetic to his position. Moreover, even if Song's work convinced them to proceed with universal one-child restrictions in 1980, the policy was loosened to a \"1.5\"-child policy just five years later, and it is that policy which has been misrepresented since as the \"one-child policy.\" Thus, it is misleading to suggest that Song Jian was either the inventor or architect of the policy.\n\nThe one-child policy was originally designed to be a \"One-Generation Policy\". It was enforced at the provincial level and enforcement varied; some provinces had more relaxed restrictions. The one-child limit was most strictly enforced in densely populated urban areas.. When this policy was first introduced, 6.1 million families that had already given birth to a child were given the \"One Child Honorary Certificates.\" This was a pledge they had to make to ensure they would not have more children.\n\nBeginning in 1980, the official policy granted local officials the flexibility to make exceptions and allow second children in the case of \"practical difficulties\" (such as cases in which the father was a disabled serviceman) or when both parents were single children, and some provinces had other exemptions worked into their policies as well. In most areas, families were allowed to apply to have a second child if their first-born was a daughter. Furthermore, families with children with disabilities have different policies and families whose first child suffers from physical disability, mental illness, or intellectual disability were allowed to have more children. However, second children were sometimes subject to birth spacing (usually 3 or 4 years). Children born in overseas countries were not counted under the policy if they did not obtain Chinese citizenship. Chinese citizens returning from abroad were allowed to have a second child. Sichuan province allowed exemptions for couples of certain backgrounds. By one estimate there were at least 22 ways in which parents could qualify for exceptions to the law towards the end of the one-child policy's existence. As of 2007, only 36% of the population were subjected to a strict one-child limit. 53% were permitted to have a second child if their first was a daughter; 9.6% of Chinese couples were permitted two children regardless of their gender; and 1.6% – mainly Tibetans – had no limit at all.\n\nFollowing the 2008 Sichuan earthquake, a new exception to the regulations was announced in Sichuan for parents who had lost children in the earthquake. Similar exceptions had previously been made for parents of severely disabled or deceased children. People have also tried to evade the policy by giving birth to a second child in Hong Kong, but at least for Guangdong residents, the one-child policy was also enforced if the birth was given in Hong Kong or abroad.\n\nIn accordance with China's affirmative action policies towards ethnic minorities, all non-Han ethnic groups are subjected to different laws and were usually allowed to have two children in urban areas, and three or four in rural areas. Han Chinese living in rural towns were also permitted to have two children. Because of couples such as these, as well as those who simply paid a fine (or \"social maintenance fee\") to have more children, the overall fertility rate of mainland China was close to 1.4 children per woman .\n\nOn 6 January 2010, the former national population and family planning commission issued the \"national population development\" 12th five-year plan.\n\nThe Family Planning Policy was enforced through a financial penalty in the form of the \"social child-raising fee\", sometimes called a \"family planning fine\" in the West, which was collected as a fraction of either the annual disposable income of city dwellers or of the annual cash income of peasants, in the year of the child's birth. For instance, in Guangdong, the fee was between 3 and 6 annual incomes for incomes below the per capita income of the district, plus 1 to 2 times the annual income exceeding the average. The family was required to pay the fine.\n\nAs part of the policy, women were required to have a contraceptive intrauterine device (IUD) surgically installed after having a first child, and to be sterilized by tubal ligation after having a second child. From 1980 to 2014, 324 million Chinese women were fitted with IUDs in this way and 108 million were sterilized. Women who refused these procedures – which many resented – could lose their government employment and their children could lose access to education or health services. The IUDs installed in this way were modified such that they could not be removed manually, but only through surgery.\n\nIn 2016, following the abolition of the one-child policy, the Chinese government announced that IUD removals would now be paid for by the government.\n\nIn 2013, Deputy Director Wang Peian of the National Health and Family Planning Commission said that \"China's population will not grow substantially in the short term\". A survey by the commission found that only about half of eligible couples wish to have two children, mostly because of the cost of living impact of a second child.\n\nIn November 2013, following the Third Plenum of the 18th Central Committee of the Chinese Communist Party, China announced the decision to relax the one-child policy. Under the new policy, families could have two children if one parent, rather than both parents, was an only child. This mainly applied to urban couples, since there were very few rural only children due to long-standing exceptions to the policy for rural couples. Zhejiang, one of the most affluent provinces, became the first area to implement this \"relaxed policy\" in January 2014, and 29 out of the 31 provinces had implemented it by July 2014, with the exceptions of Xinjiang and Tibet. Under this policy, approximately 11million couples in China are allowed to have a second child; however, only \"nearly one million\" couples applied to have a second child in 2014, less than half the expected number of 2 million per year. By May 2014, 241,000 out of 271,000 applications had been approved. Officials of China's National Health and Family Planning Commission claimed that this outcome was expected, and that \"second-child policy\" would continue progressing with a good start.\n\nIn October 2015, the Chinese news agency Xinhua announced plans of the government to abolish the one-child policy, now allowing all families to have two children, citing from a communiqué issued by the Communist Party \"to improve the balanced development of population\"an apparent reference to the country's female-to-male sex ratioand to deal with an aging population according to the Canadian Broadcasting Corporation. The new law took effect on 1 January 2016 after it was passed in the standing committee of the National People's Congress on 27 December 2015.\n\nThe rationale for the abolition was summarized by former \"Wall Street Journal\" reporter Mei Fong: \"The reason China is doing this right now is because they have too many men, too many old people, and too few young people. They have this huge crushing demographic crisis as a result of the one-child policy. And if people don't start having more children, they're going to have a vastly diminished workforce to support a huge aging population.\" China's ratio is about five working adults to one retiree; the huge retiree community must be supported, and that will dampen future growth, according to Fong.\n\nSince the citizens of China are living longer and having fewer children, the growth of the population imbalance is expected to continue, as reported by the Canadian Broadcasting Corporation which referred to a United Nations projections forecast that \"China will lose 67million working-age people by 2030, while simultaneously doubling the number of elderly. That could put immense pressure on the economy and government resources.\" The longer term outlook is also pessimistic, based on an estimate by the Chinese Academy of Social Sciences, revealed by Cai Fang, deputy director. \"By 2050, one-third of the country will be aged 60 years or older, and there will be fewer workers supporting each retired person.\"\n\nAlthough many critics of China's reproductive restrictions approve of the policy's abolition, Amnesty International said that the move to the two-child policy would not end forced sterilizations, forced abortions, or government control over birth permits. Others also stated that the abolition is not a sign of the relaxation of authoritarian control in China. A reporter for CNN said, \"It was not a sign that the party will suddenly start respecting personal freedoms more than it has in the past. No, this is a case of the party adjusting policy to conditions. ... The new policy, raising the limit to two children per couple, preserves the state's role.\"\n\nThe abolition may not achieve a significant benefit, as the Canadian Broadcasting Corporation analysis indicated: \"Repealing the one-child policy may not spur a huge baby boom, however, in part because fertility rates are believed to be declining even without the policy's enforcement. Previous easings of the one-child policy have spurred fewer births than expected, and many people among China's younger generations see smaller family sizes as ideal.\" The CNN reporter adds that China's new prosperity is also a factor in the declining birth rate, saying, \"Couples naturally decide to have fewer children as they move from the fields into the cities, become more educated, and when women establish careers outside the home.\"\n\nThe Chinese government had expected the abolishing of the one-child rule would lead to an increase in births to about 21.9 million births in 2018. The actual number of births was 15.2 million - the lowest birth rate since 1961.\n\nThe one-child policy was managed by the National Population and Family Planning Commission under the central government since 1981. The Ministry of Health of the People's Republic of China and the National Health and Family Planning Commission were made defunct and a new single agency National Health and Family Planning Commission took over national health and family planning policies in 2013. The agency reports to the State Council.\n\nThe policy was enforced at the provincial level through fines that were imposed based on the income of the family and other factors. \"Population and Family Planning Commissions\" existed at every level of government to raise awareness and carry out registration and inspection work.\n\nThe fertility rate in China continued its fall from 2.8 births per woman in 1979 (already a sharp reduction from more than five births per woman in the early 1970s) to 1.5 by the mid 1990s. Some scholars claim that this decline is similar to that observed in other places that had no one-child restrictions, such as Thailand as well as Indian states of Kerala and Tamil Nadu, a claim designed to support the argument that China's fertility might have fallen to such levels anyway without draconian fertility restrictions.\n\nAccording to a 2017 study in the Journal of Economic Perspectives, \"the one-child policy accelerated the already-occurring drop in fertility for a few years, but in the longer term, economic development played a more fundamental role in leading to and maintaining China's low fertility level.\". However, a more recent study found that China's fertility decline to very low levels by the mid 1990s was far more impressive given its lower level of socio-economic development at that time; even after taking rapid economic development into account, China's fertility restrictions likely averted over 500 million births between 1970 and 2015, with the portion caused by one-child restrictions possibly totaling 400 million. Fertility restrictions also had other unintended consequences, such as a deficit of 40 million female babies. Most of this deficit was due to sex-selective abortion as well as the 1.5 child stopping rule, which required rural parents to stop childbearing if their first born was a son. Another consequence was the acceleration of the aging of China's population.\n\nThe sex ratio of a newborn infant (between male and female births) in mainland China reached 117:100, and stabilized between 2000 and 2013, about 10% higher than the baseline, which ranges between 103:100 and 107:100. It had risen from 108:100 in 1981—at the boundary of the natural baseline—to 111:100 in 1990. According to a report by the National Population and Family Planning Commission, there will be 30million more men than women in 2020, potentially leading to social instability, and courtship-motivated emigration. The number of 30 million cited for the sex disparity is, however, likely very exaggerated, as birth statistics is skewed by late registrations and unreported births: for instance, researchers found that census statistics of women in later stages of their life do not match with the birth statistics.\n\nThe disparity in the gender ratio at birth increases dramatically after the first birth, for which the ratios remained steadily within the natural baseline over the 20 year interval between 1980 and 1999. Thus, a large majority of couples appear to accept the outcome of the first pregnancy, whether it is a boy or a girl. If the first child is a girl, and they are able to have a second child, then a couple may take extraordinary steps to assure that the second child is a boy. If a couple already has two or more boys, the sex ratio of higher parity births swings decidedly in a feminine direction. This demographic evidence indicates that while families highly value having male offspring, a secondary norm of having a girl or having some balance in the sexes of children often comes into play. reported a study based on the 1990 census in which they found sex ratios of just 65 or 70 boys per 100 girls for births in families that already had two or more boys. A study by found a similar pattern among both Han and non-Han nationalities in Xinjiang Province: a strong preference for girls in high parity births in families that had already borne two or more boys. This tendency to favour girls in high parity births to couples who had already borne sons was later also noted by Coale and Banister, who suggested as well that once a couple had achieved its goal for the number of males, it was also much more likely to engage in \"stopping behavior\", i.e., to stop having more children.\n\nThe long-term disparity has led to a significant gender imbalance or skewing of the sex ratio. As reported by the Canadian Broadcasting Corporation, China has between 32million and 36million more males than would be expected naturally, and this has led to social problems. \"Because of a traditional preference for baby boys over girls, the one-child policy is often cited as the cause of China's skewed sex ratio ... Even the government acknowledges the problem and has expressed concern about the tens of millions of young men who won't be able to find brides and may turn to kidnapping women, sex trafficking, other forms of crime or social unrest.\" The situation will not improve in the near future. According to the Chinese Academy of Social Sciences, there will be 24 million more men than women of marriageable age by 2020.\n\nAccording to a 2017 study in the Journal of Economic Perspectives, \"existing studies indicate either a modest or minimal effect of the fertility change induced by the one-child policy on children education\".\n\nFor parents who had \"unauthorized\" births or who wanted a son but had a daughter, giving up the child for adoption was a kind of strategy to avoid penalties under one-child restrictions. Many families also kept their illegal children hidden so they would not be punished by the government. In fact, \"out adoption\" was not uncommon in China even before birth planning. In the 1980s, adoptions of daughters accounted for slightly above half of the so-called \"missing girls\", as out-adopted daughters often went unreported in censuses and survey and adoptive parents were not penalized for violating birth quota. However, in 1991, a central decree attempted to close off this loophole by raising penalties and levying those penalties on any household that had an \"unauthorized\" child, including those that had adopted children. This closing of the adoption loophole resulted in the abandonment of some two million Chinese children (mostly daughters), many of whom ended up in orphanages, some 120,000 of whom would be adopted by international parents.\n\nThe peak wave of abandonment occurred in the 1990s, with a smaller wave after 2000. Around the same time, poor care and high mortality rates in some state orphanages generated intense international pressure for reform.\n\nAfter 2005, the number of international adoptions declined, due both to falling birth rates and the related increase in demand for adoptions by Chinese parents themselves. In an interview with National Public Radio on 30 October 2015, Adam Pertman, president and CEO of the National Center on Adoption and Permanency, indicated that \"the infant girls of yesteryear have not been available, if you will, for five, seven years. China has been ... trying to keep the girls within the country ... And the consequence is that, today, rather than those young girls who used to be available – primarily girls – today, it's older children, children with special needs, children in sibling groups. It's very, very different.\"\n\nSince there are no penalties for multiple births, it is believed that an increasing number of couples are turning to fertility medicines to induce the conception of twins. According to a 2006 \"China Daily\" report, the number of twins born per year was estimated to have doubled.\n\nThe one-child policy's limit on the number of children resulted in new mothers having more resources to start investing money in their own well-being. As a result of being an only child, women have increased opportunity to receive an education, and support to get better jobs. One of the side effects of the one-child policy is to have liberated women from heavy duties in terms of taking care of many children and the family in the past; instead women had a lot of spare time for themselves to pursue their career or hobbies. The other major \"side effect\" of the one child policy is that the traditional concepts of gender roles between men and women have weakened. Being one and the only \"chance\" the parents have, women are expected to compete with peer men for better educational resources or career opportunities. Especially in cities where one-child policy was much more regulated and enforced, expectations on women to succeed in life are no less than on men. Recent data has shown that the proportion of women attending college is higher than that of men. The policy also has a positive effect at 10 to 19 years of age on the likelihood of completing senior high school in women of Han ethnicity. At the same time, the one-child policy reduces the economic burden for each family. The condition for each family has become better. As a result, women also have much more freedom within the family. They are supported by their family to pursue their life achievements.\n\nIt is reported that the focus of China on population planning helps provide a better health service for women and a reduction in the risks of death and injury associated with pregnancy. At family planning offices, women receive free contraception and pre-natal classes that contributed to the policy's success in two respects. First, the average Chinese household expends fewer resources, both in terms of time and money, on children, which gives many Chinese people more money with which to invest. Second, since Chinese adults can no longer rely on children to care for them in their old age, there is an impetus to save money for the future.\n\nAs the first generation of law-enforced only-children came of age for becoming parents themselves, one adult child was left with having to provide support for his or her two parents and four grandparents. Called the \"4-2-1 Problem\", this leaves the older generations with increased chances of dependency on retirement funds or charity in order to receive support. If not for personal savings, pensions, or state welfare, most senior citizens would be left entirely dependent upon their very small family or neighbours for assistance. If for any reason, the single child is unable to care for their older adult relatives, the oldest generations would face a lack of resources and necessities. In response to such an issue, by 2007, all provinces in the nation except Henan had adopted a new policy allowing couples to have two children if both parents were only children themselves; Henan followed in 2011.\n\nHeihaizi () or \"black child\" is a term denoting children born outside the one-child policy, or generally children who are not registered in the Chinese national household registration system.\n\nBeing excluded from the family register means they do not possess a Hukou, which is \"an identifying document, similar in some ways to the American social security card.\" In this respect they do not legally exist and as a result cannot access most public services, such as education and health care, and do not receive protection under the law.\n\nSome parents may over-indulge their only child. The media referred to the indulged children in one-child families as \"little emperors\". Since the 1990s, some people have worried that this will result in a higher tendency toward poor social communication and cooperation skills among the new generation, as they have no siblings at home.This is coupled with lack of uncles and aunts for next generation.No social studies have investigated the ratio of these so-called \"over-indulged\" children and to what extent they are indulged. With the first generation of children born under the policy (which initially became a requirement for most couples with first children born starting in 1979 and extending into the 1980s) reaching adulthood, such worries were reduced.\n\nHowever, the \"little emperor syndrome\" and additional expressions, describing the generation of Chinese singletons are very abundant in the Chinese media, Chinese academia and popular discussions. Being over-indulged, lacking self-discipline and having no adaptive capabilities are traits that are highly associated with Chinese singletons.\n\nSome 30 delegates called on the government in the Chinese People's Political Consultative Conference in March 2007 to abolish the one-child rule, citing \"social problems and personality disorders in young people\". One statement read, \"It is not healthy for children to play only with their parents and be spoiled by them: it is not right to limit the number to two children per family, either.\" The proposal was prepared by Ye Tingfang, a professor at the Chinese Academy of Social Sciences, who suggested that the government at least restore the previous rule that allowed couples to have up to two children. According to a scholar, \"The one-child limit is too extreme. It violates nature's law. And in the long run, this will lead to mother nature's revenge.\"\n\nReports surfaced of Chinese women giving birth to their second child overseas, a practice known as birth tourism. Many went to Hong Kong, which is exempt from the one-child policy. Likewise, a Hong Kong passport differs from China's mainland passport by providing additional advantages. Recently though, the Hong Kong government has drastically reduced the quota of births set for non-local women in public hospitals. As a result, fees for delivering babies there have surged. As further admission cuts or a total ban on non-local births in Hong Kong are being considered, mainland agencies that arrange for expectant mothers to give birth overseas are predicting a surge in those going to North America.\n\nAs the United States practises birthright citizenship, all children born in the US will automatically have US citizenship. The closest US location from China is Saipan in the Northern Mariana Islands, a US dependency in the western Pacific Ocean that allows Chinese visitors without visa restrictions. As of 2012, the island was experiencing an upswing in Chinese births, since birth tourism there had become cheaper than to Hong Kong. This option is used by relatively affluent Chinese who often have secondary motives as well, wishing their children to be able to leave mainland China when they grow older or bring their parents to the US. Canada, compared to the US, is less achievable as their government denies many visa requests.\n\nDue to the preference in Rural Chinese society to give birth to a son, pre-natal sex determination and sex-selective abortions are illegal in China. Often argued as one of the key factors in the imbalanced sex-ratio in China, as excess female infant mortality and underreporting of female births cannot solely explain this gender disparity. Researchers have found that the gender of the firstborn child in rural parts of China impact whether or not the mother will seek an ultrasound for the second child. 40% of women with a firstborn son seek an ultrasound for their second pregnancy, versus 70% of women with firstborn daughters. This clearly depicts a desire for women to birth a son if one has not yet been birthed. In response to this, the Chinese government made sex-selective abortions illegal in 2005.\n\nThe policy is controversial outside China for many reasons, including accusations of human rights abuses in the implementation of the policy, as well as concerns about negative social consequences.\n\nThe Chinese government, quoting Zhai Zhenwu, director of Renmin University's School of Sociology and Population in Beijing, estimates that 400million births were prevented by the one-child policy as of 2011, while some demographers challenge that number, putting the figure at perhaps half that level, according to CNN. Zhai clarified that the 400million estimate referred not just to the one-child policy, but includes births prevented by predecessor policies implemented one decade before, stating that \"there are many different numbers out there but it doesn't change the basic fact that the policy prevented a really large number of births\".\n\nThis claim is disputed by Wang Feng, director of the Brookings-Tsinghua Center for Public Policy, and Cai Yong from the Carolina Population Center at University of North Carolina Chapel Hill Wang claims that \"Thailand and China have had almost identical fertility trajectories since the mid 1980s\", and \"Thailand does not have a one-child policy.\" \nChina's Health Ministry has also disclosed that at least 336million abortions were performed on account of the policy.\n\nAccording to a report by the US Embassy, scholarship published by Chinese scholars and their presentations at the October 1997 Beijing conference of the International Union for the Scientific Study of Population seemed to suggest that market-based incentives or increasing voluntariness is not morally better but that it is, in the end, more effective. In 1988, Zeng Yi and Professor T. Paul Schultz of Yale University discussed the effect of the transformation to the market on Chinese fertility, arguing that the introduction of the contract responsibility system in agriculture during the early 1980s weakened family planning controls during that period. Zeng contended that the \"big cooking pot\" system of the People's Communes had insulated people from the costs of having many children. By the late 1980s, economic costs and incentives created by the contract system were already reducing the number of children farmers wanted.\n\nA long-term experiment in a county in Shanxi, in which the family planning law was suspended, suggested that families would not have many more children even if the law were abolished. A 2003 review of the policy-making process behind the adoption of the one-child policy shows that less intrusive options, including those that emphasized delay and spacing of births, were known but not fully considered by China's political leaders.\n\nCorrupted government officials and especially wealthy individuals have often been able to violate the policy in spite of fines. Filmmaker Zhang Yimou had three children and was subsequently fined 7.48million yuan ($1.2million). For example, between 2000 and 2005, as many as 1,968 officials in Hunan province were found to be violating the policy, according to the provincial family planning commission; also exposed by the commission were 21 national and local lawmakers, 24 political advisors, 112 entrepreneurs and 6 senior intellectuals.\n\nSome of the offending officials did not face penalties, although the government did respond by raising fines and calling on local officials to \"expose the celebrities and high-income people who violate the family planning policy and have more than one child\". Also, people who lived in the rural areas of China were allowed to have two children without punishment, although the family is required to wait a couple of years before having another child.\n\nThe one-child policy has been challenged for violating a human right to determine the size of one's own proper family. According to a 1968 proclamation of the International Conference on Human Rights, \"Parents have a basic human right to determine freely and responsibly the number and the spacing of their children.\"\n\nAccording to the UK newspaper \"The Daily Telegraph\", a quota of 20,000 abortions and sterilizations was set for Huaiji County, Guangdong in one year due to reported disregard of the one-child policy. According to the article local officials were being pressured into purchasing portable ultrasound devices to identify abortion candidates in remote villages. The article also reported that women as far along as 8.5 months pregnant were forced to abort, usually by an injection of saline solution. A 1993 book by social scientist and conservative political activist Steven W. Mosher reported that women in their ninth month of pregnancy, or already in labour, were having their children killed whilst in the birth canal or immediately after birth.\n\nAccording to a 2005 news report by Australian Broadcasting Corporation correspondent John Taylor, China outlawed the use of physical force to make a woman submit to an abortion or sterilization in 2002 but ineffectively enforces the measure. In 2012, Feng Jianmei, a villager from Shaanxi province was forced into an abortion by local officials after her family refused to pay the fine for having a second child. Chinese authorities have since apologized and two officials were fired, while five others were sanctioned.\n\nIn the past, China promoted eugenics as part of its population planning policies, but the government has backed away from such policies, as evidenced by China's ratification of the Convention on the Rights of Persons with Disabilities, which compels the nation to significantly reform its genetic testing laws. Recent research has also emphasized the necessity of understanding a myriad of complex social relations that affect the meaning of informed consent in China. Furthermore, in 2003, China revised its marriage registration regulations and couples no longer have to submit to a pre-marital physical or genetic examination before being granted a marriage license.\n\nThe United Nations Population Fund's (UNFPA) support for family planning in China, which has been associated with the One-Child policy in the United States, led the United States Congress to pull out of the UNFPA during the Reagan administration, and again under George W. Bush's presidency, citing human rights abuses and stating that the right to \"found a family\" was protected under the Preamble in the Universal Declaration of Human Rights. President Obama resumed U.S. government financial support for the UNFPA shortly after taking office in 2009, intending to \"work collaboratively to reduce poverty, improve the health of women and children, prevent HIV/AIDS and provide family planning assistance to women in 154 countries\".\n\nSex-selected abortion, abandonment, and infanticide are illegal in China. Nevertheless, the United States Department of State, the Parliament of the United Kingdom, and the human rights organization Amnesty International have all declared that infanticide still exists. A writer for the Georgetown Journal of International Affairs wrote, \"The 'one-child' policy has also led to what Amartya Sen first called 'Missing Women', or the 100million girls 'missing' from the populations of China (and other developing countries) as a result of female infanticide, abandonment, and neglect\".\n\nThe Canadian Broadcasting Corporation offered the following summary as to the long term effects of sex-selective abortion and abandonment of female infants:\n\nAnthropologist G. William Skinner at the University of California, Davis and Chinese researcher Yuan Jianhua have claimed that infanticide was fairly common in China before the 1990s.\n\n\n\nGeneral:\n\n\n\n \n"}
{"id": "24872", "url": "https://en.wikipedia.org/wiki?curid=24872", "title": "Pollution", "text": "Pollution\n\nPollution is the introduction of contaminants into the natural environment that cause adverse change. Pollution can take the form of chemical substances or energy, such as noise, heat or light. Pollutants, the components of pollution, can be either foreign substances/energies or naturally occurring contaminants. Pollution is often classed as point source or nonpoint source pollution.\nIn 2015, pollution killed 9 million people in the world.\n\nMajor forms of pollution include: Air pollution, light pollution, littering, noise pollution, plastic pollution, soil contamination, radioactive contamination, thermal pollution, visual pollution, water pollution.\n\nAir pollution has always accompanied civilizations. Pollution started from prehistoric times, when man created the first fires. According to a 1983 article in the journal \"Science,\" \"soot\" found on ceilings of prehistoric caves provides ample evidence of the high levels of pollution that was associated with inadequate ventilation of open fires.\" Metal forging appears to be a key turning point in the creation of significant air pollution levels outside the home. Core samples of glaciers in Greenland indicate increases in pollution associated with Greek, Roman, and Chinese metal production.\n\nThe burning of coal and wood, and the presence of many horses in concentrated areas made the cities the primary sources of pollution. The Industrial Revolution brought an infusion of untreated chemicals and wastes into local streams that served as the water supply. King Edward I of England banned the burning of sea-coal by proclamation in London in 1272, after its smoke became a problem; the fuel was so common in England that this earliest of names for it was acquired because it could be carted away from some shores by the wheelbarrow.\n\nIt was the Industrial Revolution that gave birth to environmental pollution as we know it today. London also recorded one of the earlier extreme cases of water quality problems with the Great Stink on the Thames of 1858, which led to construction of the London sewerage system soon afterward. Pollution issues escalated as population growth far exceeded viability of neighborhoods to handle their waste problem. Reformers began to demand sewer systems and clean water.\n\nIn 1870, the sanitary conditions in Berlin were among the worst in Europe. August Bebel recalled conditions before a modern sewer system was built in the late 1870s:\n\nThe primitive conditions were intolerable for a world national capital, and the Imperial German government brought in its scientists, engineers, and urban planners to not only solve the deficiencies, but to forge Berlin as the world's model city. A British expert in 1906 concluded that Berlin represented \"the most complete application of science, order and method of public life,\" adding \"it is a marvel of civic administration, the most modern and most perfectly organized city that there is.\"\n\nThe emergence of great factories and consumption of immense quantities of coal gave rise to unprecedented air pollution and the large volume of industrial chemical discharges added to the growing load of untreated human waste. Chicago and Cincinnati were the first two American cities to enact laws ensuring cleaner air in 1881. Pollution became a major issue in the United States in the early twentieth century, as progressive reformers took issue with air pollution caused by coal burning, water pollution caused by bad sanitation, and street pollution caused by the 3 million horses who worked in American cities in 1900, generating large quantities of urine and manure. As historian Martin Melosi notes, The generation that first saw automobiles replacing the horses saw cars as \"miracles of cleanliness.\". By the 1940s, however, automobile-caused smog was a major issue in Los Angeles.\n\nOther cities followed around the country until early in the 20th century, when the short lived Office of Air Pollution was created under the Department of the Interior. Extreme smog events were experienced by the cities of Los Angeles and Donora, Pennsylvania in the late 1940s, serving as another public reminder.\n\nAir pollution would continue to be a problem in England, especially later during the industrial revolution, and extending into the recent past with the Great Smog of 1952. Awareness of atmospheric pollution spread widely after World War II, with fears triggered by reports of radioactive fallout from atomic warfare and testing. Then a non-nuclear event – the Great Smog of 1952 in London – killed at least 4000 people. This prompted some of the first major modern environmental legislation: the Clean Air Act of 1956.\n\nPollution began to draw major public attention in the United States between the mid-1950s and early 1970s, when Congress passed the Noise Control Act, the Clean Air Act, the Clean Water Act, and the National Environmental Policy Act. \n\nSevere incidents of pollution helped increase consciousness. PCB dumping in the Hudson River resulted in a ban by the EPA on consumption of its fish in 1974. National news stories in the late 1970s – especially the long-term dioxin contamination at Love Canal starting in 1947 and uncontrolled dumping in Valley of the Drums – led to the Superfund legislation of 1980. The pollution of industrial land gave rise to the name brownfield, a term now common in city planning.\n\nThe development of nuclear science introduced radioactive contamination, which can remain lethally radioactive for hundreds of thousands of years. Lake Karachay – named by the Worldwatch Institute as the \"most polluted spot\" on earth – served as a disposal site for the Soviet Union throughout the 1950s and 1960s. Chelyabinsk, Russia, is considered the \"Most polluted place on the planet\".\n\nNuclear weapons continued to be tested in the Cold War, especially in the earlier stages of their development. The toll on the worst-affected populations and the growth since then in understanding about the critical threat to human health posed by radioactivity has also been a prohibitive complication associated with nuclear power. Though extreme care is practiced in that industry, the potential for disaster suggested by incidents such as those at Three Mile Island, Chernobyl, and Fukushima pose a lingering specter of public mistrust. Worldwide publicity has been intense on those disasters. Widespread support for test ban treaties has ended almost all nuclear testing in the atmosphere.\n\nInternational catastrophes such as the wreck of the Amoco Cadiz oil tanker off the coast of Brittany in 1978 and the Bhopal disaster in 1984 have demonstrated the universality of such events and the scale on which efforts to address them needed to engage. The borderless nature of atmosphere and oceans inevitably resulted in the implication of pollution on a planetary level with the issue of global warming. Most recently the term persistent organic pollutant (POP) has come to describe a group of chemicals such as PBDEs and PFCs among others. Though their effects remain somewhat less well understood owing to a lack of experimental data, they have been detected in various ecological habitats far removed from industrial activity such as the Arctic, demonstrating diffusion and bioaccumulation after only a relatively brief period of widespread use.\nA much more recently discovered problem is the Great Pacific Garbage Patch, a huge concentration of plastics, chemical sludge and other debris which has been collected into a large area of the Pacific Ocean by the North Pacific Gyre. This is a less well known pollution problem than the others described above, but nonetheless has multiple and serious consequences such as increasing wildlife mortality, the spread of invasive species and human ingestion of toxic chemicals. Organizations such as 5 Gyres have researched the pollution and, along with artists like Marina DeBris, are working toward publicizing the issue.\n\nPollution introduced by light at night is becoming a global problem, more severe in urban centres, but nonetheless contaminating also large territories, far away from towns.\n\nGrowing evidence of local and global pollution and an increasingly informed public over time have given rise to environmentalism and the environmental movement, which generally seek to limit human impact on the environment.\n\nThe major forms of pollution are listed below along with the particular contaminant relevant to each of them:\n\nA pollutant is a waste material that pollutes air, water, or soil. Three factors determine the severity of a pollutant: its chemical nature, the concentration and the persistence.\n\nPollution has a cost. Manufacturing activities that cause air pollution impose health and clean-up costs on the whole of society, whereas the neighbors of an individual who chooses to fire-proof his home may benefit from a reduced risk of a fire spreading to their own homes. A manufacturing activity that causes air pollution is an example of a negative externality in production. A negative externality in production occurs “when a firm’s production reduces the well-being of others who are not compensated by the firm.\" For example, if a laundry firm exists near a polluting steel manufacturing firm, there will be increased costs for the laundry firm because of the dirt and smoke produced by the steel manufacturing firm. If external costs exist, such as those created by pollution, the manufacturer will choose to produce more of the product than would be produced if the manufacturer were required to pay all associated environmental costs. Because responsibility or consequence for self-directed action lies partly outside the self, an element of externalization is involved. If there are external benefits, such as in public safety, less of the good may be produced than would be the case if the producer were to receive payment for the external benefits to others. However, goods and services that involve negative externalities in production, such as those that produce pollution, tend to be over-produced and underpriced since the externality is not being priced into the market.\n\nPollution can also create costs for the firms producing the pollution. Sometimes firms choose, or are forced by regulation, to reduce the amount of pollution that they are producing. The associated costs of doing this are called abatement costs, or marginal abatement costs if measured by each additional unit. In 2005 pollution abatement capital expenditures and operating costs in the US amounted to nearly $27 billion.\n\nSociety derives some indirect utility from pollution, otherwise there would be no incentive to pollute. This utility comes from the consumption of goods and services that create pollution. Therefore, it is important that policymakers attempt to balance these indirect benefits with the costs of pollution in order to achieve an efficient outcome.\n\nIt is possible to use environmental economics to determine which level of pollution is deemed the social optimum. For economists, pollution is an “external cost and occurs only when one or more individuals suffer a loss of welfare,” however, there exists a socially optimal level of pollution at which welfare is maximized. This is because consumers derive utility from the good or service manufactured, which will outweigh the social cost of pollution until a certain point. At this point the damage of one extra unit of pollution to society, the marginal cost of pollution, is exactly equal to the marginal benefit of consuming one more unit of the good or service.\n\nIn markets with pollution, or other negative externalities in production, the free market equilibrium will not account for the costs of pollution on society. If the social costs of pollution are higher than the private costs incurred by the firm, then the true supply curve will be higher. The point at which the social marginal cost and market demand intersect gives the socially optimal level of pollution. At this point, the quantity will be lower and the price will be higher in comparison to the free market equilibrium. Therefore, the free market outcome could be considered a market failure because it “does not maximize efficiency”.\n\nThis model can be used as a basis to evaluate different methods of internalizing the externality. Some examples include tariffs, a carbon tax and cap and trade systems.\n\nAir pollution comes from both natural and human-made (anthropogenic) sources. However, globally human-made pollutants from combustion, construction, mining, agriculture and warfare are increasingly significant in the air pollution equation.\n\nMotor vehicle emissions are one of the leading causes of air pollution. China, United States, Russia, India Mexico, and Japan are the world leaders in air pollution emissions. Principal stationary pollution sources include chemical plants, coal-fired power plants, oil refineries, petrochemical plants, nuclear waste disposal activity, incinerators, large livestock farms (dairy cows, pigs, poultry, etc.), PVC factories, metals production factories, plastics factories, and other heavy industry. Agricultural air pollution comes from contemporary practices which include clear felling and burning of natural vegetation as well as spraying of pesticides and herbicides\n\nAbout 400 million metric tons of hazardous wastes are generated each year. The United States alone produces about 250 million metric tons. Americans constitute less than 5% of the world's population, but produce roughly 25% of the world's , and generate approximately 30% of world's waste. In 2007, China has overtaken the United States as the world's biggest producer of , while still far behind based on per capita pollution – ranked 78th among the world's nations.\nIn February 2007, a report by the Intergovernmental Panel on Climate Change (IPCC), representing the work of 2,500 scientists, economists, and policymakers from more than 120 countries, said that humans have been the primary cause of global warming since 1950. Humans have ways to cut greenhouse gas emissions and avoid the consequences of global warming, a major climate report concluded. But to change the climate, the transition from fossil fuels like coal and oil needs to occur within decades, according to the final report this year from the UN's Intergovernmental Panel on Climate Change (IPCC).\n\nSome of the more common soil contaminants are chlorinated hydrocarbons (CFH), heavy metals (such as chromium, cadmium – found in rechargeable batteries, and lead – found in lead paint, aviation fuel and still in some countries, gasoline), MTBE, zinc, arsenic and benzene. In 2001 a series of press reports culminating in a book called \"Fateful Harvest\" unveiled a widespread practice of recycling industrial byproducts into fertilizer, resulting in the contamination of the soil with various metals. Ordinary municipal landfills are the source of many chemical substances entering the soil environment (and often groundwater), emanating from the wide variety of refuse accepted, especially substances illegally discarded there, or from pre-1970 landfills that may have been subject to little control in the U.S. or EU. There have also been some unusual releases of polychlorinated dibenzodioxins, commonly called \"dioxins\" for simplicity, such as TCDD.\n\nPollution can also be the consequence of a natural disaster. For example, hurricanes often involve water contamination from sewage, and petrochemical spills from ruptured boats or automobiles. Larger scale and environmental damage is not uncommon when coastal oil rigs or refineries are involved. Some sources of pollution, such as nuclear power plants or oil tankers, can produce widespread and potentially hazardous releases when accidents occur.\n\nIn the case of noise pollution the dominant source class is the motor vehicle, producing about ninety percent of all unwanted noise worldwide.\n\nAdverse air quality can kill many organisms including humans. Ozone pollution can cause respiratory disease, cardiovascular disease, throat inflammation, chest pain, and congestion. Water pollution causes approximately 14,000 deaths per day, mostly due to contamination of drinking water by untreated sewage in developing countries. An estimated 500 million Indians have no access to a proper toilet, Over ten million people in India fell ill with waterborne illnesses in 2013, and 1,535 people died, most of them children. Nearly 500 million Chinese lack access to safe drinking water. A 2010 analysis estimated that 1.2 million people died prematurely each year in China because of air pollution. The high smog levels China has been facing for a long time can do damage to civilians bodies and generate different diseases The WHO estimated in 2007 that air pollution causes half a million deaths per year in India. Studies have estimated that the number of people killed annually in the United States could be over 50,000.\n\nOil spills can cause skin irritations and rashes. Noise pollution induces hearing loss, high blood pressure, stress, and sleep disturbance. Mercury has been linked to developmental deficits in children and neurologic symptoms. Older people are majorly exposed to diseases induced by air pollution. Those with heart or lung disorders are at additional risk. Children and infants are also at serious risk. Lead and other heavy metals have been shown to cause neurological problems. Chemical and radioactive substances can cause cancer and as well as birth defects.\n\nAn October 2017 study by the Lancet Commission on Pollution and Health found that global pollution, specifically toxic air, water, soils and workplaces, kill nine million people annually, which is triple the number of deaths caused by AIDS, tuberculosis and malaria combined, and 15 times higher than deaths caused by wars and other forms of human violence. The study concluded that \"pollution is one of the great existential challenges of the Anthropocene era. Pollution endangers the stability of the Earth’s support systems and threatens the continuing survival of human societies.\"\n\nPollution has been found to be present widely in the environment. There are a number of effects of this:\n\nThe Toxicology and Environmental Health Information Program (TEHIP) at the United States National Library of Medicine (NLM) maintains a comprehensive toxicology and environmental health web site that includes access to resources produced by TEHIP and by other government agencies and organizations. This web site includes links to databases, bibliographies, tutorials, and other scientific and consumer-oriented resources. TEHIP also is responsible for the Toxicology Data Network (TOXNET) an integrated system of toxicology and environmental health databases that are available free of charge on the web.\n\nTOXMAP is a Geographic Information System (GIS) that is part of TOXNET. TOXMAP uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs.\n\nA 2019 paper linked pollution to adverse school outcomes for children.\n\nA number of studies show that pollution has an adverse effect on the productivity of both indoor and outdoor workers.\n\nTo protect the environment from the adverse effects of pollution, many nations worldwide have enacted legislation to regulate various types of pollution as well as to mitigate the adverse effects of pollution.\n\nPollution control is a term used in environmental management. It means the control of emissions and effluents into air, water or soil. Without pollution control, the waste products from overconsumption, heating, agriculture, mining, manufacturing, transportation and other human activities, whether they accumulate or disperse, will degrade the environment. In the hierarchy of controls, pollution prevention and waste minimization are more desirable than pollution control. In the field of land development, low impact development is a similar technique for the prevention of urban runoff.\n\n\n\nThe earliest precursor of pollution generated by life forms would have been a natural function of their existence. The attendant consequences on viability and population levels fell within the sphere of natural selection. These would have included the demise of a population locally or ultimately, species extinction. Processes that were untenable would have resulted in a new balance brought about by changes and adaptations. At the extremes, for any form of life, consideration of pollution is superseded by that of survival.\n\nFor humankind, the factor of technology is a distinguishing and critical consideration, both as an enabler and an additional source of byproducts. Short of survival, human concerns include the range from quality of life to health hazards. Since science holds experimental demonstration to be definitive, modern treatment of toxicity or environmental harm involves defining a level at which an effect is observable. Common examples of fields where practical measurement is crucial include automobile emissions control, industrial exposure (e.g. Occupational Safety and Health Administration (OSHA) PELs), toxicology (e.g. ), and medicine (e.g. medication and radiation doses).\n\n\"The solution to pollution is dilution\", is a dictum which summarizes a traditional approach to pollution management whereby sufficiently diluted pollution is not harmful. It is well-suited to some other modern, locally scoped applications such as laboratory safety procedure and hazardous material release emergency management. But it assumes that the dilutant is in virtually unlimited supply for the application or that resulting dilutions are acceptable in all cases.\n\nSuch simple treatment for environmental pollution on a wider scale might have had greater merit in earlier centuries when physical survival was often the highest imperative, human population and densities were lower, technologies were simpler and their byproducts more benign. But these are often no longer the case. Furthermore, advances have enabled measurement of concentrations not possible before. The use of statistical methods in evaluating outcomes has given currency to the principle of probable harm in cases where assessment is warranted but resorting to deterministic models is impractical or infeasible. In addition, consideration of the environment beyond direct impact on human beings has gained prominence.\n\nYet in the absence of a superseding principle, this older approach predominates practices throughout the world. It is the basis by which to gauge concentrations of effluent for legal release, exceeding which penalties are assessed or restrictions applied. One such superseding principle is contained in modern hazardous waste laws in developed countries, as the process of diluting hazardous waste to make it non-hazardous is usually a regulated treatment process. Migration from pollution dilution to elimination in many cases can be confronted by challenging economical and technological barriers.\n\nCarbon dioxide, while vital for photosynthesis, is sometimes referred to as pollution, because raised levels of the gas in the atmosphere are affecting the Earth's climate. Disruption of the environment can also highlight the connection between areas of pollution that would normally be classified separately, such as those of water and air. Recent studies have investigated the potential for long-term rising levels of atmospheric carbon dioxide to cause slight but critical increases in the acidity of ocean waters, and the possible effects of this on marine ecosystems.\n\nThe Pure Earth, an international non-for-profit organization dedicated to eliminating life-threatening pollution in the developing world, issues an annual list of some of the world's most polluting industries.\n\nA 2018 report by the Institute for Agriculture and Trade Policy and GRAIN says that the meat and dairy industries are poised to surpass the oil industry as the world's worst polluters.\n\nPure Earth issues an annual list of some of the world's worst polluted places.\n\n"}
{"id": "10934212", "url": "https://en.wikipedia.org/wiki?curid=10934212", "title": "Air pollution", "text": "Air pollution\n\nAir pollution occurs when harmful or excessive quantities of substances including gases (such as ammonia, carbon monoxide, sulfur dioxide, nitrous oxides, methane and chlorofluorocarbons), particulates (both organic and inorganic), and biological molecules are introduced into Earth's atmosphere. It may cause diseases, allergies and even death to humans; it may also cause harm to other living organisms such as animals and food crops, and may damage the natural or built environment. Both human activity and natural processes can generate air pollution.\n\nIndoor air pollution and poor urban air quality are listed as two of the world's worst toxic pollution problems in the 2008 Blacksmith Institute World's Worst Polluted Places report. Outdoor air pollution alone causes 2.1 to 4.21 million premature deaths annually. According to the 2014 World Health Organization report, air pollution in 2012 caused the deaths of around 7 million people worldwide, an estimate roughly echoed by the International Energy Agency. \n\nAn air pollutant is a material in the air that can have adverse effects on humans and the ecosystem. The substance can be solid particles, liquid droplets, or gases. A pollutant can be of natural origin or man-made.\nPollutants are classified as primary or secondary. Primary pollutants are usually produced by processes such as ash from a volcanic eruption. Other examples include carbon monoxide gas from motor vehicle exhausts or sulfur dioxide released from factories. Secondary pollutants are not emitted directly. Rather, they form in the air when primary pollutants react or interact. Ground level ozone is a prominent example of secondary pollutants. Some pollutants may be both primary and secondary: they are both emitted directly and formed from other primary pollutants.\n\nPollutants emitted into the atmosphere by human activity include:\n\nSecondary pollutants include:\nMinor air pollutants include:\n\nPersistent organic pollutants (POPs) are organic compounds that are resistant to environmental degradation through chemical, biological, and photolytic processes. Because of this, they have been observed to persist in the environment, to be capable of long-range transport, bioaccumulate in human and animal tissue, biomagnify in food chains, and to have potentially significant impacts on human health and the environment.\n\nThese are mostly related to the burning of fuel.\n\nThere are also sources from processes other than combustion\n\n\nAir pollutant emission factors are reported representative values that attempt to relate the quantity of a pollutant released to the ambient air with an activity associated with the release of that pollutant. These factors are usually expressed as the weight of pollutant divided by a unit weight, volume, distance, or duration of the activity emitting the pollutant (e.g., kilograms of particulate emitted per tonne of coal burned). Such factors facilitate estimation of emissions from various sources of air pollution. In most cases, these factors are simply averages of all available data of acceptable quality, and are generally assumed to be representative of long-term averages.\n\nThere are 12 compounds in the list of persistent organic pollutants. Dioxins and furans are two of them and intentionally created by combustion of organics, like open burning of plastics. These compounds are also endocrine disruptors and can mutate the human genes.\n\nThe United States Environmental Protection Agency has published a compilation of air pollutant emission factors for a wide range of industrial sources. The United Kingdom, Australia, Canada and many other countries have published similar compilations, as well as the European Environment Agency.\n\nAir pollution risk is a function of the hazard of the pollutant and the exposure to that pollutant. Air pollution exposure can be expressed for an individual, for certain groups (e.g. neighborhoods or children living in a country), or for entire populations. For example, one may want to calculate the exposure to a hazardous air pollutant for a geographic area, which includes the various microenvironments and age groups. This can be calculated as an inhalation exposure. This would account for daily exposure in various settings (e.g. different indoor micro-environments and outdoor locations). The exposure needs to include different age and other demographic groups, especially infants, children, pregnant women and other sensitive subpopulations. The exposure to an air pollutant must integrate the concentrations of the air pollutant with respect to the time spent in each setting and the respective inhalation rates for each subgroup for each specific time that the subgroup is in the setting and engaged in particular activities (playing, cooking, reading, working, spending time in traffic, etc.). For example, a small child's inhalation rate will be less than that of an adult. A child engaged in vigorous exercise will have a higher respiration rate than the same child in a sedentary activity. The daily exposure, then, needs to reflect the time spent in each micro-environmental setting and the type of activities in these settings. The air pollutant concentration in each microactivity/microenvironmental setting is summed to indicate the exposure. For some pollutants such as black carbon, traffic related exposures may dominate total exposure despite short exposure times since high concentrations coincide with proximity to major roads or participation to (motorized) traffic. A large portion of total daily exposure occurs as short peaks of high concentrations, but it remains unclear how to define peaks and determine their frequency and health impact.\n\nA lack of ventilation indoors concentrates air pollution where people often spend the majority of their time. Radon (Rn) gas, a carcinogen, is exuded from the Earth in certain locations and trapped inside houses. Building materials including carpeting and plywood emit formaldehyde (HCO) gas. Paint and solvents give off volatile organic compounds (VOCs) as they dry. Lead paint can degenerate into dust and be inhaled. Intentional air pollution is introduced with the use of air fresheners, incense, and other scented items. Controlled wood fires in stoves and fireplaces can add significant amounts of smoke particulates into the air, inside and out. Indoor pollution fatalities may be caused by using pesticides and other chemical sprays indoors without proper ventilation.\n\nCarbon monoxide poisoning and fatalities are often caused by faulty vents and chimneys, or by the burning of charcoal indoors or in a confined space, such as a tent. Chronic carbon monoxide poisoning can result even from poorly-adjusted pilot lights. Traps are built into all domestic plumbing to keep sewer gas and hydrogen sulfide, out of interiors. Clothing emits tetrachloroethylene, or other dry cleaning fluids, for days after dry cleaning.\n\nThough its use has now been banned in many countries, the extensive use of asbestos in industrial and domestic environments in the past has left a potentially very dangerous material in many localities. Asbestosis is a chronic inflammatory medical condition affecting the tissue of the lungs. It occurs after long-term, heavy exposure to asbestos from asbestos-containing materials in structures. Sufferers have severe dyspnea (shortness of breath) and are at an increased risk regarding several different types of lung cancer. As clear explanations are not always stressed in non-technical literature, care should be taken to distinguish between several forms of relevant diseases. According to the World Health Organization (WHO), these may defined as; asbestosis, \"lung cancer\", and \"Peritoneal Mesothelioma\" (generally a very rare form of cancer, when more widespread it is almost always associated with prolonged exposure to asbestos).\n\nBiological sources of air pollution are also found indoors, as gases and airborne particulates. Pets produce dander, people produce dust from minute skin flakes and decomposed hair, dust mites in bedding, carpeting and furniture produce enzymes and micrometre-sized fecal droppings, inhabitants emit methane, mold forms on walls and generates mycotoxins and spores, air conditioning systems can incubate Legionnaires' disease and mold, and houseplants, soil and surrounding gardens can produce pollen, dust, and mold. Indoors, the lack of air circulation allows these airborne pollutants to accumulate more than they would otherwise occur in nature.\n\nIn 2012, air pollution caused premature deaths on average of 1 year in Europe, and was a significant risk factor for a number of pollution-related diseases, including respiratory infections, heart disease, COPD, stroke and lung cancer. The health effects caused by air pollution may include difficulty in breathing, wheezing, coughing, asthma and worsening of existing respiratory and cardiac conditions. These effects can result in increased medication use, increased doctor or emergency department visits, more hospital admissions and premature death. The human health effects of poor air quality are far reaching, but principally affect the body's respiratory system and the cardiovascular system. Individual reactions to air pollutants depend on the type of pollutant a person is exposed to, the degree of exposure, and the individual's health status and genetics.\nThe most common sources of air pollution include particulates, ozone, nitrogen dioxide, and sulfur dioxide. Children aged less than five years that live in developing countries are the most vulnerable population in terms of total deaths attributable to indoor and outdoor air pollution.\n\nThe World Health Organization estimated in 2014 that every year air pollution causes the premature death of some 7 million people worldwide. Studies published in March 2019 indicated that the number may be around 8.8 million.\n\nIndia has the highest death rate due to air pollution. India also has more deaths from asthma than any other nation according to the World Health Organization. In December 2013 air pollution was estimated to kill 500,000 people in China each year. There is a positive correlation between pneumonia-related deaths and air pollution from motor vehicle emissions.\n\nAnnual premature European deaths caused by air pollution are estimated at 430,000-800,000 An important cause of these deaths is nitrogen dioxide and other nitrogen oxides (NOx) emitted by road vehicles. In a 2015 consultation document the UK government disclosed that nitrogen dioxide is responsible for 23,500 premature UK deaths per annum. Across the European Union, air pollution is estimated to reduce life expectancy by almost nine months. Causes of deaths include strokes, heart disease, COPD, lung cancer, and lung infections.\n\nUrban outdoor air pollution is estimated to cause 1.3 million deaths worldwide per year. Children are particularly at risk due to the immaturity of their respiratory organ systems.\n\nThe US EPA estimated in 2004 that a proposed set of changes in diesel engine technology (\"Tier 2\") could result in 12,000 fewer \"premature mortalities\", 15,000 fewer heart attacks, 6,000 fewer emergency department visits by children with asthma, and 8,900 fewer respiratory-related hospital admissions each year in the United States.\n\nThe US EPA has estimated that limiting ground-level ozone concentration to 65 parts per billion, would avert 1,700 to 5,100 premature deaths nationwide in 2020 compared with the 75-ppb standard. The agency projected the more protective standard would also prevent an additional 26,000 cases of aggravated asthma, and more than a million cases of missed work or school. Following this assessment, the EPA acted to protect public health by lowering the National Ambient Air Quality Standards (NAAQS) for ground-level ozone to 70 parts per billion (ppb).\n\nA new economic study of the health impacts and associated costs of air pollution in the Los Angeles Basin and San Joaquin Valley of Southern California shows that more than 3,800 people die prematurely (approximately 14 years earlier than normal) each year because air pollution levels violate federal standards. The number of annual premature deaths is considerably higher than the fatalities related to auto collisions in the same area, which average fewer than 2,000 per year.\n\nDiesel exhaust (DE) is a major contributor to combustion-derived particulate matter air pollution. In several human experimental studies, using a well-validated exposure chamber setup, DE has been linked to acute vascular dysfunction and increased thrombus formation.\n\nThe mechanisms linking air pollution to increased cardiovascular mortality are uncertain, but probably include pulmonary and systemic inflammation.\n\nA 2007 review of evidence found that, ambient air pollution exposure is a risk factor correlating with increased total mortality from cardiovascular events (range: 12% to 14% per 10 microg/m increase).\n\nAir pollution is also emerging as a risk factor for stroke, particularly in developing countries where pollutant levels are highest. A 2007 study found that in women, air pollution is not associated with hemorrhagic but with ischemic stroke. Air pollution was also found to be associated with increased incidence and mortality from coronary stroke in a cohort study in 2011. Associations are believed to be causal and effects may be mediated by vasoconstriction, low-grade inflammation and atherosclerosis Other mechanisms such as autonomic nervous system imbalance have also been suggested.\nResearch has demonstrated increased risk of developing asthma and COPD from increased exposure to traffic-related air pollution. Additionally, air pollution has been associated with increased hospitalization and mortality from asthma and COPD. Chronic obstructive pulmonary disease (COPD) includes diseases such as chronic bronchitis and emphysema.\n\nA study conducted in 1960–1961 in the wake of the Great Smog of 1952 compared 293 London residents with 477 residents of Gloucester, Peterborough, and Norwich, three towns with low reported death rates from chronic bronchitis. All subjects were male postal truck drivers aged 40 to 59. Compared to the subjects from the outlying towns, the London subjects exhibited more severe respiratory symptoms (including cough, phlegm, and dyspnea), reduced lung function (FEV and peak flow rate), and increased sputum production and purulence. The differences were more pronounced for subjects aged 50 to 59. The study controlled for age and smoking habits, so concluded that air pollution was the most likely cause of the observed differences.\nMore recent studies have shown that air pollution exposure from traffic reduces lung function development in children and lung function may be compromised by air pollution even at low concentrations. Air pollution exposure also cause lung cancer in non smokers.\n\nIt is believed that much like cystic fibrosis, by living in a more urban environment serious health hazards become more apparent. Studies have shown that in urban areas patients suffer mucus hypersecretion, lower levels of lung function, and more self-diagnosis of chronic bronchitis and emphysema.\n\nA review of evidence regarding whether ambient air pollution exposure is a risk factor for cancer in 2007 found solid data to conclude that long-term exposure to PM2.5 (fine particulates) increases the overall risk of non-accidental mortality by 6% per a 10 microg/m increase. Exposure to PM2.5 was also associated with an increased risk of mortality from lung cancer (range: 15% to 21% per 10 microg/m increase) and total cardiovascular mortality (range: 12% to 14% per a 10 microg/m increase). The review further noted that living close to busy traffic appears to be associated with elevated risks of these three outcomes – increase in lung cancer deaths, cardiovascular deaths, and overall non-accidental deaths. The reviewers also found suggestive evidence that exposure to PM2.5 is positively associated with mortality from coronary heart diseases and exposure to SO increases mortality from lung cancer, but the data was insufficient to provide solid conclusions. Another investigation showed that higher activity level increases deposition fraction of aerosol particles in human lung and recommended avoiding heavy activities like running in outdoor space at polluted areas.\n\nIn 2011, a large Danish epidemiological study found an increased risk of lung cancer for patients who lived in areas with high nitrogen oxide concentrations. In this study, the association was higher for non-smokers than smokers. An additional Danish study, also in 2011, likewise noted evidence of possible associations between air pollution and other forms of cancer, including cervical cancer and brain cancer.\n\nIn December 2015, medical scientists reported that cancer is overwhelmingly a result of environmental factors, and not largely down to bad luck. Maintaining a healthy weight, eating a healthy diet, minimizing alcohol and eliminating smoking reduces the risk of developing the disease, according to the researchers.\n\nIn the United States, despite the passage of the Clean Air Act in 1970, in 2002 at least 146 million Americans were living in non-attainment areas—regions in which the concentration of certain air pollutants exceeded federal standards. These dangerous pollutants are known as the criteria pollutants, and include ozone, particulate matter, sulfur dioxide, nitrogen dioxide, carbon monoxide, and lead.\nProtective measures to ensure children's health are being taken in cities such as New Delhi, India where buses now use compressed natural gas to help eliminate the \"pea-soup\" smog. A recent study in Europe has found that exposure to ultrafine particles can increase blood pressure in children.\nAccording to a WHO report-2018, polluted air is a main cause poisoning millions of children under the age of 15 years and ruining their lives which resulting to death of some six hundred thousand children annually.\n\nAmbient levels of air pollution have been associated with preterm birth and low birth weight. A 2014 WHO worldwide survey on maternal and perinatal health found a statistically significant association between low birth weights (LBW) and increased levels of exposure to PM2.5. Women in regions with greater than average PM2.5 levels had statistically significant higher odds of pregnancy resulting in a low-birth weight infant even when adjusted for country-related variables. The effect is thought to be from stimulating inflammation and increasing oxidative stress.\n\nA study by the University of York found that in 2010 exposure to PM2.5 was strongly associated with 18% of preterm births globally, which was approximately 2.7 million premature births. The countries with the highest air pollution associated preterm births were in South and East Asia, the Middle East, North Africa, and West sub-Saharan Africa.\n\nThe source of PM 2.5 differs greatly by region. In South and East Asia, pregnant women are frequently exposed to indoor air pollution because of the wood and other biomass fuels used for cooking which are responsible for more than 80% of regional pollution. In the Middle East, North Africa and West sub-Saharan Africa, fine PM comes from natural sources, such as dust storms. The United States had an estimated 50,000 preterm births associated with exposure to PM2.5 in 2010.\n\nA study performed by Wang, et al. between the years of 1988 and 1991 has found a correlation between sulfur Dioxide (SO2) and total suspended particulates (TSP) and preterm births and low birth weights in Beijing. A group of 74,671 pregnant women, in four separate regions of Beijing, were monitored from early pregnancy to delivery along with daily air pollution levels of sulfur Dioxide and TSP (along with other particulates). The estimated reduction in birth weight was 7.3 g for every 100 µg/m increase in SO2 and 6.9g for each 100 µg/m increase in TSP. These associations were statistically significant in both summer and winter, although, summer was greater. The proportion of low birth weight attributable to air pollution, was 13%. This is the largest attributable risk ever reported for the known risk factors of low birth weight. Coal stoves, which are in 97% of homes, are a major source of air pollution in this area.\n\nBrauer et al. studied the relationship between air pollution and proximity to a highway with pregnancy outcomes in a Vancouver cohort of pregnant woman using addresses to estimate exposure during pregnancy. Exposure to NO, NO2, CO PM10 and PM2.5 were associated with infants born small for gestational age (SGA). Women living <50meters away from an expressway or highway were 26% more likely to give birth to a SGA infant.\n\nEven in the areas with relatively low levels of air pollution, public health effects can be significant and costly, since a large number of people breathe in such pollutants. A study published in 2017 found that even in areas of the U.S. where ozone and PM2.5 meet federal standards, Medicare recipients who are exposed to more air pollution have higher mortality rates. A 2005 scientific study for the British Columbia Lung Association showed that a small improvement in air quality (1% reduction of ambient PM2.5 and ozone concentrations) would produce $29 million in annual savings in the Metro Vancouver region in 2010. This finding is based on health valuation of lethal (death) and sub-lethal (illness) affects.\n\nData is accumulating that air pollution exposure also affects the central nervous system.\n\nIn a June 2014 study conducted by researchers at the University of Rochester Medical Center, published in the journal \"Environmental Health Perspectives\", it was discovered that early exposure to air pollution causes the same damaging changes in the brain as autism and schizophrenia. The study also shows that air pollution also affected short-term memory, learning ability, and impulsivity. Lead researcher Professor Deborah Cory-Slechta said that \"When we looked closely at the ventricles, we could see that the white matter that normally surrounds them hadn't fully developed. It appears that inflammation had damaged those brain cells and prevented that region of the brain from developing, and the ventricles simply expanded to fill the space. Our findings add to the growing body of evidence that air pollution may play a role in autism, as well as in other neurodevelopmental disorders.\" In a study of mice, air pollution also has a more significant negative effect on males than on females.\n\nIn 2015, experimental studies reported the detection of significant episodic (situational) cognitive impairment from impurities in indoor air breathed by test subjects who were not informed about changes in the air quality. Researchers at the Harvard University and SUNY Upstate Medical University and Syracuse University measured the cognitive performance of 24 participants in three different controlled laboratory atmospheres that simulated those found in \"conventional\" and \"green\" buildings, as well as green buildings with enhanced ventilation. Performance was evaluated objectively using the widely used Strategic Management Simulation software simulation tool, which is a well-validated assessment test for executive decision-making in an unconstrained situation allowing initiative and improvisation. Significant deficits were observed in the performance scores achieved in increasing concentrations of either volatile organic compounds (VOCs) or carbon dioxide, while keeping other factors constant. The highest impurity levels reached are not uncommon in some classroom or office environments. Air pollution increases the risk of dementia in people over 50 years old.\n\nIn India in 2014, it was reported that air pollution by black carbon and ground level ozone had reduced crop yields in the most affected areas by almost half in 2011 when compared to 1980 levels.\n\nAir pollution costs the world economy $5 trillion per year as a result of productivity losses and degraded quality of life, according to a joint study by the World Bank and the Institute for Health Metrics and Evaluation (IHME) at the University of Washington. These productivity losses are caused by deaths due to diseases caused by air pollution. One out of ten deaths in 2013 was caused by diseases associated with air pollution and the problem is getting worse. The problem is even more acute in the developing world. \"Children under age 5 in lower-income countries are more than 60 times as likely to die from exposure to air pollution as children in high-income countries.\" The report states that additional economic losses caused by air pollution, including health costs and the adverse effect on agricultural and other productivity were not calculated in the report, and thus the actual costs to the world economy are far higher than $5 trillion.\n\nThe world's worst short-term civilian pollution crisis was the 1984 Bhopal Disaster in India. Leaked industrial vapours from the Union Carbide factory, belonging to Union Carbide, Inc., U.S.A. (later bought by Dow Chemical Company), killed at least 3787 people and injured from 150,000 to 600,000. The United Kingdom suffered its worst air pollution event when the December 4 Great Smog of 1952 formed over London. In six days more than 4,000 died and more recent estimates put the figure at nearer 12,000. An accidental leak of anthrax spores from a biological warfare laboratory in the former USSR in 1979 near Sverdlovsk is believed to have caused at least 64 deaths. The worst single incident of air pollution to occur in the US occurred in Donora, Pennsylvania in late October, 1948, when 20 people died and over 7,000 were injured.\n\nThere are now practical alternatives to the principal causes of air pollution:\n\nVarious air pollution control technologies and strategies are available to reduce air pollution. At its most basic level, land-use planning is likely to involve zoning and transport infrastructure planning. In most developed countries, land-use planning is an important part of social policy, ensuring that land is used efficiently for the benefit of the wider economy and population, as well as to protect the environment.\n\nBecause a large share of air pollution is caused by combustion of fossil fuels such as coal and oil, the reduction of these fuels can reduce air pollution drastically. Most effective is the switch to clean power sources such as wind power, solar power, hydro power which don't cause air pollution. Efforts to reduce pollution from mobile sources includes primary regulation (many developing countries have permissive regulations), expanding regulation to new sources (such as cruise and transport ships, farm equipment, and small gas-powered equipment such as string trimmers, chainsaws, and snowmobiles), increased fuel efficiency (such as through the use of hybrid vehicles), conversion to cleaner fuels or conversion to electric vehicles.\n\nTitanium dioxide has been researched for its ability to reduce air pollution. Ultraviolet light will release free electrons from material, thereby creating free radicals, which break up VOCs and NOx gases. One form is superhydrophilic.\n\nIn 2014, Prof. Tony Ryan and Prof. Simon Armitage of University of Sheffield prepared a 10 meter by 20 meter-sized poster coated with microscopic, pollution-eating nanoparticles of titanium dioxide. Placed on a building, this giant poster can absorb the toxic emission from around 20 cars each day.\n\nA very effective means to reduce air pollution is the transition to renewable energy. According to a study published in Energy and Environmental Science in 2015 the switch to 100% renewable energy in the United States would eliminate about 62,000 premature mortalities per year and about 42,000 in 2050, if no biomass were used. This would save about $600 billion in health costs a year due to reduced air pollution in 2050, or about 3.6% of the 2014 U.S. gross domestic product.\n\nThe following items are commonly used as pollution control devices in industry and transportation. They can either destroy contaminants or remove them from an exhaust stream before it is emitted into the atmosphere.\n\nIn general, there are two types of air quality standards. The first class of standards (such as the U.S. National Ambient Air Quality Standards and E.U. Air Quality Directive) set maximum atmospheric concentrations for specific pollutants. Environmental agencies enact regulations which are intended to result in attainment of these target levels. The second class (such as the North American air quality index) take the form of a scale with various thresholds, which is used to communicate to the public the relative risk of outdoor activity. The scale may or may not distinguish between different pollutants.\n\nIn Canada, air pollution and associated health risks are measured with the Air Quality Health Index or (AQHI). It is a health protection tool used to make decisions to reduce short-term exposure to air pollution by adjusting activity levels during increased levels of air pollution.\n\nThe Air Quality Health Index or \"AQHI\" is a federal program jointly coordinated by Health Canada and Environment Canada. However, the AQHI program would not be possible without the commitment and support of the provinces, municipalities and NGOs. From air quality monitoring to health risk communication and community engagement, local partners are responsible for the vast majority of work related to AQHI implementation. The AQHI provides a number from 1 to 10+ to indicate the level of health risk associated with local air quality. Occasionally, when the amount of air pollution is abnormally high, the number may exceed 10. The AQHI provides a local air quality current value as well as a local air quality maximums forecast for today, tonight and tomorrow and provides associated health advice.\n\nAs it is now known that even low levels of air pollution can trigger discomfort for the sensitive population, the index has been developed as a continuum: The higher the number, the greater the health risk and need to take precautions. The index describes the level of health risk associated with this number as 'low', 'moderate', 'high' or 'very high', and suggests steps that can be taken to reduce exposure.\n\nThe measurement is based on the observed relationship of Nitrogen Dioxide (NO), ground-level Ozone (O) and particulates (PM) with mortality, from an analysis of several Canadian cities. Significantly, all three of these pollutants can pose health risks, even at low levels of exposure, especially among those with pre-existing health problems.\n\nWhen developing the AQHI, Health Canada's original analysis of health effects included five major air pollutants: particulates, ozone, and nitrogen dioxide (NO2), as well as sulfur dioxide (SO), and carbon monoxide (CO). The latter two pollutants provided little information in predicting health effects and were removed from the AQHI formulation.\n\nThe AQHI does not measure the effects of odour, pollen, dust, heat or humidity.\n\nTA Luft is the German air quality regulation.\n\nAir pollution hotspots are areas where air pollution emissions expose individuals to increased negative health effects. They are particularly common in highly populated, urban areas, where there may be a combination of stationary sources (e.g. industrial facilities) and mobile sources (e.g. cars and trucks) of pollution. Emissions from these sources can cause respiratory disease, childhood asthma, cancer, and other health problems. Fine particulate matter such as diesel soot, which contributes to more than 3.2 million premature deaths around the world each year, is a significant problem. It is very small and can lodge itself within the lungs and enter the bloodstream. Diesel soot is concentrated in densely populated areas, and one in six people in the U.S. live near a diesel pollution hot spot.\n\nWhile air pollution hotspots affect a variety of populations, some groups are more likely to be located in hotspots. Previous studies have shown disparities in exposure to pollution by race and/or income. Hazardous land uses (toxic storage and disposal facilities, manufacturing facilities, major roadways) tend to be located where property values and income levels are low. Low socioeconomic status can be a proxy for other kinds of social vulnerability, including race, a lack of ability to influence regulation and a lack of ability to move to neighborhoods with less environmental pollution. These communities bear a disproportionate burden of environmental pollution and are more likely to face health risks such as cancer or asthma.\n\nStudies show that patterns in race and income disparities not only indicate a higher exposure to pollution but also higher risk of adverse health outcomes. Communities characterized by low socioeconomic status and racial minorities can be more vulnerable to cumulative adverse health impacts resulting from elevated exposure to pollutants than more privileged communities. Blacks and Latinos generally face more pollution than whites and Asians, and low-income communities bear a higher burden of risk than affluent ones. Racial discrepancies are particularly distinct in suburban areas of the Southern United States and metropolitan areas of the Midwestern and Western United States. Residents in public housing, who are generally low-income and cannot move to healthier neighborhoods, are highly affected by nearby refineries and chemical plants.\n\nAir pollution is usually concentrated in densely populated metropolitan areas, especially in developing countries where environmental regulations are relatively lax or nonexistent. However, even populated areas in developed countries attain unhealthy levels of pollution, with Los Angeles and Rome being two examples. Between 2002 and 2011 the incidence of lung cancer in Beijing near doubled. While smoking remains the leading cause of lung cancer in China, the number of smokers is falling while lung cancer rates are rising.\n\nIn Europe, Council Directive 96/62/EC on ambient air quality assessment and management provides a common strategy against which member states can \"set objectives for ambient air quality in order to avoid, prevent or reduce harmful effects on human health and the environment ... and improve air quality where it is unsatisfactory\".\n\nOn 25 July 2008 in the case Dieter Janecek v Freistaat Bayern CURIA, the European Court of Justice ruled that under this directive citizens have the right to require national authorities to implement a short term action plan that aims to maintain or achieve compliance to air quality limit values.\n\nThis important case law appears to confirm the role of the EC as centralised regulator to European nation-states as regards air pollution control. It places a supranational legal obligation on the UK to protect its citizens from dangerous levels of air pollution, furthermore superseding national interests with those of the citizen.\n\nIn 2010, the European Commission (EC) threatened the UK with legal action against the successive breaching of PM10 limit values. The UK government has identified that if fines are imposed, they could cost the nation upwards of £300 million per year.\n\nIn March 2011, the Greater London Built-up Area remains the only UK region in breach of the EC's limit values, and has been given 3 months to implement an emergency action plan aimed at meeting the EU Air Quality Directive. The City of London has dangerous levels of PM10 concentrations, estimated to cause 3000 deaths per year within the city. As well as the threat of EU fines, in 2010 it was threatened with legal action for scrapping the western congestion charge zone, which is claimed to have led to an increase in air pollution levels.\n\nIn response to these charges, Boris Johnson, Mayor of London, has criticised the current need for European cities to communicate with Europe through their nation state's central government, arguing that in future \"A great city like London\" should be permitted to bypass its government and deal directly with the European Commission regarding its air quality action plan.\n\nThis can be interpreted as recognition that cities can transcend the traditional national government organisational hierarchy and develop solutions to air pollution using global governance networks, for example through transnational relations. Transnational relations include but are not exclusive to national governments and intergovernmental organisations, allowing sub-national actors including cities and regions to partake in air pollution control as independent actors.\n\nParticularly promising at present are global city partnerships. These can be built into networks, for example the C40 Cities Climate Leadership Group, of which London is a member. The C40 is a public 'non-state' network of the world's leading cities that aims to curb their greenhouse emissions. The C40 has been identified as 'governance from the middle' and is an alternative to intergovernmental policy. It has the potential to improve urban air quality as participating cities \"exchange information, learn from best practices and consequently mitigate carbon dioxide emissions independently from national government decisions\". A criticism of the C40 network is that its exclusive nature limits influence to participating cities and risks drawing resources away from less powerful city and regional actors.\n\nIt is expected that Africa could represent the half of the world's pollution emissions by 2030, warns Cathy Liousse director of research of atmospheric sounding of the CNRS, along with many other researchers. According the report the sub-Saharan Africa is experiencing a fast increasing pollution, derived to many causes, such as burning wood for cooking, burning open waste, traffic, agri-food and chemical industries, the sand dust from the Sahara carried by the winds through the Sahel area, all this reinforced by a greater demography and urban density.\n\n\n\n\n"}
{"id": "64959", "url": "https://en.wikipedia.org/wiki?curid=64959", "title": "Poverty", "text": "Poverty\n\nPoverty is not having enough material possessions or income for a person's needs. Poverty may include social, economic, and political elements.\n\n\"Absolute poverty\" is the complete lack of the means necessary to meet basic personal needs, such as food, clothing and shelter. The threshold at which \"absolute poverty\" is defined is always about the same, independent of the person's permanent location or era.\n\nOn the other hand, \"relative poverty\" occurs when a person cannot meet a minimum level of living standards, compared to others in the same time and place. Therefore, the threshold at which \"relative poverty\" is defined varies from one country to another, or from one society to another. For example, a person who cannot afford housing better than a small tent in an open field would be said to live in relative poverty if almost everyone else in that area lives in modern brick homes, but not if everyone else also lives in small tents in open fields (for example, in a nomadic tribe).\n\nGovernments and non-governmental organizations try to reduce poverty. Providing basic needs to people who are unable to earn a sufficient income can be hampered by constraints on government's ability to deliver services, such as corruption, tax avoidance, debt and loan conditionalities and by the brain drain of health care and educational professionals. Strategies of increasing income to make basic needs more affordable typically include welfare, economic freedoms and providing financial services.\n\nIn 2012 it was estimated that, using a poverty line of $1.25 a day, 1.2 billion people lived in poverty. Given the current economic model, built on GDP, it would take 100 years to bring the world's poorest up to the poverty line of $1.25 a day. UNICEF estimates half the world's children (or 1.1 billion) live in poverty.\n\nThe World Bank forecasted in 2015 that 702.1 million people were living in extreme poverty, down from 1.75 billion in 1990. Extreme poverty is observed in all parts of the world, including developed economies. Of the 2015 population, about 347.1 million people (35.2%) lived in Sub-Saharan Africa and 231.3 million (13.5%) lived in South Asia. According to the World Bank, between 1990 and 2015, the percentage of the world's population living in extreme poverty fell from 37.1% to 9.6%, falling below 10% for the first time. The People's Republic of China accounts for over three quarters of global poverty reduction from 1990 to 2005. Though, as noted, China accounted for nearly half of all extreme poverty in 1990. In public opinion around the world people surveyed tend to incorrectly think extreme poverty has not decreased.\n\nDuring the 2013 to 2015 period The World Bank reported that extreme poverty fell from 11% to 10%, however they also noted that the rate of decline had slowed by nearly half from the 25 year average with parts of sub-saharan Africa returning to early 2000 levels. The World Bank attributed this to increasing violence following the Arab Spring, population increases in Sub-Saharan Africa, and general African inflationary pressures and economic malaise were the primary drivers for this slow down.\n\nThere is disagreement among experts as to what would be considered a realistic poverty rate with one considering it \"an inaccurately measured and arbitrary cut off\". Some contend that a higher poverty line is needed, such as a minimum of $7.40 or even $10 to $15 a day. They argue that these levels would better reflect the cost of basic needs and normal life expectancy. One estimate places the true scale of poverty much higher than the World Bank, with an estimated 4.3 billion people (59% of the world's population) living with less than $5 a day and unable to meet basic needs adequately. It has been argued by some academics that the neoliberal policies promoted by global financial institutions such as the IMF and the World Bank are actually exacerbating both inequality and poverty.\n\nAn data based scientific empirical research, which studied the impact of dynastic politics on the level of poverty of the provinces, found a positive correlation between dynastic politics and poverty i.e. the higher proportion of dynastic politicians in power in a province leads to higher poverty rate. There is significant evidence that these political dynasties use their political dominance over their respective regions to enrich themselves, using methods such as graft or outright bribery of legislators.\n\nPoverty is the scarcity or the lack of a certain (variant) amount of material possessions or money. The word \"poverty\" comes from the old (Norman) French word \"poverté\" (Modern French: \"pauvreté),\" from Latin \"paupertās\" from \"pauper\" (poor).\n\nThere are several definitions of poverty depending on the context of the situation it is placed in, and the views of the person giving the definition.\n\nIncome Poverty: a family's income fails to meet a federally established threshold that differs across countries.\n\nUnited Nations: Fundamentally, poverty is the inability of having choices and opportunities, a violation of human dignity. It means lack of basic capacity to participate effectively in society. It means not having enough to feed and clothe a family, not having a school or clinic to go to, not having the land on which to grow one's food or a job to earn one's living, not having access to credit. It means insecurity, powerlessness and exclusion of individuals, households and communities. It means susceptibility to violence, and it often implies living in marginal or fragile environments, without access to clean water or sanitation.\n\nWorld Bank: Poverty is pronounced deprivation in well-being, and comprises many dimensions. It includes low incomes and the inability to acquire the basic goods and services necessary for survival with dignity. Poverty also encompasses low levels of health and education, poor access to clean water and sanitation, inadequate physical security, lack of voice, and insufficient capacity and opportunity to better one's life.\n\nPoverty is usually measured as either absolute or relative (the latter being actually an index of income inequality).\n\nIn the United Kingdom, the second Cameron ministry came under attack for their redefinition of poverty; poverty is no longer classified by a family's income, but as to whether a family is in work or not. Considering that two-thirds of people who found work were accepting wages that are below the living wage (according to the Joseph Rowntree Foundation) this has been criticised by anti-poverty campaigners as an unrealistic view of poverty in the United Kingdom.\n\nAbsolute poverty refers to a set standard which is consistent over time and between countries. First introduced in 1990, the dollar a day poverty line measured absolute poverty by the standards of the world's poorest countries. The World Bank defined the new international poverty line as $1.25 a day in 2008 for 2005 (equivalent to $1.00 a day in 1996 US prices). In October 2015, they reset it to $1.90 a day.\n\nAbsolute poverty, extreme poverty, or abject poverty is \"a condition characterized by severe deprivation of basic human needs, including food, safe drinking water, sanitation facilities, health, shelter, education and information. It depends not only on income but also on access to services.\" The term 'absolute poverty', when used in this fashion, is usually synonymous with 'extreme poverty': Robert McNamara, the former president of the World Bank, described absolute or extreme poverty as, \"a condition so limited by malnutrition, illiteracy, disease, squalid surroundings, high infant mortality, and low life expectancy as to be beneath any reasonable definition of human decency.\" Australia is one of the world's wealthier nations. In his article published in Australian Policy Online, Robert Tanton notes that, \"While this amount is appropriate for third world countries, in Australia, the amount required to meet these basic needs will naturally be much higher because prices of these basic necessities are higher.\"\n\nHowever, as the amount of wealth required for survival is not the same in all places and time periods, particularly in highly developed countries where few people would fall below the World Bank Group's poverty lines, countries often develop their own national poverty lines.\n\nAn absolute poverty line was calculated in Australia for the Henderson poverty inquiry in 1973. It was $62.70 a week, which was the disposable income required to support the basic needs of a family of two adults and two dependent children at the time. This poverty line has been updated regularly by the Melbourne Institute according to increases in average incomes; for a single employed person it was $391.85 per week (including housing costs) in March 2009. In Australia the OECD poverty would equate to a \"disposable income of less than $358 per week for a single adult (higher for larger households to take account of their greater costs). in 2015 Australia implemented the Individual Deprivation Measure which address gender disparities in poverty.\n\nFor a few years starting 1990, the World Bank anchored absolute poverty line as $1 per day. This was revised in 1993, and through 2005, absolute poverty was $1.08 a day for all countries on a purchasing power parity basis, after adjusting for inflation to the 1993 U.S. dollar. In 2005, after extensive studies of cost of living across the world, The World Bank raised the measure for global poverty line to reflect the observed higher cost of living. In 2015, the World Bank defines extreme poverty as living on less than US$1.90 (PPP) per day, and \"moderate poverty\" as less than $2 or $5 a day (but note that a person or family with access to subsistence resources, e.g., subsistence farmers, may have a low cash income without a correspondingly low standard of living – they are not living \"on\" their cash income but using it as a top up). It estimated that \"in 2001, 1.1 billion people had consumption levels below $1 a day and 2.7 billion lived on less than $2 a day.\" A 'dollar a day', in nations that do not use the U.S. dollar as currency, does not translate to living a day on the equivalent amount of local currency as determined by the exchange rate. Rather, it is determined by the purchasing power parity rate, which would look at how much local currency is needed to buy the same things that a dollar could buy in the United States. Usually, this would translate to less local currency than the exchange rate in poorer countries as the United States is a relatively more expensive country.\n\nThe poverty line threshold of $1.90 per day, as set by the World Bank, is controversial. Each nation has its own threshold for absolute poverty line; in the United States, for example, the absolute poverty line was US$15.15 per day in 2010 (US$22,000 per year for a family of four), while in India it was US$1.0 per day and in China the absolute poverty line was US$0.55 per day, each on PPP basis in 2010. These different poverty lines make data comparison between each nation's official reports qualitatively difficult. Some scholars argue that the World Bank method sets the bar too high, others argue it is low. Still others suggest that poverty line misleads as it measures everyone below the poverty line the same, when in reality someone living on $1.20 per day is in a different state of poverty than someone living on $0.20 per day. In other words, the depth and intensity of poverty varies across the world and in any regional populations, and $1.25 per day poverty line and head counts are inadequate measures.\n\nThe share of the world's population living in absolute poverty fell from 43% in 1981 to 14% in 2011. The absolute number of people in poverty fell from 1.95 billion in 1981 to 1.01 billion in 2011. The economist Max Roser estimates that the number of people in poverty is therefore roughly the same as 200 years ago. This is the case since the world population was just little more than 1 billion in 1820 and the majority (84% to 94%) of the world population was living poverty.\nThe proportion of the developing world's population living in extreme economic poverty fell from 28 percent in 1990 to 21 percent in 2001. Most of this improvement has occurred in East and South Asia. In East Asia the World Bank reported that \"The poverty headcount rate at the $2-a-day level is estimated to have fallen to about 27 percent [in 2007], down from 29.5 percent in 2006 and 69 percent in 1990.\" In Sub-Saharan Africa extreme poverty went up from 41 percent in 1981 to 46 percent in 2001, which combined with growing population increased the number of people living in extreme poverty from 231 million to 318 million.\n\nIn the early 1990s some of the transition economies of Central and Eastern Europe and Central Asia experienced a sharp drop in income. The collapse of the Soviet Union resulted in large declines in GDP per capita, of about 30 to 35% between 1990 and the through year of 1998 (when it was at its minimum). As a result, poverty rates tripled, excess mortality increased, and life expectancy declined. In subsequent years as per capita incomes recovered the poverty rate dropped from 31.4% of the population to 19.6%. The average post-communist country had returned to 1989 levels of per-capita GDP by 2005, although as of 2015 some are still far behind that. According to an article in \"Foreign Affairs\", there were generally three paths to economic reform taken post Soviet collapse. Those nations that took a \"radical\" or \"gradual\" reform rate have GDP per capita similar to other nations in their stage of economic development at generally 150% of their transition year (1991) GDP. Nations that took a \"slow\" approach (an approach that limited free market reforms generally) had much slower, and lower economic growth, higher Gini coefficients, and poorer health outcomes. Currently, those nations sit at 125% of their transition year GDP per capita. A 2009 study published in \"The Lancet\" suggested that radical economic changes and the resulting short term unemployment led to temporary increases in the mortality rate of adult males.\n\nWorld Bank data shows that the percentage of the population living in households with consumption or income per person below the poverty line has decreased in each region of the world since 1990:\n\nAccording to Chen and Ravallion, about 1.76 billion people in developing world lived \"above\" $1.25 per day and 1.9 billion people lived \"below\" $1.25 per day in 1981. The world's population increased over the next 25 years. In 2005, about 4.09 billion people in developing world lived above $1.25 per day and 1.4 billion people lived below $1.25 per day (both 1981 and 2005 data are on inflation adjusted basis). Some scholars caution that these trends are subject to various assumptions and not certain. Additionally, they note that the poverty reduction is not uniform across the world; economically prospering countries such as China, India and Brazil have made more progress in absolute poverty reduction than countries in other regions of the world.\n\nThe absolute poverty measure trends noted above are supported by human development indicators, which have also been improving. Life expectancy has greatly increased in the developing world since World War II and is starting to close the gap to the developed world. Child mortality has decreased in every developing region of the world. The proportion of the world's population living in countries where per-capita food supplies are less than 2,200 calories (9,200 kilojoules) per day decreased from 56% in the mid-1960s to below 10% by the 1990s. Similar trends can be observed for literacy, access to clean water and electricity and basic consumer items.\n\nRelative poverty views poverty as socially defined and dependent on social context, hence relative poverty is a measure of income inequality. Usually, relative poverty is measured as the percentage of the population with income less than some fixed proportion of median income. There are several other different income inequality metrics, for example, the Gini coefficient or the Theil Index.\n\nRelative poverty is the \"most useful measure for ascertaining poverty rates in wealthy developed nations\". Relative poverty measure is used by the United Nations Development Program (UNDP), the United Nations Children's Fund (UNICEF), the Organisation for Economic Co-operation and Development (OECD) and Canadian poverty researchers. In the European Union, the \"relative poverty measure is the most prominent and most-quoted of the EU social inclusion indicators\".\n\n\"Relative poverty reflects better the cost of social inclusion and equality of opportunity in a specific time and space.\"\n\n\"Once economic development has progressed beyond a certain minimum level, the rub of the poverty problem – from the point of view of both the poor individual and of the societies in which they live – is not so much the effects of poverty in any absolute form but the effects of the contrast, daily perceived, between the lives of the poor and the lives of those around them. For practical purposes, the problem of poverty in the industrialized nations today is a problem of relative poverty (page 9).\"\n\nIn 1776 Adam Smith in the Wealth of Nations argued that poverty is the inability to afford, \"not only the commodities which are indispensably necessary for the support of life but whatever the custom of the country renders it indecent for creditable people, even of the lowest order, to be without\".\n\nIn 1958 J. K. Galbraith argued that \"People are poverty stricken when their income, even if adequate for survival, falls markedly behind that of their community.\"\n\nIn 1964 in a joint committee economic President's report in the United States, Republicans endorsed the concept of relative poverty. \"No objective definition of poverty exists... The definition varies from place to place and time to time. In America as our standard of living rises, so does our idea of what is substandard.\"\n\nIn 1965 Rose Friedman argued for the use of relative poverty claiming that the definition of poverty changes with general living standards. Those labeled as poor in 1995 would have had \"a higher standard of living than many labeled not poor\" in 1965.\n\nIn 1979, British sociologist, Peter Townsend published his famous definition, \"individuals ... can be said to be in poverty when they lack the resources to obtain the types of diet, participate in the activities and have the living conditions and amenities which are customary, or are at least widely encouraged or approved, in the societies to which they belong (page 31)\".\n\nBrian Nolan and Christopher T. Whelan of the Economic and Social Research Institute (ESRI) in Ireland explained that \"Poverty has to be seen in terms of the standard of living of the society in question.\"\n\nRelative poverty measures are used as official poverty rates by the European Union, UNICEF, and the OECD. The main poverty line used in the OECD and the European Union is based on \"economic distance\", a level of income set at 60% of the median household income.\n\nMany wealthy nations have seen an increase in relative poverty rates ever since the Great Recession, in particular among children from impoverished families who often reside in substandard housing and find educational opportunities out of reach.\n\nSecondary poverty refers to those that earn enough income to not be impoverished, but who spend their income on unnecessary pleasures, such as alcoholic beverages, thus placing them below it in practice.\n\nIn 18th- and 19th-century Great Britain, the practice of temperance among Methodists, as well as their rejection of gambling, allowed them to eliminate secondary poverty and accumulate capital.\n\nEconomic aspects of poverty focus on material needs, typically including the necessities of daily living, such as food, clothing, shelter, or safe drinking water. Poverty in this sense may be understood as a condition in which a person or community is lacking in the basic needs for a minimum standard of well-being and life, particularly as a result of a persistent lack of income. The increase in poverty runs parallel sides with unemployment, hunger, and higher crime rate.\n\nAnalysis of social aspects of poverty links conditions of scarcity to aspects of the distribution of resources and power in a society and recognizes that poverty may be a function of the diminished \"capability\" of people to live the kinds of lives they value. The social aspects of poverty may include lack of access to information, education, health care, social capital or political power.\n\nPoverty levels are snapshot pictures in time that omits the transitional dynamics between levels. Mobility statistics supply additional information about the fraction who leave the poverty level. For example, one study finds that in a sixteen-year period (1975 to 1991 in the U.S.) only 5% of those in the lower fifth of the income level were still at that level, while 95% transitioned to a higher income category. Poverty levels can remain the same while those who rise out of poverty are replaced by others. The transient poor and chronic poor differ in each society. In a nine-year period ending in 2005 for the U.S., 50% of the poorest quintile transitioned to a higher quintile.\n\nPoverty may also be understood as an aspect of unequal social status and inequitable social relationships, experienced as social exclusion, dependency, and diminished capacity to participate, or to develop meaningful connections with other people in society. Such social exclusion can be minimized through strengthened connections with the mainstream, such as through the provision of relational care to those who are experiencing poverty.\n\nThe World Bank's \"Voices of the Poor,\" based on research with over 20,000 poor people in 23 countries, identifies a range of factors which poor people identify as part of poverty. These include:\n\nDavid Moore, in his book \"The World Bank\", argues that some analysis of poverty reflect pejorative, sometimes racial, stereotypes of impoverished people as powerless victims and passive recipients of aid programs.\n\nUltra-poverty, a term apparently coined by Michael Lipton, connotes being amongst poorest of the poor in low-income countries. Lipton defined ultra-poverty as receiving less than 80 percent of minimum caloric intake whilst spending more than 80% of income on food. Alternatively a 2007 report issued by International Food Policy Research Institute defined ultra-poverty as living on less than 54 cents per day.\n\nAsset poverty is an economic and social condition that is more persistent and prevalent than income poverty. It can be defined as a household's inability to access wealth resources that are enough to provide for basic needs for a period of three months. Basic needs refer to the minimum standards for consumption and acceptable needs.Wealth resources consist of home ownership, other real estate (second home, rented properties, etc.), net value of farm and business assets, stocks, checking and savings accounts, and other savings (money in savings bonds, life insurance policy cash values, etc.). Wealth is measured in three forms: net worth, net worth minus home equity, and liquid assets. Net worth consists of all the aspects mentioned above. Net worth minus home equity is the same except it does not include home ownership in asset calculations. Liquid assets are resources that are readily available such as cash, checking and savings accounts, stocks, and other sources of savings. There are two types of assets: tangible and intangible. Tangible assets most closely resemble liquid assets in that they include stocks, bonds, property, natural resources, and hard assets not in the form of real estate. Intangible assets are simply the access to credit, social capital, cultural capital, political capital, and human capital.\n\nPoverty rate is a calculation of the percentage of people whose family household income falls below the Poverty Line. The federal government typically regulates this line, and typically the cost of the line is set to three times the cost an adequate meal. This line is subject to change based on the level of price changes and size of the family.\n\nAccording to census, poverty rate helped determine that around 14.8% of U.S population had household incomes that was below the poverty line for their family size in 2014.\n\nThe effects of poverty may also be causes as listed above, thus creating a \"poverty cycle\" operating across multiple levels, individual, local, national and global.\n\nOne third of deaths around the world – some 18 million people a year or 50,000 per day – are due to poverty-related causes. People living in developing nations, among them women and children, are over represented among the global poor and these effects of severe poverty. Those living in poverty suffer disproportionately from hunger or even starvation and disease, as well as lower life expectancy. According to the World Health Organization, hunger and malnutrition are the single gravest threats to the world's public health and malnutrition is by far the biggest contributor to child mortality, present in half of all cases.\n\nAlmost 90% of maternal deaths during childbirth occur in Asia and sub-Saharan Africa, compared to less than 1% in the developed world. Those who live in poverty have also been shown to have a far greater likelihood of having or incurring a disability within their lifetime. Infectious diseases such as malaria and tuberculosis can perpetuate poverty by diverting health and economic resources from investment and productivity; malaria decreases GDP growth by up to 1.3% in some developing nations and AIDS decreases African growth by 0.3–1.5% annually.\n\nPoverty has been shown to impede cognitive function. One way in which this may happen is that financial worries put a severe burden on one's mental resources so that they are no longer fully available for solving complicated problems. The reduced capability for problem solving can lead to suboptimal decisions and further perpetuate poverty. Many other pathways from poverty to compromised cognitive capacities have been noted, from poor nutrition and environmental toxins to the effects of stress on parenting behavior, all of which lead to suboptimal psychological development. Neuroscientists have documented the impact of poverty on brain structure and function throughout the lifespan.\n\nInfectious diseases continue to blight the lives of the poor across the world. An estimated 40 million people are living with HIV/AIDS, with 3 million deaths in 2004. Every year there are 350–500 million cases of malaria, with 1 million fatalities: Africa accounts for 90 percent of malarial deaths and African children account for over 80 percent of malaria victims worldwide.\n\nPoor people often are more prone to severe diseases due to the lack of health care, and due to living in non-optimal conditions. Among the poor, girls tend to suffer even more due to gender discrimination. Economic stability is paramount in a poor household otherwise they go in a endless loop of negative income trying to treat diseases. Often time when a person in a poor household falls ill it is up to the family members to take care of their family members due to limited access to health care and lack of health insurance. The household members oftentimes have to give up their income or stop seeking further education to tend to the sick member. There is a greater opportunity cost imposed on the poor to tend to someone compared to someone with better financial stability.\n\nRises in the costs of living make poor people less able to afford items. Poor people spend a greater portion of their budgets on food than wealthy people. As a result, poor households and those near the poverty threshold can be particularly vulnerable to increases in food prices. For example, in late 2007 increases in the price of grains led to food riots in some countries. The World Bank warned that 100 million people were at risk of sinking deeper into poverty. Threats to the supply of food may also be caused by drought and the water crisis. Intensive farming often leads to a vicious cycle of exhaustion of soil fertility and decline of agricultural yields. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to United Nations University's Ghana-based Institute for Natural Resources in Africa. Every year nearly 11 million children living in poverty die before their fifth birthday. 1.02 billion people go to bed hungry every night.\n\nAccording to the Global Hunger Index, Sub-Saharan Africa had the highest child malnutrition rate of the world's regions over the 2001–2006 period.\n\nThe Associated Press reports that people gather every evening in downtown Caracas in search of food thrown out on sidewalks due to 90% of Venezuela's population living in poverty.\n\nAs part of the Sustainable Development Goals the global community has made the elimination of hunger and undernutrition a priority for the coming years. While the Goal 2 of the SDGs aims to reach this goal by 2030 a number of initiatives aim to achieve the goal 5 years earlier, by 2025:\n\nResearch has found that there is a high risk of educational underachievement for children who are from low-income housing circumstances. This is often a process that begins in primary school for some less fortunate children. Instruction in the US educational system, as well as in most other countries, tends to be geared towards those students who come from more advantaged backgrounds. As a result, children in poverty are at a higher risk than advantaged children for retention in their grade, special deleterious placements during the school's hours and even not completing their high school education. Advantage breeds advantage. There are indeed many explanations for why students tend to drop out of school. One is the conditions of which they attend school. Schools in poverty-stricken areas have conditions that hinder children from learning in a safe environment. Researchers have developed a name for areas like this: an urban war zone is a poor, crime-laden district in which deteriorated, violent, even war-like conditions and underfunded, largely ineffective schools promote inferior academic performance, including irregular attendance and disruptive or non-compliant classroom behavior. Because of poverty, \"Students from low-income families are 2.4 times more likely to drop out than middle-income kids, and over 10 times more likely than high-income peers to drop out\"\n\nFor children with low resources, the risk factors are similar to others such as juvenile delinquency rates, higher levels of teenage pregnancy, and the economic dependency upon their low-income parent or parents.\nFamilies and society who submit low levels of investment in the education and development of less fortunate children end up with less favorable results for the children who see a life of parental employment reduction and low wages. Higher rates of early childbearing with all the connected risks to family, health and well-being are major important issues to address since education from preschool to high school are both identifiably meaningful in a life.\n\nPoverty often drastically affects children's success in school. A child's \"home activities, preferences, mannerisms\" must align with the world and in the cases that they do not do these, students are at a disadvantage in the school and, most importantly, the classroom. Therefore, it is safe to state that children who live at or below the poverty level will have far less success educationally than children who live above the poverty line. Poor children have a great deal less healthcare and this ultimately results in many absences from the academic year. Additionally, poor children are much more likely to suffer from hunger, fatigue, irritability, headaches, ear infections, flu, and colds. These illnesses could potentially restrict a child or student's focus and concentration.\n\nFor a child to grow up emotionally healthy, the children under three need \"A strong, reliable primary caregiver who provides consistent and unconditional love, guidance, and support. Safe, predictable, stable environments. Ten to 20 hours each week of harmonious, reciprocal interactions. This process, known as attunement, is most crucial during the first 6–24 months of infants' lives and helps them develop a wider range of healthy emotions, including gratitude, forgiveness, and empathy. Enrichment through personalized, increasingly complex activities\".\n\nHarmful spending habits mean that the poor typically spend about 2 percent of their income educating their children but larger percentages of alcohol and tobacco (For example, 6 percent in Indonesia and 8 percent in Mexico).\n\nPoverty increases the risk of homelessness. Slum-dwellers, who make up a third of the world's urban population, live in a poverty no better, if not worse, than rural people, who are the traditional focus of the poverty in the developing world, according to a report by the United Nations.\n\nThere are over 100 million street children worldwide. Most of the children living in institutions around the world have a surviving parent or close relative, and they most commonly entered orphanages because of poverty. It is speculated that, flush with money, orphanages are increasing and push for children to join even though demographic data show that even the poorest extended families usually take in children whose parents have died. Experts and child advocates maintain that orphanages are expensive and often harm children's development by separating them from their families and that it would be more effective and cheaper to aid close relatives who want to take in the orphans.\n\nAs of 2012, 2.5 billion people lack access to sanitation services and 15% practice open defecation. The most noteworthy example is Bangladesh, which has half the GDP per capita of India but has a lower mortality from diarrhea than India or the world average, with diarrhea deaths declining by 90% since the 1990s. Even while providing latrines is a challenge, people still do not use them even when available. By strategically providing pit latrines to the poorest, charities in Bangladesh sparked a cultural change as those better off perceived it as an issue of status to not use one. The vast majority of the latrines built were then not from charities but by villagers themselves.\n\nWater utility subsidies tend to subsidize water consumption by those connected to the supply grid, which is typically skewed towards the richer and urban segment of the population and those outside informal housing. As a result of heavy consumption subsidies, the price of water decreases to the extent that only 30%, on average, of the supplying costs in developing countries is covered.\nThis results in a lack of incentive to maintain delivery systems, leading to losses from leaks annually that are enough for 200 million people.\nThis also leads to a lack of incentive to invest in expanding the network, resulting in much of the poor population being unconnected to the network. Instead, the poor buy water from water vendors for, on average, about five to 16 times the metered price. However, subsidies for laying new connections to the network rather than for consumption have shown more promise for the poor.\n\nSimilarly, the poorest fifth receive 0.1% of the world's lighting but pay a fifth of total spending on light, accounting for 25 to 30 percent of their income. Indoor air pollution from burning fuels kills 2 million, with almost half the deaths from pneumonia in children under 5. Fuel from Bamboo burns more cleanly and also matures much faster than wood, thus also reducing deforestation. Additionally, using solar panels is promoted as being cheaper over the products' lifetime even if upfront costs are higher. Thus, payment schemes such as lend-to-own programs are promoted and up to 14% of Kenyan households use solar as their primary energy source.\n\nAccording to experts, many women become victims of trafficking, the most common form of which is prostitution, as a means of survival and economic desperation. Deterioration of living conditions can often compel children to abandon school to contribute to the family income, putting them at risk of being exploited. For example, in Zimbabwe, a number of girls are turning to sex in return for food to survive because of the increasing poverty. According to studies, as poverty decreases there will be fewer and fewer instances of violence.\n\nIn one survey, 67% of children from disadvantaged inner cities said they had witnessed a serious assault, and 33% reported witnessing a homicide. 51% of fifth graders from New Orleans (median income for a household: $27,133) have been found to be victims of violence, compared to 32% in Washington, DC (mean income for a household: $40,127).\n\nMax Weber and some schools of modernization theory suggest that cultural values could affect economic success. However, researchers have gathered evidence that suggest that values are not as deeply ingrained and that changing economic opportunities explain most of the movement into and out of poverty, as opposed to shifts in values. Studies have shown that poverty changes the personalities of children who live in it. The Great Smoky Mountains Study was a ten-year study that was able to demonstrate this. During the study, about one-quarter of the families saw a dramatic and unexpected increase in income. The study showed that among these children, instances of behavioral and emotional disorders decreased, and conscientiousness and agreeableness increased.\n\nOne 2012 paper, based on a sampling of 9,646 U.S, adults, claimed that poverty tends to correlate with laziness and other such traits. A 2018 report on poverty in the United States by UN special rapporteur Philip Alston asserts that caricatured narratives about the rich and the poor, that \"the rich are industrious, entrepreneurial, patriotic and the drivers of economic success. The poor are wasters, losers and scammers\" are largely inaccurate, as \"the poor are overwhelmingly those born into poverty, or those thrust there by circumstances largely beyond their control, such as physical or mental disabilities, divorce, family breakdown, illness, old age, unliveable wages or discrimination in the job market.\"\n\nA psychological study has been conducted by four scientists during inagural Convention of Psychological Science. The results find that people who thrive with financial stability or fall under low socioeconomic status (SES), tend to perform worse cognitively due to external pressure imposed upon them. The research found that stressors such as low income, inadequate health care, discrimination, exposure to criminal activities all contribute to mental disorders. This study also found that it slows cognitive thinking in children when they are exposed to poverty stricken environments. In kids it is seen that kids perform better under the care and nourishment from their parents, and found that children tend to adopt speaking language at a younger age. Since being in poverty from childhood is especially more harmful than it is for an adult, therefore it is seen that children in poor households tend to fall behind in certain cognitive abilities compared to other average families.\n\nCultural factors, such as discrimination of various kinds, can negatively affect productivity such as age discrimination, stereotyping, discrimination against people with physical disability, gender discrimination, racial discrimination, and caste discrimination. Women are the group suffering from the highest rate of poverty after children; 14.5% of women and 22% of children are poor in the United States. In addition, the fact that women are more likely to be caregivers, regardless of income level, to either the generations before or after them, exacerbates the burdens of their poverty.\nMarking the International Day for the Eradication of Poverty, the United Nations Special Rapporteur on extreme poverty Philip Alston warned in a statement that, “The world’s poor are at disproportionate risk of torture, arrest, early death and domestic violence, but their civil and political rights are being airbrushed out of the picture.” ... people in lower socio-economic classes are much more likely to get killed, tortured or experience an invasion of their privacy, and are far less likely to realize their right to vote, or otherwise participate in the political process.”\n\nCauses of poverty is a highly ideologically charged subject, as different causes point to different remedies. Very broadly speaking, the socialist tradition locates the roots of poverty in problems of distribution and the use of the means of production as capital benefiting individuals, and calls for re-distribution of wealth as the solution, whereas the neoliberal school of thought is dedicated the idea that creating conditions for profitable private investment is the solutions. Neoliberal think tanks have received extensive funding, and the ability to apply many of their ideas in highly indebted countries in the global South as a condition for receiving emergency loans from the International Monetary Fund.\n\nVarious poverty reduction strategies are broadly categorized based on whether they make more of the basic human needs available or whether they increase the disposable income needed to purchase those needs. Some strategies such as building roads can both bring access to various basic needs, such as fertilizer or healthcare from urban areas, as well as increase incomes, by bringing better access to urban markets. Statistics of 2018 shows population living in extreme conditions has declined by more than 1 billion in the last 25 years. As per the report published by the world bank on 19 September 2018 world poverty falls below 750 million.\n\nAgricultural technologies such as nitrogen fertilizers, pesticides, new seed varieties and new irrigation methods have dramatically reduced food shortages in modern times by boosting yields past previous constraints.\n\nBefore the Industrial Revolution, poverty had been mostly accepted as inevitable as economies produced little, making wealth scarce. Geoffrey Parker wrote that \"In Antwerp and Lyon, two of the largest cities in western Europe, by 1600 three-quarters of the total population were too poor to pay taxes, and therefore likely to need relief in times of crisis.\" The initial industrial revolution led to high economic growth and eliminated mass absolute poverty in what is now considered the developed world. Mass production of goods in places such as rapidly industrializing China has made what were once considered luxuries, such as vehicles and computers, inexpensive and thus accessible to many who were otherwise too poor to afford them.\n\nEven with new products, such as better seeds, or greater volumes of them, such as industrial production, the poor still require access to these products. Improving road and transportation infrastructure helps solve this major bottleneck. In Africa, it costs more to move fertilizer from an African seaport 60 miles inland than to ship it from the United States to Africa because of sparse, low-quality roads, leading to fertilizer costs two to six times the world average. Microfranchising models such as door to door distributors who earn commission-based income or Coca-Cola's successful distribution system are used to disseminate basic needs to remote areas for below market prices.\n\nNations do not necessarily need wealth to gain health. For example, Sri Lanka had a maternal mortality rate of 2% in the 1930s, higher than any nation today. It reduced it to 0.5–0.6% in the 1950s and to 0.6% today while spending less each year on maternal health because it learned what worked and what did not. Knowledge on the cost effectiveness of healthcare interventions can be elusive and educational measures have been made to disseminate what works, such as the Copenhagen Consensus. Cheap water filters and promoting hand washing are some of the most cost effective health interventions and can cut deaths from diarrhea and pneumonia.\n\nStrategies to provide education cost effectively include deworming children, which costs about 50 cents per child per year and reduces non-attendance from anemia, illness and malnutrition, while being only a twenty-fifth as expensive as increasing school attendance by constructing schools. Schoolgirl absenteeism could be cut in half by simply providing free sanitary towels. Fortification with micronutrients was ranked the most cost effective aid strategy by the Copenhagen Consensus. For example, iodised salt costs 2 to 3 cents per person a year while even moderate iodine deficiency in pregnancy shaves off 10 to 15 IQ points. Paying for school meals is argued to be an efficient strategy in increasing school enrollment, reducing absenteeism and increasing student attention.\n\nDesirable actions such as enrolling children in school or receiving vaccinations can be encouraged by a form of aid known as conditional cash transfers. In Mexico, for example, dropout rates of 16- to 19-year-olds in rural area dropped by 20% and children gained half an inch in height. Initial fears that the program would encourage families to stay at home rather than work to collect benefits have proven to be unfounded. Instead, there is less excuse for neglectful behavior as, for example, children stopped begging on the streets instead of going to school because it could result in suspension from the program.\n\nGovernment revenue can be diverted away from basic services by corruption. Funds from aid and natural resources are often sent by government individuals for money laundering to overseas banks which insist on bank secrecy, instead of spending on the poor. A Global Witness report asked for more action from Western banks as they have proved capable of stanching the flow of funds linked to terrorism.\n\nIllicit capital flight from the developing world is estimated at ten times the size of aid it receives and twice the debt service it pays, with one estimate that most of Africa would be developed if the taxes owed were paid. About 60 per cent of illicit capital flight from Africa is from transfer mispricing, where a subsidiary in a developing nation sells to another subsidiary or shell company in a tax haven at an artificially low price to pay less tax. An African Union report estimates that about 30% of sub-Saharan Africa's GDP has been moved to tax havens. Solutions include corporate \"country-by-country reporting\" where corporations disclose activities in each country and thereby prohibit the use of tax havens where no effective economic activity occurs.\n\nDeveloping countries' debt service to banks and governments from richer countries can constrain government spending on the poor. For example, Zambia spent 40% of its total budget to repay foreign debt, and only 7% for basic state services in 1997. One of the proposed ways to help poor countries has been debt relief. Zambia began offering services, such as free health care even while overwhelming the health care infrastructure, because of savings that resulted from a 2005 round of debt relief.\n\nThe World Bank and the International Monetary Fund, as primary holders of developing countries' debt, attach structural adjustment conditionalities in return for loans which are generally geared toward loan repayment with austerity measures such as the elimination of state subsidies and the privatization of state services. For example, the World Bank presses poor nations to eliminate subsidies for fertilizer even while many farmers cannot afford them at market prices. In Malawi, almost five million of its 13 million people used to need emergency food aid but after the government changed policy and subsidies for fertilizer and seed were introduced, farmers produced record-breaking corn harvests in 2006 and 2007 as Malawi became a major food exporter. A major proportion of aid from donor nations is tied, mandating that a receiving nation spend on products and expertise originating only from the donor country. US law requires food aid be spent on buying food at home, instead of where the hungry live, and, as a result, half of what is spent is used on transport.\n\nDistressed securities funds, also known as \"vulture funds\", buy up the debt of poor nations cheaply and then sue countries for the full value of the debt plus interest which can be ten or 100 times what they paid. They may pursue any companies which do business with their target country to force them to pay to the fund instead. Considerable resources are diverted on costly court cases. For example, a court in Jersey ordered the Democratic Republic of the Congo to pay an American speculator $100 million in 2010. Now, the UK, Isle of Man and Jersey have banned such payments.\n\nThe loss of basic needs providers emigrating from impoverished countries has a damaging effect. As of 2004, there were more Ethiopia-trained doctors living in Chicago than in Ethiopia. Proposals to mitigate the problem include compulsory government service for graduates of public medical and nursing schools and promoting medical tourism so that health care personnel have more incentive to practice in their home countries. It is very easy for Ugandan doctors to emigrate to other countries. It is seen that only 69 percent of the health care jobs were filled in Uganda. Other Ugandan doctors were seeking jobs in other countries leaving inadequate or less skilled doctors to stay in Uganda.\n\nSome argue that overpopulation and lack of access to birth control can lead to population increase to exceed food production and other resources. Better education for both men and women, and more control of their lives, reduces population growth due to family planning. According to United Nations Population Fund (UNFPA), by giving better education to men and women, they can earn money for their lives and can help them to strengthen economic security.\n\nThe following are strategies used or proposed to increase personal incomes among the poor. Raising farm incomes is described as the core of the antipoverty effort as three-quarters of the poor today are farmers. Estimates show that growth in the agricultural productivity of small farmers is, on average, at least twice as effective in benefiting the poorest half of a country's population as growth generated in nonagricultural sectors.\n\nA guaranteed minimum income ensures that every citizen will be able to purchase a desired level of basic needs. A basic income (or negative income tax) is a system of social security, that periodically provides each citizen, rich or poor, with a sum of money that is sufficient to live on. Studies of large cash-transfer programs in Ethiopia, Kenya, and Malawi show that the programs can be effective in increasing consumption, schooling, and nutrition, whether they are tied to such conditions or not. Proponents argue that a basic income is more economically efficient than a minimum wage and unemployment benefits, as the minimum wage effectively imposes a high marginal tax on employers, causing losses in efficiency. In 1968, Paul Samuelson, John Kenneth Galbraith and another 1,200 economists signed a document calling for the US Congress to introduce a system of income guarantees. Winners of the Nobel Prize in Economics, with often diverse political convictions, who support a basic income include Herbert A. Simon, Friedrich Hayek, Robert Solow, Milton Friedman, Jan Tinbergen, James Tobin\nand James Meade.\n\nIncome grants are argued to be vastly more efficient in extending basic needs to the poor than subsidizing supplies whose effectiveness in poverty alleviation is diluted by the non-poor who enjoy the same subsidized prices. With cars and other appliances, the wealthiest 20% of Egypt uses about 93% of the country's fuel subsidies. In some countries, fuel subsidies are a larger part of the budget than health and education. A 2008 study concluded that the money spent on in-kind transfers in India in a year could lift all India's poor out of poverty for that year if transferred directly. The primary obstacle argued against direct cash transfers is the impractically for poor countries of such large and direct transfers. In practice, payments determined by complex iris scanning are used by war-torn Democratic Republic of Congo and Afghanistan, while India is phasing out its fuel subsidies in favor of direct transfers. Additionally, in aid models, the famine relief model increasingly used by aid groups calls for giving cash or cash vouchers to the hungry to pay local farmers instead of buying food from donor countries, often required by law, as it wastes money on transport costs.\n\nCorruption often leads to many civil services being treated by governments as employment agencies to loyal supporters and so it could mean going through 20 procedures, paying $2,696 in fees, and waiting 82 business days to start a business in Bolivia, while in Canada it takes two days, two registration procedures, and $280 to do the same. Such costly barriers favor big firms at the expense of small enterprises, where most jobs are created. Often, businesses have to bribe government officials even for routine activities, which is, in effect, a tax on business. Noted reductions in poverty in recent decades has occurred in China and India mostly as a result of the abandonment of collective farming in China and the ending of the central planning model known as the License Raj in India.\n\nThe World Bank concludes that governments and feudal elites extending to the poor the right to the land that they live and use are 'the key to reducing poverty' citing that land rights greatly increase poor people's wealth, in some cases doubling it. Although approaches varied, the World Bank said the key issues were security of tenure and ensuring land transactions costs were low.\n\nGreater access to markets brings more income to the poor. Road infrastructure has a direct impact on poverty. Additionally, migration from poorer countries resulted in $328 billion sent from richer to poorer countries in 2010, more than double the $120 billion in official aid flows from OECD members. In 2011, India got $52 billion from its diaspora, more than it took in foreign direct investment.\n\nMicroloans, made famous by the Grameen Bank, is where small amounts of money are loaned to farmers or villages, mostly women, who can then obtain physical capital to increase their economic rewards. However, microlending has been criticized for making hyperprofits off the poor even from its founder, Muhammad Yunus, and in India, Arundhati Roy asserts that some 250,000 debt-ridden farmers have been driven to suicide.\n\nThere has been significant rise of agriculture and small business that started booming up after the introduction of Grameen Banking especially in the rural areas. While making the foundation of this loan, from 1984 to 1989 loan recovery decreased from 99.4 percent to 96.9 percent.\n\nThose in poverty place overwhelming importance on having a safe place to save money, much more so than receiving loans. Additionally, a large part of microfinance loans are spent not on investments but on products that would usually be paid by a checking or savings account. Microsavings are designs to make savings products available for the poor, who make small deposits. Mobile banking utilizes the wide availability of mobile phones to address the problem of the heavy regulation and costly maintenance of saving accounts. This usually involves a network of agents of mostly shopkeepers, instead of bank branches, would take deposits in cash and translate these onto a virtual account on customers' phones. Cash transfers can be done between phones and issued back in cash with a small commission, making remittances safer.\n\nIn USA there are abundance of payday loan companies that strive to serve those people whose credit scores are much below the standard credit score in order to take an eligible loan. While these companies initially gives money to people who would likely default from traditional loans, many still frown on these loan services due to charging astonishingly high interest rate.\n\nPoverty can also be reduced as an improved economic policy is developed by the governing authorities to facilitate a more equitable distribution of the nation's wealth. Oxfam has called for an international movement to end extreme wealth concentration as a significant step towards ameliorating global poverty. The group stated that the $240 billion added to the fortunes of the world's richest billionaires in 2012 was enough to end extreme poverty four times over. Oxfam argues that the \"concentration of resources in the hands of the top 1% depresses economic activity and makes life harder for everyone else – particularly those at the bottom of the economic ladder.\" It has been reported that only 1% of the world population controls 50% of the wealth today, and the other 99% is having access to the remaining 50% only, and the gap has sharply increased in the recent past. In 2018, Oxfam reported that the gains of the world's billionaires in 2017, which amounted to $762 billion, was enough to end extreme global poverty seven times over.\nJosé Antonio Ocampo, professor at Columbia University and former finance minister of Colombia, and Magdalena Sepúlveda Carmona, former UN Special Rapporteur on Extreme Poverty and Human Rights, argue that global tax reform is integral to human development and fighting poverty, as corporate tax avoidance has disproportionately impacted those mired in poverty, noting that \"the human impact is devastatingly real. When profits are shifted out, the tax revenues from those profits that could be available to fund healthcare, schools, water sanitation and other public goods vanish from the ledger, leaving women and men, boys and girls without pathways to a better future.\"\n\nRaghuram G. Rajan, former governor of the Reserve Bank of India, former chief economist at the International Monetary Fund and professor of finance at the University of Chicago Booth School of Business has blamed the ever-widening gulf between the rich and the poor especially in the US to be one of the main \"Fault Lines\" which caused the financial institutions to pump money into subprime mortgages – on political behest, as a palliative and not a remedy, for poverty – causing the financial crisis of 2007–2009. In Rajan's view the main cause of increasing gap between the high income and low income earners, was lack of equal access to high class education for the latter.\n\nThe existence of inequality is in part due to a set of self-reinforcing behaviors that all together constitute one aspect of the cycle of poverty. These behaviors, in addition to unfavorable, external circumstances, also explain the existence of the Matthew effect, which not only exacerbates existing inequality, but is more likely to make it multigenerational. Widespread, multigenerational poverty is an important contributor to civil unrest and political instability.\n\nThe concept of business serving the world's poorest four billion or so people has been popular since CK Prahalad introduced the idea through his book \"Fortune at the Bottom of the Pyramid: Eradicating Poverty Through Profits\" in 2004, among many business corporations and business schools. Kash Rangan, John Quelch, and other faculty members at the Global Poverty Project at Harvard Business School \"believe that in pursuing its own self-interest in opening and expanding the BoP market, business can make a profit while serving the poorest of consumers and contributing to development.\" According to Rangan \"For business, the bulk of emerging markets worldwide is at the bottom of the pyramid so it makes good business sense – not a sense of do-gooding – to go after it.\".\n\nIn their 2013 book, \"The Business Solution to Poverty,\" Paul Polak and Mal Warwick directly addressed the criticism leveled against Prahalad's concept. They noted that big business often failed to create products that actually met the needs and desires of the customers who lived at the bottom-of-the-pyramid. Their answer was that a business that wanted to succeed in that market had to spend time talking to and understanding those customers. Polak had previously promoted this approach in his previous book, \"Out of Poverty,\" that described the work of International Development Enterprises (iDE), which he had formed in 1982. Polak and Warwick provided practical advice: a product needed to affect at least a billion people (i.e., have universal appeal), it had to be able to be delivered to customers living where there was not a FedEx office or even a road, and it had to be \"radically affordable\" to attract someone who earned less than $2 a day.\n\nRather than encouraging multinational businesses to meet the needs of the poor, some organizations such as iDE, the World Resources Institute, and the United Nations Development Programme began to focus on working directly with helping bottom-of-the-pyramid populations become local, small-scale entrepreneurs. Since so much of this population is engaged in agriculture, these NGOs have addressed market gaps that enable small-scale (i.e., plots less than 2 hectares) farmers to increase their production and find markets for their harvests. This is done by increasing the availability of farming equipment (e.g., pumps, tillers, seeders) and better quality seed and fertilizer, as well as expanding access for training in farming best practices (e.g., crop rotation).\n\nCreating entrepreneurs through microfinance can produce unintended outcomes: Some entrepreneurial borrowers become informal intermediaries between microfinance initiatives and poorer micro-entrepreneurs. Those who more easily qualify for microfinance split loans into smaller credit to even poorer borrowers. Informal intermediation ranges from casual intermediaries at the good or benign end of the spectrum to 'loan sharks' at the professional and sometimes criminal end of the spectrum.\n\nMilton Friedman argues that \"the social responsibility of business is to increase its profits\" only, thus, it needs to be examined whether business in BoP markets is capable of achieving the dual objective of \"making a profit while serving the poorest of consumers and contributing to development\"? Erik Simanis has reported that the model has a fatal flaw. According to Erik \"Despite achieving healthy penetration rates of 5% to 10% in four test markets, for instance, Procter & Gamble couldn't generate a competitive return on its Pur water-purification powder after launching the product on a large scale in 2001...DuPont ran into similar problems with a venture piloted from 2006 to 2008 in Andhra Pradesh, India, by its subsidiary Solae, a global manufacturer of soy protein ... Because the high costs of doing business among the very poor demand a high contribution per transaction, companies must embrace the reality that high margins and price points aren't just a top-of-the-pyramid phenomenon; they're also a necessity for ensuring sustainable businesses at the bottom of the pyramid.\" Marc Gunther states that \"The bottom-of-the-pyramid (BOP) market leader, arguably, is Unilever ... Its signature BOP product is Pureit, a countertop water-purification system sold in India, Africa and Latin America. It's saving lives, but it's not making money for shareholders.\" This leaves the ideal of \"eradicating poverty through profits\" or with a \"good business sense – not a sense of do-gooding\" rather questionable.\n\nOthers have noted that relying on BoP consumers to choose to purchase items that increase their incomes is naive. Poor consumers may spend their income disproportionately on events or goods and services that offer short-term benefits rather than invest in things that could change their lives in the long-term.\n\nA report published in 2013 by the World Bank, with support from the Climate & Development Knowledge Network, found that climate change was likely to hinder future attempts to reduce poverty. The report presented the likely impacts of present day, 2 °C and 4 °C warming on agricultural production, water resources, coastal ecosystems and cities across Sub-Saharan Africa, South Asia and South East Asia. The impacts of a temperature rise of 2 °C included: regular food shortages in Sub-Saharan Africa; shifting rain patterns in South Asia leaving some parts under water and others without enough water for power generation, irrigation or drinking; degradation and loss of reefs in South East Asia, resulting in reduced fish stocks; and coastal communities and cities more vulnerable to increasingly violent storms. In 2016, a UN report claimed that by 2030, an additional 122 million more people could be driven to extreme poverty because of climate change.\n\nMany think that poverty is the cause of environmental degradation, while there are others who claim that rather the poor are the worst sufferers of environmental degradation caused by reckless exploitation of natural resources by the rich. A Delhi-based environment organisation, the Centre for Science and Environment, points out that if the poor world were to develop and consume in the same manner as the West to achieve the same living standards, \"we would need two additional planet Earths to produce resources and absorb wastes.\", reports Anup Shah (2003). in his article \"Poverty and the Environment\" on Global Issues.\n\nAmong some individuals, poverty is considered a necessary or desirable condition, which must be embraced to reach certain spiritual, moral, or intellectual states. Poverty is often understood to be an essential element of renunciation in religions such as Buddhism, Hinduism (only for monks, not for lay persons) and Jainism, whilst in Roman Catholicism it is one of the evangelical counsels. The main aim of giving up things of the materialistic world is to withdraw oneself from sensual pleasures (as they are considered illusionary and only temporary in some religions – such as the concept of dunya in Islam). This self-invited poverty (or giving up pleasures) is different from the one caused by economic imbalance.\n\nSome Christian communities, such as the Simple Way, the Bruderhof, and the Amish value voluntary poverty; some even take a vow of poverty, similar to that of the traditional Catholic orders, in order to live a more complete life of discipleship.\n\nBenedict XVI distinguished \"poverty \"chosen\"\" (the poverty of spirit proposed by Jesus), and \"poverty \"to be fought\"\" (unjust and imposed poverty). He considered that the moderation implied in the former favors solidarity, and is a necessary condition so as to fight effectively to eradicate the abuse of the latter.\n\nAs it was indicated above the reduction of poverty results from religion, but also can result from solidarity.\n\n"}
{"id": "29501", "url": "https://en.wikipedia.org/wiki?curid=29501", "title": "Sustainable development", "text": "Sustainable development\n\nSustainable development is the organizing principle for meeting human development goals while simultaneously sustaining the ability of natural systems to provide the natural resources and ecosystem services based upon which the economy and society depend. The desired result is a state of society where living conditions and resources are used to continue to meet human needs without undermining the integrity and stability of the natural system. Sustainable development can be defined as development that meets the needs of the present without compromising the ability of future generations to meet their own needs.\nWhile the modern concept of sustainable development is derived mostly from the 1987 Brundtland Report, it is also rooted in earlier ideas about sustainable forest management and twentieth-century environmental concerns. As the concept developed, it has shifted its focus more towards the economic development, social development and environmental protection for future generations. It has been suggested that \"the term 'sustainability' should be viewed as humanity's target goal of human-ecosystem equilibrium, while 'sustainable development' refers to the holistic approach and temporal processes that lead us to the end point of sustainability\". Modern economies are endeavoring to reconcile ambitious economic development and obligations of preserving natural resources and ecosystems, as the two are usually seen as of conflicting nature. Instead of holding climate change commitments and other sustainability measures as a remedy to economic development, turning and leveraging them into market opportunities will do greater good. The economic development brought by such organized principles and practices in an economy is called Managed Sustainable Development (MSD).\n\nThe concept of sustainable development has been, and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as a sustainable use of a non-renewable resource, since any positive rate of exploitation will eventually lead to the exhaustion of earth's finite stock; this perspective renders the Industrial Revolution as a whole unsustainable. It has also been argued that the meaning of the concept has opportunistically been stretched from 'conservation management' to 'economic development', and that the Brundtland Report promoted nothing but a business as usual strategy for world development, with an ambiguous and insubstantial concept attached as a public relations slogan (see below). \n\nSustainability can be defined as the practice of maintaining world processes of productivity indefinitely—natural or human-made—by replacing resources used with resources of equal or greater value without degrading or endangering natural biotic systems. Sustainable development ties together concern for the carrying capacity of natural systems with the social, political, and economic challenges faced by humanity. Sustainability Science is the study of the concepts of sustainable development and environmental science. There is an additional focus on the present generations' responsibility to regenerate, maintain and improve planetary resources for use by future generations.\n\nSustainable development has its roots in ideas about sustainable forest management which were developed in Europe during the 17th and 18th centuries. In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued that \"sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over-exploitation of natural resources\" in his 1662 essay \"Sylva\". In 1713 Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published \"Sylvicultura economics\", a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield. His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, eventually leading to the development of a science of forestry. This, in turn, influenced people like Gifford Pinchot, first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.\n\nFollowing the publication of Rachel Carson's \"Silent Spring\" in 1962, the developing environmental movement drew attention to the relationship between economic growth and development and environmental degradation. Kenneth E. Boulding in his influential 1966 essay \"The Economics of the Coming Spaceship Earth\" identified the need for the economic system to fit itself to the ecological system with its limited pools of resources. One of the first uses of the term sustainable in the contemporary sense was by the Club of Rome in 1972 in its classic report on the \"Limits to Growth\", written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable \"state of global equilibrium\", the authors wrote: \"We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people.\"\n\nFollowing the Club of Rome report, an MIT research group prepared ten days of hearings on \"Growth and Its Implication for the Future\" (Roundtable Press, 1973) for the US Congress, the first hearings ever held on sustainable development. William Flynn Martin, David Dodson Gray, and Elizabeth Gray prepared the hearings under the Chairmanship of Congressman John Dingell.\n\nIn 1980 the International Union for the Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority and introduced the term \"sustainable development\". Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged. In 1987 the United Nations World Commission on Environment and Development released the report \"Our Common Future\", commonly called the Brundtland Report. The report included what is now one of the most widely recognised definitions of sustainable development.\n\nSince the Brundtland Report, the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of \"socially inclusive and environmentally sustainable economic growth\". In 1992, the UN Conference on Environment and Development published the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognises these interdependent pillars. It emphasises that in sustainable development everyone is a user and provider of information. It stresses the need to change from old sector-centered ways of doing business to new approaches that involve cross-sectoral co-ordination and the integration of environmental and social concerns into all development processes. Furthermore, Agenda 21 emphasises that broad public participation in decision making is a fundamental prerequisite for achieving sustainable development.\n\nUnder the principles of the United Nations Charter the Millennium Declaration identified principles and treaties on sustainable development, including economic development, social development and environmental protection. Broadly defined, sustainable development is a systems approach to growth and development and to manage natural, produced, and social capital for the welfare of their own and future generations. The term sustainable development as used by the United Nations incorporates both issues associated with land development and broader issues of human development such as education, public health, and standard of living.\n\nA 2013 study concluded that sustainability reporting should be reframed through the lens of four interconnected domains: ecology, economics, politics and culture.\n\nEducation for Sustainable\nDevelopment (ESD) is defined as education that encourages changes in knowledge, skills, values and attitudes to enable a more sustainable and equitable society. ESD aims to empower and equip current and future generations to meet the needs using a balanced and integrated approach to the economic, social and environmental dimensions of sustainable development.\n\nThe concept of ESD was born from the need for education to address the growing and changing environmental challenges facing the planet. In order to do this, education must change to provide the knowledge, skills, values and attitudes that empower learners to contribute to sustainable development. At the same time, education must be strengthened in all agendas, programmes, and activities that promote sustainable development. Sustainable development must be integrated into education and education must be integrated into sustainable development. ESD promotes the integration of these critical sustainability issues in local and global contexts into the curriculum to prepare learners to understand and respond to the changing world. ESD aims to produce learning outcomes that include core competencies such as critical and systematic thinking, collaborative decision-making, and taking responsibility for the present and future generations. Since traditional single-directional delivery of knowledge is not sufficient to inspire learners to take action as responsible citizens, ESD entails rethinking the learning environment, physical and virtual. The learning environment itself must adapt and apply a whole-institution approach to embed the philosophy of sustainable development. Building the capacity of educators and policy support at international, regional, national and local levels helps drive changes in learning institutions. Empowered youth and local communities interacting with education institutions become key actors in advancing sustainable development.\n\nThe launch of the UN Decade of Education for Sustainable Development (2005-2014) started a global movement to reorient education to address the challenges of sustainable development. Building on the achievement of the Decade, stated in the Aichi-Nagoya Declaration on ESD, UNESCO endorsed the Global Action Programme on ESD (GAP) in the 37th session of its General Conference. Acknowledged by UN General Assembly Resolution A/RES/69/211 and launched at the UNESCO World Conference on ESD in 2014, the GAP aims to scale-up actions and good practices. UNESCO has a major role, along with its partners, in bringing about key achievements to ensure the principles of ESD are promoted through formal, non-formal and informal education.\n\nInternational recognition of ESD as the key enabler for sustainable development is growing steadily. The role of ESD was recognized in three major UN summits on sustainable development: the 1992 UN Conference on Environment and Development (UNCED) in Rio de Janeiro, Brazil; the 2002 World Summit on Sustainable Development (WSSD) in Johannesburg, South Africa; and the 2012 UN Conference on Sustainable Development (UNCSD) in Rio de Janeiro. Other key global agreements such as the Paris Agreement (Article 12) also recognize the importance of ESD. Today, ESD is arguably at the heart of the 2030 Agenda for Sustainable Development and its 17 Sustainable Development Goals (SDGs) (United Nations, 2015). The SDGs recognize that all countries must stimulate action in the following key areas - people, planet, prosperity, peace and partnership - in order to tackle the global challenges that are crucial for the survival of humanity. ESD is explicitly mentioned in Target 4.7 of SDG4, which aims to ensure that all learners acquire the knowledge and skills needed to promote sustainable development and is understood as an important means to achieve all the other 16 SDGs (UNESCO, 2017).\n\nSustainable development can be thought of in terms of three spheres, dimensions, domains or pillars, i.e. the environment, the economy and society. The three-sphere framework was initially proposed by the economist Rene Passet in 1979. It has also been worded as \"economic, environmental and social\" or \"ecology, economy and equity\". This has been expanded by some authors to include a fourth pillar of culture, institutions or governance, or alternatively reconfigured as four domains of the social - ecology, economics, politics and culture, thus bringing economics back inside the social, and treating ecology as the intersection of the social and the natural.\n\nThe ecological stability of human settlements is part of the relationship between humans and their natural, social and built environments. Also termed human ecology, this broadens the focus of sustainable development to include the domain of human health. Fundamental human needs such as the availability and quality of air, water, food and shelter are also the ecological foundations for sustainable development; addressing public health risk through investments in ecosystem services can be a powerful and transformative force for sustainable development which, in this sense, extends to all species.\n\nEnvironmental sustainability concerns the natural environment and how it endures and remains diverse and productive. Since natural resources are derived from the environment, the state of air, water, and the climate are of particular concern. The IPCC Fifth Assessment Report outlines current knowledge about scientific, technical and socio-economic information concerning climate change, and lists options for adaptation and mitigation. Environmental sustainability requires society to design activities to meet human needs while preserving the life support systems of the planet. This, for example, entails using water sustainably, utilizing renewable energy, and sustainable material supplies (e.g. harvesting wood from forests at a rate that maintains the biomass and biodiversity).\n\nAn unsustainable situation occurs when natural capital (the sum total of nature's resources) is used up faster than it can be replenished. Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. Inherently the concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life. Such degradation on a global scale should imply an increase in human death rate until population falls to what the degraded environment can support. If the degradation continues beyond a certain tipping point or critical threshold it would lead to eventual extinction for humanity.\n\nIntegral elements for a sustainable development are research and innovation activities. A telling example is the European environmental research and innovation policy, which aims at defining and implementing a transformative agenda to greening the economy and the society as a whole so to achieve a truly sustainable development. Research and innovation in Europe is financially supported by the programme Horizon 2020, which is also open to participation worldwide. A promising direction towards sustainable development is to design systems that are flexible and reversible.\n\nPollution of the public resources is really not a different action, it just is a reverse tragedy of the commons, in that instead of taking something out, something is put into the commons. When the costs of polluting the commons are not calculated into the cost of the items consumed, then it becomes only natural to pollute, as the cost of pollution is external to the cost of the goods produced and the cost of cleaning the waste before it is discharged exceeds the cost of releasing the waste directly into the commons. So, the only way to solve this problem is by protecting the ecology of the commons by making it, through taxes or fines, more costly to release the waste directly into the commons than would be the cost of cleaning the waste before discharge.\n\nSo, one can try to appeal to the ethics of the situation by doing the right thing as an individual, but in the absence of any direct consequences, the individual will tend to do what is best for the person and not what is best for the common good of the public. Once again, this issue needs to be addressed. Because, left unaddressed, the development of the commonly owned property will become impossible to achieve in a sustainable way. So, this topic is central to the understanding of creating a sustainable situation from the management of the public resources that are used for personal use.\n\nSustainable agriculture consists of environment friendly methods of farming that allow the production of crops or livestock without damage to human or natural systems. It involves preventing adverse effects to soil, water, biodiversity, surrounding or downstream resources—as well as to those working or living on the farm or in neighbouring areas. The concept of sustainable agriculture extends intergenerationally, passing on a conserved or improved natural resource, biotic, and economic base rather than one which has been depleted or polluted. Elements of sustainable agriculture include permaculture, agroforestry, mixed farming, multiple cropping, and crop rotation. It involves agricultural methods that do not undermine the environment, smart farming technologies that enhance a quality environment for humans to thrive and reclaiming and transforming deserts into farmlands(Herman Daly, 2017). \n\nNumerous sustainability standards and certification systems exist, including organic certification, Rainforest Alliance, Fair Trade, UTZ Certified, Bird Friendly, and the Common Code for the Coffee Community (4C).\n\nIt has been suggested that because of rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital. Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption.\nAccording to ecological economist , ecological economics is defined by its focus on nature, justice, and time. Issues of intergenerational equity, irreversibility of environmental change, uncertainty of long-term outcomes, and sustainable development guide ecological economic analysis and valuation.\n\nAs early as the 1970s, the concept of sustainability was used to describe an economy \"in equilibrium with basic ecological support systems\". Scientists in many fields have highlighted \"The Limits to Growth\", and economists have presented alternatives, for example a 'steady-state economy', to address concerns over the impacts of expanding human development on the planet. In 1987 the economist Edward Barbier published the study \"The Concept of Sustainable Economic Development\", where he recognised that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.\n\nA World Bank study from 1999 concluded that based on the theory of genuine savings, policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental. Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule steady state.\n\nThe study, \"Interpreting Sustainability in Economic Terms\", found three pillars of sustainable development, interlinkage, intergenerational equity, and dynamic efficiency.\n\nBut Gilbert Rist points out that the World Bank has twisted the notion of sustainable development to prove that economic development need not be deterred in the interest of preserving the ecosystem. He writes: \"From this angle, 'sustainable development' looks like a cover-up operation. ... The thing that is meant to be sustained is really 'development', not the tolerance capacity of the ecosystem or of human societies.\"\n\nThe World Bank, a leading producer of environmental knowledge, continues to advocate the win-win prospects for economic growth and ecological stability even as its economists express their doubts. Herman Daly, an economist for the Bank from 1988 to 1994, writes:\nWhen authors of \"WDR\" '92 [the highly influential 1992 \"World Development Report\" that featured the environment] were drafting the report, they called me asking for examples of \"win-win\" strategies in my work. What could I say? None exists in that pure form; there are trade-offs, not \"win-wins.\" But they want to see a world of \"win-wins\" based on articles of faith, not fact. I wanted to contribute because \"WDR\"s are important in the Bank, [because] task managers read [them] to find philosophical justification for their latest round of projects. But they did not want to hear about how things really are, or what I find in my work...\n\nA meta review in 2002 looked at environmental and economic valuations and found a lack of \"sustainability policies\". A study in 2004 asked if we consume too much. A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world. It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics. A meta review in 2009 identified conditions for a strong case to act on climate change, and called for more work to fully account of the relevant economics and how it affects human welfare. According to free-market environmentalist John Baden \"the improvement of environment quality depends on the market economy and the existence of legitimate and protected property rights\". They enable the effective practice of personal responsibility and the development of mechanisms to protect the environment. The State can in this context \"create conditions which encourage the people to save the environment\".\n\nMisum, Mistra Center for Sustainable Markets, based at Stockholm School of Economics, aims to provide policy research and advice to Swedish and international actors on Sustainable Markets. Misum is a cross-disciplinary and multi-stakeholder knowledge center dedicated to sustainability and sustainable markets and contains three research platforms: Sustainability in Financial Markets (Mistra Financial Systems), Sustainability in Production and Consumption and Sustainable Socio-Economic Development.\n\nThe total environment includes not just the biosphere of earth, air, and water, but also human interactions with these things, with nature, and what humans have created as their surroundings.\n\nAs countries around the world continue to advance economically, they put a strain on the ability of the natural environment to absorb the high level of pollutants that are created as a part of this economic growth. Therefore, solutions need to be found so that the economies of the world can continue to grow, but not at the expense of the public good. In the world of economics the amount of environmental quality must be considered as limited in supply and therefore is treated as a scarce resource. This is a resource to be protected. One common way to analyze possible outcomes of policy decisions on the scarce resource is to do a cost-benefit analysis. This type of analysis contrasts different options of resource allocation and, based on an evaluation of the expected courses of action and the consequences of these actions, the optimal way to do so in the light of different policy goals can be elicited.\n\nBenefit-cost analysis basically can look at several ways of solving a problem and then assigning the best route for a solution, based on the set of consequences that would result from the further development of the individual courses of action, and then choosing the course of action that results in the least amount of damage to the expected outcome for the environmental quality that remains after that development or process takes place. Further complicating this analysis are the interrelationships of the various parts of the environment that might be impacted by the chosen course of action. Sometimes it is almost impossible to predict the various outcomes of a course of action, due to the unexpected consequences and the amount of unknowns that are not accounted for in the benefit-cost analysis.\n\nSustainable energy is clean and can be used over a long period of time. Unlike fossil fuels and biofuels that provide the bulk of the worlds energy, renewable energy sources like hydroelectric, solar and wind energy produce far less pollution. Solar energy is commonly used on public parking meters, street lights and the roof of buildings. Wind power has expanded quickly, its share of worldwide electricity usage at the end of 2014 was 3.1%. Most of California's fossil fuel infrastructures are sited in or near low-income communities, and have traditionally suffered the most from California's fossil fuel energy system. These communities are historically left out during the decision-making process, and often end up with dirty power plants and other dirty energy projects that poison the air and harm the area. These toxicants are major contributors to health problems in the communities. As renewable energy becomes more common, fossil fuel infrastructures are replaced by renewables, providing better social equity to these communities.\nOverall, and in the long run, sustainable development in the field of energy is also deemed to contribute to economic sustainability and national security of communities, thus being increasingly encouraged through investment policies.\n\nMain article: Green manufacturing and Distributed manufacturing\n\nOne of the core concepts in sustainable development is that technology can be used to assist people to meet their developmental needs. Technology to meet these sustainable development needs is often referred to as appropriate technology, which is an ideological movement (and its manifestations) originally articulated as intermediate technology by the economist E. F. Schumacher in his influential work, \"Small is Beautiful.\" And now covers a wide range of technologies. Both Schumacher and many modern-day proponents of appropriate technology also emphasise the technology as people-centered. Today appropriate technology is often developed using open source principles, which have led to Open-Source Appropriate Technology (OSAT) and thus many of the plans of the technology can be freely found on the Internet. OSAT has been proposed as a new model of enabling innovation for sustainable development.\n\nTransportation is a large contributor to greenhouse gas emissions. It is said that one-third of all gases produced are due to transportation. Motorized transport also releases exhaust fumes that contain particulate matter which is hazardous to human health and a contributor to climate change.\n\nSustainable transport has many social and economic benefits that can accelerate local sustainable development. According to a series of reports by the Low Emission Development Strategies Global Partnership (LEDS GP), sustainable transport can help create jobs, improve commuter safety through investment in bicycle lanes and pedestrian pathways, make access to employment and social opportunities more affordable and efficient. It also offers a practical opportunity to save people's time and household income as well as government budgets, making investment in sustainable transport a 'win-win' opportunity.\n\nSome Western countries are making transportation more sustainable in both long-term and short-term implementations. An example is the modification in available transportation in Freiburg, Germany. The city has implemented extensive methods of public transportation, cycling, and walking, along with large areas where cars are not allowed.\n\nSince many Western countries are highly automobile-oriented, the main transit that people use is personal vehicles. About 80% of their travel involves cars. Therefore, California, is one of the highest greenhouse gases emitters in the United States. The federal government has to come up with some plans to reduce the total number of vehicle trips in order to lower greenhouse gases emission. Such as:\n\n\nOther states and nations have built efforts to translate knowledge in behavioral economics into evidence-based sustainable transportation policies.\n\nThe most broadly accepted criterion for corporate sustainability constitutes a firm's efficient use of natural capital. This eco-efficiency is usually calculated as the economic value added by a firm in relation to its aggregated ecological impact. This idea has been popularised by the World Business Council for Sustainable Development (WBCSD) under the following definition: \"Eco-efficiency is achieved by the delivery of competitively priced goods and services that satisfy human needs and bring quality of life, while progressively reducing ecological impacts and resource intensity throughout the life-cycle to a level at least in line with the earth's carrying capacity\" (DeSimone and Popoff, 1997: 47).\n\nSimilar to the eco-efficiency concept but so far less explored is the second criterion for corporate sustainability. Socio-efficiency describes the relation between a firm's value added and its social impact. Whereas, it can be assumed that most corporate impacts on the environment are negative (apart from rare exceptions such as the planting of trees) this is not true for social impacts. These can be either positive (e.g. corporate giving, creation of employment) or negative (e.g. work accidents, mobbing of employees, human rights abuses). Depending on the type of impact socio-efficiency thus either tries to minimise negative social impacts (i.e. accidents per value added) or maximise positive social impacts (i.e. donations per value added) in relation to the value added.\n\nBoth eco-efficiency and socio-efficiency are concerned primarily with increasing economic sustainability. In this process they instrumentalise both natural and social capital aiming to benefit from win-win situations. However, as Dyllick and Hockerts point out the business case alone will not be sufficient to realise sustainable development. They point towards eco-effectiveness, socio-effectiveness, sufficiency, and eco-equity as four criteria that need to be met if sustainable development is to be reached.\n\nCASI Global, New York \"CSR & Sustainability together lead to sustainable development. CSR as in corporate social responsibility is not what you do with your profits, but is the way you make profits. This means CSR is a part of every department of the company value chain and not a part of HR / independent department. Sustainability as in effects towards Human resources, Environment and Ecology has to be measured within each department of the company.\" CASI Global\n\nAt the present time, sustainable development can reduce poverty. Sustainable development reduces poverty through financial (among other things, a balanced budget), environmental (living conditions), and social (including equality of income) means.\n\nIn sustainable architecture the recent movements of New Urbanism and New Classical architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and International Style architecture, as well as opposing to solitary housing estates and suburban sprawl, with long commuting distances and large ecological footprints. Both trends started in the 1980s. (Sustainable architecture is predominantly relevant to the economics domain while architectural landscaping pertains more to the ecological domain.)\n\nA study concluded that social indicators and, therefore, sustainable development indicators, are scientific constructs whose principal objective is to inform public policy-making. The International Institute for Sustainable Development has similarly developed a political policy framework, linked to a sustainability index for establishing measurable entities and metrics. The framework consists of six core areas:\n\nThe United Nations Global Compact Cities Programme has defined sustainable political development in a way that broadens the usual definition beyond states and governance. The political is defined as the domain of practices and meanings associated with basic issues of social power as they pertain to the organisation, authorisation, legitimation and regulation of a social life held in common. This definition is in accord with the view that political change is important for responding to economic, ecological and cultural challenges. It also means that the politics of economic change can be addressed. They have listed seven subdomains of the domain of politics:\n\n\nThis accords with the Brundtland Commission emphasis on development that is guided by human rights principles (see above).\n\nWorking with a different emphasis, some researchers and institutions have pointed out that a fourth dimension should be added to the dimensions of sustainable development, since the triple-bottom-line dimensions of economic, environmental and social do not seem to be enough to reflect the complexity of contemporary society. In this context, the Agenda 21 for culture and the United Cities and Local Governments (UCLG) Executive Bureau lead the preparation of the policy statement \"Culture: Fourth Pillar of Sustainable Development\", passed on 17 November 2010, in the framework of the World Summit of Local and Regional Leaders – 3rd World Congress of UCLG, held in Mexico City. This document inaugurates a new perspective and points to the relation between culture and sustainable development through a dual approach: developing a solid cultural policy and advocating a cultural dimension in all public policies. The Circles of Sustainability approach distinguishes the four domains of economic, ecological, political and cultural sustainability.\n\nOther organizations have also supported the idea of a fourth domain of sustainable development. The Network of Excellence \"Sustainable Development in a Diverse World\", sponsored by the European Union, integrates multidisciplinary capacities and interprets cultural diversity as a key element of a new strategy for sustainable development. The Fourth Pillar of Sustainable Development Theory has been referenced by executive director of IMI Institute at UNESCO Vito Di Bari in his manifesto of art and architectural movement Neo-Futurism, whose name was inspired by the 1987 United Nations’ report Our Common Future. The Circles of Sustainability approach used by Metropolis defines the (fourth) cultural domain as practices, discourses, and material expressions, which, over time, express continuities and discontinuities of social meaning.\n\nRecently, human-centered design and cultural collaboration have been popular frameworks for sustainable development in marginalized communities. These frameworks involve open dialogue which entails sharing, debating, and discussing, as well as holistic evaluation of the site of development. Especially when working on sustainable development in marginalized communities, cultural emphasis is a crucial factor in project decisions, since it largely affects aspects of their lives and traditions. Collaborators utilize articulation theory in co-designing. This allows for them to understand each other's thought process and their comprehension of the sustainable projects. By using the method of co-design, the beneficiaries' holistic needs are being considered. Final decisions and implementations are made with respect to sociocultural and ecological factors.\n\nThe user-oriented framework relies heavily on user participation and user feedback in the planning process. Users are able to provide new perspective and ideas, which can be considered in a new round of improvements and changes. It is said that increased user participation in the design process can garner a more comprehensive understanding of the design issues, due to more contextual and emotional transparency between researcher and participant. A key element of human centered design is applied ethnography, which was a research method adopted from cultural anthropology. This research method requires researchers to be fully immersed in the observation so that implicit details are also recorded.\n\nMany communities express environmental concerns, so life cycle analysis is often conducted when assessing the sustainability of a product or prototype. The assessment is done in stages with meticulous cycles of planning, design, implementation, and evaluation. The decision to choose materials is heavily weighted on its longevity, renewability, and efficiency. These factors ensure that researchers are conscious of community values that align with positive environmental, social, and economic impacts.\n\nThe United Nations Conference on Sustainable Development (UNCSD; also known as Rio 2012) was the third international conference on sustainable development, which aimed at reconciling the economic and environmental goals of the global community. An outcome of this conference was the development of the Sustainable Development Goals that aim to promote sustainable progress and eliminate inequalities around the world. However, few nations met the World Wide Fund for Nature's definition of sustainable development criteria established in 2006. Although some nations are more developed than others, all nations are constantly developing because each nation struggles with perpetuating disparities, inequalities and unequal access to fundamental rights and freedoms.\n\nIn 2007 a report for the U.S. Environmental Protection Agency stated: \"While much discussion and effort has gone into sustainability indicators, none of the resulting systems clearly tells us whether our society is sustainable. At best, they can tell us that we are heading in the wrong direction, or that our current activities are not sustainable. More often, they simply draw our attention to the existence of problems, doing little to tell us the origin of those problems and nothing to tell us how to solve them.\" Nevertheless, a majority of authors assume that a set of well defined and harmonised indicators is the only way to make sustainability tangible. Those indicators are expected to be identified and adjusted through empirical observations (trial and error).\n\nThe most common critiques are related to issues like data quality, comparability, objective function and the necessary resources. However a more general criticism is coming from the project management community: How can a sustainable development be achieved at global level if we cannot monitor it in any single project?\n\nThe Cuban-born researcher and entrepreneur Sonia Bueno suggests an alternative approach that is based upon the integral, long-term cost-benefit relationship as a measure and monitoring tool for the sustainability of every project, activity or enterprise. Furthermore, this concept aims to be a practical guideline towards sustainable development following the principle of conservation and increment of value rather than restricting the consumption of resources.\n\nReasonable qualifications of sustainability are seen U.S. Green Building Council's (USGBC) Leadership in Energy and Environmental Design (LEED). This design incorporates some ecological, economic, and social elements. The goals presented by LEED design goals are sustainable sites, water efficiency, energy consumption and atmospheric emission reduction, material and resources efficiency, and indoor environmental quality. Although amount of structures for sustainability development is many, these qualification has become a standard for sustainable building.\n\nRecent research efforts created also the SDEWES Index to benchmark the performance of cities across aspects that are related to energy, water and environment systems. The SDEWES Index consists of 7 dimensions, 35 indicators, and close to 20 sub-indicators. It is currently applied to 58 cities.\n\nThe sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible. Leading ecological economist and steady-state theorist Herman Daly, for example, points to the fact that natural capital can not necessarily be substituted by economic capital. While it is possible that we can find ways to replace some natural resources, it is much more unlikely that they will ever be able to replace eco-system services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest. In fact natural capital, social capital and economic capital are often complementarities. A further obstacle to substitutability lies also in the multi-functionality of many natural resources. Forests, for example, not only provide the raw material for paper (which can be substituted quite easily), but they also maintain biodiversity, regulate water flow, and absorb CO2.\n\nAnother problem of natural and social capital deterioration lies in their partial irreversibility. The loss of biodiversity, for example, is often definitive. The same can be true for cultural diversity. For example, with globalisation advancing quickly the number of indigenous languages is dropping at alarming rates. Moreover, the depletion of natural and social capital may have non-linear consequences. Consumption of natural and social capital may have no observable impact until a certain threshold is reached. A lake can, for example, absorb nutrients for a long time while actually increasing its productivity. However, once a certain level of algae is reached lack of oxygen causes the lake's ecosystem to break down suddenly.\n\nIf the degradation of natural and social capital has such important consequence the question arises why action is not taken more systematically to alleviate it. Cohen and Winn point to four types of market failure as possible explanations: First, while the benefits of natural or social capital depletion can usually be privatised, the costs are often externalised (i.e. they are borne not by the party responsible but by society in general). Second, natural capital is often undervalued by society since we are not fully aware of the real cost of the depletion of natural capital. Information asymmetry is a third reason—often the link between cause and effect is obscured, making it difficult for actors to make informed choices. Cohen and Winn close with the realization that contrary to economic theory many firms are not perfect optimisers. They postulate that firms often do not optimise resource allocation because they are caught in a \"business as usual\" mentality.\n\n\"Main page: Education for sustainable development\"\n\nEducation must be revisited in light of a renewed vision of sustainable human and social development that is both equitable and viable. This vision of sustainability must take into consideration the social, environmental and economic dimensions of human development and the various ways in which these relate to education: ‘An empowering education is one that builds the human resources we need to be productive, to continue to learn, to solve problems, to be creative, and to live together and with nature in peace and harmony. When nations ensure that such an education is accessible to all throughout their lives, a quiet revolution is set in motion: education becomes the engine of sustainable development and the key to a better world.’\n\nIt has been argued that since the 1960s, the concept of sustainable development has changed from \"conservation management\" to \"economic development\", whereby the original meaning of the concept has been stretched somewhat.\n\nIn the 1960s, the international community realised that many African countries needed national plans to safeguard wildlife habitats, and that rural areas had to confront the limits imposed by soil, climate and water availability. This was a strategy of conservation management. In the 1970s, however, the focus shifted to the broader issues of the provisioning of basic human needs, community participation as well as appropriate technology use throughout the developing countries (and not just in Africa). This was a strategy of economic development, and the strategy was carried even further by the Brundtland Commission's report on \"Our Common Future\" when the issues went from regional to international in scope and application. In effect, the conservationists were crowded out and superseded by the developers.\n\nBut shifting the focus of sustainable development from conservation to development has had the imperceptible effect of stretching the original forest management term of sustainable yield from the use of renewable resources only (like forestry), to now also accounting for the use of non-renewable resources (like minerals). This stretching of the term has been questioned. Thus, environmental economist Kerry Turner has argued that literally, there can be no such thing as overall \"sustainable development\" in an industrialised world economy that remains heavily dependent on the extraction of earth's finite stock of exhaustible mineral resources: \"It makes no sense to talk about the sustainable use of a non-renewable resource (even with substantial recycling effort and reduction in use rates). Any positive rate of exploitation will eventually lead to exhaustion of the finite stock.\"\n\nIn effect, it has been argued that the industrial revolution as a whole is unsustainable. \n\nOne critic has argued that the Brundtland Commission promoted nothing but a business as usual strategy for world development, with the ambiguous and insubstantial concept of \"sustainable development\" attached as a public relations slogan: The report on \"Our Common Future\" was largely the result of a political bargaining process involving many special interest groups, all put together to create a common appeal of political acceptability across borders. After World War II, the notion of \"development\" had been established in the West to imply the projection of the American model of society onto the rest of the world. In the 1970s and 1980s, this notion was broadened somewhat to also imply human rights, basic human needs and finally, ecological issues. The emphasis of the report was on helping poor nations out of poverty and meeting the basic needs of their growing populations—as usual. This issue demanded more economic growth, also in the rich countries, who would then import more goods from the poor countries to help them out—as usual. When the discussion switched to , the obvious dilemma was left aside by calling for economic growth with improved resource efficiency, or what was termed \"a change in the \"quality\" of growth\". However, most countries in the West had experienced such improved resource efficiency since the early-20th century already and as usual; only, this improvement had been more than offset by continuing industrial expansion, to the effect that world resource consumption was now higher than ever before—and these two historical trends were completely ignored in the report. Taken together, the policy of perpetual economic growth for the entire planet remained virtually intact. Since the publication of the report, the ambiguous and insubstantial slogan of \"sustainable development\" has marched on worldwide.\n\n\n"}
{"id": "27992", "url": "https://en.wikipedia.org/wiki?curid=27992", "title": "Slavery", "text": "Slavery\n\nSlavery is any system in which principles of property law are applied to people, allowing individuals to own, buy and sell other individuals, as a \"de jure\" form of property. A slave is unable to withdraw unilaterally from such an arrangement and works without remuneration. Many scholars now use the term chattel slavery to refer to this specific sense of legalized, \"de jure\" slavery. In a broader sense, however, the word slavery may also refer to any situation in which an individual is \"de facto\" forced to work against their own will. Scholars also use the more generic terms such as unfree labour or forced labour to refer to such situations. However, and especially under slavery in broader senses of the word, slaves may have some rights and protections according to laws or customs.\n\nSlavery existed in many cultures, dating back to early human civilizations. A person could become enslaved from the time of their birth, capture, or purchase.\n\nSlavery was legal in most societies at some time in the past, but is now outlawed in all recognized countries. The last country to officially abolish slavery was Mauritania in 1981. Nevertheless, there are an estimated 40.3 million people worldwide subject to some form of modern slavery. The most common form of modern slave trade is commonly referred to as human trafficking. In other areas, slavery (or unfree labour) continues through practices such as debt bondage, the most widespread form of slavery today, serfdom, domestic servants kept in captivity, certain adoptions in which children are forced to work as slaves, child soldiers, and forced marriage.\n\nThe English word \"slave\" comes from Old French \"sclave\", from the Medieval Latin \"sclavus\", from the Byzantine Greek σκλάβος, which, in turn, comes from the etymon of the ethnonym \"Slav\", because in some early Medieval wars many Slavs were captured and enslaved. An older interpretation connected it to the Greek verb \"skyleúo\" 'to strip a slain enemy'.\n\nThere is a dispute among historians about whether terms such as \"unfree labourer\" or enslaved person, rather than \"slave\", should be used when describing the victims of slavery. According to those proposing a change in terminology, \"slave\" perpetuates the crime of slavery in language; by reducing its victims to a nonhuman noun instead of \"carry[ing] them forward as people, not the property that they were\". Other historians prefer \"slave\" because the term is familiar and shorter, or because it accurately reflects the inhumanity of slavery, with \"person\" implying a degree of autonomy that slavery does not allow for.\n\nIndenture, otherwise known as bonded labour or debt bondage, is a form of unfree labour under which a person pledges himself or herself against a loan. The services required to repay the debt, and their duration, may be undefined. Debt bondage can be passed on from generation to generation, with children required to pay off their progenitors' debt. It is the most widespread form of slavery today. Debt bondage is most prevalent in South Asia.\n\nChattel slavery, also called traditional slavery, is so named because people are treated as the chattel (personal property) of the owner and are bought and sold as commodities. Typically, under the chattel slave system, slave status was imposed on children of the enslaved at birth. Although it dominated many different societies throughout human history, this form of slavery has been formally abolished and is very rare today. Even when it can be said to survive, it is not upheld by the legal system of any internationally recognized government.\n\n\"Slavery\" has also been used to refer to a legal state of dependency to somebody else. For example, in Persia, the situations and lives of such slaves could be better than those of common citizens.\n\nForced labour, or unfree labour, is sometimes used to refer to when an individual is forced to work against their own will, under threat of violence or other punishment, but the generic term unfree labour is also used to describe chattel slavery, as well as any other situation in which a person is obliged to work against their own will and a person's ability to work productively is under the complete control of another person. This may also include institutions not commonly classified as slavery, such as serfdom, conscription and penal labour. While some unfree labourers, such as serfs, have substantive, \"de jure\" legal or traditional rights, they also have no ability to terminate the arrangements under which they work, and are frequently subject to forms of coercion, violence, and restrictions on their activities and movement outside their place of work.\n\nHuman trafficking primarily involves women and children forced into prostitution and is the fastest growing form of forced labour, with Thailand, Cambodia, India, Brazil and Mexico having been identified as leading hotspots of commercial sexual exploitation of children. Examples of sexual slavery, often in military contexts, include detention in \"rape camps\" or \"comfort stations,\" \"comfort women\", forced \"marriages\" to soldiers and other practices involving the treatment of women or men as chattel and, as such, violations of the peremptory norm prohibiting slavery.\n\nIn 2007, Human Rights Watch estimated that 200,000 to 300,000 children served as soldiers in current conflicts. More girls under 16 work as domestic workers than any other category of child labor, often sent to cities by parents living in rural poverty such as in restaveks in Haiti.\n\nForced marriages or early marriages are often considered types of slavery. Forced marriage continues to be practiced in parts of the world including some parts of Asia and Africa and in immigrant communities in the West. Sacred prostitution is where girls and women are pledged to priests or those of higher castes, such as the practice of Devadasi in South Asia or fetish slaves in West Africa. Marriage by abduction occurs in many places in the world today, with a national average of 69% of marriages in Ethiopia being through abduction.\n\nEconomists have attempted to model the circumstances under which slavery (and variants such as serfdom) appear and disappear. One observation is that slavery becomes more desirable for landowners where land is abundant but labour is scarce, such that rent is depressed and paid workers can demand high wages. If the opposite holds true, then it becomes more costly for landowners to have guards for the slaves than to employ paid workers who can only demand low wages due to the amount of competition. Thus, first slavery and then serfdom gradually decreased in Europe as the population grew, but were reintroduced in the Americas and in Russia as large areas of new land with few people became available.\n\nSlavery is more common when the labor done is relatively simple and thus easy to supervise, such as large-scale growing of a single crop, like sugar and cotton, in which output was based on economies of scale. This enables such systems of labor, such as the gang system in the United States, to become prominent on large plantations where field hands were monitored and worked with factory-like precision.\n\nFor example, each work gang was based on an internal division of labour that not only assigned every member of the gang to a precise task, but also simultaneously made their own performance dependent on the actions of the others. The hoe hands chopped out the weeds that surrounded the cotton plants as well as excessive sprouts. The plow gangs followed behind, stirring the soil near the rows of cotton plants and tossing it back around the plants. Thus, the gang system worked like an assembly line.\n\nSince the 18th century, critics have argued that slavery tends to retard technological advancement because the focus is on increasing the number of slaves doing simple tasks rather than upgrading the efficiency of labour. For example, it is sometime argued that, because of this narrow focus, theoretical knowledge and learning in Greece – and later in Rome – was not applied to ease physical labour or improve manufacturing.\nAdam Smith made the argument that free labour was economically better than slave labour, and that it is nearly impossible to end slavery in a free, democratic, or republican form of government since many of its legislators, or political figures were slave owners, and would not punish themselves. He further argued that slaves would be better able to gain their freedom when there was centralized government, or a central authority like a king or the church.\n\nSimilar arguments appear later in the works of Auguste Comte, especially when it comes to Adam Smith's belief in the separation of powers, or what Comte called the \"separation of the spiritual and the temporal\" during the Middle Ages and the end of slavery, and Smith's criticism of masters, past and present. As Smith stated in the Lectures on Jurisprudence, \"The great power of the clergy thus concurring with that of the king set the slaves at liberty. But it was absolutely necessary both that the authority of the king and of the clergy should be great. Where ever any one of these was wanting, slavery still continues...\"\nSlaves can be an attractive investment because the slave-owner only needs to pay for sustenance and enforcement. This is sometimes lower than the wage-cost of free laborers because free workers earn more than sustenance, resulting in slaves having a positive price. When the cost of sustenance and enforcement exceeds the wage rate, slave-owning would no longer be profitable, and owners would simply release their slaves. Slaves are thus a more attractive investment in high-wage, cheap-enforcement environments, and less attractive in low-wage-rate, expensive-enforcement environments.\n\nFree workers also earn compensating differentials, whereby they are paid more for doing unpleasant work. However, since neither sustenance nor enforcement costs rise with the unpleasantness of the work, the cost of slaves do not rise by the same amount. As such, slaves are more attractive for unpleasant work, and less attractive for pleasant work. Because the unpleasantness of the work is not internalised, being borne by the slave rather than the owner, it is a negative externality and leads to over-use of slaves in these situations.\n\nCurrently, the weighted average global sales price of a slave is calculated to be approximately $340, with a high of $1,895 for the average trafficked sex slave, and a low of $40 to $50 for debt bondage slaves in part of Asia and Africa.\n\nWorldwide slavery is a criminal offense but slave owners can get very high returns for their risk. According to researcher Siddharth Kara, the profits generated worldwide by all forms of slavery in 2007 were $91.2 billion. That is second only to drug trafficking, in terms of global criminal enterprises. The weighted average annual profits generated by a slave in 2007 was $3,175, with a low of an average $950 for bonded labor and $29,210 for a trafficked sex slave. Approximately 40% of slave profits each year are generated by trafficked sex slaves, representing slightly more than 4% of the world's 29 million slaves.\n\nRobert E. Wright has developed a model, based on economic conditions, that helps to predict when firms (individuals, companies) will be more likely to use slaves rather than wage workers, indentured servants, family members, or other types of labourers.\n\nThroughout history, slaves were clothed in a distinctive fashion, particularly with respect to the frequent lack of footwear, as they were rather commonly forced to go barefoot. This was partly due to economic reasons, but also served as a distinguishing feature, especially in South Africa and South America. For example, the Cape Town slave code stated that \"Slaves must go barefoot and must carry passes.\" It also puts slaves at a physical disadvantage due to the lack of protection against environmental adversities and also in situations of possible confrontation, thereby making it more difficult to escape or to rebel against their owners.\n\nThis was the case in the majority of states that abolished slavery later in history, as most images from the respective historical period suggest that slaves were barefoot.\nTo quote Brother Riemer (1779): \"[the slaves] are, even in their most beautiful suit, obliged to go barefoot. Slaves were forbidden to wear shoes. This was a prime mark of distinction between the free and the bonded and no exceptions were permitted.\"\n\nAccording to the bible shoes have been considered badges of freedom since antiquity: \"But the father said to his servants, Bring forth the best robe, and put [it] on him; and put a ring on his hand, and shoes on [his] feet\" (). This aspect can be viewed as an informal law in areas where slavery existed as any person sighted barefoot in public would be conclusively regarded as a slave.\n\nIn certain societies this rule is valid to this day. As with the Tuareg, where slavery is still unofficially practiced, their slaves are constantly forced to remain barefoot as a recognition mark. Mainly through their bare feet their societal status and rank opposite their owners is displayed to the public in a plainly visible way.\n\nAnother widespread practice was branding the slaves either to generally mark them as property or as punishment usually reserved for fugitives.\n\nEvidence of slavery predates written records, and has existed in many cultures. Slavery is rare among hunter-gatherer populations because it requires economic surpluses and a high population density to be viable. Thus, although it has existed among unusually resource-rich hunter gatherers, such as the American Indian peoples of the salmon-rich rivers of the Pacific Northwest Coast, slavery became widespread only with the invention of agriculture during the Neolithic Revolution about 11,000 years ago.\n\nIn the earliest known records, slavery is treated as an established institution. The Code of Hammurabi (c. 1760 BC), for example, prescribed death for anyone who helped a slave escape or who sheltered a fugitive. The Bible mentions slavery as an established institution.\n\nSlavery was known in almost every ancient civilization and society including Sumer, Ancient Egypt, Ancient China, the Akkadian Empire, Assyria, Ancient India, Ancient Greece, Carolingian Europe, the Roman Empire, the Hebrew kingdoms of the ancient Levant, and the pre-Columbian civilizations of the Americas. Such institutions included debt-slavery, punishment for crime, the enslavement of prisoners of war, child abandonment, and the birth of slave children to slaves.\n\nSlavery existed in Pharaonic Egypt but studying it is complicated by terminology used by the Egyptians to refer to different classes of servitude over the course of history. Interpretation of the textual evidence of classes of slaves in ancient Egypt has been difficult to differentiate by word usage alone. There were three apparent types of enslavement in Ancient Egypt: chattel slavery, bonded labor, and forced labor.\n\nSlavery is known to have existed in ancient China as early as the Shang Dynasty. Slavery was largely employed by governments as a means of maintaining a public labor force.\n\nRecords of slavery in Ancient Greece date as far back as Mycenaean Greece. It is certain that Classical Athens had the largest slave population, with as many as 80,000 in the 6th and 5th centuries BC; two- to four-fifths of the population were slaves. As the Roman Republic expanded outward, entire populations were enslaved, thus creating an ample supply from all over Europe and the Mediterranean. Greeks, Illyrians, Berbers, Germans, Britons, Thracians, Gauls, Jews, Arabs, and many more were slaves used not only for labour, but also for amusement (e.g. gladiators and sex slaves). This oppression, by an elite minority, eventually led to slave revolts (see Roman Servile Wars); the Third Servile War, led by Spartacus, (a Thracian) being the most famous.\n\nBy the late Republican era, slavery had become a vital economic pillar in the wealth of Rome, as well as a very significant part of Roman society. It is estimated that 25% or more of the population of Ancient Rome was enslaved, although the actual percentage is debated by scholars, and varied from region to region. Slaves represented 15–25% of Italy's population, mostly captives in war, especially from Gaul and Epirus. Estimates of the number of slaves in the Roman Empire suggest that the majority of slaves were scattered throughout the provinces outside of Italy. Generally, slaves in Italy were indigenous Italians, with a minority of foreigners (including both slaves and freedmen) born outside of Italy estimated at 5% of the total in the capital at its peak, where their number was largest. Those from outside of Europe were predominantly of Greek descent, while the Jewish ones never fully assimilated into Roman society, remaining an identifiable minority. These slaves (especially the foreigners) had higher death rates and lower birth rates than natives, and were sometimes even subjected to mass expulsions. The average recorded age at death for the slaves of the city of Rome was seventeen and a half years (17.2 for males; 17.9 for females).\n\nSlavery was widespread in Africa, with both internal and external slave trade. In the Senegambia region, between 1300 and 1900, close to one-third of the population was enslaved. In early Islamic states of the western Sahel, including Ghana (750–1076), Mali (1235–1645), Segou (1712–1861), and Songhai (1275–1591), about a third of the population were enslaved.\n\nThe Arab slave trade, across the Sahara desert and across the Indian Ocean, began after Muslim Arab and Swahili traders won control of the Swahili Coast and sea routes during the 9th century (see Sultanate of Zanzibar). These traders captured Bantu peoples (Zanj) from the interior in present-day Kenya, Mozambique and Tanzania and brought them to the coast. There, the slaves gradually assimilated in the rural areas, particularly on the Unguja and Pemba islands.\n\nSlavery in Mexico can be traced back to the Aztecs. Other Amerindians, such as the Inca of the Andes, the Tupinambá of Brazil, the Creek of Georgia, and the Comanche of Texas, also owned slaves.\n\nMany Han Chinese were enslaved in the process of the Mongol invasion of China proper. According to Japanese historians Sugiyama Masaaki (杉山正明) and Funada Yoshiyuki (舩田善之), there were also a certain number of Mongolian slaves owned by Han Chinese during the Yuan dynasty. Moreover, there is no evidence that the Han Chinese, who were at the bottom of Yuan society according to some research, suffered particularly cruel abuse.\n\nSlavery in Korea existed since before the Three Kingdoms of Korea period, approximately 2,000 years ago. Slavery has been described as \"very important in medieval Korea, probably more important than in any other East Asian country, but by the 16th century, population growth was making [it] unnecessary\". Slavery went into decline around the 10th century, but came back in the late Goryeo period when Korea also experienced a number of slave rebellions.\n\nIn the Joseon period of Korea, members of the slave class were known as \"nobi\". The nobi were socially indistinct from freemen (i.e., the middle and common classes) other than the ruling yangban class, and some possessed property rights, legal entities and civil rights. Hence, some scholars argue that it is inappropriate to call them \"slaves\", while some scholars describe them as serfs. The nobi population could fluctuate up to about one-third of the population, but on average the nobi made up about 10% of the total population. In 1801, the vast majority of government nobi were emancipated, and by 1858 the nobi population stood at about 1.5 percent of the total population of Korea.\n\nSlavery largely disappeared from Western Europe in the Middle Ages, but persisted longer in Eastern Europe. Large-scale trading in slaves was mainly confined to the South and East of early medieval Europe: the Byzantine Empire and the Muslim world were the destinations, while pagan Central and Eastern Europe (along with the Caucasus and Tartary) were important sources. Viking, Arab, Greek, and Radhanite Jewish merchants were all involved in the slave trade during the Early Middle Ages. The trade in European slaves reached a peak in the 10th century following the Zanj rebellion which dampened the use of African slaves in the Arab world.\n\nSlavery in early medieval Europe was so common that the Catholic Church repeatedly prohibited it, or at least the export of Christian slaves to non-Christian lands, as for example at the Council of Koblenz (922), the Council of London (1102) (which aimed mainly at the sale of English slaves to Ireland) and the Council of Armagh (1171). Serfdom, on the contrary, was widely accepted. In 1452, Pope Nicholas V issued the papal bull Dum Diversas, granting the kings of Spain and Portugal the right to reduce any \"Saracens (Muslims), pagans and any other unbelievers\" to perpetual slavery, legitimizing the slave trade as a result of war. The approval of slavery under these conditions was reaffirmed and extended in his Romanus Pontifex bull of 1455.\n\nIn Britain, slavery continued to be practiced following the fall of Rome and sections of Hywel the Good's laws dealt with slaves in medieval Wales. The trade particularly picked up after the Viking invasions, with major markets at Chester and Bristol supplied by Danish, Mercian, and Welsh raiding of one another's borderlands. At the time of the \"Domesday Book\", nearly 10% of the English population were slaves. William the Conqueror introduced a law preventing the sale of slaves overseas. According to historian John Gillingham, by the year 1200, slavery in the British Isles was non-existent. The slave trade was abolished by the Slave Trade Act 1807, although slavery itself remained legal in possessions outside Europe until the passage of the Slavery Abolition Act 1833 and the Indian Slavery Act, 1843.\n\nThe Byzantine-Ottoman wars and the Ottoman wars in Europe brought large numbers of slaves into the Islamic world.\n\nTo staff its bureaucracy, the Ottoman Empire established a janissary system which seized hundreds of thousands of Christian boys through the devşirme system. They were well cared for but were legally slaves owned by the government and were not allowed to marry. They were never bought or sold. The Empire gave them significant administrative and military roles. The system began about 1365; there were 135,000 janissaries in 1826, when the system ended.\n\nAfter the Battle of Lepanto, 12,000 Christian galley slaves were recaptured and freed from the Ottoman fleet. Eastern Europe suffered a series of Tatar invasions, the goal of which was to loot and capture slaves into \"jasyr\". Seventy-five Crimean Tatar raids were recorded into Poland–Lithuania between 1474 and 1569.\n\nSlavery in Poland was forbidden in the 15th century; in Lithuania, slavery was formally abolished in 1588; they were replaced by the second serfdom.\n\nThe maritime town of Lagos was the first slave market created in Portugal (one of the earliest colonizers of the Americas) for the sale of imported African slaves – the \"Mercado de Escravos\", opened in 1444. In 1441, the first slaves were brought to Portugal from northern Mauritania.\n\nBy 1552, black African slaves made up 10% of the population of Lisbon. In the second half of the 16th century, the Crown gave up the monopoly on slave trade and the focus of European trade in African slaves shifted from import to Europe to slave transports directly to tropical colonies in the Americas – in the case of Portugal, especially Brazil. In the 15th century one-third of the slaves were resold to the African market in exchange of gold.\n\nIn Kievan Rus and Muscovy, slaves were usually classified as kholops. According to David P. Forsythe, \"In 1649 up to three-quarters of Muscovy's peasants, or 13 to 14 million people, were serfs whose material lives were barely distinguishable from slaves. Perhaps another 1.5 million were formally enslaved, with Russian slaves serving Russian masters.\" Slavery remained a major institution in Russia until 1723, when Peter the Great converted the household slaves into house serfs. Russian agricultural slaves were formally converted into serfs earlier in 1679.\n\nIn Scandinavia, thralldom was abolished in the mid-14th century.\n\nMedieval Spain and Portugal were the scene of almost constant Muslim invasion of the predominantly Christian area. Periodic raiding expeditions were sent from Al-Andalus to ravage the Iberian Christian kingdoms, bringing back booty and slaves. In a raid against Lisbon, Portugal in 1189, for example, the Almohad caliph Yaqub al-Mansur took 3,000 female and child captives, while his governor of Córdoba, in a subsequent attack upon Silves, Portugal, in 1191, took 3,000 Christian slaves. From the 11th to the 19th century, North African Barbary Pirates engaged in \"Razzias\", raids on European coastal towns, to capture Christian slaves to sell at slave markets in places such as Algeria and Morocco.\n\nThe Arab slave trade lasted more than a millennium. As recently as the early 1960s, Saudi Arabia's slave population was estimated at 300,000. Along with Yemen, the Saudis abolished slavery only in 1962. Historically, slaves in the Arab World came from many different regions, including Sub-Saharan Africa (mainly \"Zanj\"), the Caucasus (mainly Circassians), Central Asia (mainly Tartars), and Central and Eastern Europe (mainly \"Saqaliba\").\n\nSome historians assert that as many as 17 million people were sold into slavery on the coast of the Indian Ocean, the Middle East, and North Africa, and approximately 5 million African slaves were bought by Muslim slave traders and taken from Africa across the Red Sea, Indian Ocean, and Sahara desert between 1500 and 1900.\n\nThe captives were sold throughout the Middle East. This trade accelerated as superior ships led to more trade and greater demand for labour on plantations in the region. Eventually, tens of thousands of captives were being taken every year.\n\nThe Indian Ocean slave trade was multi-directional and changed over time. To meet the demand for menial labor, Bantu slaves bought by Arab slave traders from southeastern Africa were sold in cumulatively large numbers over the centuries to customers in Egypt, Arabia, the Persian Gulf, India, European colonies in the Far East, the Indian Ocean islands, Ethiopia and Somalia.\n\nAccording to the \"Encyclopedia of African History\", \"It is estimated that by the 1890s the largest slave population of the world, about 2 million people, was concentrated in the territories of the Sokoto Caliphate. The use of slave labor was extensive, especially in agriculture.\" The Anti-Slavery Society estimated there were 2 million slaves in Ethiopia in the early 1930s out of an estimated population of 8 to 16 million.\n\nSlave labor in East Africa was drawn from the \"Zanj,\" Bantu peoples that lived along the East African coast. The Zanj were for centuries shipped as slaves by Arab traders to all the countries bordering the Indian Ocean. The Umayyad and Abbasid caliphs recruited many Zanj slaves as soldiers and, as early as 696, there were slave revolts of the Zanj against their Arab enslavers in Iraq.\n\nThe Zanj Rebellion, a series of uprisings that took place between 869 and 883 AD near the city of Basra (also known as Basara), situated in present-day Iraq, is believed to have involved enslaved Zanj that had originally been captured from the African Great Lakes region and areas further south in East Africa. It grew to involve over 500,000 slaves and free men who were imported from across the Muslim empire and claimed over \"tens of thousands of lives in lower Iraq\".\nThe Zanj who were taken as slaves to the Middle East were often used in strenuous agricultural work. As the plantation economy boomed and the Arabs became richer, agriculture and other manual labor work was thought to be demeaning. The resulting labor shortage led to an increased slave market.\nIn Algiers, the capital of Algeria, captured Christians and Europeans were forced into slavery. About 1650 there are said to have been as many as 35,000 Christian slaves in Algiers. By one estimate, raids by Barbary pirates on coastal villages and ships extending from Italy to Iceland, enslaved an estimated 1 million to 1¼ million Europeans between the 16th and 19th centuries. However, to this estimate is extrapolated by assuming the number of European, slaves captured by Barbary pirates, was constant for 250 years period:\n\nDavis' numbers have been refuted by other historians, such as David Earle, who cautions that true picture of Europeans slaves is clouded by the fact the corsairs also seized non-Christian whites from eastern Europe.\n\nIn addition, the number of slaves traded was hyperactive, with exaggerated estimates relying on peak years to calculate averages for entire centuries, or millennia. Hence, there were wide fluctuations year-to-year, particularly in the 18th and 19th centuries, given slave imports, and also given the fact that, prior to the 1840s, there are no consistent records. Middle East expert, John Wright, cautions that modern estimates are based on back-calculations from human observation.\n\nSuch observations, across the late 1500s and early 1600s observers, account for around 35,000 European Christian slaves held throughout this period on the Barbary Coast, across Tripoli, Tunis, but mostly in Algiers. The majority were sailors (particularly those who were English), taken with their ships, but others were fishermen and coastal villagers. However, most of these captives were people from lands close to Africa, particularly Spain and Italy.\n\nThis eventually led to the bombardment of Algiers by an Anglo-Dutch fleet in 1816.\nUnder Omani Arabs, Zanzibar became East Africa's main slave port, with as many as 50,000 enslaved Africans passing through every year during the 19th century. Some historians estimate that between 11 and 18 million African slaves crossed the Red Sea, Indian Ocean, and Sahara Desert from 650 to 1900 AD. Eduard Rüppell described the losses of Sudanese slaves being transported on foot to Egypt: \"after the Daftardar bey's 1822 campaign in the southern Nuba mountains, nearly 40,000 slaves were captured. However, through bad treatment, disease and desert travel barely 5000 made it to Egypt..\" W.A. Veenhoven wrote: \"The German doctor, Gustav Nachtigal, an eye-witness, believed that for every slave who arrived at a market three or four died on the way ... Keltie (\"The Partition of Africa\", London, 1920) believes that for every slave the Arabs brought to the coast at least six died on the way or during the slavers' raid. Livingstone puts the figure as high as ten to one.\"\n\nSystems of servitude and slavery were common in parts of Africa, as they were in much of the ancient world. In many African societies where slavery was prevalent, the enslaved people were not treated as chattel slaves and were given certain rights in a system similar to indentured servitude elsewhere in the world.\n\nThe forms of slavery in Africa were closely related to kinship structures. In many African communities, where land could not be owned, enslavement of individuals was used as a means to increase the influence a person had and expand connections. This made slaves a permanent part of a master's lineage and the children of slaves could become closely connected with the larger family ties. Children of slaves born into families could be integrated into the master's kinship group and rise to prominent positions within society, even to the level of chief in some instances. However, stigma often remained attached and there could be strict separations between slave members of a kinship group and those related to the master.\n\nSlavery, in historical Africa, was practiced in many different forms: Debt slavery, enslavement of war captives, military slavery, and criminal slavery were all practiced in various parts of Africa. Slavery for domestic and court purposes was widespread throughout Africa.\nWhen the Atlantic slave trade began, many of the local slave systems began supplying captives for chattel slave markets outside Africa. Although the Atlantic slave trade was not the only slave trade from Africa, it was the largest in volume and intensity. As Elikia M’bokolo wrote in \"Le Monde diplomatique\":\n\nThe trans-Atlantic slave trade peaked in the late 18th century, when the largest number of slaves were captured on raiding expeditions into the interior of West Africa. These expeditions were typically carried out by African kingdoms, such as the Oyo empire (Yoruba), the Ashanti Empire, the kingdom of Dahomey, and the Aro Confederacy. It is estimated that about 15 percent of slaves died during the voyage, with mortality rates considerably higher in Africa itself in the process of capturing and transporting indigenous peoples to the ships.\n\nSlavery in America remains a contentious issue and played a major role in the history and evolution of some countries, triggering a revolution, a civil war, and numerous rebellions.\n\nIn order to establish itself as an American empire, Spain had to fight against the relatively powerful civilizations of the New World. The Spanish conquest of the indigenous peoples in the Americas included using the Natives as forced labour. The Spanish colonies were the first Europeans to use African slaves in the New World on islands such as Cuba and Hispaniola (see Atlantic slave trade.) Bartolomé de las Casas, a 16th-century Dominican friar and Spanish historian, participated in campaigns in Cuba (at Bayamo and Camagüey) and was present at the massacre of Hatuey; his observation of that massacre led him to fight for a social movement away from the use of natives as slaves. Also, the alarming decline in the native population had spurred the first royal laws protecting the native population (Laws of Burgos, 1512–1513).\n\nThe first African slaves arrived in Hispaniola in 1501. England played a prominent role in the Atlantic slave trade. The \"slave triangle\" was pioneered by Francis Drake and his associates.\n\nMany Africans who arrived in North America during the 17th and 18th centuries came under contract as indentured servants. The transformation from indentured servitude to slavery was a gradual process in Virginia. The earliest legal documentation of such a shift was in 1640 where a negro, John Punch, was sentenced to lifetime slavery, forcing him to serve his master, Hugh Gwyn, for the remainder of his life, for attempting to run away. This case was significant because it established the disparity between his sentence as a black man, and that of the two white indentured servants who escaped with him (one described as Dutch and one as a Scotchman). It is the first documented case of a black man sentenced to lifetime servitude, and is considered one of the first legal cases to make a racial distinction between black and white indentured servants.\n\nAfter 1640, planters started to ignore the expiration of indentured contracts and kept their servants as slaves for life. This was demonstrated by the 1655 case \"Johnson v. Parker\", where the court ruled that a black man, Anthony Johnson of Virginia, was granted ownership of another black man, John Casor, as the result of a civil case. This was the first instance of a judicial determination in the Thirteen Colonies holding that a person who had committed no crime could be held in servitude for life.\n\nIn the very early years (1620–1640s) the majority of the labour was provided by European indentured servants, mainly English, Irish and Scottish, with enslaved Africans and enslaved Amerindian providing little of the workforce.\n\nThe introduction of sugar cane from Dutch Brazil in 1640 completely transformed society and the economy. Barbados eventually had one of the world's biggest sugar industries.\n\nAs the effects of the new crop increased, so did the shift in the ethnic composition of Barbados and surrounding islands. The workable sugar plantation required a large investment and a great deal of heavy labour. At first, Dutch traders supplied the equipment, financing, and enslaved Africans, in addition to transporting most of the sugar to Europe. In 1644 the population of Barbados was estimated at 30,000, of which about 800 were of African descent, with the remainder mainly of English descent. These English smallholders were eventually bought out and the island filled up with large sugar plantations worked by enslaved Africans. By 1660 there was near parity with 27,000 blacks and 26,000 whites. By 1666 at least 12,000 white smallholders had been bought out, died, or left the island. Many of the remaining whites were increasingly poor. By 1680 there were 17 slaves for every indentured servant. By 1700, there were 15,000 free whites and 50,000 enslaved Africans.\n\nDue to the increased implementation of slave codes, which created differential treatment between Africans and the white workers and ruling planter class, the island became increasingly unattractive to poor whites. Black or slave codes were implemented in 1661, 1676, 1682, and 1688. In response to these codes, several slave rebellions were attempted or planned during this time, but none succeeded. Nevertheless, poor whites who had or acquired the means to emigrate often did so. Planters expanded their importation of enslaved Africans to cultivate sugar cane.\n\nSlavery in Brazil began long before the first Portuguese settlement was established in 1532, as members of one tribe would enslave captured members of another.\n\nLater, Portuguese colonists were heavily dependent on indigenous labor during the initial phases of settlement to maintain the subsistence economy, and natives were often captured by expeditions called \"\". The importation of African slaves began midway through the 16th century, but the enslavement of indigenous peoples continued well into the 17th and 18th centuries.\n\nDuring the Atlantic slave trade era, Brazil imported more African slaves than any other country. Nearly 5 million slaves were brought from Africa to Brazil during the period from 1501 to 1866. Until the early 1850s, most enslaved Africans who arrived on Brazilian shores were forced to embark at West Central African ports, especially in Luanda (present-day Angola). Today, with the exception of Nigeria, the largest population of people of African descent is in Brazil.\n\nSlave labor was the driving force behind the growth of the sugar economy in Brazil, and sugar was the primary export of the colony from 1600 to 1650. Gold and diamond deposits were discovered in Brazil in 1690, which sparked an increase in the importation of African slaves to power this newly profitable market. Transportation systems were developed for the mining infrastructure, and population boomed from immigrants seeking to take part in gold and diamond mining.\n\nThe largest number of slaves were shipped to Brazil.\n\nDemand for African slaves did not wane after the decline of the mining industry in the second half of the 18th century. Cattle ranching and foodstuff production proliferated after the population growth, both of which relied heavily on slave labor. 1.7 million slaves were imported to Brazil from Africa from 1700 to 1800, and the rise of coffee in the 1830s further enticed expansion of the slave trade.\n\nBrazil was the last country in the Western world to abolish slavery. By the time it was abolished, in 1888, an estimated four million slaves had been imported from Africa to Brazil, 40% of the total number of slaves brought to the Americas. For reference, the United States received 10 percent.\n\nDespite being abolished, there are still people working in slavery-like conditions in Brazil in the 21st century.\n\nIn 1789 the Spanish Crown led an effort to reform slavery, as the demand for slave labor in Cuba was growing. The Crown issued a decree, \"Código Negro Español\" (Spanish Black Codex), that specified food and clothing provisions, put limits on the number of work hours, limited punishments, required religious instruction, and protected marriages, forbidding the sale of young children away from their mothers. The British made other changes to the institution of slavery in Cuba. But, planters often flouted the laws and protested against them, considering them a threat to their authority and an intrusion into their personal lives.\n\nThe slaveowners did not protest against all the measures of the codex, many of which they argued were already common practices. They objected to efforts to set limits on their ability to apply physical punishment. For instance, the Black Codex limited whippings to 25 and required the whippings \"not to cause serious bruises or bleeding\". The slave-owners thought that the slaves would interpret these limits as weaknesses, ultimately leading to resistance. Another contested issue was the work hours that were restricted \"from sunrise to sunset\"; plantation owners responded by explaining that cutting and processing of cane needed 20-hour days during the harvest season.\n\nThose slaves who worked on sugar plantations and in sugar mills were often subject to the harshest of conditions. The field work was rigorous manual labor which the slaves began at an early age. The work days lasted close to 20 hours during harvest and processing, including cultivating and cutting the crops, hauling wagons, and processing sugarcane with dangerous machinery. The slaves were forced to reside in barracoons, where they were crammed in and locked in by a padlock at night, getting about three and four hours of sleep. The conditions of the barracoons were harsh; they were highly unsanitary and extremely hot. Typically there was no ventilation; the only window was a small barred hole in the wall.\nCuba's slavery system was gendered in a way that some duties were performed only by male slaves, some only by female slaves. Female slaves in the city of Havana, from the sixteenth century onwards, performed duties such as operating the town taverns, eating houses, and lodges, as well as being laundresses and domestic laborers and servants. Female slaves also served as the town prostitutes.\n\nSome Cuban women could gain freedom by having children with white men. As in other Latin cultures, there were looser borders with the mulatto or mixed-race population. Sometimes men who took slaves as wives or concubines freed both them and their children. As in New Orleans and Saint-Domingue, mulattos began to be classified as a third group between the European colonists and African slaves. Freedmen, generally of mixed race, came to represent 20% of the total Cuban population and 41% of the non-white Cuban population.\n\nBut, planters encouraged Afro-Cuban slaves to have children in order to reproduce their work force. The masters wanted to pair strong and large-built black men with healthy black women. They were placed in the barracoons and forced to have sex and create offspring of “breed stock” children, who would sell for around 500 pesos. The planters needed children to be born to replace slaves who died under the harsh regime. Sometimes if the overseers did not like the quality of children, they separate the parents and sent the mother back to working in the fields.\n\nBoth women and men were subject to the punishments of violence and humiliating abuse. Slaves who misbehaved or disobeyed their masters were often placed in stocks in the depths of the boiler houses where they were abandoned for days at a time, and oftentimes two to three months. These wooden stocks were made in two types: lying-down or stand-up types. women were punished, even when pregnant. They were subjected to whippings: they had to lay \"face down over a scooped-out piece of round [earth] to protect their bellies.\" Some masters reportedly whipped pregnant women in the belly, often causing miscarriages. The wounds were treated with “compresses of tobacco leaves, urine and salt.\" \n\nSlavery in Haiti started with the arrival of Christopher Columbus on the island in 1492. The practice was devastating to the native population. Following the indigenous Taino's near decimation from forced labour, disease and war, the Spanish, under advisement of the Catholic priest Bartolomeu de las Casas, and with the blessing of the Catholic church began engaging in earnest in the kidnapped and forced labour of enslaved Africans. During the French colonial period beginning in 1625, the economy of Haiti (then known as Saint-Domingue) was based on slavery, and the practice there was regarded as the most brutal in the world.\n\nFollowing the Treaty of Ryswick of 1697, Hispaniola was divided between France and Spain. France received the western third and subsequently named it Saint-Domingue. To develop it into sugarcane plantations, the French imported thousands of slaves from Africa. Sugar was a lucrative commodity crop throughout the 18th century. By 1789, approximately 40,000 white colonists lived in Saint-Domingue. In contrast, by 1763 the white population of French Canada, a vast territory, had numbered 65,000. The whites were vastly outnumbered by the tens of thousands of African slaves they had imported to work on their plantations, which were primarily devoted to the production of sugarcane. In the north of the island, slaves were able to retain many ties to African cultures, religion and language; these ties were continually being renewed by newly imported Africans. Blacks outnumbered whites by about ten to one.\nThe French-enacted \"Code Noir\" (\"Black Code\"), prepared by Jean-Baptiste Colbert and ratified by Louis XIV, had established rules on slave treatment and permissible freedoms. Saint-Domingue has been described as one of the most brutally efficient slave colonies; one-third of newly imported Africans died within a few years. Many slaves died from diseases such as smallpox and typhoid fever. They had birth rates around 3 percent, and there is evidence that some women aborted fetuses, or committed infanticide, rather allow their children to live within the bonds of slavery.\n\nAs in its Louisiana colony, the French colonial government allowed some rights to free people of color: the mixed-race descendants of white male colonists and black female slaves (and later, mixed-race women). Over time, many were released from slavery. They established a separate social class. White French Creole fathers frequently sent their mixed-race sons to France for their education. Some men of color were admitted into the military. More of the free people of color lived in the south of the island, near Port-au-Prince, and many intermarried within their community. They frequently worked as artisans and tradesmen, and began to own some property. Some became slave holders. The free people of color petitioned the colonial government to expand their rights.\n\nSlaves that made it to Haiti from the trans-Atlantic journey and slaves born in Haiti were first documented in Haiti's archives and transferred to France's Ministry of Defense and the Ministry of Foreign Affairs. , these records are in The National Archives of France. According to the 1788 Census, Haiti's population consisted of nearly 40,000 whites, 30,000 free coloureds and 450,000 slaves.\n\nThe Haitian Revolution of 1804, the only successful slave revolt in human history, precipitated the end of slavery in all French colonies.\n\nThe Caribbean island of Jamaica was colonized by the Taino tribes prior to the arrival of Columbus in 1494.\n\nThe Spanish enslaved many of the Taino; some escaped, but most died from European diseases and overwork. The Spaniards also introduced the first African slaves.\n\nThe Spanish colonists did not bring women in the first expeditions and took Taíno women for their common-law wives, resulting in mestizo children. Sexual violence with the Taíno women by the Spanish was also common.\n\nAlthough the African slave population in the 1670s and 1680s never exceeded 10,000, by 1800 it had increased to over 300,000.\n\nIn 1519, Hernán Cortés brought the first modern slave to the area.\n\nIn the mid-16th century, the second viceroy to Mexico, Luis de Velasco, prohibited slavery of the Aztecs. A labor shortage resulted as the Aztecs were either killed or died due to disease. This led to the African slaves being imported, as they were not susceptible to smallpox. In exchange, many Africans were afforded the opportunity to buy their freedom, while eventually, others were granted their freedom by their masters.\n\nWhen Ponce de León and the Spaniards arrived on the island of Borikén (Puerto Rico), they Taíno tribes on the island, forcing them to work in the gold mines and in the construction of forts. Many Taíno died, particularly due to smallpox, of which they had no immunity. Other Taínos committed suicide or left the island after the failed Taíno revolt of 1511. The Spanish colonists, fearing the loss of their labor force, complained the courts that they needed manpower to work in the mines, build forts, and work sugar cane plantations. As an alternative, Las Casas suggested the importation and use of African slaves. In 1517, the Spanish Crown permitted its subjects to import twelve slaves each, thereby beginning the slave trade on the colonies.\n\nAfrican slaves were legally branded with a hot iron on the forehead, prevented their \"theft\" or lawsuits that challenged their captivity. The colonist continued this branding practice for more than 250 years. They were sent to work in the gold mines, or in the island's ginger and sugar fields. They were allowed to live with their families in a hut on the master's land, and given a patch of land where they could farm, but otherwise were subjected to harsh treatment; including sexual abuse as the majority of colonists had arrived without women; many of them intermarried with the Africans or Taínos. Their mixed-race descendants formed the first generations of the early Puerto Rican population.\n\nThe slaves faced heavy discrimination, and had no opportunity for advancement, though they were educated by their masters. The Spaniards considered the Africans superior to the Taíno, since the latter were unwilling to assimilate. The slaves, in contrast, had little choice but to adapt. Many converted to Christianity and were given their masters' surnames.\n\nBy 1570, the colonists found that the gold mines were depleted, relegating the island to a garrison for passing ships. The cultivation of crops such as tobacco, cotton, cocoa, and ginger became the cornerstone of the economy. With rising demand for sugar on the international market, major planters increased their labor-intensive cultivation and processing of sugar cane. Sugar plantations supplanted mining as Puerto Rico's main industry and kept demand high for African slavery.\n\nAfter 1784, Spain provided five ways by which slaves could obtain freedom. Five years later, the Spanish Crown issued the \"Royal Decree of Graces of 1789\", which set new rules related to the slave trade and added restrictions to the granting of freedman status. The decree granted its subjects the right to purchase slaves and to participate in the flourishing slave trade in the Caribbean. Later that year a new slave code, also known as \"El Código Negro\" (The Black Code), was introduced.\n\nUnder \"El Código Negro\", a slave could buy his freedom, in the event that his master was willing to sell, by paying the price sought in installments. Slaves were allowed to earn money during their spare time by working as shoemakers, cleaning clothes, or selling the produce they grew on their own plots of land. For the freedom of their newborn child, not yet baptized, they paid at half the going price for a baptized child. Many of these freedmen started settlements in the areas which became known as Cangrejos (Santurce), Carolina, Canóvanas, Loíza, and Luquillo. Some became slave owners themselves.\n\nDespite these paths to freedom, from 1790 onwards, the number of slaves more than doubled in Puerto Rico as a result of the dramatic expansion of the sugar industry in the island.\n\nOn March 22, 1873, slavery was legally abolished in Puerto Rico. However, slaves were not emancipated but rather had to \"buy\" their own freedom, at whatever price was set by their last masters. They were also required to work for another three years for their former masters, for other colonists interested in their services, or for the state in order to pay some compensation.\n\nBetween 1527 and 1873, slaves in Puerto Rico had carried out more than twenty revolts.\n\nThe planters of the Dutch colony relied heavily on African slaves to cultivate, harvest and process the commodity crops of coffee, cocoa, sugar cane and cotton plantations along the rivers. Planters' treatment of the slaves was notoriously bad. Historian C. R. Boxer wrote that \"man's inhumanity to man just about reached its limits in Surinam.\"\n\nMany slaves escaped the plantations. With the help of the native South Americans living in the adjoining rain forests, these runaway slaves established a new and unique culture in the interior that was highly successful in its own right. They were known collectively in English as Maroons, in French as \"Nèg'Marrons\" (literally meaning \"brown negroes\", that is \"pale-skinned negroes\"), and in Dutch as \"Marrons.\" The Maroons gradually developed several independent tribes through a process of ethnogenesis, as they were made up of slaves from different African ethnicities. These tribes include the Saramaka, Paramaka, Ndyuka or Aukan, Kwinti, Aluku or Boni, and Matawai.\n\nThe Maroons often raided plantations to recruit new members from the slaves and capture women, as well as to acquire weapons, food and supplies. They sometimes killed planters and their families in the raids. The colonists also mounted armed campaigns against the Maroons, who generally escaped through the rain forest, which they knew much better than did the colonists.\n\nTo end hostilities, in the 18th century the European colonial authorities signed several peace treaties with different tribes. They granted the Maroons sovereign status and trade rights in their inland territories, giving them autonomy.\n\nIn 1861-63, President Abraham Lincoln of the United States and his administration looked abroad for places to relocate freed slaves who wanted to leave the United States. It opened negotiations with the Dutch government regarding African-American emigration to and colonization of the Dutch colony of Suriname in South America. Nothing came of the idea and, after 1864, the idea was dropped.\n\nThe Netherlands abolished slavery in Suriname, in 1863, under a gradual process that required slaves to work on plantations for 10 transition years for minimal pay, which was considered as partial compensation for their masters. After 1873, most freedmen largely abandoned the plantations where they had worked for several generations in favor of the capital city, Paramaribo.\n\nSlavery in the United States was the legal institution of human chattel enslavement, primarily of Africans and African Americans, that existed in the United States of America in the 18th and 19th centuries after it gained independence and before the end of the American Civil War. Slavery had been practiced in British America from early colonial days, and was legal in all Thirteen Colonies at the time of the Declaration of Independence in 1776.\n\nBy the time of the American Revolution (1775–1783), the status of slave had been institutionalized as a racial caste associated with African ancestry. The United States became polarized over the issue of slavery, represented by the slave and free states divided by the Mason–Dixon line, which separated free Pennsylvania from slave Maryland and Delaware.\n\nCongress, during the Jefferson administration prohibited the importation of slaves, effective 1808, although smuggling (illegal importing) was not unusual. Domestic slave trading, however, continued at a rapid pace, driven by labor demands from the development of cotton plantations in the Deep South. Those states attempted to extend slavery into the new Western territories to keep their share of political power in the nation.\n\nThe treatment of slaves in the United States varied widely depending on conditions, times and places. The power relationships of slavery corrupted many whites who had authority over slaves, with children showing their own cruelty. Masters and overseers resorted to physical punishments to impose their wills. Slaves were punished by whipping, shackling, hanging, beating, burning, mutilation, branding and imprisonment. Punishment was most often meted out in response to disobedience or perceived infractions, but sometimes abuse was carried out to re-assert the dominance of the master or overseer of the slave. Treatment was usually harsher on large plantations, which were often managed by overseers and owned by absentee slaveholders, conditions permitting abuses.\nWilliam Wells Brown, who escaped to freedom, reported that on one plantation, slave men were required to pick 80 pounds per day of cotton, while women were required to pick 70 pounds; if any slave failed in his or her quota, they were subject to whip lashes for each pound they were short. The whipping post stood next to the cotton scales. A New York man who attended a slave auction in the mid-19th century reported that at least three-quarters of the male slaves he saw at sale had scars on their backs from whipping. By contrast, small slave-owning families had closer relationships between the owners and slaves; this sometimes resulted in a more humane environment but was not a given.\n\nMore than one million slaves were sold from the Upper South, which had a surplus of labor, and taken to the Deep South in a forced migration, splitting up many families. New communities of African-American culture were developed in the Deep South, and the total slave population in the South eventually reached 4 million before liberation.\n\nIn the 19th century, proponents of slavery often defended the institution as a \"necessary evil\". White people of that time feared that emancipation of black slaves would have more harmful social and economic consequences than the continuation of slavery. The French writer and traveler Alexis de Tocqueville, in \"Democracy in America\" (1835), expressed opposition to slavery while observing its effects on American society. He felt that a multiracial society without slavery was untenable, as he believed that prejudice against blacks increased as they were granted more rights. Others, like James Henry Hammond argued that slavery was a \"positive good\" stating: \"Such a class you must have, or you would not have that other class which leads progress, civilization, and refinement.\"\n\nThe Southern state governments wanted to keep a balance between the number of slave and free states to maintain a political balance of power in Congress. The new territories acquired from Britain, France, and Mexico were the subject of major political compromises. By 1850, the newly rich cotton-growing South was threatening to secede from the Union, and tensions continued to rise. Many white Southern Christians, including church ministers, attempted to justify their support for slavery as modified by Christian paternalism. The largest denominations, the Baptist, Methodist, and Presbyterian churches, split over the slavery issue into regional organizations of the North and South.\nWhen Abraham Lincoln won the 1860 election on a platform of halting the expansion of slavery, according to the 1860 U.S. census, roughly 400,000 individuals, representing 8 percent of all US families, owned nearly 4,000,000 slaves. One-third of Southern families owned slaves. The south was heavily invested in slavery. As such, upon Lincoln's election, seven states broke away to form the Confederacy. The first six states to secede held the greatest number of slaves in the South. Shortly after, over the issue of slavery, the United States erupted into an all out Civil War, with slavery not legally ceasing as an institution, until December 1865.\n\nIn 2018, the \"Orlando Sentinel\" reported some private Christian schools in Florida as teaching students a Creationist curriculum which includes assertions such as, “most black and white southerners had long lived together in harmony” and that “power-hungry individuals stirred up the people” leading to the Civil Rights Movement.\n\nSlavery has existed all throughout Asia, and forms of slavery still exist today.\n\nSlavery has taken various forms throughout China's history. It was reportedly abolished as a legally recognized institution, including in a 1909 law fully enacted in 1910, although the practice continued until at least 1949.\n\nThe Tang dynasty purchased Western slaves from the Radhanite Jews. Tang Chinese soldiers and pirates enslaved Koreans, Turks, Persians, Indonesians, and people from Inner Mongolia, central Asia, and northern India. The greatest source of slaves came from southern tribes, including Thais and aboriginals from the southern provinces of Fujian, Guangdong, Guangxi, and Guizhou. Malays, Khmers, Indians, and \"black skinned\" peoples (who were either Austronesian Negritos of Southeast Asia and the Pacific Islands, or Africans, or both) were also purchased as slaves in the Tang dynasty.\n\nIn the 17th century Qing Dynasty, there was a hereditarily servile people called \"Booi Aha\" (Manchu:booi niyalma; Chinese transliteration: 包衣阿哈), which is a Manchu word literally translated as \"household person\" and sometimes rendered as \"nucai.\" The Manchu was establishing close personal and paternalist relationship between masters and their slaves, as Nurhachi said, \"The Master should love the slaves and eat the same food as him\". However, booi aha \"did not correspond exactly to the Chinese category of \"bond-servant slave\" (Chinese:奴僕); instead, it was a relationship of personal dependency on a master which in theory guaranteed close personal relationships and equal treatment, even though many western scholars would directly translate \"booi\" as \"bond-servant\" (some of the \"booi\" even had their own servant).\n\nChinese Muslim (Tungans) Sufis who were charged with practicing xiejiao (heterodox religion), were punished by exile to Xinjiang and being sold as a slave to other Muslims, such as the Sufi begs.\n\nHan Chinese who committed crimes such as those dealing with opium became slaves to the begs, this practice was administered by Qing law. Most Chinese in Altishahr were exile slaves to Turkestani Begs. Ironically, while free Chinese merchants generally did not engage in relationships with East Turkestani women, some of the Chinese slaves belonging to begs, along with Green Standard soldiers, Bannermen, and Manchus, engaged in affairs with the East Turkestani women that were serious in nature.\n\nSlavery in India intensified during the Muslim domination of northern India after the 11th-century, however, Muslim rulers did not introduce slavery to the subcontinent. Slavery existed in Portuguese India after the 16th century. The Dutch, too, largely dealt in Abyssian slaves, known in India as Habshis or Sheedes.\n\nArakan/Bengal, Malabar, and Coromandel remained the most important source of forced labour until the 1660s. Between 1626 and 1662, the Dutch exported on an average 150–400 slaves annually from the Arakan-Bengal coast. During the first thirty years of Batavia's existence, Indian and Arakanese slaves provided the main labour force of the Dutch East India Company, Asian headquarters.\n\nAn increase in Coromandel slaves occurred during a famine following the revolt of the Nayaka Indian rulers of South India (Tanjavur, Senji, and Madurai) against Bijapur overlordship (1645) and the subsequent devastation of the Tanjavur countryside by the Bijapur army. Reportedly, more than 150,000 people were taken by the invading Deccani Muslim armies to Bijapur and Golconda. In 1646, 2,118 slaves were exported to Batavia, the overwhelming majority from southern Coromandel. Some slaves were also acquired further south at Tondi, Adirampatnam, and Kayalpatnam. Another increase in slaving took place between 1659 and 1661 from Tanjavur as a result of a series of successive Bijapuri raids. At Nagapatnam, Pulicat, and elsewhere, the company purchased 8,000–10,000 slaves, the bulk of whom were sent to Ceylon while a small portion were exported to Batavia and Malacca. Finally, following a long drought in Madurai and southern Coromandel, in 1673, which intensified the prolonged Madurai-Maratha struggle over Tanjavur and punitive fiscal practices, thousands of people from Tanjavur, mostly girls and little boys, were sold into slavery and exported by Asian traders from Nagapattinam to Aceh, Johor, and other slave markets.\n\nIn September 1687, 665 slaves were exported by the English from Fort St. George, Madras. And, in 1694–96, when warfare once more ravaged South India, a total of 3,859 slaves were imported from Coromandel by private individuals into Ceylon.\n\nThe volume of the total Dutch Indian Ocean slave trade has been estimated to be about 15–30 percent of the Atlantic slave trade, slightly smaller than the trans-Saharan slave trade, and one-and-a-half to three times the size of the Swahili and Red Sea coast and the Dutch West India Company slave trades.\n\nAccording to Sir Henry Bartle Frere (who sat on the Viceroy's Council), there were an estimated 8 or 9 million slaves in India in 1841. About 15 percent of the population of Malabar were slaves. Slavery was legally abolished in the possessions of the East India Company by the Indian Slavery Act, 1843.\n\nThe hill tribe people in Indochina were \"hunted incessantly and carried off as slaves by the Siamese (Thai), the Anamites (Vietnamese), and the Cambodians\". A Siamese military campaign in Laos in 1876 was described by a British observer as having been \"transformed into slave-hunting raids on a large scale\". The census, taken in 1879, showed that 6% of the population in the Malay sultanate of Perak were slaves. Enslaved people made up about two-thirds of the population in part of North Borneo in the 1880s.\n\nAfter the Portuguese first made contact with Japan in 1543, a large scale slave trade developed in which Portuguese purchased Japanese as slaves in Japan and sold them to various locations overseas, including Portugal itself, throughout the sixteenth and seventeenth centuries. Many documents mention the large slave trade along with protests against the enslavement of Japanese. Japanese slaves are believed to be the first of their nation to end up in Europe, and the Portuguese purchased large numbers of Japanese slave girls to bring to Portugal for sexual purposes, as noted by the Church in 1555. Sebastian of Portugal feared that this was having a negative effect on Catholic proselytization since the slave trade in Japanese was growing to massive proportions, so he commanded that it be banned in 1571.\n\nJapanese slave women were even sold as concubines to Asian lascar and African crewmembers, along with their European counterparts serving on Portuguese ships trading in Japan, mentioned by Luis Cerqueira, a Portuguese Jesuit, in a 1598 document. Japanese slaves were brought by the Portuguese to Macau, where some of them not only ended up being enslaved to Portuguese, but as slaves to other slaves, with the Portuguese owning Malay and African slaves, who in turn owned Japanese slaves of their own.\n\nHideyoshi was so disgusted that his own Japanese people were being sold \"en masse\" into slavery on Kyushu, that he wrote a letter to Jesuit Vice-Provincial Gaspar Coelho on July 24, 1587, to demand the Portuguese, Siamese (Thai), and Cambodians stop purchasing and enslaving Japanese and return Japanese slaves who ended up as far as India. Hideyoshi blamed the Portuguese and Jesuits for this slave trade and banned Christian proselytizing as a result.\n\nSome Korean slaves were bought by the Portuguese and brought back to Portugal from Japan, where they had been among the tens of thousands of Korean prisoners of war transported to Japan during the Japanese invasions of Korea (1592–98). Historians pointed out that at the same time Hideyoshi expressed his indignation and outrage at the Portuguese trade in Japanese slaves, he himself was engaging in a mass slave trade of Korean prisoners of war in Japan.\n\nFillippo Sassetti saw some Chinese and Japanese slaves in Lisbon among the large slave community in 1578, although most of the slaves were black.\n\nThe Portuguese \"highly regarded\" Asian slaves like Chinese and Japanese, much more \"than slaves from sub-Saharan Africa\". The Portuguese attributed qualities like intelligence and industriousness to Chinese and Japanese slaves which is why they favoured them.\n\nIn 1595 a law was passed by Portugal banning the selling and buying of Chinese and Japanese slaves.\n\nDuring the Joseon period, the nobi population could fluctuate up to about one-third of the population, but on average the nobi made up about 10% of the total population. The nobi system declined beginning in the 18th century. Since the outset of the Joseon dynasty and especially beginning in the 17th century, there was harsh criticism among prominent thinkers in Korea about the nobi system. Even within the Joseon government, there were indications of a shift in attitude toward the nobi. King Yeongjo implemented a policy of gradual emancipation in 1775, and he and his successor King Jeongjo made many proposals and developments that lessened the burden on nobi, which led to the emancipation of the vast majority of government nobi in 1801. In addition, population growth, numerous escaped slaves, growing commercialization of agriculture, and the rise of the independent small farmer class contributed to the decline in the number of nobi to about 1.5% of the total population by 1858. The hereditary nobi system was officially abolished around 1886–87, and the rest of the nobi system was abolished with the Gabo Reform of 1894. However, slavery did not completely disappear in Korea until 1930, during Imperial Japanese rule.\n\nDuring the Imperial Japanese occupation of Korea around World War II, some Koreans were used in forced labor by the Imperial Japanese, in conditions which have been compared to slavery. These included women forced into sexual slavery by the Imperial Japanese Army before and during World War II, known as \"comfort women\".\n\nDuring the Second World War (1939–1945) Nazi Germany effectively enslaved about 12 million people, both those considered undesirable and citizens of conquered countries, with the avowed intention of treating these \"Untermenschen\" (sub-humans) as a permanent slave-class of inferior beings who could be worked until they died, and who possessed neither the rights nor the legal status of members of the Aryan race.\n\nSlaves (\"he mōkai\") had a recognised social role in traditional Māori society in New Zealand.\n\nBlackbirding occurred in the Pacific, especially in the 19th century.\n\nIn Constantinople, about one-fifth of the population consisted of slaves. The city was a major centre of the slave trade in the 15th and later centuries.\nSlaves were provided by Tatar raids on Slavic villages but also by conquest and the suppression of rebellions, in the aftermath of which, entire populations were sometimes enslaved and sold across the Empire, reducing the risk of future rebellion. The Ottomans also purchased slaves from traders who brought slaves into the Empire from Europe and Africa. It has been estimated that some 200,000 slaves – mainly Circassians – were imported into the Ottoman Empire between 1800 and 1909. As late as 1908, women slaves were still sold in the Ottoman Empire.\n\nUntil the late 18th century, the Crimean Khanate (a Muslim Tatar state) maintained a massive slave trade with the Ottoman Empire and the Middle East. The slaves were captured in southern Russia, Poland-Lithuania, Moldavia, Wallachia, and Circassia by Tatar horsemen and sold in the Crimean port of Kaffa. About 2 million, mostly Christian, slaves were exported over the period 1500–1700 until the Crimean Khanate was destroyed by the Russian Empire in 1783.\nA slave market for captured Russian and Persian slaves was centred in the Central Asian khanate of Khiva. In the early 1840s, the population of the Uzbek states of Bukhara and Khiva included about 900,000 slaves. Darrel P. Kaiser wrote, \"Kazakh-Kirghiz tribesmen kidnapped 1573 settlers from colonies [German settlements in Russia] in 1774 alone and only half were successfully ransomed. The rest were killed or enslaved.\"\n\nDuring the Second Libyan Civil War, Libyans started capturing Sub-Saharan African migrants trying to get to Europe through Libya and selling them on slave markets or holding them hostage for ransom Women are often raped, used as sex slaves, or sold to brothels. Child migrants also suffer from abuse and child rape in Libya.\n\nIn Mauritania, the last country to abolish slavery (in 1981), it is estimated that 20 percent of its 3 million population, are enslaved as bonded laborers. Slavery in Mauritania was criminalized in August 2007. However, although slavery, as a practice, was legally banned in 1981, it was not a crime to own a slave until 2007. Although many slaves have escaped or have been freed since 2007, , only one slave-owner had been sentenced to serve time in prison. Slavery in Niger is also a current phenomenon.\n\nEven though slavery is now outlawed in every country, the number of slaves today is estimated as between 12 million and 29.8 million. Several estimates of the number of slaves in the world have been provided. According to a broad definition of slavery, there were 27 million people in slavery in 1999, spread all over the world. In 2005, the International Labour Organization provided an estimate of 12.3 million forced labourers. Siddharth Kara has also provided an estimate of 28.4 million slaves at the end of 2006 divided into three categories: bonded labour/debt bondage (18.1 million), forced labour (7.6 million), and trafficked slaves (2.7 million). Kara provides a dynamic model to calculate the number of slaves in the world each year, with an estimated 29.2 million at the end of 2009.\nAccording to a 2003 report by Human Rights Watch, an estimated 15 million children in debt bondage in India work in slavery-like conditions to pay off their family's debts.\n\nA report by the Walk Free Foundation in 2013, found India had the highest number of slaves, nearly 14 million, followed by China (2.9 million), Pakistan (2.1 million), Nigeria, Ethiopia, Russia, Thailand, Democratic Republic of Congo, Myanmar and Bangladesh; while the countries with the highest proportions of slaves were Mauritania, Haiti, Pakistan, India and Nepal.\n\nIn June 2013, U.S. State Department released a report on slavery. It placed Russia, China, Uzbekistan in the worst offenders category. Cuba, Iran, North Korea, Sudan, Syria, and Zimbabwe were also at the lowest level. The list also included Algeria, Libya, Saudi Arabia and Kuwait among a total of 21 countries.\n\nThe Walk Free Foundation reported in 2018 that slavery in wealthy Western societies is much more prevalent than previously known, in particular the United States and Great Britain, which have 403,000 (one in 800) and 136,000 slaves respectively. Andrew Forrest, founder of the organization, said that \"The United States is one of the most advanced countries in the world yet has more than 400,000 modern slaves working under forced labor conditions.\" An estimated 40.3 million are enslaved globally, with North Korea having the most slaves at 2.6 million (one in 10). The Foundation defines contemporary slavery as \"situations of exploitation that a person cannot refuse or leave because of threats, violence, coercion, abuse of power, or deception.\"\n\nWhile American slaves in 1809 were sold for around $40,000 (in inflation adjusted dollars), a slave nowadays can be bought for just $90, making replacement more economical than providing long term care. Slavery is a multibillion-dollar industry with estimates of up to $35 billion generated annually.\n\nTrafficking in human beings (also called human trafficking) is one method of obtaining slaves. Victims are typically recruited through deceit or trickery (such as a false job offer, false migration offer, or false marriage offer), sale by family members, recruitment by former slaves, or outright abduction. Victims are forced into a \"debt slavery\" situation by coercion, deception, fraud, intimidation, isolation, threat, physical force, debt bondage or even force-feeding with drugs of abuse to control their victims. \"Annually, according to U.S. government-sponsored research completed in 2006, approximately 800,000 people are trafficked across national borders, which does not include millions trafficked within their own countries. Approximately 80 percent of transnational victims are women and girls, and up to 50 percent are minors, reports the U.S. State Department in a 2008 study.\n\nWhile the majority of trafficking victims are women, and sometimes children, who are forced into prostitution (in which case the practice is called sex trafficking), victims also include men, women and children who are forced into manual labour. Due to the illegal nature of human trafficking, its exact extent is unknown. A U.S. government report, published in 2005, estimates that about 700,000 people worldwide are trafficked across borders each year. This figure does not include those who are trafficked internally. Another research effort revealed that roughly 1.5 million individuals are trafficked either internally or internationally each year, of which about 500,000 are sex trafficking victims.\n\nSlavery has existed, in one form or another, through recorded human history – as have, in various periods, movements to free large or distinct groups of slaves.\n\nAshoka, who ruled the Maurya Empire from 269–232 BCE, abolished the slave trade but not slavery. The Qin dynasty, which ruled China from 221 to 206 BC, abolished slavery and discouraged serfdom. However, many of its laws were overturned when the dynasty was overthrown. Slavery was again abolished, by Wang Mang, in China in 17 CE but was reinstituted after his assassination.\n\nThe Spanish colonization of the Americas sparked a discussion about the right to enslave Native Americans. A prominent critic of slavery in the Spanish New World colonies was Bartolomé de las Casas, who opposed the enslavement of Native Americans, and later also of Africans in America.\n\nOne of the first protests against slavery came from German and Dutch Quakers in Pennsylvania in 1688. One of the most significant milestones in the campaign to abolish slavery throughout the world occurred in England in 1772, with British judge Lord Mansfield, whose opinion in Somersett's Case was widely taken to have held that slavery was illegal in England. This judgement also laid down the principle that slavery contracted in other jurisdictions could not be enforced in England. In 1777, Vermont, at the time an independent nation, became the first portion of what would become the United States to abolish slavery. France abolished slavery in 1794. All the Northern states abolished slavery; New Jersey in 1804 was the last to act. None of the Southern or border states abolished slavery before the American Civil War.\n\nSons of Africa was a late 18th-century British group that campaigned to end slavery. Its members were Africans in London, freed slaves who included Ottobah Cugoano, Olaudah Equiano and other leading members of London's black community. It was closely connected to the Society for the Abolition of the Slave Trade, a non-denominational group founded in 1787, whose members included Thomas Clarkson.\n\nBritish Member of Parliament William Wilberforce led the anti-slavery movement in the United Kingdom, although the groundwork was an anti-slavery essay by Thomas Clarkson. Wilberforce was also urged by his close friend, Prime Minister William Pitt the Younger, to make the issue his own, and was also given support by reformed Evangelical John Newton. The Slave Trade Act was passed by the British Parliament on March 25, 1807, making the slave trade illegal throughout the British Empire, Wilberforce also campaigned for abolition of slavery in the British Empire, which he lived to see in the Slavery Abolition Act 1833. After the 1807 act abolishing the slave trade was passed, these campaigners switched to encouraging other countries to follow suit, notably France and the British colonies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard. Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against \"the usurping King of Lagos\", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.\n\nIn 1839, the world's oldest international human rights organization, Anti-Slavery International, was formed in Britain by Joseph Sturge, which campaigned to outlaw slavery in other countries. There were celebrations in 2007 to commemorate the 200th anniversary of the abolition of the slave trade in the United Kingdom through the work of the British Anti-Slavery Society.\n\nIn the United States, abolitionist pressure produced a series of small steps towards emancipation. After the Act Prohibiting Importation of Slaves went into effect on January 1, 1808, the importation of slaves into the United States was prohibited, but not the internal slave trade, nor involvement in the international slave trade externally. Legal slavery persisted; most of those slaves already in the U.S. were legally emancipated only in 1863. Many American abolitionists took an active role in opposing slavery by supporting the Underground Railroad. Violent clashes between anti-slavery and pro-slavery Americans included Bleeding Kansas, a series of political and armed disputes in 1854–1861 as to whether Kansas would join the United States as a slave or free state. By 1860, the total number of slaves reached almost four million, and the American Civil War, beginning in 1861, led to the end of slavery in the United States. In 1863, Lincoln issued the Emancipation Proclamation, which freed slaves held in the Confederate States; the 13th Amendment to the U. S. Constitution prohibited most forms of slavery throughout the country.\n\nIn the case of freed slaves of the United States, many became sharecroppers and indentured servants. In this manner, some became tied to the very parcel of land into which they had been born a slave having little freedom or economic opportunity due to Jim Crow laws which perpetuated discrimination, limited education, promoted persecution without due process and resulted in continued poverty. Fear of reprisals such as unjust incarcerations and lynchings deterred upward mobility further.\n\nIn the 1860s, David Livingstone's reports of atrocities within the Arab slave trade in Africa stirred up the interest of the British public, reviving the flagging abolitionist movement. The Royal Navy throughout the 1870s attempted to suppress \"this abominable Eastern trade\", at Zanzibar in particular. In 1905, the French abolished indigenous slavery in most of French West Africa.\n\nOn December 10, 1948, the United Nations General Assembly adopted the Universal Declaration of Human Rights, which declared freedom from slavery is an internationally recognized human right. Article 4 of the Universal Declaration of Human Rights states:\nIn 2014, for the first time in history, major leaders of many religions, Buddhist, Anglican, Catholic, Orthodox Christian, Hindu, Jewish, and Muslim met to sign a shared commitment against modern-day slavery; the declaration they signed calls for the elimination of slavery and human trafficking by the year 2020. The signatories were: Pope Francis, Mātā Amṛtānandamayī, Bhikkhuni Thich Nu Chân Không (representing Zen Master Thích Nhất Hạnh), Datuk K Sri Dhammaratana, Chief High Priest of Malaysia, Rabbi Abraham Skorka, Rabbi David Rosen, Abbas Abdalla Abbas Soliman, Undersecretary of State of Al Azhar Alsharif (representing Mohamed Ahmed El-Tayeb, Grand Imam of Al-Azhar), Grand Ayatollah Mohammad Taqi al-Modarresi, Sheikh Naziyah Razzaq Jaafar, Special advisor of Grand Ayatollah (representing Grand Ayatollah Sheikh Basheer Hussain al Najafi), Sheikh Omar Abboud, Justin Welby, Archbishop of Canterbury, and Metropolitan Emmanuel of France (representing Ecumenical Patriarch Bartholomew.)\n\nGroups such as the American Anti-Slavery Group, Anti-Slavery International, Free the Slaves, the Anti-Slavery Society, and the Norwegian Anti-Slavery Society continue to campaign to eliminate slavery.\n\nOn May 21, 2001, the National Assembly of France passed the Taubira law, recognizing slavery as a crime against humanity. Apologies on behalf of African nations, for their role in trading their countrymen into slavery, remain an open issue since slavery was practiced in Africa even before the first Europeans arrived and the Atlantic slave trade was performed with a high degree of involvement of several African societies. The black slave market was supplied by well-established slave trade networks controlled by local African societies and individuals. Indeed, as already mentioned in this article, slavery persists in several areas of West Africa until the present day.\n\nThere is adequate evidence citing case after case of African control of segments of the trade. Several African nations such as the Calabar and other southern parts of Nigeria had economies depended solely on the trade. African peoples such as the Imbangala of Angola and the Nyamwezi of Tanzania would serve as middlemen or roving bands warring with other African nations to capture Africans for Europeans.\n\nSeveral historians have made important contributions to the global understanding of the African side of the Atlantic slave trade. By arguing that African merchants determined the assemblage of trade goods accepted in exchange for slaves, many historians argue for African agency and ultimately a shared responsibility for the slave trade.\n\nIn 1999, President Mathieu Kerekou of Benin (formerly the Kingdom of Dahomey) issued a national apology for the central role Africans played in the Atlantic slave trade. Luc Gnacadja, minister of environment and housing for Benin, later said: \"The slave trade is a shame, and we do repent for it.\" Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin. President Jerry Rawlings of Ghana also apologized for his country's involvement in the slave trade.\n\nThe issue of an apology is linked to reparations for slavery and is still being pursued by a number of entities across the world. For example, the Jamaican Reparations Movement approved its declaration and action Plan.\n\nIn September 2006, it was reported that the UK government might issue a \"statement of regret\" over slavery. This was followed by a \"public statement of sorrow\" from Tony Blair on November 27, 2006, and a formal apology on March 14, 2007.\n\nOn February 25, 2007, the Commonwealth of Virginia resolved to 'profoundly regret' and apologize for its role in the institution of slavery. Unique and the first of its kind in the U.S., the apology was unanimously passed in both Houses as Virginia approached the 400th anniversary of the founding of Jamestown.\n\nOn August 24, 2007, Mayor Ken Livingstone of London, apologized publicly for Britain's role in colonial slave trade. \"You can look across there to see the institutions that still have the benefit of the wealth they created from slavery,\" he said, pointing towards the financial district. He claimed that London was still tainted by the horrors of slavery. Specifically, London outfitted, financed, and insured many of the ships, which helped fund the building of London's docks. The City of Liverpool, which was a large slave trading port, apologized in 1999.\n\nOn July 30, 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws. In June 2009, the US Senate passed a resolution apologizing to African-Americans for the \"fundamental injustice, cruelty, brutality, and inhumanity of slavery\". The news was welcomed by President Barack Obama, the nation's first President of African descent. Some of President Obama's ancestors may have been slave owners.\n\nIn 2010, Libyan leader Muammar Gaddafi apologized for Arab involvement in the slave trade, saying: \"I regret the behavior of the Arabs… They brought African children to North Africa, they made them slaves, they sold them like animals, and they took them as slaves and traded them in a shameful way.\"\n\nThere have been movements to achieve reparations for those formerly held as slaves, or sometimes their descendants. Claims for reparations for being held in slavery are handled as a civil law matter in almost every country. This is often decried as a serious problem, since former slaves' relative lack of money means they often have limited access to a potentially expensive and futile legal process. Mandatory systems of fines and reparations paid to an as yet undetermined group of claimants from fines, paid by unspecified parties, and collected by authorities have been proposed by advocates to alleviate this \"civil court problem.\" Since in almost all cases there are no living ex-slaves or living ex-slave owners these movements have gained little traction. In nearly all cases the judicial system has ruled that the statute of limitations on these possible claims has long since expired.\n\nThe word \"slavery\" is often used as a pejorative to describe any activity in which one is coerced into performing.\n\nSome argue that military drafts, and other forms of coerced government labour constitute \"state-operated slavery.\" Some libertarians and anarcho-capitalists view government taxation as a form of slavery.\n\n\"Slavery\" has been used by some anti-psychiatry proponents to define involuntary psychiatric patients due to there are no unbiased physical tests for mental illness, yet the psychiatric patient must follow the orders of his/her psychiatrist. Instead of chains to control the slave, the psychiatrist uses drugs to control the slaves mind. Drapetomania was a psychiatric diagnosis for a slave who did not want to be a slave. Thomas Szasz wrote a book titled \"Psychiatric Slavery\", published in 1998 and a book titled \"Liberation by Oppression: A Comparative Study of Slavery and Psychiatry\", published in 2003.\n\nProponents of animal rights apply the term \"slavery\" to the condition of some or all human-owned animals, arguing that their status is comparable to that of human slaves.\n\nThe labor market, as institutionalized under today's market economic systems, has been criticized by mainstream socialists and by anarcho-syndicalists, who utilise the term wage slavery as a pejorative or dysphemism for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.\n\nFilm has been the most influential medium in the presentation of the history of slavery to the general public around the world. The American film industry has had a complex relationship with slavery and until recent decades often avoided the topic. Films such as \"Birth of a Nation\" (1915) and \"Gone with the Wind\" (1939) became controversial because they gave a favourable depiction. The last favourable treatment was \"Song of the South\" from Disney in 1946. In 1940 \"The Santa Fe Trail\" gave a liberal but ambiguous interpretation of John Brown's attacks on slavery – the film does not know what to do with slavery. The Civil Rights Movement in the 1950s made defiant slaves into heroes. The question of slavery in American memory necessarily involves its depictions in feature films. \n\nMost Hollywood films used American settings, although \"Spartacus\" (1960), dealt with an actual revolt in the Roman Empire known as the Third Servile War. It failed and all the rebels were executed, but their spirit lived on according to the film. \"The Last Supper\" (\"La última cena\" in Spanish) was a 1976 film directed by Cuban Tomás Gutiérrez Alea about the teaching of Christianity to slaves in Cuba, and emphasizes the role of ritual and revolt. \"Burn!\" takes place on the imaginary Portuguese island of Queimada (where the locals speak Spanish) and it merges historical events that took place in Brazil, Cuba, Santo Domingo, Jamaica, and elsewhere. \"Spartacus\" stays surprisingly close to the historical record.\n\nHistorians agree that films have largely shaped historical memories, but they debate issues of accuracy, plausibility, moralism, sensationalism, how facts are stretched in search of broader truths, and suitability for the classroom. Berlin argues that critics complain if the treatment emphasizes historical brutality, or if it glosses over the harshness to highlight the emotional impact of slavery.\n\n\n\n\n\n\n"}
{"id": "234984", "url": "https://en.wikipedia.org/wiki?curid=234984", "title": "Social movement", "text": "Social movement\n\nA social movement is a type of group action. There is no single consensus definition of a social movement. They are large, sometimes informal, groupings of individuals or organizations which focus on specific political or social issues. In other words, they carry out, resist, or undo a social change. They provide a way of social change from the bottom within nations.\n\nSocial movements can be defined as \"organizational structures and strategies that may empower oppressed populations to mount effective challenges and resist the more powerful and advantaged elites\".\n\nPolitical science and sociology have developed a variety of theories and empirical research on social movements. For example, some research in political science highlights the relation between popular movements and the formation of new political parties as well as discussing the function of social movements in relation to agenda setting and influence on politics. Sociologists distinguish between several types of social movement examining things such as scope, type of change, method of work, range, and time frame.\n\nModern Western social movements became possible through education (the wider dissemination of literature) and increased mobility of labor due to the industrialization and urbanization of 19th-century societies. It is sometimes argued that the freedom of expression, education and relative economic independence prevalent in the modern Western culture are responsible for the unprecedented number and scope of various contemporary social movements. Many of the social movements of the last hundred years grew up, like the Mau Mau in Kenya, to oppose Western colonialism. Social movements have been and continue to be closely connected with democratic political systems. Occasionally, social movements have been involved in democratizing nations, but more often they have flourished after democratization. Over the past 200 years, they have become part of a popular and global expression of dissent.\n\nModern movements often utilize technology and the internet to mobilize people globally. Adapting to communication trends is a common theme among successful movements. Research is beginning to explore how advocacy organizations linked to social movements in the U.S. and Canada use social media to facilitate civic engagement and collective action.\n\nMario Diani argues that nearly all definitions share three criteria: \"a network of informal interactions between a plurality of individuals, groups and/or organizations, engaged in a political or cultural conflict, on the basis of a shared collective identity\"\n\nSociologist Charles Tilly defines social movements as a series of contentious performances, displays and campaigns by which ordinary people make collective claims on others. For Tilly, social movements are a major vehicle for ordinary people's participation in public politics. He argues that there are three major elements to a social movement:\n\n\nSidney Tarrow defines a social movement as \"collective challenges [to elites, authorities, other groups or cultural codes] by people with common purposes and solidarity in sustained interactions with elites, opponents and authorities.\" He specifically distinguishes social movements from political parties and advocacy groups.\n\nThe sociologists John McCarthy and Mayer Zald define as a social movement as \"a set of opinions and beliefs in a population which represents preferences for changing some elements of the social structure and/or reward distribution of a society.\"\n\nAccording to Paul van Seeters and Paul James defining a social movement entails a few minimal conditions of ‘coming together’: \n\nThe early growth of social movements was connected to broad economic and political changes in England in the mid-18th century, including political representation, market capitalization, and proletarianization.\n\nThe first mass social movement catalyzed around the controversial political figure John Wilkes. As editor of the paper \"The North Briton\", Wilkes vigorously attacked the new administration of Lord Bute and the peace terms that the new government accepted at the 1763 Treaty of Paris at the end of the Seven Years' War. Charged with seditious libel, Wilkes was arrested after the issue of a general warrant, a move that Wilkes denounced as unlawful - the Lord Chief Justice eventually ruled in Wilkes favour. As a result of this, Wilkes became a figurehead to the growing movement for popular sovereignty among the middle classes - people began chanting \"Wilkes and Liberty\" in the streets.\n\nAfter a later period of exile brought about by further charges of libel and obscenity, Wilkes stood for the Parliamentary seat at Middlesex, where most of his support was located. When Wilkes was imprisoned in the King's Bench Prison on 10 May 1768, a mass movement of support emerged, with large demonstrations in the streets under the slogan \"No liberty, no King.\"\n\nStripped of the right to sit in Parliament, Wilkes became an Alderman of London in 1769, and an activist group called the \"Society for the Supporters of the Bill of Rights\" began aggressively promoting his policies. This was the first ever sustained social movement: it involved public meetings, demonstrations, the distribution of pamphlets on an unprecedented scale and the mass petition march. However, the movement was careful not to cross the line into open rebellion; - it tried to rectify the faults in governance through appeals to existing legal precedents and was conceived of as an extra-Parliamentary form of agitation to arrive at a consensual and constitutional arrangement. The force and influence of this social movement on the streets of London compelled the authorities to concede to the movement's demands. Wilkes was returned to Parliament, general warrants were declared as unconstitutional and press freedom was extended to the coverage of Parliamentary debates.\nA much larger movement of anti-Catholic protest was triggered by the Papists Act 1778, which eliminated a number of the penalties and disabilities endured by Roman Catholics in England, and formed around Lord George Gordon, who became the President of the Protestant Association in 1779. The Association had the support of leading Calvinist religious figures, including Rowland Hill, Erasmus Middleton, and John Rippon. Gordon was an articulate propagandist and he inflamed the mob with fears of Papism and a return to absolute monarchical rule. The situation deteriorated rapidly, and in 1780, after a meeting of the Protestant Association, its members subsequently marched on the House of Commons to deliver a petition demanding the repeal of the Act, which the government refused to do. Soon, large riots broke out across London and embassies and Catholic owned businesses were attacked by angry mobs.\n\nOther political movements that emerged in the late 18th century included the British abolitionist movement against slavery (becoming one between the sugar boycott of 1791 and the second great petition drive of 1806), and possibly the upheaval surrounding the French and American Revolutions. In the opinion of Eugene Black (1963), \"...association made possible the extension of the politically effective public. Modern extra parliamentary political organization is a product of the late eighteenth century [and] the history of the age of reform cannot be written without it.\n\nFrom 1815, Britain after victory in the Napoleonic Wars entered a period of social upheaval characterised by the growing maturity of the use of social movements and special-interest associations. Chartism was the first mass movement of the growing working-class in the world. It campaigned for political reform between 1838 and 1848 with the People's Charter of 1838 as its manifesto – this called for universal suffrage and the implementation of the secret ballot, amongst other things. The term \"social movements\" was introduced in 1848 by the German Sociologist Lorenz von Stein in his book \"Socialist and Communist Movements since the Third French Revolution (1848)\" in which he introduced the term \"social movement\" into scholarly discussions - actually depicting in this way political movements fighting for the social rights understood as welfare rights.\nThe labor movement and socialist movement of the late 19th century are seen as the prototypical social movements, leading to the formation of communist and social democratic parties and organisations. These tendencies were seen in poorer countries as pressure for reform continued, for example in Russia with the Russian Revolution of 1905 and of 1917, resulting in the collapse of the Czarist regime around the end of the First World War.\n\nIn 1945, Britain after victory in the Second World War entered a period of radical reform and change. In the post-war period, Feminism, gay rights movement, peace movement, Civil Rights Movement, anti-nuclear movement and environmental movement emerged, often dubbed the New Social Movements They led, among other things, to the formation of green parties and organisations influenced by the new left. Some find in the end of the 1990s the emergence of a new global social movement, the anti-globalization movement. Some social movement scholars posit that with the rapid pace of globalization, the potential for the emergence of new \"type\" of social movement is latent—they make the analogy to national movements of the past to describe what has been termed a global citizens movement.\n\nSeveral key processes lie behind the history of social movements. Urbanization led to larger settlements, where people of similar goals could find each other, gather and organize. This facilitated social interaction between scores of people, and it was in urban areas that those early social movements first appeared. Similarly, the process of industrialization which gathered large masses of workers in the same region explains why many of those early social movements addressed matters such as economic wellbeing, important to the worker class. Many other social movements were created at universities, where the process of mass education brought many people together. With the development of communication technologies, creation and activities of social movements became easier – from printed pamphlets circulating in the 18th century coffeehouses to newspapers and Internet, all those tools became important factors in the growth of the social movements. Finally, the spread of democracy and political rights like the freedom of speech made the creation and functioning of social movements much easier.\n\nNascent social movements often fail to achieve their objectives because they fail to mobilize sufficient numbers of people. Srdja Popovic, author of Blueprint for Revolution, and spokesperson for OTPOR!, says that movements succeed when they address issues that people actually care about. “It’s unrealistic to expect people to care about more than what they already care about, and any attempt to make them do so is bound to fail.” Activists too often make the mistake of trying to convince people to address their issues. A mobilization strategy aimed at large-scale change often begins with action a small issue that concerns many people. For instance, Mahatma Gandhi’s successful overthrow of British rule in India began as a small protest focused on the British tax on salt.\n\nPopovic also argues that a social movement has little chance of growing if it relies on boring speeches and the usual placard waving marches. He argues for creating movements that people actually want to join. OTPOR! succeeded because it was fun, funny, and invented graphic ways of ridiculing dictator Slobodan Milosevic. It turned fatalism and passivity into action by making it easy, even cool, to become a revolutionary; branding itself within hip slogans, rock music and street theatre. Tina Rosenberg, in Join the Club, How Peer Pressure can Transform the World, shows how movements grow when there is a core of enthusiastic players who encourage others to join them.\n\nSociologists distinguish between several types of social movement:\n\nA difficulty for scholarship of movements is that for most, neither insiders to a movement nor outsiders apply consistent labels or even descriptive phrases. Unless there is a single leader who does, or a formal system of membership agreements, activists will typically use diverse labels and descriptive phrases that require scholars to discern when they are referring to the same or similar ideas, declare similar goals, adopt similar programs of action, and use similar methods. There can be great differences in the way that is done, to recognize who is and who is not a member or an allied group:\n\n\nIt is often outsiders rather than insiders that apply the identifying labels for a movement, which the insiders then may or may not adopt and use to self-identify. For example, the label for the levellers political movement in 17th-century England was applied to them by their antagonists, as a term of disparagement. Yet admirers of the movement and its aims later came to use the term, and it is the term by which they are known to history.\n\nCaution must always be exercised in any discussion of amorphous phenomena such as movements to distinguish between the views of insiders and outsiders, supporters and antagonists, each of whom may have their own purposes and agendas in characterization or mischaracterization of it.\n\nSocial movements have a life cycle: they are created, they grow, they achieve successes or failures and eventually, they dissolve and cease to exist.\n\nThey are more likely to evolve in the time and place which is friendly to the social movements: hence their evident symbiosis with the 19th century proliferation of ideas like individual rights, freedom of speech and civil disobedience. Social movements occur in liberal and authoritarian societies but in different forms. These new movements are activated by a wish for change in social customs, ethics and values which oppress certain communities. The birth of a social movement needs what sociologist Neil Smelser calls an \"initiating event\": a particular, individual event that will begin a chain reaction of events in the given society leading to the creation of a social movement. The root of this event must be the result of some common discontent among a community. Hence, making \"emergence\" the first step to a social movement. This discontent will act as the chain that links common people together, as they share the same experiences and feelings of oppression. \"Within this stage, social movements are very preliminary and there is little to no organization. Instead this stage can be thought of as widespread discontent (Macionis, 2001; Hopper, 1950).\" Emergence is prior to any sort of organized resistance to the condition of society. Jonathan Christiansen's essay on the four stages of social movement dissects further into the historical sociology of how each stage affects the whole movement. The Civil Rights Movement's early stages are an example of the public display of protest that is utilized to push a movement into the next stages. \"It was not until after the Brown v. the Board of Education Supreme court decision (1954), which outlawed segregation in Public schools, and following the arrest of Rosa Parks in Montgomery, Alabama for refusing to comply with segregation laws on city buses by giving up her bus seat to a white man, that the American Civil Rights Movement would proceed to the next stage – coalescence.\" The impact of a black woman, Rosa Parks, riding in the whites-only section of the bus (although she was not acting alone or spontaneously—typically activist leaders lay the groundwork behind the scenes of interventions designed to spark a movement). This leads into coalesce because now the common dilemma and source of oppression is being pinned down, allowing for organizations and appearance to the public eye to be established. The Polish Solidarity movement, which eventually toppled the communist regimes of Eastern Europe, developed after trade union activist Anna Walentynowicz was fired from work. The South African shack dwellers' movement Abahlali baseMjondolo grew out of a road blockade in response to the sudden selling off of a small piece of land promised for housing to a developer. Such an event is also described as a \"volcanic model\" – a social movement is often created after a large number of people realize that there are others sharing the same value and desire for a particular social change.\n\nThis third stage, bureaucratization, is when movements must become more organized, centered around a more systematic model. The set up and system for going about the construct must be more formal, with people taking on specific roles and responsibilities. \"In this phase their political power is greater than in the previous stages in that they may have more regular access to political elites.\" In this stage, one organization may take over another one in order to obtain a greater status and formal alliance. This 'taking over' may be a positive or negative move for organizations. Ella Baker, an activist who played a role in the NAACP, had proposed to the students of the student movement to start their own organization. This becomes known as the SNCC, the student nonviolent coordinating committee (1960s). The students could have join forces with the SCLC, an already existing organization, but that would have been a poor bureaucratizing decision, as they would succumb to old ideologies. New and progressive ideas that challenge prior authority are crucial to social change.\n\nThe declining of a social movement does not necessarily mean failure. There are multiple routes in which a movement may take before proceeding into decline. Success of a movement would result in permanent changes within the society and/or government that would result in a loss of need for protest. Failure is often the result of the incapability to keep a common focus, and work towards the goal in mind. \"Failure of social movements due to organizational or strategic failings is common for many organizations.\" Such a route would result in the gradual breaking up of an organization, and out of the stages of movement. Co-optation results when people or groups are integrated and shift away from the social movement's initial concerns and values. Repression is another example, when the movement is slowly wiped away from the public platform through means of an outside force, usually being the government. The last route into declining is going mainstream, which is generally perceived as an overall success. This is when goals of the movement are taken into society as a part of daily life, making it a 'social norm.' For example, birth control is still a greatly debated topic on a government level, but it has been accepted into social life as a common thing that exists.\n\nIt is important to recognize that though movements may disintegrate and cease to be active, the impact that they have in the social realm is success in its own way. It sparks the notion in new generations that the possibility to organize and make change is there.\n\nSociologists have developed several theories related to social movements [Kendall, 2005]. Some of the better-known approaches are outlined below. Chronologically they include:\n\nDeprivation theory argues that social movements have their foundations among people who feel deprived of some good(s) or resource(s). According to this approach, individuals who are lacking some good, service, or comfort are more likely to organize a social movement to improve (or defend) their conditions.\n\nThere are two significant problems with this theory. First, since most people feel deprived at one level or another almost all the time, the theory has a hard time explaining why the groups that form social movements do when other people are also deprived. Second, the reasoning behind this theory is circular – often the only evidence for deprivation is the social movement. If deprivation is claimed to be the cause but the only evidence for such is the movement, the reasoning is circular.\n\nMass society theory argues that social movements are made up of individuals in large societies who feel insignificant or socially detached. Social movements, according to this theory, provide a sense of empowerment and belonging that the movement members would otherwise not have.\n\nVery little support has been found for this theory. Aho (1990), in his study of Idaho Christian Patriotism, did not find that members of that movement were more likely to have been socially detached. In fact, the key to joining the movement was having a friend or associate who was a member of the movement.\n\nSocial strain theory, also known as value-added theory, proposes six factors that encourage social movement development:\n\nThis theory is also subject to circular reasoning as it incorporates, at least in part, deprivation theory and relies upon it, and social/structural strain for the underlying motivation of social movement activism. However, social movement activism is, like in the case of deprivation theory, often the only indication that there was strain or deprivation.\n\nResource mobilization theory emphasizes the importance of resources in social movement development and success. Resources are understood here to include: knowledge, money, media, labor, solidarity, legitimacy, and internal and external support from power elite. The theory argues that social movements develop when individuals with grievances are able to mobilize sufficient resources to take action.The emphasis on resources offers an explanation why some discontented/deprived individuals are able to organize while others are not.\n\nIn contrast to earlier collective behavior perspectives on social movements—which emphasized the role of exceptional levels of deprivation, grievance, or social strain in motivating mass protest—Resource Mobilization perspectives hold \"that there is always enough discontent in any society to supply the grass-roots support for a movement if the movement is effectively organized and has at its disposal the power and resources of some established elite group\" Movement emergence is contingent upon the aggregation of resources by social movement entrepreneurs and movement organizations, who use these resources to turn collective dissent in to political pressure. Members are recruited through networks; commitment is maintained by building a collective identity, and through interpersonal relationships. \n\nResource Mobilization Theory views social movement activity as \"politics by other means\": a rational and strategic effort by ordinary people to change society or politics. The form of the resources shapes the activities of the movement (e.g., access to a TV station will result in the extensive use TV media). Movements develop in contingent \"opportunity structures\" that influence their efforts to mobilize; and each movement's response to the opportunity structures depends on the movement's organization and resources \n\nCritics of this theory argue that there is too much of an emphasis on resources, especially financial resources. Some movements are effective without an influx of money and are more dependent upon the movement members for time and labor (e.g., the civil rights movement in the U.S.).\n\nPolitical process theory is similar to resource mobilization in many regards, but tends to emphasize a different component of social structure that is important for social movement development: political opportunities. Political process theory argues that there are three vital components for movement formation: insurgent consciousness, organizational strength, and political opportunities.\n\nInsurgent consciousness refers back to the ideas of deprivation and grievances. The idea is that certain members of society feel like they are being mistreated or that somehow the system is unjust. The insurgent consciousness is the collective sense of injustice that movement members (or potential movement members) feel and serves as the motivation for movement organization.\n\nOrganizational strength falls inline with resource-mobilization theory, arguing that in order for a social movement to organize it must have strong leadership and sufficient resources.\n\nPolitical opportunity refers to the receptivity or vulnerability of the existing political system to challenge. This vulnerability can be the result of any of the following (or a combination thereof):\n\nOne of the advantages of the political process theory is that it addresses the issue of timing or emergence of social movements. Some groups may have the insurgent consciousness and resources to mobilize, but because political opportunities are closed, they will not have any success. The theory, then, argues that all three of these components are important.\n\nCritics of the political process theory and resource-mobilization theory point out that neither theory discusses movement culture to any great degree. This has presented culture theorists an opportunity to expound on the importance of culture.\n\nOne advance on the political process theory is the \"political mediation model,\" which outlines the way in which the political context facing movement actors intersects with the strategic choices that movements make. An additional strength of this model is that it can look at the outcomes of social movements not only in terms of success or failure but also in terms of consequences (whether intentional or unintentional, positive or negative) and in terms of collective benefits.\n\nReflecting the cultural turn in the social sciences and humanities more broadly, recent strains of social movement theory and research add to the largely structural concerns seen in the resource mobilization and political process theories by emphasizing the cultural and psychological aspects of social movement processes, such as collectively shared interpretations and beliefs, ideologies, values and other meanings about the world. In doing so, this general cultural approach also attempts to address the free-rider problem. One particularly successful take on some such cultural dimensions is manifested in the framing perspective on social movements.\n\nWhile both resource mobilization theory and political process theory include, or at least accept, the idea that certain shared understandings of, for example, perceived unjust societal conditions must exist for mobilization to occur at all, this is not explicitly problematized within those approaches. The framing perspective has brought such shared understandings to the forefront of the attempt to understand movement creation and existence by, e.g., arguing that, in order for social movements to successfully mobilize individuals, they must develop an \"injustice frame\". An injustice frame is a collection of ideas and symbols that illustrate both how significant the problem is as well as what the movement can do to alleviate it,\n\nImportant characteristics of the injustice frames include:\nIn emphasizing the injustice frame, culture theory also addresses the free-rider problem. The free-rider problem refers to the idea that people will not be motivated to participate in a social movement that will use up their personal resources (e.g., time, money, etc.) if they can still receive the benefits without participating. In other words, if person X knows that movement Y is working to improve environmental conditions in his neighborhood, he is presented with a choice: join or not join the movement. If he believes the movement will succeed without him, he can avoid participation in the movement, save his resources, and still reap the benefits - this is \"free-riding\". A significant problem for social movement theory has been to explain why people join movements if they believe the movement can/will succeed without their contribution. Culture theory argues that, in conjunction with social networks being an important contact tool, the injustice frame will provide the motivation for people to contribute to the movement.\n\nFraming processes includes three separate components:\n\nFor more than ten years, social movement groups have been using the Internet to accomplish organizational goals. It has been argued that the Internet helps to increase the speed, reach and effectiveness of social movement-related communication as well as mobilization efforts, and as a result, it has been suggested that the Internet has had a positive impact on the social movements in general. The systematic literature review of Buettner & Buettner analyzed the role of Twitter during a wide range of social movements (2007 WikiLeaks, 2009 Moldova, 2009 Austria student protest, 2009 Israel-Gaza, 2009 Iran green revolution, 2009 Toronto G20, 2010 Venezuela, 2010 Germany Stuttgart21, 2011 Egypt, 2011 England, 2011 US Occupy movement, 2011 Spain Indignados, 2011 Greece Aganaktismenoi movements, 2011 Italy, 2011 Wisconsin labor protests, 2012 Israel Hamas, 2013 Brazil Vinegar, 2013 Turkey).\n\nMany discussions have been generated recently on the topic of social networking and the effect it may play on the formation and mobilization of social movement. For example, the emergence of the Coffee Party first appeared on the social networking site, Facebook. The party has continued to gather membership and support through that site and file sharing sites, such as Flickr. The 2009–2010 Iranian election protests also demonstrated how social networking sites are making the mobilization of large numbers of people quicker and easier. Iranians were able to organize and speak out against the election of Mahmoud Ahmadinejad by using sites such as Twitter and Facebook. This in turn prompted widespread government censorship of the web and social networking sites.\n\nThe sociological study of social movements is quite new. The traditional view of movements often perceived them as chaotic and disorganized, treating activism as a threat to the social order. The activism experienced in the 1960s and 1970s shuffled in a new world opinion about the subject. Models were now introduced to understand the organizational and structural powers embedded in social movements.\n\n\n"}
{"id": "1866009", "url": "https://en.wikipedia.org/wiki?curid=1866009", "title": "Environmentalism", "text": "Environmentalism\n\nEnvironmentalism or environmental rights is a broad philosophy, ideology, and social movement regarding concerns for environmental protection and improvement of the health of the environment, particularly as the measure for this health seeks to incorporate the impact of changes to the environment on humans, animals, plants and non-living matter. While environmentalism focuses more on the environmental and nature-related aspects of green ideology and politics, ecology combines the ideology of social ecology and environmentalism.\n\nEnvironmentalism advocates the preservation, restoration and improvement of the natural environment and critical earth system elements or processes such as the climate, and may be referred to as a movement to control pollution or protect plant and animal diversity. For this reason, concepts such as a land ethic, environmental ethics, biodiversity, ecology, and the biophilia hypothesis figure predominantly.\n\nAt its crux, environmentalism is an attempt to balance relations between humans and the various natural systems on which they depend in such a way that all the components are accorded a proper degree of sustainability. The exact measures and outcomes of this balance is controversial and there are many different ways for environmental concerns to be expressed in practice. Environmentalism and environmental concerns are often represented by the colour green, but this association has been appropriated by the marketing industries for the tactic known as greenwashing.\n\nEnvironmentalism is opposed by anti-environmentalism, which says that the Earth is less fragile than some environmentalists maintain, and portrays environmentalism as overreacting to the human contribution to climate change or opposing human advancement.\n\n\"Environmentalism\" denotes a social movement that seeks to influence the political process by lobbying, activism, and education in order to protect natural resources and ecosystems.\n\nAn \"environmentalist\" is a person who may speak out about our natural environment and the sustainable management of its resources through changes in public policy or individual behaviour. This may include supporting practices such as informed consumption, conservation initiatives, investment in renewable resources, improved efficiencies in the materials economy, transitioning to new accounting paradigms such as Ecological economics, renewing and revitalizing our connections with non-human life or even opting to have one less child to reduce consumption and pressure on resources.\n\nIn various ways (for example, grassroots activism and protests), environmentalists and environmental organisations seek to give the natural world a stronger voice in human affairs.\n\nIn general terms, environmentalists advocate the sustainable management of resources, and the protection (and restoration, when necessary) of the natural environment through changes in public policy and individual behaviour. In its recognition of humanity as a participant in ecosystems, the movement is centered around ecology, health, and human rights.\n\nA concern for environmental protection has recurred in diverse forms, in different parts of the world, throughout history.\nThe earliest ideas of environment protectionism can be traced in Jainism, which was revived by Mahavira in 6th century BC in ancient India. Jainism offers a view that may seem readily compatible with core values associated with environmental activism, i.e., protection of life by nonviolence; which could form the basis of strong ecological ethos thus adding its voice to global calls for protection of environment. His teachings on the symbiosis between all living beings and the five elements—earth, water, air, fire, and space—form the basis of environmental sciences today.\n\nIn Europe, King Edward I of England banned the burning of sea-coal by proclamation in London in 1272, after its smoke had become a problem. The fuel was so common in England that this earliest of names for it was acquired because it could be carted away from some shores by the wheelbarrow.\n\nEarlier in the Middle East, the Caliph Abu Bakr in the 630s commanded his army to \"Bring no harm to the trees, nor burn them with fire,\" and \"Slay not any of the enemy's flock, save for your food.\" Arabic medical treatises during the 9th to 13th centuries dealing with environmentalism and environmental science, including pollution, were written by Al-Kindi, Qusta ibn Luqa, Al-Razi, Ibn Al-Jazzar, al-Tamimi, al-Masihi, Avicenna, Ali ibn Ridwan, Ibn Jumay, Isaac Israeli ben Solomon, Abd-el-latif, Ibn al-Quff, and Ibn al-Nafis. Their works covered a number of subjects related to pollution, such as air pollution, water pollution, soil contamination, municipal solid waste mishandling, and environmental impact assessments of certain localities.\n\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.\n\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash and gritty particles and to empower local authorities to impose their own regulations.\n\nDuring the Spanish Revolution, anarchist controlled territories undertook several environmental reforms which were possibly the largest in the world at the time. Daniel Guerin notes that anarchist territories would diversify crops, extend irrigation, initiate reforestation, start tree nurseries and helped establish naturist communities. Once there was a link discovered between air pollution and tuberculosis, the CNT shut down several metal factories.\n\nIt was, however, only under the impetus of the Great Smog of 1952 in London, which almost brought the city to a standstill and may have caused upward of 6,000 deaths that the Clean Air Act 1956 was passed and airborne pollution in the city was first tackled. Financial incentives were offered to householders to replace open coal fires with alternatives (such as installing gas fires), or for those who preferred, to burn coke instead (a byproduct of town gas production) which produces minimal smoke. 'Smoke control areas' were introduced in some towns and cities where only smokeless fuels could be burnt and power stations were relocated away from cities. The act formed an important impetus to modern environmentalism, and caused a rethinking of the dangers of environmental degradation to people's quality of life.\n\nThe late 19th century also saw the passage of the first wildlife conservation laws.\nThe zoologist Alfred Newton published a series of investigations into the \"Desirability of establishing a 'Close-time' for the preservation of indigenous animals\" between 1872 and 1903. His advocacy for legislation to protect animals from hunting during the mating season led to the formation of the Royal Society for the Protection of Birds and influenced the passage of the Sea Birds Preservation Act in 1869 as the first nature protection law in the world.\n\nEarly interest in the environment was a feature of the Romantic movement in the early 19th century. One of the earliest modern pronouncements on thinking about human industrial advancement and its influence on the environment was written by Japanese geographer, educator, philosopher and author Tsunesaburo Makiguchi in his 1903 publication \"Jinsei Chirigaku\" (\"A Geography of Human Life\"). In Britain the poet William Wordsworth travelled extensively in the Lake District and wrote that it is a \"sort of national property in which every man has a right and interest who has an eye to perceive and a heart to enjoy\".\nSystematic efforts on behalf of the environment only began in the late 19th century; it grew out of the amenity movement in Britain in the 1870s, which was a reaction to industrialisation, the growth of cities, and worsening air and water pollution. Starting with the formation of the Commons Preservation Society in 1865, the movement championed rural preservation against the encroachments of industrialisation. Robert Hunter, solicitor for the society, worked with Hardwicke Rawnsley, Octavia Hill, and John Ruskin to lead a successful campaign to prevent the construction of railways to carry slate from the quarries, which would have ruined the unspoilt valleys of Newlands and Ennerdale. This success led to the formation of the Lake District Defence Society (later to become The Friends of the Lake District).\n\nPeter Kropotkin wrote about ecology in economics, agricultural science, conservation, ethology, criminology, urban planning, geography, geology and biology. He observed in Swiss and Siberian glaciers that they had been slowly melting since the dawn of the industrial revolution, possibly making him one of the first predictors for climate change. He also observed the damage done from deforestation and hunting. Kropotkin's writings would become influential in the 1970s and became a major inspiration for the intentional community movement as well as his ideas becoming the basis for the theory of social ecology.\n\nIn 1893 Hill, Hunter and Rawnsley agreed to set up a national body to coordinate environmental conservation efforts across the country; the \"National Trust for Places of Historic Interest or Natural Beauty\" was formally inaugurated in 1894. The organisation obtained secure footing through the 1907 National Trust Bill, which gave the trust the status of a statutory corporation. and the bill was passed in August 1907.\n\nAn early \"Back-to-Nature\" movement, which anticipated the romantic ideal of modern environmentalism, was advocated by intellectuals such as John Ruskin, William Morris, George Bernard Shaw and Edward Carpenter, who were all against consumerism, pollution and other activities that were harmful to the natural world. The movement was a reaction to the urban conditions of the industrial towns, where sanitation was awful, pollution levels intolerable and housing terribly cramped. Idealists championed the rural life as a mythical utopia and advocated a return to it. John Ruskin argued that people should return to a \"small piece of English ground, beautiful, peaceful, and fruitful. We will have no steam engines upon it . . . we will have plenty of flowers and vegetables . . . we will have some music and poetry; the children will learn to dance to it and sing it.\"\n\nPractical ventures in the establishment of small cooperative farms were even attempted and old rural traditions, without the \"taint of manufacture or the canker of artificiality\", were enthusiastically revived, including the Morris dance and the maypole.\n\nThese ideas also inspired various environmental groups in the UK, such as the Royal Society for the Protection of Birds, established in 1889 by Emily Williamson as a protest group to campaign for greater protection for the indigenous birds of the island. The Society attracted growing support from the suburban middle-classes as well as support from many other influential figures, such as the ornithologist Professor Alfred Newton. By 1900, public support for the organisation had grown, and it had over 25,000 members. The Garden city movement incorporated many environmental concerns into its urban planning manifesto; the Socialist League and The Clarion movement also began to advocate measures of nature conservation.\nThe movement in the United States began in the late 19th century, out of concerns for protecting the natural resources of the West, with individuals such as John Muir and Henry David Thoreau making key philosophical contributions. Thoreau was interested in peoples' relationship with nature and studied this by living close to nature in a simple life. He published his experiences in the book \"Walden\", which argues that people should become intimately close with nature. Muir came to believe in nature's inherent right, especially after spending time hiking in Yosemite Valley and studying both the ecology and geology. He successfully lobbied congress to form Yosemite National Park and went on to set up the Sierra Club in 1892. The conservationist principles as well as the belief in an inherent right of nature were to become the bedrock of modern environmentalism.\n\nIn the 20th century, environmental ideas continued to grow in popularity and recognition. Efforts were starting to be made to save some wildlife, particularly the American bison. The death of the last passenger pigeon as well as the endangerment of the American bison helped to focus the minds of conservationists and popularise their concerns. In 1916 the National Park Service was founded by US President Woodrow Wilson.\n\nThe Forestry Commission was set up in 1919 in Britain to increase the amount of woodland in Britain by buying land for afforestation and reforestation. The commission was also tasked with promoting forestry and the production of timber for trade. During the 1920s the Commission focused on acquiring land to begin planting out new forests; much of the land was previously used for agricultural purposes. By 1939 the Forestry Commission was the largest landowner in Britain.\nDuring the 1930s the Nazis had elements that were supportive of animal rights, zoos and wildlife, and took several measures to ensure their protection. In 1933 the government created a stringent animal-protection law and in 1934, \"Das Reichsjagdgesetz\" (The Reich Hunting Law) was enacted which limited hunting. Several Nazis were environmentalists (notably Rudolf Hess), and species protection and animal welfare were significant issues in the regime. In 1935, the regime enacted the \"Reich Nature Protection Act\" (\"Reichsnaturschutzgesetz\"). The concept of the \"Dauerwald\" (best translated as the \"perpetual forest\") which included concepts such as forest management and protection was promoted and efforts were also made to curb air pollution.\n\nIn 1949, \"A Sand County Almanac\" by Aldo Leopold was published. It explained Leopold's belief that humankind should have moral respect for the environment and that it is unethical to harm it. The book is sometimes called the most influential book on conservation.\n\nThroughout the 1950s, 1960s, 1970s and beyond, photography was used to enhance public awareness of the need for protecting land and recruiting members to environmental organisations. David Brower, Ansel Adams and Nancy Newhall created the Sierra Club Exhibit Format Series, which helped raise public environmental awareness and brought a rapidly increasing flood of new members to the Sierra Club and to the environmental movement in general. \"This Is Dinosaur\" edited by Wallace Stegner with photographs by Martin Litton and Philip Hyde prevented the building of dams within Dinosaur National Monument by becoming part of a new kind of activism called environmentalism that combined the conservationist ideals of Thoreau, Leopold and Muir with hard-hitting advertising, lobbying, book distribution, letter writing campaigns, and more. The powerful use of photography in addition to the written word for conservation dated back to the creation of Yosemite National Park, when photographs persuaded Abraham Lincoln to preserve the beautiful glacier carved landscape for all time. The Sierra Club Exhibit Format Series galvanised public opposition to building dams in the Grand Canyon and protected many other national treasures. The Sierra Club often led a coalition of many environmental groups including the Wilderness Society and many others.\n\nAfter a focus on preserving wilderness in the 1950s and 1960s, the Sierra Club and other groups broadened their focus to include such issues as air and water pollution, population concern, and curbing the exploitation of natural resources.\n\nIn 1962, \"Silent Spring\" by American biologist Rachel Carson was published. The book cataloged the environmental impacts of the indiscriminate spraying of DDT in the US and questioned the logic of releasing large amounts of chemicals into the environment without fully understanding their effects on human health and ecology. The book suggested that DDT and other pesticides may cause cancer and that their agricultural use was a threat to wildlife, particularly birds. The resulting public concern led to the creation of the United States Environmental Protection Agency in 1970 which subsequently banned the agricultural use of DDT in the US in 1972. The limited use of DDT in disease vector control continues to this day in certain parts of the world and remains controversial. The book's legacy was to produce a far greater awareness of environmental issues and interest into how people affect the environment. With this new interest in environment came interest in problems such as air pollution and petroleum spills, and environmental interest grew. New pressure groups formed, notably Greenpeace and Friends of the Earth (US), as well as notable local organisations such as the Wyoming Outdoor Council, which was founded in 1967.\n\nIn the 1970s, the environmental movement gained rapid speed around the world as a productive outgrowth of the counterculture movement.\n\nThe world's first political parties to campaign on a predominantly environmental platform were the United Tasmania Group Tasmania, Australia and the Values Party of New Zealand. The first green party in Europe was the Popular Movement for the Environment, founded in 1972 in the Swiss canton of Neuchâtel. The first national green party in Europe was PEOPLE, founded in Britain in February 1973, which eventually turned into the Ecology Party, and then the Green Party.\n\nProtection of the environment also became important in the developing world; the Chipko movement was formed in India under the influence of Mhatmas Gandhi and they set up peaceful resistance to deforestation by literally hugging trees (leading to the term \"tree huggers\"). Their peaceful methods of protest and slogan \"ecology is permanent economy\" were very influential.\n\nAnother milestone in the movement was the creation of Earth Day. Earth Day was first observed in San Francisco and other cities on 21 March 1970, the first day of spring. It was created to give awareness to environmental issues. On 21 March 1971, United Nations Secretary-General U Thant spoke of a spaceship Earth on Earth Day, hereby referring to the ecosystem services the earth supplies to us, and hence our obligation to protect it (and with it, ourselves). Earth Day is now coordinated globally by the Earth Day Network, and is celebrated in more than 192 countries every year.\n\nThe UN's first major conference on international environmental issues, the United Nations Conference on the Human Environment (also known as the Stockholm Conference), was held on 5–16 June 1972. It marked a turning point in the development of international environmental politics.\n\nBy the mid-1970s, many felt that people were on the edge of environmental catastrophe. The Back-to-the-land movement started to form and ideas of environmental ethics joined with anti-Vietnam War sentiments and other political issues. These individuals lived outside normal society and started to take on some of the more radical environmental theories such as deep ecology. Around this time more mainstream environmentalism was starting to show force with the signing of the Endangered Species Act in 1973 and the formation of CITES in 1975. Significant amendments were also enacted to the United States Clean Air Act and Clean Water Act.\n\nIn 1979, James Lovelock, a British scientist, published \"Gaia: A new look at life on Earth\", which put forth the Gaia hypothesis; it proposes that life on earth can be understood as a single organism. This became an important part of the Deep Green ideology. Throughout the rest of the history of environmentalism there has been debate and argument between more radical followers of this Deep Green ideology and more mainstream environmentalists.\n\nEnvironmentalism continues to evolve to face up to new issues such as global warming, overpopulation, genetic engineering, and plastic pollution.\n\nResearch demonstrates a precipitous decline in the US public's interest in 19 different areas of environmental concern. Americans are less likely be actively participating in an environmental movement or organisation and more likely to identify as \"unsympathetic\" to an environmental movement than in 2000. This is likely a lingering factor of the Great Recession in 2008. Since 2005, the percentage of Americans agreeing that the environment should be given priority over economic growth has dropped 10 points, in contrast, those feeling that growth should be given priority \"even if the environment suffers to some extent\" has risen 12 percent. \n\nTree sitting is a form of activism in which the protester sits in a tree in an attempt to stop the removal of a tree or to impede the demolition of an area with the longest and most famous tree-sitter being Julia Butterfly Hill, who spent 738 days in a California Redwood, saving a three-acre tract of forest.\n\nSit-in can be used to encourage social change, such as the Greensboro sit-ins, a series of protests in 1960 to stop racial segregation, but can also be used in ecoactivism, as in the Dakota Access Pipeline Protest.\n\nBefore the Syrian Civil War, Rojava had been ecologically damaged by monoculture, oil extraction, damming of rivers, deforestation, drought, topsoil loss and general pollution. The DFNS launched a campaign titled 'Make Rojava Green Again' (a parody of Make America Great Again) which is attempting to provide renewable energy to communities (especially solar energy), reforestation, protecting water sources, planting gardens, promoting urban agriculture, creating wildlife reserves, water recycling, beekeeping, expanding public transportation and promoting environmental awareness within their communities.\n\nThe Rebel Zapatista Autonomous Municipalities are firmly environmentalist and have stopped the extraction of oil, uranium, timber and metal from the Lacandon Jungle and stopped the use of pesticides and chemical fertilisers in farming.\n\nThe CIPO-RFM has engaged in sabotage and direct action against wind farms, shrimp farms, eucalyptus plantations and the timber industry. They have also set up corn and coffee worker cooperatives and built schools and hospitals to help the local populations. They have also created a network of autonomous community radio stations to educate people about dangers to the environment and inform the surrounding communities about new industrial projects that would destroy more land. In 2001, the CIPO-RFM defeated the construction of a highway that was part of Plan Puebla Panama.\n\nThe \"environmental movement\" (a term that sometimes includes the conservation and green movements) is a diverse scientific, social, and political movement. Though the movement is represented by a range of organisations, because of the inclusion of environmentalism in the classroom curriculum, the environmental movement has a younger demographic than is common in other social movements (see green seniors).\n\nEnvironmentalism as a movement covers broad areas of institutional oppression, including for example: consumption of ecosystems and natural resources into waste, dumping waste into disadvantaged communities, air pollution, water pollution, weak infrastructure, exposure of organic life to toxins, mono-culture, anti-polythene drive (jhola movement) and various other focuses. Because of these divisions, the environmental movement can be categorized into these primary focuses: environmental science, environmental activism, environmental advocacy, and environmental justice.\n\nFree market environmentalism is a theory that argues that the free market, property rights, and tort law provide the best tools to preserve the health and sustainability of the environment. It considers environmental stewardship to be natural, as well as the expulsion of polluters and other aggressors through individual and class action.\n\nEvangelical environmentalism is an environmental movement in the United States of America in which some Evangelicals have emphasized biblical mandates concerning humanity's role as steward and subsequent responsibility for the care taking of Creation. While the movement has focused on different environmental issues, it is best known for its focus of addressing climate action from a biblically grounded theological perspective. This movement is controversial among some non-Christian environmentalists due to its rooting in a specific religion.\n\nEnvironmental preservation in the United States and other parts of the world, including Australia, is viewed as the setting aside of natural resources to prevent damage caused by contact with humans or by certain human activities, such as logging, mining, hunting, and fishing, often to replace them with new human activities such as tourism and recreation. Regulations and laws may be enacted for the preservation of natural resources.\n\nEnvironmental organisations can be global, regional, national or local; they can be government-run or private (NGO). Environmentalist activity exists in almost every country. Moreover, groups dedicated to community development and social justice also focus on environmental concerns.\n\nSome US environmental organisations, among them the Natural Resources Defense Council and the Environmental Defense Fund, specialise in bringing lawsuits (a tactic seen as particularly useful in that country). Other groups, such as the US-based National Wildlife Federation, Earth Day, National Cleanup Day, the Nature Conservancy, and The Wilderness Society, and global groups like the World Wide Fund for Nature and Friends of the Earth, disseminate information, participate in public hearings, lobby, stage demonstrations, and may purchase land for preservation. Statewide nonprofit organisations such as the Wyoming Outdoor Council often collaborate with these national organisations and employ similar strategies. Smaller groups, including Wildlife Conservation International, conduct research on endangered species and ecosystems. More radical organisations, such as Greenpeace, Earth First!, and the Earth Liberation Front, have more directly opposed actions they regard as environmentally harmful. While Greenpeace is devoted to nonviolent confrontation as a means of bearing witness to environmental wrongs and bringing issues into the public realm for debate, the underground \"Earth Liberation Front\" engages in the clandestine destruction of property, the release of caged or penned animals, and other criminal acts. Such tactics are regarded as unusual within the movement, however.\n\nOn an international level, concern for the environment was the subject of a United Nations Conference on the Human Environment in Stockholm in 1972, attended by 114 nations. Out of this meeting developed UNEP (United Nations Environment Programme) and the follow-up United Nations Conference on Environment and Development in 1992. Other international organisations in support of environmental policies development include the Commission for Environmental Cooperation (as part of NAFTA), the European Environment Agency (EEA), and the Intergovernmental Panel on Climate Change (IPCC).\n\nNotable environmental protests and campaigns include:\nNotable advocates for environmental protection and sustainability include:\n\nEvery year, more than 100 environmental activists are murdered throughout the world. Most recent deaths are in Brazil, where activists combat logging in the Amazon rainforest.\n\n116 environmental activists were assassinated in 2014, and 185 in 2015. This represents more than two environmentalists assassinated every week in 2014 and three every week in 2015. More than 200 environmental activists were assassinated worldwide between 2016 and early 2018.\n\n\nMany environmentalists believe that human interference with 'nature' should be restricted or minimised as a matter of urgency (for the sake of life, or the planet, or just for the benefit of the human species), whereas environmental skeptics and anti-environmentalists do not believe that there is such a need. One can also regard oneself as an environmentalist and believe that human 'interference' with 'nature' should be \"increased\". Nevertheless, there is a risk that the shift from emotional environmentalism into the technical management of natural resources and hazards could decrease the touch of humans with nature, leading to less concern with environment preservation.\n\n"}
{"id": "11185", "url": "https://en.wikipedia.org/wiki?curid=11185", "title": "Feminism", "text": "Feminism\n\nFeminism is a range of social movements, political movements, and ideologies that aim to define, establish, and achieve the political, economic, personal, and social equality of the sexes. Feminism incorporates the position that societies prioritize the male point of view, and that women are treated unfairly within those societies. Efforts to change that include fighting gender stereotypes and seeking to establish educational and professional opportunities for women that are equal to those for men.\n\nFeminist movements have campaigned and continue to campaign for women's rights, including the right to vote, to hold public office, to work, to earn fair wages, equal pay and eliminate the gender pay gap, to own property, to receive education, to enter contracts, to have equal rights within marriage, and to have maternity leave. Feminists have also worked to ensure access to legal abortions and social integration and to protect women and girls from rape, sexual harassment, and domestic violence. Changes in dress and acceptable physical activity have often been part of feminist movements.\n\nSome scholars consider feminist campaigns to be a main force behind major historical societal changes for women's rights, particularly in the West, where they are near-universally credited with achieving women's suffrage, gender-neutral language, reproductive rights for women (including access to contraceptives and abortion), and the right to enter into contracts and own property. Although feminist advocacy is, and has been, mainly focused on women's rights, some feminists, including bell hooks, argue for the inclusion of men's liberation within its aims, because they believe that men are also harmed by traditional gender roles.\nFeminist theory, which emerged from feminist movements, aims to understand the nature of gender inequality by examining women's social roles and lived experience; it has developed theories in a variety of disciplines in order to respond to issues concerning gender.\n\nNumerous feminist movements and ideologies have developed over the years and represent different viewpoints and aims. Some forms of feminism have been criticized for taking into account only white, middle class, and college-educated perspectives. This criticism led to the creation of ethnically specific or multicultural forms of feminism, including black feminism and intersectional feminism.\n\nCharles Fourier, a utopian socialist and French philosopher, is credited with having coined the word \"féminisme\" in 1837. The words \"féminisme\" (\"feminism\") and \"féministe\" (\"feminist\") first appeared in France and the Netherlands in 1872, Great Britain in the 1890s, and the United States in 1910. The \"Oxford English Dictionary\" lists 1852 as the year of the first appearance of \"feminist\" and 1895 for \"feminism\". Depending on the historical moment, culture and country, feminists around the world have had different causes and goals. Most western feminist historians contend that all movements working to obtain women's rights should be considered feminist movements, even when they did not (or do not) apply the term to themselves. Other historians assert that the term should be limited to the modern feminist movement and its descendants. Those historians use the label \"protofeminist\" to describe earlier movements.\n\nThe history of the modern western feminist movement is divided into four \"waves\". The first comprised women's suffrage movements of the 19th and early-20th centuries, promoting women's right to vote. The second wave, the women's liberation movement, began in the 1960s and campaigned for legal and social equality for women. In or around 1992, a third wave was identified, characterized by a focus on individuality and diversity. The fourth wave, from around 2012, used social media to combat sexual harassment, violence against women and rape culture; it is best known for the Me Too movement.\n\nFirst-wave feminism was a period of activity during the 19th and early-20th centuries. In the UK and US, it focused on the promotion of equal contract, marriage, parenting, and property rights for women. New legislation included the Custody of Infants Act 1839 in the UK, which introduced the tender years doctrine for child custody and gave women the right of custody of their children for the first time. Other legislation, such as the Married Women's Property Act 1870 in the UK and extended in the 1882 Act, became models for similar legislation in other British territories. Victoria passed legislation in 1884 and New South Wales in 1889; the remaining Australian colonies passed similar legislation between 1890 and 1897. With the turn of the 19th century, activism focused primarily on gaining political power, particularly the right of women's suffrage, though some feminists were active in campaigning for women's sexual, reproductive, and economic rights too.\n\nWomen's suffrage (the right to vote and stand for parliamentary office) began in Britain's Australasian colonies at the close of the 19th century, with the self-governing colonies of New Zealand granting women the right to vote in 1893; South Australia followed suit in 1895. This was followed by Australia granting female suffrage in 1902.\n\nIn Britain the suffragettes and suffragists campaigned for the women's vote, and in 1918 the Representation of the People Act was passed granting the vote to women over the age of 30 who owned property. In 1928 this was extended to all women over 21. Emmeline Pankhurst was the most notable activist in England. \"Time\" named her one of the , stating: \"she shaped an idea of women for our time; she shook society into a new pattern from which there could be no going back.\" In the US, notable leaders of this movement included Lucretia Mott, Elizabeth Cady Stanton, and Susan B. Anthony, who each campaigned for the abolition of slavery before championing women's right to vote. These women were influenced by the Quaker theology of spiritual equality, which asserts that men and women are equal under God. In the US, first-wave feminism is considered to have ended with the passage of the Nineteenth Amendment to the United States Constitution (1919), granting women the right to vote in all states. The term \"first wave\" was coined retroactively when the term \"second-wave feminism\" came into use.\n\nDuring the late Qing period and reform movements such as the Hundred Days' Reform, Chinese feminists called for women's liberation from traditional roles and Neo-Confucian gender segregation. Later, the Chinese Communist Party created projects aimed at integrating women into the workforce, and claimed that the revolution had successfully achieved women's liberation.\n\nAccording to Nawar al-Hassan Golley, Arab feminism was closely connected with Arab nationalism. In 1899, Qasim Amin, considered the \"father\" of Arab feminism, wrote \"The Liberation of Women\", which argued for legal and social reforms for women. He drew links between women's position in Egyptian society and nationalism, leading to the development of Cairo University and the National Movement. In 1923 Hoda Shaarawi founded the Egyptian Feminist Union, became its president and a symbol of the Arab women's rights movement.\n\nThe Iranian Constitutional Revolution in 1905 triggered the Iranian women's movement, which aimed to achieve women's equality in education, marriage, careers, and legal rights. However, during the Iranian revolution of 1979, many of the rights that women had gained from the women's movement were systematically abolished, such as the Family Protection Law.\n\nIn France, women obtained the right to vote only with the Provisional Government of the French Republic of 21 April 1944. The Consultative Assembly of Algiers of 1944 proposed on 24 March 1944 to grant eligibility to women but following an amendment by Fernand Grenier, they were given full citizenship, including the right to vote. Grenier's proposition was adopted 51 to 16. In May 1947, following the November 1946 elections, the sociologist Robert Verdier minimized the \"gender gap\", stating in \"Le Populaire\" that women had not voted in a consistent way, dividing themselves, as men, according to social classes. During the baby boom period, feminism waned in importance. Wars (both World War I and World War II) had seen the provisional emancipation of some women, but post-war periods signalled the return to conservative roles.\n\nBy the mid-20th century, women still lacked significant rights. In Switzerland, women gained the right to vote in federal elections in 1971; but in the canton of Appenzell Innerrhoden women obtained the right to vote on local issues only in 1991, when the canton was forced to do so by the Federal Supreme Court of Switzerland. In Liechtenstein, women were given the right to vote by the women's suffrage referendum of 1984. Three prior referendums held in 1968, 1971 and 1973 had failed to secure women's right to vote.\nFeminists continued to campaign for the reform of family laws which gave husbands control over their wives. Although by the 20th century coverture had been abolished in the UK and US, in many continental European countries married women still had very few rights. For instance, in France married women did not receive the right to work without their husband's permission until 1965. Feminists have also worked to abolish the \"marital exemption\" in rape laws which precluded the prosecution of husbands for the rape of their wives. Earlier efforts by first-wave feminists such as Voltairine de Cleyre, Victoria Woodhull and Elizabeth Clarke Wolstenholme Elmy to criminalize marital rape in the late 19th century had failed; this was only achieved a century later in most Western countries, but is still not achieved in many other parts of the world.\n\nFrench philosopher Simone de Beauvoir provided a Marxist solution and an existentialist view on many of the questions of feminism with the publication of \"Le Deuxième Sexe\" (\"The Second Sex\") in 1949. The book expressed feminists' sense of injustice. Second-wave feminism is a feminist movement beginning in the early 1960s and continuing to the present; as such, it coexists with third-wave feminism. Second-wave feminism is largely concerned with issues of equality beyond suffrage, such as ending gender discrimination.\n\nSecond-wave feminists see women's cultural and political inequalities as inextricably linked and encourage women to understand aspects of their personal lives as deeply politicized and as reflecting sexist power structures. The feminist activist and author Carol Hanisch coined the slogan \"The Personal is Political\", which became synonymous with the second wave.\n\nSecond- and third-wave feminism in China has been characterized by a reexamination of women's roles during the communist revolution and other reform movements, and new discussions about whether women's equality has actually been fully achieved.\n\nIn 1956, President Gamal Abdel Nasser of Egypt initiated \"state feminism\", which outlawed discrimination based on gender and granted women's suffrage, but also blocked political activism by feminist leaders. During Sadat's presidency, his wife, Jehan Sadat, publicly advocated further women's rights, though Egyptian policy and society began to move away from women's equality with the new Islamist movement and growing conservatism. However, some activists proposed a new feminist movement, Islamic feminism, which argues for women's equality within an Islamic framework.\n\nIn Latin America, revolutions brought changes in women's status in countries such as Nicaragua, where feminist ideology during the Sandinista Revolution aided women's quality of life but fell short of achieving a social and ideological change.\n\nIn 1963, Betty Friedan's book \"The Feminine Mystique\" helped voice the discontent that American women felt. The book is widely credited with sparking the beginning of second-wave feminism in the United States. Within ten years, women made up over half the First World workforce.\n\nThird-wave feminism is traced to the emergence of the Riot grrrl feminist punk subculture in Olympia, Washington, in the early 1990s, and to Anita Hill's televised testimony in 1991—to an all-male, all-white Senate Judiciary Committee—that Clarence Thomas, nominated for the Supreme Court of the United States, had sexually harassed her. The term \"third wave\" is credited to Rebecca Walker, who responded to Thomas's appointment to the Supreme Court with an article in \"Ms.\" magazine, \"Becoming the Third Wave\" (1992). She wrote:\n\nThird-wave feminism also sought to challenge or avoid what it deemed the second wave's essentialist definitions of femininity, which, third-wave feminists argued, over-emphasized the experiences of upper middle-class white women. Third-wave feminists often focused on \"micro-politics\" and challenged the second wave's paradigm as to what was, or was not, good for women, and tended to use a post-structuralist interpretation of gender and sexuality. Feminist leaders rooted in the second wave, such as Gloria Anzaldúa, bell hooks, Chela Sandoval, Cherríe Moraga, Audre Lorde, Maxine Hong Kingston, and many other non-white feminists, sought to negotiate a space within feminist thought for consideration of race-related subjectivities. Third-wave feminism also contained internal debates between difference feminists, who believe that there are important psychological differences between the sexes, and those who believe that there are no inherent psychological differences between the sexes and contend that gender roles are due to social conditioning.\n\nStandpoint theory is a feminist theoretical point of view stating that a person's social position influences their knowledge. This perspective argues that research and theory treats women and the feminist movement as insignificant and refuses to see traditional science as unbiased. Since the 1980s, standpoint feminists have argued that the feminist movement should address global issues (such as rape, incest, and prostitution) and culturally specific issues (such as female genital mutilation in some parts of Africa and Arab societies, as well as glass ceiling practices that impede women's advancement in developed economies) in order to understand how gender inequality interacts with racism, homophobia, classism and colonization in a \"matrix of domination\".\n\nFourth-wave feminism refers to a resurgence of interest in feminism that began around 2012 and is associated with the use of social media. According to feminist scholar Prudence Chamberlain, the focus of the fourth wave is justice for women and opposition to sexual harassment and violence against women. Its essence, she writes, is \"incredulity that certain attitudes can still exist\".\n\nFourth-wave feminism is \"defined by technology\", according to Kira Cochrane, and is characterized particularly by the use of Facebook, Twitter, Instagram, YouTube, Tumblr, and blogs such as Feministing to challenge misogyny and further gender equality.\n\nIssues that fourth-wave feminists focus on include street and workplace harassment, campus sexual assault and rape culture. Scandals involving the harassment, abuse, and murder of women and girls have galvanized the movement. These have included the 2012 Delhi gang rape, 2012 Jimmy Savile allegations, the Bill Cosby allegations, 2014 Isla Vista killings, 2016 trial of Jian Ghomeshi, 2017 Harvey Weinstein allegations and subsequent Weinstein effect, and the 2017 Westminster sexual scandals.\n\nExamples of fourth-wave feminist campaigns include the Everyday Sexism Project, No More Page 3, Stop Bild Sexism, \"Mattress Performance\", \"10 Hours of Walking in NYC as a Woman\", #YesAllWomen, Free the Nipple, One Billion Rising, the 2017 Women's March, the 2018 Women's March, and the #MeToo movement. In December 2017, \"Time\" magazine chose several prominent female activists involved in the #MeToo movement, dubbed \"the silence breakers\", as Person of the Year.\n\nThe term postfeminism is used to describe a range of viewpoints reacting to feminism since the 1980s. While not being \"anti-feminist\", postfeminists believe that women have achieved second wave goals while being critical of third- and fourth-wave feminist goals. The term was first used to describe a backlash against second-wave feminism, but it is now a label for a wide range of theories that take critical approaches to previous feminist discourses and includes challenges to the second wave's ideas. Other postfeminists say that feminism is no longer relevant to today's society. Amelia Jones has written that the postfeminist texts which emerged in the 1980s and 1990s portrayed second-wave feminism as a monolithic entity. Dorothy Chunn notes a \"blaming narrative\" under the postfeminist moniker, where feminists are undermined for continuing to make demands for gender equality in a \"post-feminist\" society, where \"gender equality has (already) been achieved.\" According to Chunn, \"many feminists have voiced disquiet about the ways in which rights and equality discourses are now used against them.\"\n\nFeminist theory is the extension of feminism into theoretical or philosophical fields. It encompasses work in a variety of disciplines, including anthropology, sociology, economics, women's studies, literary criticism, art history, psychoanalysis and philosophy. Feminist theory aims to understand gender inequality and focuses on gender politics, power relations, and sexuality. While providing a critique of these social and political relations, much of feminist theory also focuses on the promotion of women's rights and interests. Themes explored in feminist theory include discrimination, stereotyping, objectification (especially sexual objectification), oppression, and patriarchy.\nIn the field of literary criticism, Elaine Showalter describes the development of feminist theory as having three phases. The first she calls \"feminist critique\", in which the feminist reader examines the ideologies behind literary phenomena. The second Showalter calls \"gynocriticism\", in which the \"woman is producer of textual meaning\". The last phase she calls \"gender theory\", in which the \"ideological inscription and the literary effects of the sex/gender system are explored\".\n\nThis was paralleled in the 1970s by French feminists, who developed the concept of \"écriture féminine\" (which translates as 'female or feminine writing'). Helene Cixous argues that writing and philosophy are \"\" and along with other French feminists such as Luce Irigaray emphasize \"writing from the body\" as a subversive exercise. The work of Julia Kristeva, a feminist psychoanalyst and philosopher, and Bracha Ettinger, artist and psychoanalyst, has influenced feminist theory in general and feminist literary criticism in particular. However, as the scholar Elizabeth Wright points out, \"none of these French feminists align themselves with the feminist movement as it appeared in the Anglophone world\". More recent feminist theory, such as that of Lisa Lucile Owens, has concentrated on characterizing feminism as a universal emancipatory movement.\n\nMany overlapping feminist movements and ideologies have developed over the years.\n\nSome branches of feminism closely track the political leanings of the larger society, such as liberalism and conservatism, or focus on the environment. Liberal feminism seeks individualistic equality of men and women through political and legal reform without altering the structure of society. Catherine Rottenberg has argued that the neoliberal shirt in Liberal feminism has led to that form of feminism being individualized rather than collectivized and becoming detached from social inequality. Due to this she argues that Liberal Feminism cannot offer any sustained analysis of the structures of male dominance, power, or privilege.\n\nRadical feminism considers the male-controlled capitalist hierarchy as the defining feature of women's oppression and the total uprooting and reconstruction of society as necessary. Conservative feminism is conservative relative to the society in which it resides. Libertarian feminism conceives of people as self-owners and therefore as entitled to freedom from coercive interference. Separatist feminism does not support heterosexual relationships. Lesbian feminism is thus closely related. Other feminists criticize separatist feminism as sexist. Ecofeminists see men's control of land as responsible for the oppression of women and destruction of the natural environment; ecofeminism has been criticized for focusing too much on a mystical connection between women and nature.\n\nRosemary Hennessy and Chrys Ingraham say that materialist forms of feminism grew out of Western Marxist thought and have inspired a number of different (but overlapping) movements, all of which are involved in a critique of capitalism and are focused on ideology's relationship to women. Marxist feminism argues that capitalism is the root cause of women's oppression, and that discrimination against women in domestic life and employment is an effect of capitalist ideologies. Socialist feminism distinguishes itself from Marxist feminism by arguing that women's liberation can only be achieved by working to end both the economic and cultural sources of women's oppression. Anarcha-feminists believe that class struggle and anarchy against the state require struggling against patriarchy, which comes from involuntary hierarchy.\n\nSara Ahmed argues that Black and Postcolonial feminisms pose a challenge \"to some of the organizing premises of Western feminist thought.\" During much of its history, feminist movements and theoretical developments were led predominantly by middle-class white women from Western Europe and North America. However, women of other races have proposed alternative feminisms. This trend accelerated in the 1960s with the civil rights movement in the United States and the collapse of European colonialism in Africa, the Caribbean, parts of Latin America, and Southeast Asia. Since that time, women in developing nations and former colonies and who are of colour or various ethnicities or living in poverty have proposed additional feminisms. Womanism emerged after early feminist movements were largely white and middle-class. Postcolonial feminists argue that colonial oppression and Western feminism marginalized postcolonial women but did not turn them passive or voiceless. Third-world feminism and Indigenous feminism are closely related to postcolonial feminism. These ideas also correspond with ideas in African feminism, motherism, Stiwanism, negofeminism, femalism, transnational feminism, and Africana womanism.\n\nIn the late twentieth century various feminists began to argue that gender roles are socially constructed, and that it is impossible to generalize women's experiences across cultures and histories. Post-structural feminism draws on the philosophies of post-structuralism and deconstruction in order to argue that the concept of gender is created socially and culturally through discourse. Postmodern feminists also emphasize the social construction of gender and the discursive nature of reality; however, as Pamela Abbott et al. note, a postmodern approach to feminism highlights \"the existence of multiple truths (rather than simply men and women's standpoints)\".\n\nFeminist views on transgender people differ. Some feminists do not view trans women as women, believing that they have male privilege due to their sex assignment at birth. Additionally, some feminists reject the concept of transgender identity due to views that all behavioral differences between genders are a result of socialization. In contrast, other feminists and transfeminists believe that the liberation of trans women is a necessary part of feminist goals. Third-wave feminists are overall more supportive of trans rights. A key concept in transfeminism is of transmisogyny, which is the irrational fear of, aversion to, or discrimination against transgender women or feminine gender-nonconforming people.\n\nRiot grrrls took an anti-corporate stance of self-sufficiency and self-reliance. Riot grrrl's emphasis on universal female identity and separatism often appears more closely allied with second-wave feminism than with the third wave. The movement encouraged and made \"adolescent girls' standpoints central\", allowing them to express themselves fully. Lipstick feminism is a cultural feminist movement that attempts to respond to the backlash of second-wave radical feminism of the 1960s and 1970s by reclaiming symbols of \"feminine\" identity such as make-up, suggestive clothing and having a sexual allure as valid and empowering personal choices.\n\nAccording to 2014 Ipsos poll covering 15 developed countries, 53 percent of respondents identified as feminists, and 87% agreed that \"women should be treated equally to men in all areas based on their competency, not their gender\". However, only 55% of women agreed that they have \"full equality with men and the freedom to reach their full dreams and aspirations\". Taken together, these studies reflect the importance differentiating between claiming a \"feminist identity\" and holding \"feminist attitudes or beliefs\"\n\nAccording to a 2015 poll, 18 percent of Americans consider themselves feminists, while 85 percent reported they believe in \"equality for women\". Despite the popular belief in equal rights, 52 percent did not identify as feminist, 26 percent were unsure, and four percent provided no response.\n\nSociological research shows that, in the US, increased educational attainment is associated with greater support for feminist issues. In addition, politically liberal people are more likely to support feminist ideals compared to those who are conservative.\n\nAccording to numerous polls, 7% of Britons consider themselves feminists, with 83% saying they support equality of opportunity for women – this included even higher support from men (86%) than women (81%).\n\nFeminist views on sexuality vary, and have differed by historical period and by cultural context. Feminist attitudes to female sexuality have taken a few different directions. Matters such as the sex industry, sexual representation in the media, and issues regarding consent to sex under conditions of male dominance have been particularly controversial among feminists. This debate has culminated in the late 1970s and the 1980s, in what came to be known as the feminist sex wars, which pitted anti-pornography feminism against sex-positive feminism, and parts of the feminist movement were deeply divided by these debates. Feminists have taken a variety of positions on different aspects of the sexual revolution from the 1960s and 70s. Over the course of the 1970s, a large number of influential women accepted lesbian and bisexual women as part of feminism.\n\nOpinions on the sex industry are diverse. Feminists critical of the sex industry generally see it as the exploitative result of patriarchal social structures which reinforce sexual and cultural attitudes complicit in rape and sexual harassment. Alternately, feminists who support at least part of the sex industry argue that it can be a medium of feminist expression and a means for women to take control of their sexuality. For the views of feminism on male prostitutes see the article on male prostitution.\n\nFeminist views of pornography range from condemnation of pornography as a form of violence against women, to an embracing of some forms of pornography as a medium of feminist expression. Similarly, feminists' views on prostitution vary, ranging from critical to supportive.\n\nFor feminists, a woman's right to control her own sexuality is a key issue. Feminists such as Catharine MacKinnon argue that women have very little control over their own bodies, with female sexuality being largely controlled and defined by men in patriarchal societies. Feminists argue that sexual violence committed by men is often rooted in ideologies of male sexual entitlement and that these systems grant women very few legitimate options to refuse sexual advances. Feminists argue that all cultures are, in one way or another, dominated by ideologies that largely deny women the right to decide how to express their sexuality, because men under patriarchy feel entitled to define sex on their own terms. This entitlement can take different forms, depending on the culture. In conservative and religious cultures marriage is regarded as an institution which requires a wife to be sexually available at all times, virtually without limit; thus, forcing or coercing sex on a wife is not considered a crime or even an abusive behaviour. In more liberal cultures, this entitlement takes the form of a general sexualization of the whole culture. This is played out in the sexual objectification of women, with pornography and other forms of sexual entertainment creating the fantasy that all women exist solely for men's sexual pleasure and that women are readily available and desiring to engage in sex at any time, with any man, on a man's terms.\n\nSandra Harding says that the \"moral and political insights of the women's movement have inspired social scientists and biologists to raise critical questions about the ways traditional researchers have explained gender, sex and relations within and between the social and natural worlds.\" Some feminists, such as Ruth Hubbard and Evelyn Fox Keller, criticize traditional scientific discourse as being historically biased towards a male perspective. A part of the feminist research agenda is the examination of the ways in which power inequities are created or reinforced in scientific and academic institutions. Physicist Lisa Randall, appointed to a task force at Harvard by then-president Lawrence Summers after his controversial discussion of why women may be underrepresented in science and engineering, said, \"I just want to see a whole bunch more women enter the field so these issues don't have to come up anymore.\"\n\nLynn Hankinson Nelson notes that feminist empiricists find fundamental differences between the experiences of men and women. Thus, they seek to obtain knowledge through the examination of the experiences of women and to \"uncover the consequences of omitting, misdescribing, or devaluing them\" to account for a range of human experience. Another part of the feminist research agenda is the uncovering of ways in which power inequities are created or reinforced in society and in scientific and academic institutions. Furthermore, despite calls for greater attention to be paid to structures of gender inequity in the academic literature, structural analyses of gender bias rarely appear in highly cited psychological journals, especially in the commonly studied areas of psychology and personality.\n\nOne criticism of feminist epistemology is that it allows social and political values to influence its findings. Susan Haack also points out that feminist epistemology reinforces traditional stereotypes about women's thinking (as intuitive and emotional, etc.); Meera Nanda further cautions that this may in fact trap women within \"traditional gender roles and help justify patriarchy\".\n\nModern feminism challenges the essentialist view of gender as biologically intrinsic. For example, Anne Fausto-Sterling's book, \"Myths of Gender\", explores the assumptions embodied in scientific research that support a biologically essentialist view of gender. In \"Delusions of Gender,\" Cordelia Fine disputes scientific evidence that suggests that there is an innate biological difference between men's and women's minds, asserting instead that cultural and societal beliefs are the reason for differences between individuals that are commonly perceived as sex differences.\n\nFeminism in psychology emerged as a critique of the dominant male outlook on psychological research where only male perspectives were studied with all male subjects. As women earned doctorates in psychology, females and their issues were introduced as legitimate topics of study. Feminist psychology emphasizes social context, lived experience, and qualitative analysis. Projects such as Psychology's Feminist Voices have emerged to catalogue the influence of feminist psychologists on the discipline.\n\nGender-based inquiries into and conceptualization of architecture have also come about, leading to feminism in modern architecture. Piyush Mathur coined the term \"archigenderic\". Claiming that \"architectural planning has an inextricable link with the defining and regulation of gender roles, responsibilities, rights, and limitations\", Mathur came up with that term \"to explore ... the meaning of 'architecture' in terms of gender\" and \"to explore the meaning of 'gender' in terms of architecture\".\n\nFeminist activists have established a range of feminist businesses, including women's bookstores, feminist credit unions, feminist presses, feminist mail-order catalogs, and feminist restaurants. These businesses flourished as part of the second and third-waves of feminism in the 1970s, 1980s, and 1990s.\n\nCorresponding with general developments within feminism, and often including such self-organizing tactics as the consciousness-raising group, the movement began in the 1960s and flourished throughout the 1970s. Jeremy Strick, director of the Museum of Contemporary Art in Los Angeles, described the feminist art movement as \"the most influential international movement of any during the postwar period\", and Peggy Phelan says that it \"brought about the most far-reaching transformations in both artmaking and art writing over the past four decades\". Feminist artist Judy Chicago, who created \"The Dinner Party\", a set of vulva-themed ceramic plates in the 1970s, said in 2009 to \"ARTnews\", \"There is still an institutional lag and an insistence on a male Eurocentric narrative. We are trying to change the future: to get girls and boys to realize that women's art is not an exception—it's a normal part of art history.\" A feminist approach to the visual arts has most recently developed through Cyberfeminism and the posthuman turn, giving voice to the ways \"contemporary female artists are dealing with gender, social media and the notion of embodiment\".\n\nThe feminist movement produced feminist fiction, feminist non-fiction, and feminist poetry, which created new interest in women's writing. It also prompted a general reevaluation of women's historical and academic contributions in response to the belief that women's lives and contributions have been underrepresented as areas of scholarly interest. There has also been a close link between feminist literature and activism, with feminist writing typically voicing key concerns or ideas of feminism in a particular era.\n\nMuch of the early period of feminist literary scholarship was given over to the rediscovery and reclamation of texts written by women. In Western feminist literary scholarship, Studies like Dale Spender's \"Mothers of the Novel\" (1986) and Jane Spencer's \"The Rise of the Woman Novelist\" (1986) were ground-breaking in their insistence that women have always been writing.\n\nCommensurate with this growth in scholarly interest, various presses began the task of reissuing long-out-of-print texts. Virago Press began to publish its large list of 19th and early-20th-century novels in 1975 and became one of the first commercial presses to join in the project of reclamation. In the 1980s Pandora Press, responsible for publishing Spender's study, issued a companion line of 18th-century novels written by women. More recently, Broadview Press continues to issue 18th- and 19th-century novels, many hitherto out of print, and the University of Kentucky has a series of republications of early women's novels.\n\nParticular works of literature have come to be known as key feminist texts. \"A Vindication of the Rights of Woman\" (1792) by Mary Wollstonecraft, is one of the earliest works of feminist philosophy. \"A Room of One's Own\" (1929) by Virginia Woolf, is noted in its argument for both a literal and figural space for women writers within a literary tradition dominated by patriarchy.\n\nThe widespread interest in women's writing is related to a general reassessment and expansion of the literary canon. Interest in post-colonial literatures, gay and lesbian literature, writing by people of colour, working people's writing, and the cultural productions of other historically marginalized groups has resulted in a whole scale expansion of what is considered \"literature\", and genres hitherto not regarded as \"literary\", such as children's writing, journals, letters, travel writing, and many others are now the subjects of scholarly interest. Most genres and subgenres have undergone a similar analysis, so literary studies has entered new territories such as the \"female gothic\" or women's science fiction.\n\nAccording to Elyce Rae Helford, \"Science fiction and fantasy serve as important vehicles for feminist thought, particularly as bridges between theory and practice.\" Feminist science fiction is sometimes taught at the university level to explore the role of social constructs in understanding gender. Notable texts of this kind are Ursula K. Le Guin's \"The Left Hand of Darkness\" (1969), Joanna Russ' \"The Female Man\" (1970), Octavia Butler's \"Kindred\" (1979) and Margaret Atwood's \"Handmaid's Tale\" (1985).\n\nFeminist nonfiction has played an important role in voicing concerns about women's lived experiences. For example, Maya Angelou's \"I Know Why the Caged Bird Sings\" was extremely influential, as it represented the specific racism and sexism experienced by black women growing up in the United States.\n\nIn addition, many feminist movements have embraced poetry as a vehicle through which to communicate feminist ideas to public audiences through anthologies, poetry collections, and public readings.\n\nMoreover, historical pieces of writing by women have been used by feminists to speak about what women's lives would have been like in the past, while demonstrating the power that they held and the impact they had in their communities even centuries ago. An important figure in the history of women in relation to literature is Hrothsvitha. Hrothsvitha was a canoness from 935 - 973, as the first female poetess in the German lands, and first female historian Hrothsvitha is one of the few people to speak about women's lives from a woman's perspective during the Middle Ages.\n\nWomen's music (or womyn's music or wimmin's music) is the music by women, for women, and about women. The genre emerged as a musical expression of the second-wave feminist movement as well as the labour, civil rights, and peace movements. The movement was started by lesbians such as Cris Williamson, Meg Christian, and Margie Adam, African-American women activists such as Bernice Johnson Reagon and her group Sweet Honey in the Rock, and peace activist Holly Near. Women's music also refers to the wider industry of women's music that goes beyond the performing artists to include studio musicians, producers, sound engineers, technicians, cover artists, distributors, promoters, and festival organizers who are also women.\nRiot grrrl is an underground feminist hardcore punk movement described in the cultural movements section of this article.\n\nFeminism became a principal concern of musicologists in the 1980s as part of the New Musicology. Prior to this, in the 1970s, musicologists were beginning to discover women composers and performers, and had begun to review concepts of canon, genius, genre and periodization from a feminist perspective. In other words, the question of how women musicians fit into traditional music history was now being asked. Through the 1980s and 1990s, this trend continued as musicologists like Susan McClary, Marcia Citron and Ruth Solie began to consider the cultural reasons for the marginalizing of women from the received body of work. Concepts such as music as gendered discourse; professionalism; reception of women's music; examination of the sites of music production; relative wealth and education of women; popular music studies in relation to women's identity; patriarchal ideas in music analysis; and notions of gender and difference are among the themes examined during this time.\n\nWhile the music industry has long been open to having women in performance or entertainment roles, women are much less likely to have positions of authority, such as being the leader of an orchestra. In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process.\n\nFeminist cinema, advocating or illustrating feminist perspectives, arose largely with the development of feminist film theory in the late '60s and early '70s. Women who were radicalized during the 1960s by political debate and sexual liberation; but the failure of radicalism to produce substantive change for women galvanized them to form consciousness-raising groups and set about analysing, from different perspectives, dominant cinema's construction of women. Differences were particularly marked between feminists on either side of the Atlantic. 1972 saw the first feminist film festivals in the U.S. and U.K. as well as the first feminist film journal, \"Women and Film\". Trailblazers from this period included Claire Johnston and Laura Mulvey, who also organized the Women's Event at the Edinburgh Film Festival. Other theorists making a powerful impact on feminist film include Teresa de Lauretis, Anneke Smelik and Kaja Silverman. Approaches in philosophy and psychoanalysis fuelled feminist film criticism, feminist independent film and feminist distribution.\n\nIt has been argued that there are two distinct approaches to independent, theoretically inspired feminist filmmaking. 'Deconstruction' concerns itself with analysing and breaking down codes of mainstream cinema, aiming to create a different relationship between the spectator and dominant cinema. The second approach, a feminist counterculture, embodies feminine writing to investigate a specifically feminine cinematic language. Some recent criticism of \"feminist film\" approaches has centred around a Swedish rating system called the Bechdel test.\n\nDuring the 1930s–1950s heyday of the big Hollywood studios, the status of women in the industry was abysmal. Since then female directors such as Sally Potter, Catherine Breillat, Claire Denis and Jane Campion have made art movies, and directors like Kathryn Bigelow and Patty Jenkins have had mainstream success. This progress stagnated in the 90s, and men outnumber women five to one in behind the camera roles.\n\nFeminism had complex interactions with the major political movements of the twentieth century.\n\nSince the late nineteenth century, some feminists have allied with socialism, whereas others have criticized socialist ideology for being insufficiently concerned about women's rights. August Bebel, an early activist of the German Social Democratic Party (SPD), published his work \"Die Frau und der Sozialismus\", juxtaposing the struggle for equal rights between sexes with social equality in general. In 1907 there was an International Conference of Socialist Women in Stuttgart where suffrage was described as a tool of class struggle. Clara Zetkin of the SPD called for women's suffrage to build a \"socialist order, the only one that allows for a radical solution to the women's question\".\n\nIn Britain, the women's movement was allied with the Labour party. In the U.S., Betty Friedan emerged from a radical background to take leadership. Radical Women is the oldest socialist feminist organization in the U.S. and is still active. During the Spanish Civil War, Dolores Ibárruri (\"La Pasionaria\") led the Communist Party of Spain. Although she supported equal rights for women, she opposed women fighting on the front and clashed with the anarcha-feminist Mujeres Libres.\n\nFeminists in Ireland in the early 20th century included the revolutionary Irish Republican, suffragette and socialist Constance Markievicz who in 1918 was the first woman elected to the British House of Commons. However, in line with Sinn Féin abstentionist policy, she would not take her seat in the House of Commons. She was re-elected to the Second Dáil in the elections of 1921. She was also a commander of the Irish Citizens Army which was led by the socialist & self-described feminist, Irish leader James Connolly during the 1916 Easter Rising.\n\nFascism has been prescribed dubious stances on feminism by its practitioners and by women's groups. Amongst other demands concerning social reform presented in the Fascist manifesto in 1919 was expanding the suffrage to all Italian citizens of age 18 and above, including women (accomplished only in 1946, after the defeat of fascism) and eligibility for all to stand for office from age 25. This demand was particularly championed by special Fascist women's auxiliary groups such as the \"fasci femminilli\" and only partly realized in 1925, under pressure from dictator Benito Mussolini's more conservative coalition partners.\n\nCyprian Blamires states that although feminists were among those who opposed the rise of Adolf Hitler, feminism has a complicated relationship with the Nazi movement as well. While Nazis glorified traditional notions of patriarchal society and its role for women, they claimed to recognize women's equality in employment. However, Hitler and Mussolini declared themselves as opposed to feminism, and after the rise of Nazism in Germany in 1933, there was a rapid dissolution of the political rights and economic opportunities that feminists had fought for during the pre-war period and to some extent during the 1920s. Georges Duby et al. note that in practice fascist society was hierarchical and emphasized male virility, with women maintaining a largely subordinate position. Blamires also notes that Neofascism has since the 1960s been hostile towards feminism and advocates that women accept \"their traditional roles\".\n\nThe civil rights movement has influenced and informed the feminist movement and vice versa. Many Western feminists adapted the language and theories of black equality activism and drew parallels between women's rights and the rights of non-white people. Despite the connections between the women's and civil rights movements, some tensions arose during the late 1960s and the 1970s as non-white women argued that feminism was predominantly white, straight, and middle class, and did not understand and was not concerned with issues of race and sexuality. Similarly, some women argued that the civil rights movement had sexist and homophobic elements and did not adequately address minority women's concerns. These criticisms created new feminist social theories about identity politics and the intersections of racism, classism, and sexism; they also generated new feminisms such as black feminism and Chicana feminism in addition to making large contributions to lesbian feminism and other integrations of queer of colour identity. \n\nNeoliberalism has been criticized by feminist theory for having a negative effect on the female workforce population across the globe, especially in the global south. Masculinist assumptions and objectives continue to dominate economic and geopolitical thinking. Women's experiences in non-industrialized countries reveal often deleterious effects of modernization policies and undercut orthodox claims that development benefits everyone.\n\nProponents of neoliberalism have theorized that by increasing women's participation in the workforce, there will be heightened economic progress, but feminist critics have noted that this participation alone does not further equality in gender relations. Neoliberalism has failed to address significant problems such as the devaluation of feminized labour, the structural privileging of men and masculinity, and the politicization of women's subordination in the family and the workplace. The \"feminization of employment\" refers to a conceptual characterization of deteriorated and devalorized labour conditions that are less desirable, meaningful, safe and secure. Employers in the global south have perceptions about feminine labour and seek workers who are perceived to be undemanding, docile and willing to accept low wages. Social constructs about feminized labour have played a big part in this, for instance, employers often perpetuate ideas about women as 'secondary income earners to justify their lower rates of pay and not deserving of training or promotion.\n\nThe feminist movement has effected change in Western society, including women's suffrage; greater access to education; more nearly equitable pay with men; the right to initiate divorce proceedings; the right of women to make individual decisions regarding pregnancy (including access to contraceptives and abortion); and the right to own property.\n\nFrom the 1960s on, the campaign for women's rights was met with mixed results in the U.S. and the U.K. Other countries of the EEC agreed to ensure that discriminatory laws would be phased out across the European Community.\n\nSome feminist campaigning also helped reform attitudes to child sexual abuse. The view that young girls cause men to have sexual intercourse with them was replaced by that of men's responsibility for their own conduct, the men being adults.\n\nIn the U.S., the National Organization for Women (NOW) began in 1966 to seek women's equality, including through the Equal Rights Amendment (ERA), which did not pass, although some states enacted their own. Reproductive rights in the U.S. centred on the court decision in \"Roe\" v. \"Wade\" enunciating a woman's right to choose whether to carry a pregnancy to term. Western women gained more reliable birth control, allowing family planning and careers. The movement started in the 1910s in the U.S. under Margaret Sanger and elsewhere under Marie Stopes. In the final three decades of the 20th century, Western women knew a new freedom through birth control, which enabled women to plan their adult lives, often making way for both career and family.\n\nThe division of labour within households was affected by the increased entry of women into workplaces in the 20th century. Sociologist Arlie Russell Hochschild found that, in two-career couples, men and women, on average, spend about equal amounts of time working, but women still spend more time on housework, although Cathy Young responded by arguing that women may prevent equal participation by men in housework and parenting. Judith K. Brown writes, \"Women are most likely to make a substantial contribution when subsistence activities have the following characteristics: the participant is not obliged to be far from home; the tasks are relatively monotonous and do not require rapt concentration; and the work is not dangerous, can be performed in spite of interruptions, and is easily resumed once interrupted.\"\n\nIn international law, the \"Convention on the Elimination of All Forms of Discrimination Against Women\" (CEDAW) is an international convention adopted by the United Nations General Assembly and described as an international bill of rights for women. It came into force in those nations ratifying it.\n\nFeminist jurisprudence is a branch of jurisprudence that examines the relationship between women and law. It addresses questions about the history of legal and social biases against women and about the enhancement of their legal rights.\n\nFeminist jurisprudence signifies a reaction to the philosophical approach of modern legal scholars, who typically see law as a process for interpreting and perpetuating a society's universal, gender-neutral ideals. Feminist legal scholars claim that this fails to acknowledge women's values or legal interests or the harms that they may anticipate or experience.\n\nProponents of gender-neutral language argue that the use of gender-specific language often implies male superiority or reflects an unequal state of society. According to \"The Handbook of English Linguistics\", generic masculine pronouns and gender-specific job titles are instances \"where English linguistic convention has historically treated men as prototypical of the human species.\"\n\nMerriam-Webster chose \"feminism\" as its 2017 Word of the Year, noting that \"Word of the Year is a quantitative measure of interest in a particular word.\"\n\nFeminist theology is a movement that reconsiders the traditions, practices, scriptures, and theologies of religions from a feminist perspective. Some of the goals of feminist theology include increasing the role of women among the clergy and religious authorities, reinterpreting male-dominated imagery and language about God, determining women's place in relation to career and motherhood, and studying images of women in the religion's sacred texts.\n\nChristian feminism is a branch of feminist theology which seeks to interpret and understand Christianity in light of the equality of women and men, and that this interpretation is necessary for a complete understanding of Christianity. While there is no standard set of beliefs among Christian feminists, most agree that God does not discriminate on the basis of sex, and are involved in issues such as the ordination of women, male dominance and the balance of parenting in Christian marriage, claims of moral deficiency and inferiority of women compared to men, and the overall treatment of women in the church.\n\nIslamic feminists advocate women's rights, gender equality, and social justice grounded within an Islamic framework. Advocates seek to highlight the deeply rooted teachings of equality in the Quran and encourage a questioning of the patriarchal interpretation of Islamic teaching through the Quran, \"hadith\" (sayings of Muhammad), and \"sharia\" (law) towards the creation of a more equal and just society. Although rooted in Islam, the movement's pioneers have also utilized secular and Western feminist discourses and recognize the role of Islamic feminism as part of an integrated global feminist movement.\n\nBuddhist feminism is a movement that seeks to improve the religious, legal, and social status of women within Buddhism. It is an aspect of feminist theology which seeks to advance and understand the equality of men and women morally, socially, spiritually, and in leadership from a Buddhist perspective. The Buddhist feminist Rita Gross describes Buddhist feminism as \"the radical practice of the co-humanity of women and men.\"\n\nJewish feminism is a movement that seeks to improve the religious, legal, and social status of women within Judaism and to open up new opportunities for religious experience and leadership for Jewish women. The main issues for early Jewish feminists in these movements were the exclusion from the all-male prayer group or \"minyan\", the exemption from positive time-bound \"mitzvot\", and women's inability to function as witnesses and to initiate divorce. Many Jewish women have become leaders of feminist movements throughout their history.\n\nDianic Wicca is a feminist-centred thealogy.\n\nSecular or atheist feminists have engaged in feminist criticism of religion, arguing that many religions have oppressive rules towards women and misogynistic themes and elements in religious texts.\n\nPatriarchy is a social system in which society is organized around male authority figures. In this system, fathers have authority over women, children, and property. It implies the institutions of male rule and privilege, and is dependent on female subordination. Most forms of feminism characterize patriarchy as an unjust social system that is oppressive to women. Carole Pateman argues that the patriarchal distinction \"between masculinity and femininity is the political difference between freedom and subjection.\" In feminist theory the concept of patriarchy often includes all the social mechanisms that reproduce and exert male dominance over women. Feminist theory typically characterizes patriarchy as a social construction, which can be overcome by revealing and critically analyzing its manifestations. Some radical feminists have proposed that because patriarchy is too deeply rooted in society, separatism is the only viable solution. Other feminists have criticized these views as being anti-men.\n\nFeminist theory has explored the social construction of masculinity and its implications for the goal of gender equality. The social construct of masculinity is seen by feminism as problematic because it associates males with aggression and competition, and reinforces patriarchal and unequal gender relations. Patriarchal cultures are criticized for \"limiting forms of masculinity\" available to men and thus narrowing their life choices. Some feminists are engaged with men's issues activism, such as bringing attention to male rape and spousal battery and addressing negative social expectations for men.\n\nMale participation in feminism is generally encouraged by feminists and is seen as an important strategy for achieving full societal commitment to gender equality. Many male feminists and pro-feminists are active in both women's rights activism, feminist theory, and masculinity studies. However, some argue that while male engagement with feminism is necessary, it is problematic because of the ingrained social influences of patriarchy in gender relations. The consensus today in feminist and masculinity theories is that men and women should cooperate to achieve the larger goals of feminism. It has been proposed that, in large part, this can be achieved through considerations of women's agency.\n\nDifferent groups of people have responded to feminism, and both men and women have been among its supporters and critics. Among American university students, for both men and women, support for feminist ideas is more common than self-identification as a feminist. The US media tends to portray feminism negatively and feminists \"are less often associated with day-to-day work/leisure activities of regular women.\" However, as recent research has demonstrated, as people are exposed to self-identified feminists and to discussions relating to various forms of feminism, their own self-identification with feminism increases.\n\nPro-feminism is the support of feminism without implying that the supporter is a member of the feminist movement. The term is most often used in reference to men who are actively supportive of feminism. The activities of pro-feminist men's groups include anti-violence work with boys and young men in schools, offering sexual harassment workshops in workplaces, running community education campaigns, and counselling male perpetrators of violence. Pro-feminist men also may be involved in men's health, activism against pornography including anti-pornography legislation, men's studies, and the development of gender equity curricula in schools. This work is sometimes in collaboration with feminists and women's services, such as domestic violence and rape crisis centres.\n\nAnti-feminism is opposition to feminism in some or all of its forms.\n\nIn the nineteenth century, anti-feminism was mainly focused on opposition to women's suffrage. Later, opponents of women's entry into institutions of higher learning argued that education was too great a physical burden on women. Other anti-feminists opposed women's entry into the labour force, or their right to join unions, to sit on juries, or to obtain birth control and control of their sexuality.\n\nSome people have opposed feminism on the grounds that they believe it is contrary to traditional values or religious beliefs. These anti-feminists argue, for example, that social acceptance of divorce and non-married women is wrong and harmful, and that men and women are fundamentally different and thus their different traditional roles in society should be maintained. Other anti-feminists oppose women's entry into the workforce, political office, and the voting process, as well as the lessening of male authority in families.\n\nWriters such as Camille Paglia, Christina Hoff Sommers, Jean Bethke Elshtain, Elizabeth Fox-Genovese, Lisa Lucile Owens and Daphne Patai oppose some forms of feminism, though they identify as feminists. They argue, for example, that feminism often promotes misandry and the elevation of women's interests above men's, and criticize radical feminist positions as harmful to both men and women. Daphne Patai and Noretta Koertge argue that the term \"anti-feminist\" is used to silence academic debate about feminism. Lisa Lucile Owens argues that certain rights extended exclusively to women are patriarchal because they relieve women from exercising a crucial aspect of their moral agency.\n\nSecular humanism is an ethical framework that attempts to dispense with any unreasoned dogma, pseudoscience, and superstition. Critics of feminism sometimes ask \"Why feminism and not humanism?\". Some humanists argue, however, that the goals of feminists and humanists largely overlap, and the distinction is only in motivation. For example, a humanist may consider abortion in terms of a utilitarian ethical framework, rather than considering the motivation of any particular woman in getting an abortion. In this respect, it is possible to be a humanist without being a feminist, but this does not preclude the existence of feminist humanism. Humanism plays a significant role in protofeminism during the renaissance period in such that humanists made educated women a popular figure despite the challenge to the male patriarchal organization of society.\n\n\n\n\n\n"}
{"id": "437868", "url": "https://en.wikipedia.org/wiki?curid=437868", "title": "Minority group", "text": "Minority group\n\nIn sociology, a minority group refers to a category of people who experience relative disadvantage as compared to members of a dominant social group. Minority group membership is typically based on differences in observable characteristics or practices, such as: ethnicity (ethnic minority), race (racial minority), religion (religious minority), sexual orientation (sexual minority), or disability. Utilizing the framework of intersectionality, it is important to recognize that an individual may simultaneously hold membership in multiple minority groups (e.g. both a racial and religious minority). Likewise, individuals may also be part of a minority group in regard to some characteristics, but part of a dominant group in regard to others.\n\nThe term \"minority group\" often occurs within the discourse of civil rights and collective rights, as members of minority groups are prone to differential treatment in the countries and societies in which they live. Minority group members often face discrimination in multiple areas of social life, including housing, employment, healthcare, and education, among others. While discrimination may be committed by individuals, it may also occur through structural inequalities, in which rights and opportunities are not equally accessible to all. The language of minority rights is often used to discuss laws designed to protect minority groups from discrimination and afford them equal social status to the dominant group.\n\nLouis Wirth defined a minority group as \"a group of people who, because of their physical or cultural characteristics, are singled out from the others in the society in which they live for differential and unequal treatment, and who therefore regard themselves as objects of collective discrimination\". The definition includes both objective and subjective criteria: membership of a minority group is objectively ascribed by society, based on an individual's physical or behavioral characteristics; it is also subjectively applied by its members, who may use their status as the basis of group identity or solidarity. Thus, minority group status is categorical in nature: an individual who exhibits the physical or behavioral characteristics of a given minority group is accorded the status of that group and is subject to the same treatment as other members of that group.\n\nJoe Feagin, states that a minority group has five characteristics: (1) suffering discrimination and subordination, (2) physical and/or cultural traits that set them apart, and which are disapproved by the dominant group, (3) a shared sense of collective identity and common burdens, (4) socially shared rules about who belongs and who does not determine minority status, and (5) tendency to marry within the group.\n\nThere is a controversy with the use of the word minority, as it has a generic and an academic usage. Common usage of the term indicates a statistical minority; however, academics refer to power differences among groups rather than differences in population size among groups.\n\nThe above criticism is based on the idea that a group can be considered minority even if it includes such a large number of people that it is numerically not a minority in society.\n\nSome sociologists have criticized the concept of \"minority/majority\", arguing this language excludes or neglects changing or unstable cultural identities, as well as cultural affiliations across national boundaries. As such, the term historically excluded groups (HEGs) is often similarly used to highlight the role of historical oppression and domination, and how this results in the under-representation of particular groups in various areas of social life.\n\nThe term national minority is often used to discuss minority groups in international and national politics. All countries contain some degree of racial, ethnic, or linguistic diversity. In addition, minorities may also be immigrant, indigenous or landless nomadic communities. This often results in variations in language, culture, beliefs, practices, that set some groups apart from the dominant group. As these differences are usually perceived negatively by, this results in loss of social and political power for members of minority groups.\n\nThere is no legal definition of national minorities in international law, though protection of minority groups is outlined by the United Nations Declaration on the Rights of Persons Belonging to National or Ethnic, Religious and Linguistic Minorities. International criminal law can protect the rights of racial or ethnic minorities in a number of ways. The right to self-determination is a key issue. The Council of Europe regulates minority rights in the European Charter for Regional or Minority Languages and the Framework Convention for the Protection of National Minorities.\n\nIn some places, subordinate ethnic groups may constitute a numerical majority, such as Blacks in South Africa under apartheid. In the United States, for example, non-Hispanic Whites constitute the majority (63.4%) and all other racial and ethnic groups (Hispanic or Latino, African Americans, Asian Americans, American Indian, and Native Hawaiians) are classified as \"minorities\". If the non-Hispanic White population falls below 50% the group will only be the \"plurality\", not the majority.\n\nAlso known as \"castelike minorities,\" involuntary minorities are a term for people who were originally brought into any society against their will. In the United States, for instance, it includes but is not limited to Native Americans, Puerto Ricans, African Americans, and native-born Mexican Americans. For reasons of cultural differences, involuntary minorities may experience difficulties in school more than members of other (voluntary) minority groups. Social capital helps children engage with different age groups that share a common goal.\n\nImmigrants take on minority status in their new country, usually in hopes of a better future economically, educationally, and politically than in their homeland. Because of their focus on success, voluntary minorities are more likely to do better in school than other migrating minorities. Adapting to a very different culture and language make difficulties in the early stages of life in the new country. Voluntary immigrants do not experience a sense of divided identity as much as involuntary minorities, and are often rich in social capital because of their educational ambitions. Major immigrant groups in the United States include Mexicans, Central and South Americans, Cubans, Africans, and Indians.\n\nThe term sexual minority is frequently used by public health researchers to recognize a wide variety of individuals who engage in same-sex sexual behavior, including those who do not identify under the LGBTQ umbrella. For example, men who have sex with men (MSM), but do not identify as gay. In addition, the term gender minorities can include many types of gender variant people, such as intersex people, transgender people, or gender non-conforming individuals. However, the terms sexual and gender minority are often not preferred by LGBTQ people, as they represent clinical categories rather than individual identity.\n\nThough lesbian, gay, bisexual, transgender, and queer (LGBTQ) people have existed throughout human history, LGBT rights movements across many western countries led to the recognition of LGBTQ people as members of a minority group. LGBTQ people represent a numerical and social minority. They experience numerous social inequalities stemming from their group membership as LGBTQ people. These inequalities include social discrimination and isolation, unequal access to healthcare, employment, and housing, and experience negative mental and physical health outcomes due to these experiences.\n\nThe disability rights movement has contributed to an understanding of people with disabilities as a minority or a coalition of minorities who are disadvantaged by society, not just as people who are disadvantaged by their impairments. Advocates of disability rights emphasize difference in physical or psychological functioning, rather than inferiority. For example, some people with autism argue for acceptance of neurodiversity, much as opponents of racism argue for acceptance of ethnic diversity. The deaf community is often regarded as a linguistic and cultural minority rather than a group with disabilities, and some deaf people do not see themselves as having a disability at all. Rather, they are disadvantaged by technologies and social institutions that are designed to cater for the dominant group. (\"See the Convention on the Rights of Persons with Disabilities\".)\n\nPeople belonging to religious minorities have a faith which is different from that held by the majority. Most countries of the world have religious minorities. It is now widely accepted in the west that people should have the freedom to choose their own religion, including not having any religion (atheism and/or agnosticism), and including the right to convert from one religion to another. However, in many countries this freedom is constricted. In Egypt, a new system of identity cards requires all citizens to state their religion—and the only choices are Islam, Christianity, or Judaism (See Egyptian identification card controversy). Another example is the case of decreasing population of minorities in Pakistan, where they are being forcefully converted or killed.\n\nIn most societies, numbers of men and women are not equal. The status of women as a subordinate group has led to many social scientists to study them as a minority group. Though women's legal rights and status vary widely across countries, women experience social inequalities relative to men in most societies. Women are often denied access to education, subject to violence, and lack access to the same economic opportunities as men.\n\nIn the politics of some countries, a \"minority\" is an ethnic group recognized by law, and having specified rights. Speakers of a legally recognized minority language, for instance, might have the right to education or communication with the government in their mother tongue. Countries with special provisions for minorities include Canada, China, Ethiopia, Germany, India, the Netherlands, Poland, Romania, Russia, Croatia, and the United Kingdom.\n\nThe various minority groups in a country are often not given equal treatment. Some groups are too small or indistinct to obtain minority protections. For example, a member of a particularly small ethnic group might be forced to check \"Other\" on a checklist of different backgrounds and so might receive fewer privileges than a member of a more defined group.\n\nMany contemporary governments prefer to assume the people they rule all belong to the same nationality rather than separate ones based on ethnicity. The United States asks for race and ethnicity on its official census forms, which thus breaks up and organizes its population into sub-groups, primarily racial rather than national. Spain does not divide its nationals by ethnic group, although it does maintain an official notion of minority languages.\n\nSome especially significant or powerful minorities receive comprehensive protection and political representation. For example, the former Yugoslav republic of Bosnia and Herzegovina recognizes the three constitutive nations, none of which constitutes a numerical majority (see nations of Bosnia and Herzegovina). However, other minorities such as Romani and Jews, are officially labelled \"foreign\" and are excluded from many of these protections. For example, they may be excluded from political positions, including the presidency.\n\nThere is debate over recognizing minority groups and their privileges. One view is that the application of special rights to minority groups may harm some countries, such as new states in Africa or Latin America not founded on the European nation-state model, since minority recognition may interfere with establishing a national identity. It may hamper the integration of the minority into mainstream society, perhaps leading to separatism or supremacism. In Canada, some feel that the failure of the dominant English-speaking majority to integrate French Canadians has provoked Quebec separatism.\n\nOthers assert that minorities require specific protections to ensure that they are not marginalised: for example, bilingual education may be needed to allow linguistic minorities to fully integrate into the school system and compete equally in society. In this view, rights for minorities strengthen the nation-building project, as members of minorities see their interests well served, and willingly accept the legitimacy of the nation and their integration (not assimilation) within it.\n\n"}
{"id": "1182927", "url": "https://en.wikipedia.org/wiki?curid=1182927", "title": "Social stratification", "text": "Social stratification\n\nSocial stratification refers to society’s categorization of its people into groups based on socioeconomic factors like wealth, income, race, education, gender, occupation, and social status, or derived power (social and political). As such, stratification is the relative social position of persons within a social group, category, geographic region, or social unit. \n\nIn modern Western societies, social stratification is typically defined in terms of three social classes: (i) the upper class, (ii) the middle class, and (iii) the lower class; in turn, each class can be subdivided into , e.g. the upper-stratum, the middle-stratum, and the lower stratum. Moreover, a social stratum can be formed upon the bases of kinship, clan, tribe or caste, or all four.\n\nThe categorization of people by social strata occurs most clearly in complex state-based, polycentric, or feudal societies, the latter being based upon socio-economic relations among classes of nobility and classes of peasants. Historically, whether or not hunter-gatherer, tribal, and band societies can be defined as socially stratified, or if social stratification otherwise began with agriculture and large-scale means of social exchange, remains a debated matter in the social sciences. Determining the structures of social stratification arises from inequalities of status among persons, therefore, the degree of social inequality determines a person's social stratum. Generally, the greater the social complexity of a society, the more social exist, by way of social differentiation.\n\nSocial stratification is a term used in the social sciences to describe the relative social position of persons in a given social group, category, geographical region or other social unit. It derives from the Latin \"strātum\" (plural '; parallel, horizontal layers) referring to a given society's categorization of its people into rankings of socioeconomic tiers based on factors like wealth, income, social status, occupation and power. In modern Western societies, stratification is often broadly classified into three major divisions of social class: upper class, middle class, and lower class. Each of these classes can be further subdivided into smaller classes (e.g. \"upper middle\"). Social may also be delineated on the basis of kinship ties or caste relations.\n\nThe concept of social stratification is often used and interpreted differently within specific theories. In sociology, for example, proponents of action theory have suggested that social stratification is commonly found in developed societies, wherein a dominance hierarchy may be necessary in order to maintain social order and provide a stable social structure. Conflict theories, such as Marxism, point to the inaccessibility of resources and lack of social mobility found in stratified societies. Many sociological theorists have criticized the fact that the working classes are often unlikely to advance socioeconomically while the wealthy tend to hold political power which they use to exploit the proletariat (laboring class). Talcott Parsons, an American sociologist, asserted that stability and social order are regulated, in part, by universal values. Such values are not identical with \"consensus\" but can indeed be an impetus for social conflict, as has been the case multiple times through history. Parsons never claimed that universal values, in and by themselves, \"satisfied\" the functional prerequisites of a society. Indeed, the constitution of society represents a much more complicated codification of emerging historical factors. Theorists such as Ralf Dahrendorf alternately note the tendency toward an enlarged middle-class in modern Western societies due to the necessity of an educated workforce in technological economies. Various social and political perspectives concerning globalization, such as dependency theory, suggest that these effects are due to changes in the status of workers to the third world.\n\nFour principles are posited to underlie social stratification. First, social stratification is socially defined as a property of a society rather than individuals in that society. Second, social stratification is reproduced from generation to generation. Third, social stratification is universal (found in every society) but variable (differs across time and place). Fourth, social stratification involves not just quantitative inequality but qualitative beliefs and attitudes about social status.\n\nAlthough stratification is not limited to complex societies, all complex societies exhibit features of stratification. In any complex society, the total stock of valued goods is distributed unequally, wherein the most privileged individuals and families enjoy a disproportionate share of income, power, and other valued social resources. The term \"stratification system\" is sometimes used to refer to the complex social relationships and social structures that generate these observed inequalities. The key components of such systems are: (a) social-institutional processes that define certain types of goods as valuable and desirable, (b) the rules of allocation that distribute goods and resources across various positions in the division of labor (e.g., physician, farmer, ‘housewife’), and (c) the social mobility processes that link individuals to positions and thereby generate unequal control over valued resources.\n\nSocial mobility is the movement of individuals, social groups or categories of people between the layers or within a stratification system. This movement can be intragenerational (within a generation) or intergenerational (between two or more generations). Such mobility is sometimes used to classify different systems of social stratification. Open stratification systems are those that allow for mobility between , typically by placing value on the achieved status characteristics of individuals. Those societies having the highest levels of intragenerational mobility are considered to be the most open and malleable systems of stratification. Those systems in which there is little to no mobility, even on an intergenerational basis, are considered closed stratification systems. For example, in caste systems, all aspects of social status are ascribed, such that one's social position at birth persists throughout one's lifetime.\n\nIn Marxist theory, the modern mode of production consists of two main economic parts: the base and the superstructure. The base encompasses the relations of production: employer–employee work conditions, the technical division of labour, and property relations. Social class, according to Marx, is determined by one's relationship to the means of production. There exist at least two classes in any class-based society: the owners of the means of production and those who sell their labor to the owners of the means of production. At times, Marx almost hints that the ruling classes seem to own the working class itself as they only have their own labor power ('wage labor') to offer the more powerful in order to survive. These relations fundamentally determine the ideas and philosophies of a society and additional classes may form as part of the superstructure. Through the ideology of the ruling class—throughout much of history, the land-owning aristocracy—false consciousness is promoted both through political and non-political institutions but also through the arts and other elements of culture. When the aristocracy falls, the bourgeoisie become the owners of the means of production in the capitalist system. Marx predicted the capitalist mode would eventually give way, through its own internal conflict, to revolutionary consciousness and the development of more egalitarian, more communist societies.\n\nMarx also described two other classes, the petite bourgeoisie and the lumpenproletariat. The petite bourgeoisie is like a small business class that never really accumulates enough profit to become part of the bourgeoisie, or even challenge their status. The lumpenproletariat is the underclass, those with little to no social status. This includes prostitutes, beggars, the homeless or other untouchables in a given society. Neither of these subclasses has much influence in Marx's two major classes, but it is helpful to know that Marx did recognize differences within the classes.\n\nAccording to Marvin Harris and Tim Ingold, Lewis Henry Morgan's accounts of egalitarian hunter-gatherers formed part of Karl Marx' and Friedrich Engels' inspiration for communism. Morgan spoke of a situation in which people living in the same community pooled their efforts and shared the rewards of those efforts fairly equally. He called this \"communism in living.\" But when Marx expanded on these ideas, he still emphasized an economically oriented culture, with property defining the fundamental relationships between people. Yet, issues of ownership and property are arguably less emphasized in hunter-gatherer societies. This, combined with the very different social and economic situations of hunter-gatherers may account for many of the difficulties encountered when implementing communism in industrialized states. As Ingold points out: \"The notion of communism, removed from the context of domesticity and harnessed to support a project of social engineering for large-scale, industrialized states with populations of millions, eventually came to mean something quite different from what Morgan had intended: namely, a principle of redistribution that would override all ties of a personal or familial nature, and cancel out their effects.\"\n\nThe counter-argument to Marxist's conflict theory is the theory of structural functionalism, argued by Kingsley Davis and Wilbert Moore, which states that social inequality places a vital role in the smooth operation of a society. The Davis–Moore hypothesis argues that a position does not bring power and prestige because it draws a high income; rather, it draws a high income because it is functionally important and the available personnel is for one reason or another scarce. Most high-income jobs are difficult and require a high level of education to perform, and their compensation is a motivator in society for people to strive to achieve more.\n\nMax Weber was strongly influenced by Marx's ideas but rejected the possibility of effective communism, arguing that it would require an even greater level of detrimental social control and bureaucratization than capitalist society. Moreover, Weber criticized the dialectical presumption of a proletariat revolt, maintaining it to be unlikely. Instead, he develops a three-component theory of stratification and the concept of life chances. Weber held there are more class divisions than Marx suggested, taking different concepts from both functionalist and Marxist theories to create his own system. He emphasizes the difference between class, status and power, and treats these as separate but related sources of power, each with different effects on social action. Working half a century later than Marx, Weber claims there to be four main social classes: the upper class, the white collar workers, the petite bourgeoisie, and the manual working class. Weber's theory more-closely resembles contemporary Western class structures, although economic status does not currently seem to depend strictly on earnings in the way Weber envisioned.\n\nWeber derives many of his key concepts on social stratification by examining the social structure of Germany. He notes that, contrary to Marx's theories, stratification is based on more than simple ownership of capital. Weber examines how many members of the aristocracy lacked economic wealth yet had strong political power. Many wealthy families lacked prestige and power, for example, because they were Jewish. Weber introduced three independent factors that form his theory of stratification hierarchy, which are; class, status, and power:\n\nC. Wright Mills, drawing from the theories of Vilfredo Pareto and Gaetano Mosca, contends that the imbalance of power in society derives from the complete absence of countervailing powers against corporate leaders of the Power elite. Mills both incorporated and revised Marxist ideas. While he shared Marx's recognition of a dominant wealthy and powerful class, Mills believed that the source for that power lay not only in the economic realm but also in the political and military arenas. During the 1950s, Mills stated that hardly anyone knew about the power elite's existence, some individuals (including the elite themselves) denied the idea of such a group, and other people vaguely believed that a small formation of a powerful elite existed. \"Some prominent individuals knew that Congress had permitted a handful of political leaders to make critical decisions about peace and war; and that two atomic bombs had been dropped on Japan in the name of the United States, but neither they nor anyone they knew had been consulted.\"\n\nMills explains that the power elite embody a privileged class whose members are able to recognize their high position within society. In order to maintain their highly exalted position within society, members of the power elite tend to marry one another, understand and accept one another, and also work together. The most crucial aspect of the power elite's existence lays within the core of education. \"Youthful upper-class members attend prominent preparatory schools, which not only open doors to such elite universities as Harvard, Yale, and Princeton but also to the universities' highly exclusive clubs. These memberships in turn pave the way to the prominent social clubs located in all major cities and serving as sites for important business contacts.\" Examples of elite members who attended prestigious universities and were members of highly exclusive clubs can be seen in George W. Bush and John Kerry. Both Bush and Kerry were members of the Skull and Bones club while attending Yale University. This club includes members of some of the most powerful men of the twentieth century, all of which are forbidden to tell others about the secrets of their exclusive club. Throughout the years, the Skull and Bones club has included presidents, cabinet officers, Supreme Court justices, spies, captains of industry, and often their sons and daughters join the exclusive club, creating a social and political network like none ever seen before.\n\nThe upper class individuals who receive elite educations typically have the essential background and contacts to enter into the three branches of the power elite: The political leadership, the military circle, and the corporate elite.\n\n\nMills shows that the power elite has an \"inner-core\" made up of individuals who are able to move from one position of institutional power to another; for example, a prominent military officer who becomes a political adviser or a powerful politician who becomes a corporate executive. \"These people have more knowledge and a greater breadth of interests than their colleagues. Prominent bankers and financiers, who Mills considered 'almost professional go-betweens of economic, political, and military affairs,' are also members of the elite's inner core.\n\nMost if not all anthropologists dispute the \"universal\" nature of social stratification, holding that it is not the standard among all societies. John Gowdy (2006) writes, \"Assumptions about human behaviour that members of market societies believe to be universal, that humans are naturally competitive and acquisitive, and that social stratification is natural, do not apply to many hunter-gatherer peoples. Non-stratified egalitarian or acephalous (\"headless\") societies exist which have little or no concept of social hierarchy, political or economic status, class, or even permanent leadership.\n\nAnthropologists identify egalitarian cultures as \"kinship-oriented,\" because they appear to value social harmony more than wealth or status. These cultures are contrasted with economically oriented cultures (including states) in which status and material wealth are prized, and stratification, competition, and conflict are common. Kinship-oriented cultures actively work to prevent social hierarchies from developing because they believe that such stratification could lead to conflict and instability. Reciprocal altruism is one process by which this is accomplished.\n\nA good example is given by Richard Borshay Lee in his account of the Khoisan, who practice \"insulting the meat.\" Whenever a hunter makes a kill, he is ceaselessly teased and ridiculed (in a friendly, joking fashion) to prevent him from becoming too proud or egotistical. The meat itself is then distributed evenly among the entire social group, rather than kept by the hunter. The level of teasing is proportional to the size of the kill. Lee found this out when he purchased an entire cow as a gift for the group he was living with, and was teased for weeks afterward about it (since obtaining that much meat could be interpreted as showing off).\n\nAnother example is the Indigenous Australians of Groote Eylandt and Bickerton Island, off the coast of Arnhem Land, who have arranged their entire society—spiritually and economically—around a kind of gift economy called \"renunciation.\" According to David H. Turner, in this arrangement, every person is expected to give \"everything\" of any resource they have to any other person who needs or lacks it at the time. This has the benefit of largely eliminating social problems like theft and relative poverty. However, misunderstandings obviously arise when attempting to reconcile Aboriginal \"renunciative economics\" with the competition/scarcity-oriented economics introduced to Australia by Anglo-European colonists.\n\nThe social status variables underlying social stratification are based in social perceptions and attitudes about various characteristics of persons and peoples. While many such variables cut across time and place, the relative weight placed on each variable and specific combinations of these variables will differ from place to place over time. One task of research is to identify accurate mathematical models that explain how these many variables combine to produce stratification in a given society. Grusky (2011) provides a good overview of the historical development of sociological theories of social stratification and a summary of contemporary theories and research in this field. While many of the variables that contribute to an understanding of social stratification have long been identified, models of these variables and their role in constituting social stratification are still an active topic of theory and research. In general, sociologists recognize that there are no \"pure\" economic variables, as social factors are integral to economic value. However, the variables posited to affect social stratification can be loosely divided into economic and other social factors.\n\nStrictly quantitative economic variables are more useful to describing social stratification than explaining how social stratification is constituted or maintained. Income is the most common variable used to describe stratification and associated economic inequality in a society. However, the distribution of individual or household accumulation of surplus and wealth tells us more about variation in individual well-being than does income, alone. Wealth variables can also more vividly illustrate salient variations in the well-being of groups in stratified societies. Gross Domestic Product (GDP), especially \"per capita\" GDP, is sometimes used to describe economic inequality and stratification at the international or global level.\n\nSocial variables, both quantitative and qualitative, typically provide the most explanatory power in causal research regarding social stratification, either as independent variables or as intervening variables. Three important social variables include gender, race, and ethnicity, which, at the least, have an intervening effect on social status and stratification in most places throughout the world. Additional variables include those that describe other ascribed and achieved characteristics such as occupation and skill levels, age, education level, education level of parents, and geographic area. Some of these variables may have both causal and intervening effects on social status and stratification. For example, absolute age may cause a low income if one is too young or too old to perform productive work. The social perception of age and its role in the workplace, which may lead to ageism, typically has an intervening effect on employment and income.\n\nSocial scientists are sometimes interested in quantifying the degree of economic stratification between different social categories, such as men and women, or workers with different levels of education. An index of stratification has been recently proposed by Zhou for this purpose.\n\nGender is one of the most pervasive and prevalent social characteristics which people use to make social distinctions between individuals. Gender distinctions are found in economic-, kinship- and caste-based stratification systems. Social role expectations often form along sex and gender lines. Entire societies may be classified by social scientists according to the rights and privileges afforded to men or women, especially those associated with ownership and inheritance of property. In patriarchal societies, such rights and privileges are normatively granted to men over women; in matriarchal societies, the opposite holds true. Sex- and gender-based division of labor is historically found in the annals of most societies and such divisions increased with the advent of industrialization. Sex-based wage discrimination exists in some societies such that men, typically, receive higher wages than women for the same type of work. Other differences in employment between men and women lead to an overall gender-based pay-gap in many societies, where women as a category earn less than men due to the types of jobs which women are offered and take, as well as to differences in the number of hours worked by women. These and other gender-related values affect the distribution of income, wealth, and property in a given social order.\n\nRacism consists of both prejudice and discrimination based in social perceptions of observable biological differences between peoples. It often takes the form of social actions, practices or beliefs, or political systems in which different races are perceived to be ranked as inherently superior or inferior to each other, based on presumed shared inheritable traits, abilities, or qualities. In a given society, those who share racial characteristics socially perceived as undesirable are typically under-represented in positions of social power, i.e., they become a minority category in that society. Minority members in such a society are often subjected to discriminatory actions resulting from majority policies, including assimilation, exclusion, oppression, expulsion, and extermination. Overt racism usually feeds directly into a stratification system through its effect on social status. For example, members associated with a particular race may be assigned a slave status, a form of oppression in which the majority refuses to grant basic rights to a minority that are granted to other members of the society. More covert racism, such as that which many scholars posit is practiced in more contemporary societies, is socially hidden and less easily detectable. Covert racism often feeds into stratification systems as an intervening variable affecting income, educational opportunities, and housing. Both overt and covert racism can take the form of structural inequality in a society in which racism has become institutionalized.\n\nEthnic prejudice and discrimination operate much the same as do racial prejudice and discrimination in society. In fact, only recently have scholars begun to differentiate race and ethnicity; historically, the two were considered to be identical or closely related. With the scientific development of genetics and the human genome as fields of study, most scholars now recognize that race is socially defined on the basis of biologically determined characteristics that can be observed within a society while ethnicity is defined on the basis of culturally learned behavior. Ethnic identification can include shared cultural heritage such as language and dialect, symbolic systems, religion, mythology and cuisine. As with race, ethnic categories of persons may be socially defined as minority categories whose members are under-represented in positions of social power. As such, ethnic categories of persons can be subject to the same types of majority policies. Whether ethnicity feeds into a stratification system as a direct, causal factor or as an intervening variable may depend on the level of ethnographic entrism within each of the various ethnic populations in a society, the amount of conflict over scarce resources, and the relative social power held within each ethnic category.\n\nThe world and the pace of social change today are very different than in the time of Karl Marx, Max Weber, or even C. Wright Mills. Globalizing forces lead to rapid international integration arising from the interchange of world views, products, ideas, and other aspects of culture. Advances in transportation and telecommunications infrastructure, including the rise of the telegraph and its modern representation the Internet, are major factors in globalization, generating further interdependence of economic and cultural activities.\n\nLike a stratified class system within a nation, looking at the world economy one can see class positions in the unequal distribution of capital and other resources between nations. Rather than having separate national economies, nations are considered as participating in this world economy. The world economy manifests a global division of labor with three overarching classes: core countries, semi-periphery countries and periphery countries, according to World-systems and Dependency theories. Core nations primarily own and control the major means of production in the world and perform the higher-level production tasks and provide international financial services. Periphery nations own very little of the world's means of production (even when factories are located in periphery nations) and provide low to non-skilled labor. Semiperipheral nations are midway between the core and periphery. They tend to be countries moving towards industrialization and more diversified economies. Core nations receive the greatest share of surplus production, and periphery nations receive the least. Furthermore, core nations are usually able to purchase raw materials and other goods from noncore nations at low prices, while demanding higher prices for their exports to noncore nations. A global workforce employed through a system of global labor arbitrage ensures that companies in core countries can utilize the cheapest semi-and non-skilled labor for production.\n\nToday we have the means to gather and analyze data from economies across the globe. Although many societies worldwide have made great strides toward more equality between differing geographic regions, in terms of the standard of living and life chances afforded to their peoples, we still find large gaps between the wealthiest and the poorest within a nation and between the wealthiest and poorest nations of the world. A January 2014 Oxfam report indicates that the 85 wealthiest individuals in the world have a combined wealth equal to that of the bottom 50% of the world's population, or about 3.5 billion people. By contrast, for 2012, the World Bank reports that 21 percent of people worldwide, around 1.5 billion, live in extreme poverty, at or below $1.25 a day. Zygmunt Bauman has provocatively observed that the rise of the rich is linked to their capacity to lead highly mobile lives: \"Mobility climbs to the rank of the uppermost among coveted values -and the freedom to move, perpetually a scarce and unequally distributed commodity, fast becomes the main stratifying factor of our late modern or postmodern time.\"\n"}
{"id": "7257", "url": "https://en.wikipedia.org/wiki?curid=7257", "title": "Caste", "text": "Caste\n\nCaste is a form of social stratification characterized by endogamy, hereditary transmission of a style of life which often includes an occupation, ritual status in a hierarchy, and customary social interaction and exclusion based on cultural notions of purity and pollution. Its paradigmatic ethnographic example is the division of India's Hindu society into rigid social groups, with roots in India's ancient history and persisting to the present time. However, the economic significance of the caste system in India has been declining as a result of urbanization and affirmative action programs. A subject of much scholarship by sociologists and anthropologists, the Hindu caste system is sometimes used as an analogical basis for the study of caste-like social divisions existing outside Hinduism and India. The term \"caste\" is also applied to morphological groupings in female populations of ants and bees.\n\nThe English word \"caste\" derives from the Spanish and Portuguese \"casta\", which, according to the John Minsheu's Spanish dictionary (1569), means \"race, lineage, tribe or breed\". When the Spanish colonized the New World, they used the word to mean a \"clan or lineage\". It was, however, the Portuguese who first employed \"casta\" in the primary modern sense of the English word 'caste' when they applied it to the thousands of endogamous, hereditary Indian social groups they encountered upon their arrival in India in 1498. The use of the spelling \"caste\", with this latter meaning, is first attested in English in 1613.\n\nModern India's caste system is based on the artificial superimposition of a four-fold theoretical classification called the \"Varna\" on the natural social groupings called the \"Jāti\". From 1901 onwards, for the purposes of the Decennial Census, the British classified all Jātis into one or the other of the \"Varna\" categories as described in ancient texts. Herbert Hope Risley, the Census Commissioner, noted that \"The principle suggested as a basis was that of classification by social precedence as recognized by native public opinion at the present day, and manifesting itself in the facts that particular castes are supposed to be the modern representatives of one or other of the castes of the theoretical Indian system.\" The system of \"Varnas\" propounded in ancient Hindu texts envisages the society divided into four classes: Brahmins (scholars and yajna priests), Kshatriyas (rulers and warriors), Vaishyas (farmers, merchants and artisans) and Shudras (workmen/service providers). The texts do not mention any separate, untouchable category in \"Varna\" classification. Scholars believe that the \"Varnas\" system was never truly operational in society and there is no evidence of it ever being a reality in Indian history. The practical division of the society had always been in terms of \"Jātis\" (birth groups), which are not based on any specific principle, but could vary from ethnic origins to occupations to geographic areas. The \"Jātis\" have been endogamous groups without any fixed hierarchy but subject to vague notions of rank articulated over time based on lifestyle and social, political or economic status. Many of India's major empires and dynasties like the Mauryas, Shalivahanas, Chalukyas, Kakatiyas among many others, were founded by people who would have been classified as Shudras, under the \"Varnas\" system. It is well established that by the 9th century, kings from all the four castes, including Brahmins and Vaishyas, had occupied the highest seat in the monarchical system in Hindu India, contrary to the Varna theory. In many instances, as in Bengal, historically the kings and rulers had been called upon, when required, to mediate on the ranks of \"Jātis\", which might number in thousands all over the subcontinent and vary by region. In practice, the \"jātis\" may or may not fit into the \"Varna\" classes and many prominent \"Jatis\", for example the Jats and Yadavs, straddled two Varnas i.e. Kshatriyas and Vaishyas, and the \"Varna\" status of \"Jātis\" itself was subject to articulation over time.\n\nStarting with the British colonial Census of 1901 led by Herbert Hope Risley, all the \"jātis\" were grouped under the theoretical \"varnas\" categories. According to political scientist Lloyd Rudolph, Risley believed that \"varna\", however ancient, could be applied to all the modern castes found in India, and \"[he] meant to identify and place several hundred million Indians within it.\" In an effort to arrange various castes in order of precedence functional grouping was based less on the occupation that prevailed in each case in the present day than on that which was traditional with it, or which gave rise to its differentiation from the rest of the community. \"This action virtually removed Indians from the progress of history and condemned them to an unchanging position and place in time. In one sense, it is rather ironic that the British, who continually accused the Indian people of having a static society, should then impose a construct that denied progress\" The terms \"varna\" (conceptual classification based on occupation) and \"jāti\" (groups) are two distinct concepts: while \"varna\" is the idealised four-part division envisaged by the Twice-Born, \"jāti\" (community) refers to the thousands of actual endogamous groups prevalent across the subcontinent. The classical authors scarcely speak of anything other than the \"varnas\", as it provided a convenient shorthand; but a problem arises when even Indologists sometimes confuse the two. Thus, starting with the 1901 Census, Caste officially became India's essential institution, with an imprimatur from the British administrators, augmenting a discourse that had already dominated Indology. “Despite India's acquisition of formal political independence, it has still not regained the power to know its own past and present apart from that discourse”.\n\nUpon independence from Britain, the Indian Constitution listed 1,108 castes across the country as Scheduled Castes in 1950, for positive discrimination. The Untouchable communities are sometimes called \"Scheduled Castes\", \"Dalit\" or \"Harijan\" in contemporary literature. In 2001, Dalits were 16.2% of India's population. Most of the 15 million bonded child workers are from the lowest castes.\n\nIndependent India has witnessed caste-related violence. In 2005, government recorded approximately 110,000 cases of reported violent acts, including rape and murder, against Dalits. For 2012, the government recorded 651 murders, 3,855 injuries, 1,576 rapes, 490 kidnappings, and 214 cases of arson.\n\nThe socio-economic limitations of the caste system are reduced due to urbanization and affirmative action. Nevertheless, the caste system still exists in endogamy and patrimony, and thrives in the politics of democracy, where caste provides ready made constituencies to politicians. The globalization and economic opportunities from foreign businesses has influenced the growth of India's middle-class population. Some members of the Chhattisgarh Potter Caste Community (CPCC) are middle-class urban professionals and no longer potters unlike the remaining majority of traditional rural potter members. There is persistence of caste in Indian politics. Caste associations have evolved into caste-based political parties. Political parties and the state perceive caste as an important factor for mobilization of people and policy development.\n\nStudies by Bhatt and Beteille have shown changes in status, openness, mobility in the social aspects of Indian society. As a result of the modern social pressures on the country, India is experiencing a change in their social sphere dynamic as well as economically in the caste system. While arranged marriages are still the most common practice in India, the internet has provided a network for younger Indians to take control of their relationships through the use of dating apps. This remains isolated to informal terms, as marriage is not often achieved through the use of these apps. Hypergamy is still a common practice in India and Hindu culture. Men are expected to marry within their caste, or one below, with no social repercussions. If a woman marries into a higher caste, then her children will take the status of their father. If she marries down, her family is reduced to the social status of their son in law. In this case, the women are bearers of the egalitarian principle of the marriage. There would be no benefit in marrying a higher caste if the terms of the marriage did not imply equality. However, men are systematically shielded from the negative implications of the agreement.\n\nGeographical factors also determine adherence to the caste system. Many Northern villages are more likely to participate in exogamous marriage, due to a lack of eligible suitors within the same caste. Women in North India have been found to be less likely to leave or divorce their husbands since they are of a relatively lower caste system, and have higher restrictions on their freedoms. On the other hand, Pahari women, of the northern mountains, have much more freedom to leave their husbands without stigma. This often leads to better husbandry as his actions are not protected by social expectations.\n\nChiefly among the factors influencing the rise of exogamy is the rapid urbanisation in India experienced over the last century. It is well known that urban centers tend to be less reliant on tradition and are more progressive as a whole. As India’s cities boomed in population, the job market grew to keep pace. Prosperity and stability were now more easily attained by an individual, and the anxiety to marry quickly and effectively was reduced as traditional marriage was viewed as a means to attain these principles. Thus, younger, more progressive generations of urban Indians are less likely than ever to participate in the antiquated system of arranged endogamy.\n\nIndia has also experimented with Affirmative Action, locally known as “reservation groups”. Quota system jobs, as well as placements in publicly funded colleges, hold spots for the 8% of India’s minority, and underprivileged groups. As a result, in states such as Tamil Nadu or those in the north-east, where underprivileged populations predominate, over 80% of government jobs are set aside in quotas. In education, colleges lower the marks necessary for the Dalits to enter.\n\nThe Nepalese caste system resembles that of the Indian \"jāti\" system with numerous \"jāti\" divisions with a \"varna\" system superimposed for a rough equivalence. But since the culture and the society is different some of the things are different. Inscriptions attest the beginnings of a caste system during the Licchavi period. Jayasthiti Malla (1382–1395) categorized Newars into 64 castes (Gellner 2001). A similar exercise was made during the reign of Mahindra Malla (1506–1575). The Hindu social code was later set up in Gorkha by Ram Shah (1603–1636).\n\nMcKim Marriott claims a social stratification that is hierarchical, closed, endogamous and hereditary is widely prevalent, particularly in western parts of Pakistan. Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes.\n\nThe caste system in Sri Lanka is a division of society into strata, influenced by the textbook \"varnas\" and \"jāti\" system found in India. Ancient Sri Lankan texts such as the Pujavaliya, Sadharmaratnavaliya and Yogaratnakaraya and inscriptional evidence show that the above hierarchy prevailed throughout the feudal period. The repetition of the same caste hierarchy even as recently as the 18th century, in the British/Kandyan period Kadayimpoth – Boundary books as well, indicates the continuation of the tradition right up to the end of Sri Lanka's monarchy.\n\nBalinese caste structure has been described in early 20th-century European literature to be based on three categories – triwangsa (thrice born) or the nobility, \"dwijāti\" (twice born) in contrast to \"ekajāti\" (once born) the low folks. Four statuses were identified in these sociological studies, spelled a bit differently from the caste categories for India:\n\nThe Brahmana caste was further subdivided by these Dutch ethnographers into two: Siwa and Buda. The Siwa caste was subdivided into five: Kemenuh, Keniten, Mas, Manuba and Petapan. This classification was to accommodate the observed marriage between higher-caste Brahmana men with lower-caste women. The other castes were similarly further sub-classified by these 19th-century and early-20th-century ethnographers based on numerous criteria ranging from profession, endogamy or exogamy or polygamy, and a host of other factors in a manner similar to \"castas\" in Spanish colonies such as Mexico, and caste system studies in British colonies such as India.\n\nIn the Philippines, pre-colonial societies do not have a single social structure. The class structures can be roughly categorized into four types:\n\nDuring the period of Yuan Dynasty, ruler Kublai Khan enforced a \"Four Class System\", which was a legal caste system. The order of four classes of people was maintained by the information of the descending order were:\n\nToday, the Hukou system is considered by various sources as the current caste system of China.\n\nThere is also significant controversy over the social classes of Tibet, especially with regards to the serfdom in Tibet controversy.\n\nIn Japan's history, social strata based on inherited position rather than personal merit, were rigid and highly formalized in a system called \"mibunsei\" (身分制). At the top were the Emperor and Court nobles (kuge), together with the Shōgun and daimyō. Below them, the population was divided into four classes: samurai, peasants, craftsmen and merchants. Only samurai were allowed to bear arms. A samurai had a right to kill any peasants, craftsman or merchant who he felt were disrespectful. Merchants were the lowest caste because they did not produce any products. The castes were further sub-divided; for example, peasants were labelled as \"furiuri\", \"tanagari\", \"mizunomi-byakusho\" among others. As in Europe, the castes and sub-classes were of the same race, religion and culture.\n\nHowell, in his review of Japanese society notes that if a Western power had colonized Japan in the 19th century, they would have discovered and imposed a rigid four-caste hierarchy in Japan.\n\nDe Vos and Wagatsuma observe that Japanese society had a systematic and extensive caste system. They discuss how alleged caste impurity and alleged racial inferiority, concepts often assumed to be different, are superficial terms, and are due to identical inner psychological processes, which expressed themselves in Japan and elsewhere.\n\nEndogamy was common because marriage across caste lines was socially unacceptable.\n\nJapan had its own untouchable caste, shunned and ostracized, historically referred to by the insulting term \"Eta\", now called \"Burakumin\". While modern law has officially abolished the class hierarchy, there are reports of discrimination against the Buraku or Burakumin underclasses. The Burakumin are regarded as \"ostracised\". The burakumin are one of the main minority groups in Japan, along with the Ainu of Hokkaidō and those of residents of Korean and Chinese descent.\n\nThe baekjeong (백정) were an \"untouchable\" outcaste of Korea. The meaning today is that of butcher. It originates in the Khitan invasion of Korea in the 11th century. The defeated Khitans who surrendered were settled in isolated communities throughout Goryeo to forestall rebellion. They were valued for their skills in hunting, herding, butchering, and making of leather, common skill sets among nomads. Over time, their ethnic origin was forgotten, and they formed the bottom layer of Korean society.\n\nIn 1392, with the foundation of the Confucian Joseon dynasty, Korea systemised its own native class system. At the top were the two official classes, the Yangban, which literally means \"two classes\". It was composed of scholars (\"munban\") and warriors (\"muban\"). Scholars had a significant social advantage over the warriors. Below were the \"jung-in\" (중인-中人: literally \"middle people\". This was a small class of specialized professions such as medicine, accounting, translators, regional bureaucrats, etc. Below that were the \"sangmin\" (상민-常民: literally 'commoner'), farmers working their own fields. Korea also had a serf population known as the \"nobi\". The nobi population could fluctuate up to about one third of the population, but on average the nobi made up about 10% of the total population. In 1801, the vast majority of government nobi were emancipated, and by 1858 the nobi population stood at about 1.5% of the total population of Korea. The hereditary nobi system was officially abolished around 1886–87 and the rest of the nobi system was abolished with the Gabo Reform of 1894, but traces remained until 1930.\n\nThe opening of Korea to foreign Christian missionary activity in the late 19th century saw some improvement in the status of the \"baekjeong\". However, everyone was not equal under the Christian congregation, and even so protests erupted when missionaries tried to integrate \"baekjeong\" into worship, with non-\"baekjeong\" finding this attempt insensitive to traditional notions of hierarchical advantage. Around the same time, the \"baekjeong\" began to resist open social discrimination. They focused on social and economic injustices affecting them, hoping to create an egalitarian Korean society. Their efforts included attacking social discrimination by upper class, authorities, and \"commoners\", and the use of degrading language against children in public schools.\n\nWith the Gabo reform of 1896, the class system of Korea was officially abolished. Following the collapse of the Gabo government, the new cabinet, which became the Gwangmu government after the establishment of the Korean Empire, introduced systematic measures for abolishing the traditional class system. One measure was the new household registration system, reflecting the goals of formal social equality, which was implemented by the loyalists' cabinet. Whereas the old registration system signified household members according to their hierarchical social status, the new system called for an occupation.\n\nWhile most Koreans by then had surnames and even bongwan, although still substantial number of cheonmin, mostly consisted of serfs and slaves, and untouchables did not. According to the new system, they were then required to fill in the blanks for surname in order to be registered as constituting separate households. Instead of creating their own family name, some cheonmins appropriated their masters' surname, while others simply took the most common surname and its bongwan in the local area. Along with this example, activists within and outside the Korean government had based their visions of a new relationship between the government and people through the concept of citizenship, employing the term \"inmin\" (\"people\") and later, \"kungmin\" (\"citizen\").\n\nThe Committee for Human Rights in North Korea reported that \"Every North Korean citizen is assigned a heredity-based class and socio-political rank over which the individual exercises no control but which determines all aspects of his or her life.\" Called \"Songbun\", Barbara Demick describes this \"class structure\" as an updating of the hereditary \"caste system\", a combination of Confucianism and Stalinism. She claims that a bad family background is called \"tainted blood\", and that by law this \"tainted blood\" lasts three generations.\n\n has put forth the argument that pre-1950s Tibetan society was functionally a caste system, in contrast to previous scholars who defined the Tibetan social class system as similar to European feudal serfdom, as well as non-scholarly western accounts which seek to romanticize a supposedly 'egalitarian' ancient Tibetan society.\n\nYezidi society is hierarchical. The secular leader is a hereditary emir or prince, whereas a chief sheikh heads the religious hierarchy. The Yazidi are strictly endogamous; members of the three Yazidi castes, the murids, sheikhs and pirs, marry only within their group.\n\nPre-Islamic Sassanid society was immensely complex, with separate systems of social organization governing numerous different groups within the empire. Historians believe society comprised four\nsocial classes:\n\n\nIn Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Estimates put their number at over 3.5 million residents who are discriminated, out of a total Yemeni population of around 22 million.\n\nVarious sociologists have reported caste systems in Africa. The specifics of the caste systems have varied in ethnically and culturally diverse Africa, however the following features are common – it has been a closed system of social stratification, the social status is inherited, the castes are hierarchical, certain castes are shunned while others are merely endogamous and exclusionary. In some cases, concepts of purity and impurity by birth have been prevalent in Africa. In other cases, such as the \"Nupe\" of Nigeria, the \"Beni Amer\" of East Africa, and the \"Tira\" of Sudan, the exclusionary principle has been driven by evolving social factors.\n\nAmong the Igbo of Nigeria – especially Enugu, Anambra, Imo, Abia, Ebonyi, Edo and Delta states of the country – Obinna finds Osu caste system has been and continues to be a major social issue. The Osu caste is determined by one's birth into a particular family irrespective of the religion practised by the individual. Once born into Osu caste, this Nigerian person is an outcast, shunned and ostracized, with limited opportunities or acceptance, regardless of his or her ability or merit. Obinna discusses how this caste system-related identity and power is deployed within government, Church and indigenous communities.\n\nThe \"osu\" class systems of eastern Nigeria and southern Cameroon are derived from indigenous religious beliefs and discriminate against the \"Osus\" people as \"owned by deities\" and outcasts.\n\nThe Songhai economy was based on a caste system. The most common were metalworkers, fishermen, and carpenters. Lower caste participants consisted of mostly non-farm working immigrants, who at times were provided special privileges and held high positions in society. At the top were noblemen and direct descendants of the original Songhai people, followed by freemen and traders.\n\nIn a review of social stratification systems in Africa, Richter reports that the term caste has been used by French and American scholars to many groups of West African artisans. These groups have been described as inferior, deprived of all political power, have a specific occupation, are hereditary and sometimes despised by others. Richter illustrates caste system in Ivory Coast, with six sub-caste categories. Unlike other parts of the world, mobility is sometimes possible within sub-castes, but not across caste lines. Farmers and artisans have been, claims Richter, distinct castes. Certain sub-castes are shunned more than others. For example, exogamy is rare for women born into families of woodcarvers.\n\nSimilarly, the Mandé societies in Gambia, Ghana, Guinea, Ivory Coast, Liberia, Senegal and Sierra Leone have social stratification systems that divide society by ethnic ties. The Mande class system regards the \"jonow\" slaves as inferior. Similarly, the Wolof in Senegal is divided into three main groups, the \"geer\" (freeborn/nobles), \"jaam\" (slaves and slave descendants) and the underclass \"neeno\". In various parts of West Africa, Fulani societies also have class divisions. Other castes include \"Griots\", \"Forgerons\", and \"Cordonniers\".\n\nTamari has described endogamous castes of over fifteen West African peoples, including the Tukulor, Songhay, Dogon, Senufo, Minianka, Moors, Manding, Soninke, Wolof, Serer, Fulani, and Tuareg. Castes appeared among the \"Malinke\" people no later than 14th century, and was present among the \"Wolof\" and \"Soninke\", as well as some \"Songhay\" and \"Fulani\" populations, no later than 16th century. Tamari claims that wars, such as the \"Sosso-Malinke\" war described in the \"Sunjata\" epic, led to the formation of blacksmith and bard castes among the people that ultimately became the Mali empire.\n\nAs West Africa evolved over time, sub-castes emerged that acquired secondary specializations or changed occupations. Endogamy was prevalent within a caste or among a limited number of castes, yet castes did not form demographic isolates according to Tamari. Social status according to caste was inherited by off-springs automatically; but this inheritance was paternal. That is, children of higher caste men and lower caste or slave concubines would have the caste status of the father.\n\nEthel M. Albert in 1960 claimed that the societies in Central Africa were caste-like social stratification systems. Similarly, in 1961, Maquet notes that the society in Rwanda and Burundi can be best described as castes. The Tutsi, noted Maquet, considered themselves as superior, with the more numerous Hutu and the least numerous Twa regarded, by birth, as respectively, second and third in the hierarchy of Rwandese society. These groups were largely endogamous, exclusionary and with limited mobility.\n\nIn a review published in 1977, Todd reports that numerous scholars report a system of social stratification in different parts of Africa that resembles some or all aspects of caste system. Examples of such caste systems, he claims, are to be found in Ethiopia in communities such as the Gurage and Konso. He then presents the Dime of Southwestern Ethiopia, amongst whom there operates a system which Todd claims can be unequivocally labelled as caste system. The Dime have seven castes whose size varies considerably. Each broad caste level is a hierarchical order that is based on notions of purity, non-purity and impurity. It uses the concepts of defilement to limit contacts between caste categories and to preserve the purity of the upper castes. These caste categories have been exclusionary, endogamous and the social identity inherited. Alula Pankhurst has published a study of caste groups in SW Ethiopia.\n\nAmong the Kafa, there were also traditionally groups labeled as castes. \"Based on research done before the Derg regime, these studies generally presume the existence of a social hierarchy similar to the caste system. At the top of this hierarchy were the Kafa, followed by occupational groups including blacksmiths (Qemmo), weavers (Shammano), bards (Shatto), potters, and tanners (Manno). In this hierarchy, the Manjo were commonly referred to as hunters, given the lowest status equal only to slaves.\"\n\nThe Borana Oromo of southern Ethiopia in the Horn of Africa also have a class system, wherein the Wata, an acculturated hunter-gatherer group, represent the lowest class. Though the Wata today speak the Oromo language, they have traditions of having previously spoken another language before adopting Oromo.\n\nThe traditionally nomadic Somali people are divided into clans, wherein the Rahanweyn agro-pastoral clans and the occupational clans such as the Madhiban were traditionally sometimes treated as outcasts. As Gabboye, the Madhiban along with the Yibir and Tumaal (collectively referred to as \"sab\") have since obtained political representation within Somalia, and their general social status has improved with the expansion of urban centers.\n\nFor centuries, through the modern times, the majority regarded Cagots who lived primarily in the Basque region of France and Spain as an inferior caste, the untouchables. While they had the same skin color and religion as the majority, in the churches they had to use segregated doors, drink from segregated fonts, and receive communion on the end of long wooden spoons. It was a closed social system. The socially isolated Cagots were endogamous, and chances of social mobility non-existent.\n\nIn July 2013, the UK government announced its intention to amend the Equality Act 2010, to \"introduce legislation on caste, including any necessary exceptions to the caste provisions, within the framework of domestic discrimination law\". Section 9(5) of the Equality Act 2010 provides that \"a Minister may by order amend the statutory definition of race to include caste and may provide for exceptions in the Act to apply or not to apply to caste\".\n\nFrom September 2013 to February 2014, Meena Dhanda led a project on \"Caste in Britain\" for the UK Equality and Human Rights Commission (EHRC).\n\nIn W. Lloyd Warner's view, the historic relationship between Blacks and Whites in the US showed many caste-like features such as residential segregation and marriage restrictions. Discrimination based upon socio-economic factors are historically prevalent within the country.\n\nAccording to Gerald D. Berreman, in the two systems, there are rigid rules of avoidance and certain types of contacts are defined as contaminating. In India, there are complex religious features which make up the system, whereas in the United States race and color are the basis for differentiation. The caste systems in India and the United States have higher groups which desire to retain their positions for themselves and thus perpetuate the two systems.\n\nThe process of creating a homogenized society by social engineering in both India and the US has created other institutions that have made class distinctions among different groups evident. Anthropologist James C. Scott elaborates on how “global capitalism is perhaps the most powerful force for homogenization, whereas the state may be the defender of local difference and variety in some instances.” The caste system further emphasizes differences between the socio-economic classes that arise as a product of capitalism, which makes social mobility more difficult. Parts of the United States are sometimes divided by race and class status despite the national narrative of integration.\n\nAs a result of increased immigration, many Indian Americans have brought their traditional caste values to the United States. A survey commissioned by Equality Labs finds that caste discrimination is also playing out in the United States. 2/3 of the members of the lowest caste, called Dalits, said that they have faced workplace discrimination due to their caste. 41% of the Dalits who were surveyed said that they have experienced discrimination in education because the caste system is now being practiced in the United States.\n\n\n\n\n"}
{"id": "1364467", "url": "https://en.wikipedia.org/wiki?curid=1364467", "title": "Dalit", "text": "Dalit\n\nDalit, meaning \"broken/scattered\" in Sanskrit and Hindi, is a term mostly used for the ethnic groups in India that have been kept repressed (often termed backward castes). Dalits were excluded from the four-fold varna system of Hinduism and were seen as forming a fifth varna, also known by the name of \"Panchama\". Dalits now profess various religious beliefs, including Hinduism, Buddhism, Sikhism, Christianity and various folk religions. The 2011 Census of India recorded their numbers at over 200 million people, representing 16 percent of India's population.\n\nThe term \"dalits\" was in use as a translation for the British Raj census classification of \"Depressed Classes\" prior to 1935. It was popularised by the economist and reformer B. R. Ambedkar (1891–1956), who included all depressed people irrespective of their caste into the definition of dalits. Hence the first group he made was called the \"Labour Party\" and included as its members all people of the society who were kept depressed, including women, small scale farmers and people from backward castes. Ambedkar himself was a Mahar, and in the 1970s the use of the word \"dalit\" was invigorated when it was adopted by the Dalit Panthers activist group. Gradually, political parties used it to gain mileage. New leaders like Kanhaiya Kumar subscribe to this definition of \"dalits\", thus a Brahmin marginal farmer trying to eke out a living, but unable to do so also falls in the \"dalit\" category.\n\nIndia's National Commission for Scheduled Castes considers official use of \"dalit\" as a label to be \"unconstitutional\" because modern legislation prefers \"Scheduled Castes\"; however, some sources say that \"Dalit\" has encompassed more communities than the official term of \"Scheduled Castes\" and is sometimes used to refer to all of India's oppressed peoples. A similar all-encompassing situation prevails in Nepal.\n\nScheduled Caste communities exist across India, although they are mostly concentrated in four states; they do not share a single language or religion. They comprise 16.6 per cent of India's population, according to the 2011 Census of India. Similar communities are found throughout the rest of South Asia, in Nepal, Pakistan, Bangladesh and Sri Lanka, and are part of the global Indian diaspora.\n\nIn 1932, the British Raj recommended separate electorates to select leaders for Dalits in the Communal Award. This was favoured by Ambedkar but when Mahatma Gandhi opposed the proposal it resulted in the Poona Pact. That in turn influenced the Government of India Act, 1935, which introduced the reservation of seats for the Depressed Classes, now renamed as Scheduled Castes.\n\nFrom soon after its independence in 1947, India introduced a reservation system to enhance the ability of Dalits to have political representation and to obtain government jobs and education. In 1997, India elected its first Dalit President, K. R. Narayanan. Many social organisations have promoted better conditions for Dalits through education, healthcare and employment. Nonetheless, while caste-based discrimination was prohibited and untouchability abolished by the Constitution of India, such practices are still widespread. To prevent harassment, assault, discrimination and similar acts against these groups, the Government of India enacted the Prevention of Atrocities Act, also called the SC/ST Act, on 31 March 1995.\n\nIn accordance with the order of the Bombay High Court, the Information and Broadcasting Ministry (I&B Ministry) of the Government of India issued an advisory to all media channels in September 2018, asking them to use \"Scheduled Castes\" instead of the word \"Dalit\".\n\nThe word \"dalit\" is a vernacular form of the Sanskrit दलित (\"dalita\"). In Classical Sanskrit, this means \"divided, split, broken, scattered\". This word was repurposed in 19th-century Sanskrit to mean \"(a person) not belonging to one of the four Brahminic castes\". It was perhaps first used in this sense by Pune-based social reformer Jyotirao Phule, in the context of the oppression faced by the erstwhile \"untouchable\" castes from other Hindus.\n\n\"Dalit\" is mostly used to describe communities that have been subjected to untouchability. Such people were excluded from the four-fold varna system of Hinduism and thought of themselves as forming a fifth varna, describing themselves as \"Panchama\".\n\nThe term was in use as a translation for the British Raj census classification of \"Depressed Classes\" prior to 1935. It was popularised by the economist and reformer B. R. Ambedkar (1891–1956), himself a Dalit, and in the 1970s its use was invigorated when it was adopted by the Dalit Panthers activist group.\n\n\"Dalit\" has become a political identity, similar to how the LGBTQ community reclaimed \"queer\" from its pejorative use as a neutral or positive self-identifier and as a political identity. Socio-legal scholar Oliver Mendelsohn and political economist Marika Vicziany wrote in 1998 that the term had become \"intensely political ... While use of the term might seem to express an appropriate solidarity with the contemporary face of Untouchable politics, there remain major problems in adopting it as a generic term. Although the word is now quite widespread, it still has deep roots in a tradition of political radicalism inspired by the figure of B. R. Ambedkar.\" They suggested its use risked erroneously labelling the entire population of untouchables in India as being united by a radical politics. Anand Teltumbde also detects a trend towards denial of the politicised identity, for example among educated middle-class people who have converted to Buddhism and argue that, as Buddhists, they cannot be Dalits. This may be due to their improved circumstances giving rise to a desire not to be associated with the what they perceive to be the demeaning Dalit masses.\n\n\"Scheduled Castes\" is the official term for Dalits in the opinion of India's National Commissions for Scheduled Castes (NCSC), who took legal advice that indicated modern legislation does not refer to Dalit and that therefore, it says, it is \"unconstitutional\" for official documents to do so. In 2004, the NCSC noted that some state governments used \"Dalits\" rather than \"Scheduled Castes\" in documentation and asked them to desist.\n\nSome sources say that \"Dalit\" encompasses a broader range of communities than the official \"Scheduled Caste\" definition. It can include nomadic tribes and another official classification that also originated with the British Raj positive discrimination efforts in 1935, being the \"Scheduled Tribes\". It is also sometimes used to refer to the entirety of India's oppressed peoples, which is the context that applies to its use in Nepalese society. An example of the limitations of the \"Scheduled Caste\" category is that, under Indian law, such people can only be followers of Buddhism, Hinduism or Sikhism, yet there are communities who claim to be Dalit Christians and Muslims, and the tribal communities often practise folk religions.\n\nMahatma Gandhi coined the word \"Harijan\", translated roughly as \"people of God\", to identify untouchables in 1933. The name was disliked by Ambedkar as it emphasised the Dalits as belonging to the Greater Hindu Nation rather than being an independent community like Muslims. In addition, many Dalits saw the term to be patronizing and derogatory. Some have even claimed that the term really refers to children of devadasis, South Indian girls who were married to a temple and served as concubines and prostitutes for upper-caste Hindus, but this claim cannot be verified.. When untouchability was outlawed after Indian independence, the use of the word \"Harijan\" to describe the ex-untouchables was more common among other castes than the Dalits themselves.\n\n\"Harijan\" (Hindustani: हरिजन , ہریجن ; translation: \"person of Hari/Vishnu\") was a term popularized by Indian political leader Mohandas Gandhi for referring communities traditionally considered so-called \"Untouchable\" (formerly called \"acchoot\" अछूत in Hindi ). The term was later considered derogatory and patronising; hence the term Harijan is no longer used by people belonging to the castes that were kept back in medieval and modern India.\n\nThey are now called Dalits, though even this term is banned in some states of India such as Kerala. The term Harijan is regarded as condescending by many, with some Dalit activists calling it insulting. As a result, the Government of India and several state governments forbid or discourage its use for official purposes.\n\nThough Gandhi popularized the term harijan, which literally meant children of god, some contested that as per certain religious texts, brahmins are said to be children of God. The term may have been suggested to Gandhi based on the term used in the works by the Gujarati Bhakti era poet-saint Narsi Mehta. It has been claimed that in Narsi's work, the term refers to the children of Devadasis. Others state that the claim cannot be verified. According to other source the medieval devotional poet Gangasati used the term to refer to herself during the Bhakti movement, a period in India that gave greater status and voice to women while challenging the legitimacy of caste. Gangasati lived around the 12th-14th centuries and wrote in the Gujarati language.\n\nGandhi started publishing a weekly journal called \"Harijan\" on 11 February 1933 from Yerwada Jail during British rule. He created three publications: \"Harijan\" in English (from 1933 to 1948), \"Harijan Bandu\" in Gujarati, and \"Harijan Sevak\" in Hindi. These newspapers found Gandhi concentrating on social and economic problems, much as his earlier English newspaper, \"Young India\", had done from 1919 to 1932.\n\nIn Southern India, Dalits are sometimes known as \"Adi Dravida\", \"Adi Karnataka\", and \"Adi Andhra\". This practice began around 1917, when the \"Adi-\" prefix was appropriated by Southern Dalit leaders, who believed that they were the indigenous inhabitants of India. The terms are used in the states of Tamil Nadu, Karnataka, and Andhra Pradesh respectively, to identify Dalits in official documents.\n\nIn the Indian state of Maharashtra, according to historian and women's studies academic Shailaja Paik, \"Dalit\" is a term mostly used by members of the Mahar caste, into which Ambedkar was born. Most other communities prefer to use their own caste name.\n\nIn Nepal, aside from \"Harijan\" and, most commonly, \"Dalit\", terms such as \"Haris\" (among Muslims), \"Achhoot\", \"outcastes\" and \"neech jati\" are used.\n\nScheduled Caste communities exist across India and comprised 16.6 per cent of the country's population, 2011 Census of India. Uttar Pradesh (21 per cent), West Bengal (11 per cent), Bihar (8 per cent) and Tamil Nadu (7%) between them accounted for almost half the country's total Scheduled Caste population. They were most prevalent as a proportion of the states' population in Punjab, at about 32 per cent, while Mizoram had the lowest at approximately zero.\n\nSimilar groups are found throughout the rest of the Indian subcontinent, in Nepal, Pakistan, Bangladesh and Sri Lanka. They are also found as part of the Indian diaspora in many countries, including the United States, United Kingdom, Singapore, and the Caribbean.\n\nDalits have had lowest social status in the traditional Hindu social structure but James Lochtefeld, a professor of religion and Asian studies, said in 2002 that the \"adoption and popularization of [the term \"Dalit\"] reflects their growing awareness of the situation, and their greater assertiveness in demanding their legal and constitutional rights\".\n\nIn the past, they were believed to be so impure that caste Hindus considered their presence to be polluting. The impure status was related to their historic hereditary occupations that caste Hindus considered to be \"polluting\" or debased, such as working with leather, working with feces and other dirty work.\n\nGopal Baba Walangkar (ca. 1840–1900) is generally considered to be the pioneer of the Dalit movement, seeking a society in which they were not discriminated against. Another pioneer was Harichand Thakur (ca. 1812–1878) with his Matua organisation that involved the Namasudra (Chandala) community in the Bengal Presidency. Ambedkar himself believed Walangkar to be the progenitor. Another early social reformer who worked to improve conditions for Dalits was Jyotirao Phule (1827-1890).\n\nThe 1950 Constitution of India, introduced after the country gained independence, included measures to improve the socioeconomic conditions of Dalits. Aside from banning untouchability, these included the reservation system, a means of positive discrimination that created the classifications of Scheduled Castes, Scheduled Tribes and Other Backward Classes (OBCs). Communities that were categorised as being one of those groups were guaranteed a percentage of the seats in the national and state legislatures, as well as in government jobs and places of education. The system has its origins in the 1932 Poona Pact between Ambedkar and Gandhi, when Ambedkar conceded his demand that the Dalits should have an electorate separate from the caste Hindus in return for Gandhi accepting measures along these lines. The notion of a separate electorate had been proposed in the Communal Award made by the British Raj authorities, and the outcome of the Pact - the Government of India Act of 1935 - both introduced the new term of \"Scheduled Castes\" in replacement for \"Depressed Classes\" and reserved seats for them in the legislatures.\n\nBy 1995, of all federal government jobs in India - 10.1 per cent of Class I, 12.7 per cent of Class II, 16.2 per cent of Class III, and 27.2 per cent of Class IV jobs were held by Dalits. Of the most senior jobs in government agencies and government-controlled enterprises, only 1 per cent were held by Dalits, not much change in 40 years. In the 21st century, Dalits have been elected to India's highest judicial and political offices.\n\nIn 2001, the quality of life of the Dalit population in India was worse than that of the overall Indian population on metrics such as access to health care, life expectancy, education attainability, access to drinking water and housing. In 2010, Dalits received international attention due to a portrait exhibition by Marcus Perkins that depicted Dalits.\n\nAccording to a 2007 report by Human Rights Watch (HRW), the treatment of Dalits has been like a \"hidden apartheid\" and that they \"endure segregation in housing, schools, and access to public services\". HRW noted that Manmohan Singh, then Prime Minister of India, saw a parallel between the apartheid system and untouchability. Eleanor Zelliot also notes Singh's 2006 comment but says that, despite the obvious similarities, race prejudice and the situation of Dalits \"have a different basis and perhaps a different solution.\" Though the Indian Constitution abolished untouchability, the oppressed status of Dalits remains a reality. In rural India, stated Klaus Klostermaier in 2010, \"they still live in secluded quarters, do the dirtiest work, and are not allowed to use the village well and other common facilities\". In the same year, Zelliot noted that \"In spite of much progress over the last sixty years, Dalits are still at the social and economic bottom of society.\"\n\nAccording to a 2014 report to the Ministry of Minority Affairs, over 44.8 per cent of Scheduled Tribe (ST) and 33.8 per cent of Scheduled Caste (SC) populations in rural India were living below the poverty line in 2011–12. In urban areas, 27.3 per cent of ST and 21.8 per cent of SC populations were below the poverty line.\n\nSome Dalits have achieved affluence, although most remain poor. Some intellectuals, such as Chandra Bhan Prasad, have argued that the living standards of many Dalits have improved since the economic system became more liberalized starting in 1991 and have supported their claims through large surveys. According to the Socio Economic and Caste Census 2011, nearly 79 per cent of Adivasi households and 73 per cent of Dalit households were the most deprived among rural households in India. While 45 per cent of SC households are landless and earn a living by manual casual labour, the figure is 30 per cent for Adivasis.\n\nA 2012 survey by Mangalore University in Karnataka found that 93 per cent of Dalit families in the state of Karnataka live below the poverty line.\n\nAccording to an analysis by The IndiaGoverns Research Institute, Dalits constituted nearly half of primary school dropouts in Karnataka during the period 2012–14.\n\nA sample survey in 2014, conducted by Dalit Adhikar Abhiyan and funded by ActionAid, found that among state schools in Madhya Pradesh, 88 percent discriminated against Dalit children. In 79 percent of the schools studied, Dalit children are forbidden from touching mid-day meals. They are required to sit separately at lunch in 35 percent of schools, and are required to eat with specially marked plates in 28 percent.\n\nThere have been incidents and allegations of SC and ST teachers and professors being discriminated against and harassed by authorities, upper castes colleagues and upper caste students in different education institutes of India. In some cases, such as in Gujarat, state governments have argued that, far from being discriminatory, their rejection when applying for jobs in education has been because there are no suitably qualified candidates from those classifications.\n\nDiscrimination can also exist in access to healthcare and nutrition. A sample survey of Dalits, conducted over several months in Madhya Pradesh and funded by ActionAid in 2014, found that health field workers did not visit 65 per cent of Dalit settlements. 47 per cent of Dalits were not allowed entry into ration shops; and 64 per cent were given less grains than non-Dalits. In Haryana state, 49 per cent of Dalit children under five years were underweight and malnourished while 80 per cent of those in the 6–59 months age group were anaemic in 2015.\n\nDalits comprise a slightly disproportionate number of India's prison inmates. While Dalits (including both SCs and STs) constitute 25 per cent of the Indian population, they account for 33.2 per cent of prisoners. About 24.5 per cent of death row inmates in India are from Scheduled Castes and Scheduled Tribes which is proportionate to their population. The percentage is highest in Maharashtra (50 per cent), Karnataka (36.4 per cent) and Madhya Pradesh (36 per cent).\n\nCaste-related violence between Dalit and non-Dalits allegedly stems from Dalit's economic success amidst ongoing prejudice. The Bhagana rape case, which arose out of a dispute of allocation of land, is an example of atrocities against Dalit girls and women. In August 2015, due to continued alleged discrimination from upper castes of the village, about 100 Dalit inhabitants converted to Islam in a ceremony at Jantar Mantar, New Delhi. Inter-caste marriage has been proposed as a remedy, but according to a 2014 survey of 42,000 households by the New Delhi-based National Council of Applied Economic Research (NCAER) and the University of Maryland, it was estimated that only 5 per cent of Indian marriages cross caste boundaries.\n\nA 2006 article reported incidents of violence, disputes and discrimination against Dalits in Maharashtra. The article noted that non-Dalit families claimed they do not treat Dalits differently. A carpenter caste person said, \"We tell them anything and they tell us you are pointing fingers at us because of our caste; we all live together, and there are bound to be fights, but they think we target them.\"\n\nThere have been reports of Dalits being forced to eat human faeces and drink urine by Christian Thevars, an OBC. In one such instance, a 17-year-old girl was set on fire by Yadav (an OBC) youth, allegedly because she was allowed school-education. In September 2015, a 45-year-old dalit woman was allegedly stripped naked and was forced to drink urine by perpetrators from the Yadav community in Madhya Pradesh.\n\nThe Government of India has attempted on several occasions to legislate specifically to address the issue of caste-related violence that affects SCs and STs. Aside from the Constitutional abolition of untouchability, there has been the Untouchability (Offences) Act of 1955, which was amended in the same year to become the Protection of Civil Rights Act. It was determined that neither of those Acts were effective, so the Scheduled Caste and Scheduled Tribe (Prevention of Atrocities) Act of 1989 (POA) came into force.\n\nThe POA designated specific crimes against SCs and STs as \"atrocities\" – a criminal act that has \"the quality of being shockingly cruel and inhumane\" – which should be prosecuted under its terms rather than existing criminal law. It created corresponding punishments. Its purpose was to curb and punish violence against Dalits, including humiliations such as the forced consumption of noxious substances. Other atrocities included forced labour, denial of access to water and other public amenities, and sexual abuse. The Act permitted Special Courts exclusively to try POA cases. The Act called on states with high levels of caste violence (said to be \"atrocity-prone\") to appoint qualified officers to monitor and maintain law and order.\n\nIn 2015, the Parliament of India passed the Scheduled Castes and Scheduled Tribes (Prevention of Atrocities) Amendment Act to address issues regarding implementation of the POA, including instances where the police put procedural obstacles in the way of alleged victims or indeed outright colluded with the accused. It also extended the number of acts that were deemed to be atrocities. One of those remedies, in an attempt to address the slow process of cases, was to make it mandatory for states to set up the exclusive Special Courts that the POA had delineated. Progress in doing so, however, was reported in April 2017 to be unimpressive. P. L. Punia, a former chairman of the NCSC, said that the number of pending cases was high because most of the extant Special Courts were in fact not exclusive but rather being used to process some non-POA cases, and because \"The special prosecutors are not bothered and the cases filed under this Act are as neglected as the victims\". While Dalit rights organisations were cautiously optimistic that the amended Act would improve the situation, legal experts were pessimistic.\n\nFa Xian, a Chinese Buddhist pilgrim who recorded his visit to India in the early 5th century, mentioned segregation in the context of the untouchable Chandala community:\n\nWhile discrimination against Dalits has declined in urban areas and in the public sphere, it still exists in rural areas and in the private sphere, in everyday matters such as access to eating places, schools, temples and water sources. Some Dalits successfully integrated into urban Indian society, where caste origins are less obvious. In rural India, however, caste origins are more readily apparent and Dalits often remain excluded from local religious life, though some qualitative evidence suggests that exclusion is diminishing.\n\nAccording to the 2014 NCAER/University of Maryland survey, 27 per cent of the Indian population still practices untouchability. The figure may be higher because many people refuse to acknowledge doing so when questioned, although the methodology of the survey was also criticised for potentially inflating the figure. Across India, Untouchability was practised among 52 per cent of Brahmins, 33 per cent of Other Backward Classes and 24 per cent of non-Brahmin forward castes. Untouchability was also practiced by people of minority religions – 23 per cent of Sikhs, 18 per cent of Muslims and 5 per cent of Christians. According to statewide data, Untouchability is most commonly practiced in Madhya Pradesh (53 per cent), followed by Himachal Pradesh (50 per cent), Chhattisgarh (48 per cent), Rajasthan and Bihar (47 per cent), Uttar Pradesh (43 per cent), and Uttarakhand (40 per cent).\n\nExamples of segregation have included the Madhya Pradesh village of Ghatwani, where the Scheduled Tribe population of Bhilala do not allow Dalit villagers to use public borewell for fetching water and thus they are forced to drink dirty water. In metropolitan areas around New Delhi and Bangalore, Dalits and Muslims face discrimination from upper caste landlords when seeking places to rent.\n\nIn several incidents if dalits were found burning holika for Holika Dahan ceremony, they are tonsured and paraded naked in the villages. Also in some parts of India, there have been allegations that Dalit grooms riding horses for wedding ceremonies have been beaten up and ostracised by upper caste people. In August 2015, upper caste people burned houses and vehicles belonging to Dalit families and slaughtered their livestock in reaction to Dalits daring to hold a temple car procession at a village in Tamil Nadu. In August 2015, it was claimed that a Jat Khap Panchayat ordered the rape of two Dalit sisters because their brother eloped with a married Jat girl of the same village.\n\nMost Dalits in India practice Hinduism. According to the 61st round Survey of the Ministry of Statistics and Programme Implementation, 90 per cent of Buddhists, one-third of Sikhs, and one-third of Christians in India belonged to Scheduled Castes or Scheduled Tribes.\n\nAmbedkar said that untouchability came into Indian society around 400 AD, due to the struggle for supremacy between Buddhism and Brahmanism (an ancient term for Brahmanical Hinduism). Some Hindu priests befriended Dalits and were demoted to low-caste ranks. Eknath, another excommunicated Brahmin, fought for the rights of untouchables during the Bhakti period. Historical examples of Dalit priests include Chokhamela in the 14th century, who was India's first recorded Dalit poet. Raidas (Ravidass), born into a family of cobblers, is considered a guru by Dalits and is held in high regard. His teachings and writings form part of the Sikh holy book, the \"Guru Granth Sahib\". The 15th-century saint Ramananda Ray accepted all castes, including Untouchables, into his fold. Most of these saints subscribed to the medieval era Bhakti movement in Hinduism that rejected casteism. The story of Nandanar describes a low-caste Hindu devotee who was rejected by the priests but accepted by God.\n\nIn the 19th century, the Brahmo Samaj, Arya Samaj and the Ramakrishna Mission actively participated in Dalit emancipation. While Dalits had places to worship, the first upper-caste temple to openly welcome Dalits was the Laxminarayan Temple in Wardha in 1928. It was followed by the Temple Entry Proclamation issued by the last King of Travancore in the Indian state of Kerala in 1936.\n\nThe Punjabi reformist Satnami movement was founded by Dalit Guru Ghasidas. Guru Ravidas was also a Dalit. Giani Ditt Singh, a Dalit Sikh reformer, started Singh Sabha movement to convert Dalits. Other reformers, such as Jyotirao Phule, Ayyankali of Kerala and Iyothee Thass of Tamil Nadu worked for Dalit emancipation.\n\nIn the 1930s, Gandhi and Ambedkar disagreed regarding retention of the caste system. Whilst Ambedkar wanted to see it destroyed, Gandhi thought that it could be modified by reinterpreting Hindu texts so that the untouchables were absorbed into the Shudra varna. This was this disagreement that led to the Poona Pact. Despite the disagreement, Gandhi began the Harijan Yatra to help the Dalits.\n\nThe declaration by princely states of Kerala between 1936 and 1947 that temples were open to all Hindus went a long way towards ending Untouchability there. However, educational opportunities to Dalits in Kerala remain limited.\n\nOther Hindu groups attempted to reconcile with the Dalit community. Hindu temples are increasingly receptive to Dalit priests, a function formerly reserved for Brahmins.\n\nThe fight for temple entry rights for Dalits continues to cause controversy. Brahmins such as Subramania Bharati passed Brahminhood onto a Dalit, while in Shivaji's Maratha Empire Dalit warriors (the Mahar Regiment) joined his forces. In a 2015 incident in Meerut, when a Dalit belonging to Valmiki caste was denied entry to a Hindu temple he converted to Islam. In September 2015, four Dalit women were fined by the upper-caste Hindus for entering a temple in Karnataka.\n\nThere have been allegations that Dalits in Nepal are denied entry to Hindu temples. In at least one reported case were beaten up by some upper caste people for doing so.\n\nIn Maharashtra, Uttar Pradesh, Tamil Nadu and a few other regions, Dalits came under the influence of the neo-Buddhist movement initiated by Ambedkar. In the 1950s, he turned his attention to Buddhism and travelled to Sri Lanka (formerly Ceylon) to attend a convention of Buddhist scholars and monks. While dedicating a new Buddhist vihara near Pune, he announced that he was writing a book on Buddhism, and that he planned a formal conversion. Ambedkar twice visited Burma in 1954; the second time to attend a conference of the World Fellowship of Buddhists in Rangoon. In 1955, he founded the Bharatiya Bauddha Mahasabha (Buddhist Society of India). He completed writing \"The Buddha and His Dhamma\" in May 1956.\n\nAfter meetings with the Buddhist monk Hammalawa Saddhatissa, Ambedkar organised a public ceremony for himself and his supporters in Nagpur on 14 October 1956. Accepting the Three Refuges and Five Precepts in the traditional manner, he completed his conversion. He then proceeded to convert an estimated 500,000 of his supporters. Taking the 22 Vows, they explicitly condemned and rejected Hinduism and Hindu philosophy.\n\nGuru Nanak in \"Guru Granth Sahib\" calls for everyone to treat each other equally. Subsequent Sikh Gurus, all of whom came from the Khatri caste, also denounced the hierarchy of the caste system. Despite this, social stratification exists in the Sikh community. The bulk of the Sikhs of Punjab belong to the Jat caste; there are also two Dalit Sikh castes in the state, called the Mazhabis and the Ramdasias.\n\nSunrinder S. Jodhka says that, in practice, Sikhs belonging to the landowning dominant castes have not shed all their prejudices against the dalit castes. While dalits would be allowed entry into the village gurudwaras they would not be permitted to cook or serve langar (the communal meal). Therefore, wherever they could mobilise resources, the Sikh dalits of Punjab have tried to construct their own gurudwara and other local-level institutions in order to attain a certain degree of cultural autonomy. In 1953, Sikh leader, Master Tara Singh, succeeded in winning the demands from the Government to include Sikh castes of the converted untouchables in the list of scheduled castes. In the Shiromani Gurdwara Prabandhak Committee (SGPC), 20 of the 140 seats are reserved for low-caste Sikhs.\n\nSikhs adopt standard surnames such as Singh to disguise caste identities. Nevertheless, families generally do not marry across caste boundaries.\n\nIn 2003 the Talhan village Gurudwara endured a bitter dispute between Jat Sikhs and Chamars. The Chamars came out in force and confronted the Randhawa and Bains Jat Sikh landlords, who refused to give the Chamars a share on the governing committee of a shrine dedicated to Shaheed Baba Nihal Singh. The shrine earned 3–7 crore Indian Rupees, and the Jat Sikh landlords allegedly \"gobbled up a substantial portion of the offerings\". Though Dalits form more than 60 per cent of Talhan's 5,000-strong population, local traditions ensured that they were denied a place on the committee. The landlords, in league with radical Sikh organisations and the SGPC, attempted to keep out the Dalits by razing the shrine overnight and constructing a gurdwara on it, but the Dalit quest for a say in the governing committee did not end.\n\nChamars fought a four-year court battle with the landlords and their allies, including the Punjab Police. In that time Dalits conducted several boycotts against the Chamars. The Jat Sikhs and their allies cut off the power supply to their homes. In addition, various scuffles and fights set Chamar youths armed with lathis, rocks, bricks, soda bottles and anything they could find fought Jat Sikh landlords, youths and the Punjab police. Dalit youngsters painted their homes and motorcycles with the slogan, \"Putt Chamar De\" (\"proud sons of Chamars\") in retaliation to the Jat slogan, \"Putt Jattan De\".\n\nBant Singh is a Mazhabi Sikh farmer and singer from Jhabhar village in Mansa district, Punjab, India, who has emerged as an agricultural labour activist, fighting landowners.\n\nAfter his minor daughter was raped in 2000, Bant took the rapists to court, braving threats of violence and attempted bribes. Rapes of Dalits by non-Dalits are not commonly reported. The 2004 trial culminated in life sentences for three of the culprits.\n\nOn the evening of 7 January 2006 Bant Singh was returning home from campaigning for a national agricultural labour rally. He was assaulted by seven men, allegedly sent by Jaswant and Niranjan Singh, the headman of his village, who have links with the Indian National Congress party. One of them brandished a revolver to prevent any resistance while the other six beat him with iron rods and axes. He was left for dead, but survived.\n\nHe was first taken to civil hospital in Mansa but was not properly treated there. Then he moved to the PGI at Chandigarh, where both lower arms and one leg had to be amputated since gangrene had set in and his kidneys had collapsed due to blood loss. The original doctor was eventually suspended for misconduct.\n\nHistorically Jainism was practiced by many communities across India. They are often conservative and are generally considered upper-caste.\n\nIn 1958, a Sthanakvasi Jain called Muni Sameer Muni came into contact with members of the Khatik community in the Udaipur region, who decided to adopt Jainism. Their centre, Ahimsa Nagar, located about four miles from Chittorgarh, was inaugurated by Mohanlal Sukhadia in 1966. Sameer Muni termed them \"Veerwaal\", i.e. belonging to Mahavira. A 22-year-old youth, Chandaram Meghwal, was initiated as a Jain monk at Ahore town in Jalore district in 2005. In 2010 a Mahar engineer called Vishal Damodar was initiated as a Jain monk by Acharya Navaratna Sagar Suriji at Samet Shikhar. Acharya Nanesh, the eighth Achayra of Sadhumargi Jain Shravak Sangha had preached among the Balai community in 1963 near Ratlam. His followers are called \"Dharmapal\". In 1984, some of the Bhangis of Jodhpur came under the influence of Acharya Shri Tulsi and adopted Jainism.\n\nChristian communities in India believe that every human being is equal in Christ. The social stratification in some communities such as the Goan Catholics remained but varied from the Hindu system.\n\nDalit political parties include:\n\nAnti-Dalit prejudices exist in groups such as the extremist militia Ranvir Sena, largely run by upper-caste landlords in Bihar. They oppose equal treatment of Dalits and have resorted to violence. The Ranvir Sena is considered a terrorist organisation by the government of India. In 2015, Cobrapost exposed many leaders especially like C. P. Thakur alongside former PM Chandra Shekhar associated with Ranvir Sena in Bihar Dalit massacres while governments of Nitish Kumar (under pressure from BJP), Lalu Prasad Yadav and Rabri Devi did nothing to get justice for Dalits.\n\nThe rise of Hindutva's (Hindu nationalism) role in Indian politics has accompanied allegations that religious conversions of Dalits are due to allurements like education and jobs rather than faith. Critics argue that laws banning conversion and limiting social relief for converts mean that conversion impedes economic success. However, Bangaru Laxman, a Dalit politician, was a prominent member of the Hindutva movement.\n\nAnother political issue is Dalit affirmative-action quotas in government jobs and university admissions. About 8 per cent of the seats in the National and State Parliaments are reserved for Scheduled Caste and Tribe candidates.\n\nJagjivan Ram(1908–1986) was the first scheduled caste leader to emerge at the national level from Bihar. He was member of the Constituent assembly that drafted India's constitution. Ram also served in the interim national government of 1946 He served in the cabinets of Congress party Prime ministers Jawaharlal Nehru, Lal Bahadur Shastri and Indira Gandhi. His last position in government was as Deputy Prime Minister of India in the Janata Party government of 1977–1979,\n\nIn modern times several Bharatiya Janata Party leaders were Dalits, including Dinanath Bhaskar, Ramchandra Veerappa and Dr. Suraj Bhan.\n\nIn India's most populous state, Uttar Pradesh, Dalits have had a major political impact. The Dalit-led Bahujan Samaj Party (BSP) had previously run the government and that party's leader, Mayawati, served several times as chief minister. Regarding her election in 2007, some reports claimed her victory was due to her ability to win support from both 17 per cent of Muslims and nearly 17 per cent Brahmins alongside 80 per cent of Dalits. However, surveys of voters on the eve of elections, indicated that caste loyalties were not the voters' principal concern. Instead, inflation and other issues of social and economic development dictated the outcome. Mayawati's success in reaching across castes has led to speculation about her as a potential future Prime Minister of India.\n\nAside from Mayawati in Uttar Pradesh, Damodaram Sanjivayya was chief minister of Andhra Pradesh (from 11 January 1960 – 12 March 1962) and Jitan Ram Manjhi was chief minister of Bihar for just less than a year. In 1997, K. R. Narayanan, who was a Dalit, was elected as President of India.\n\nVotebank politics are common in India, usually based on religion or caste. Indeed, the term itself was coined by the Indian sociologist, M. N. Srinivas. Dalits are often used as a votebank. There have been instances where it has been alleged that an election-winning party reneged on promises made to the Dalits made during the election campaign or have excluded them from party affairs.\n\nThe SC, ST Sub-Plan, or \"Indiramma Kalalu\", is a budget allocation by the Government of Andhra Pradesh for the welfare of Dalits. The law was enacted in May 2013. SCs and STs have separate panels for spending. The plan was meant to prevent the government from diverting funds meant for SCs and STs to other programs, which was historically the case. , no equivalent national plan existed. Scheduled Castes Sub Plan and Tribal Sub-Plan funds are often diverted by state governments to other purposes.\n\nWhile the Indian Constitution has provisions for the social and economic uplift of Dalits to support their upward social mobility, these concessions are limited to Hindus. Dalits who have converted to other religions have asked that benefits be extended to them.\n\nAfter World War II, immigration from the former British Empire was largely driven by labour shortages. Like the rest of the Indian subcontinent diaspora, Dalits immigrated and established their own communities.\n\nA 2009 report alleged that caste discrimination is \"rife\" in the United Kingdom. The report alleged that casteism persists in the workplace and within the National Health Service and at doctor's offices.\n\nIndians are divided on the subject and such claims are disputed by the UK Hindu Council who assert that the issue was being \"manipulated\" by Christians and other anti-Indian activists eager to convert Hindus.\n\nHindu groups asserted that caste issues will be resolved as generations pass and that a trend towards inter-caste marriages should help. Some claim that caste discrimination is non-existent. Some have rejected the government's right to interfere in the community. The Hindu Forum of Britain conducted their own research, concluding that caste discrimination was \"not endemic in British society\", that reports to the contrary aimed to increase discrimination by legislating expression and behaviour and that barriers should instead be removed through education.\n\nA 2010 study found that caste discrimination occurs in Britain at work and in service provision. While not ruling out the possibility of discrimination in education, no such incidents were uncovered. The report found favourable results from educational activities. However, non-legislative approaches were claimed to be less effective in the workplace and would not help when the authorities were discriminating. One criticism of discrimination law was the difficulty in obtaining proof of violations. Perceived benefits of legislation were that it provides redress, leads to greater understanding and reduces the social acceptance of such discrimination.\n\nMore recent studies in Britain were inconclusive and found that discrimination was \"not religion specific and is subscribed to by members of any or no religion\". Equalities Minister Helen Grant found insufficient evidence to justify specific legislation, while Shadow Equalities minister Kate Green said that the impact is on a relatively small number of people. Religious studies professor Gavin Flood of the Oxford Centre for Hindu Studies concluded that the Hindu community in Britain is particularly well integrated, loosening caste ties. Casteist beliefs were prevalent mainly among first generation immigrants, with such prejudices declining with each successive generation due to greater assimilation.\n\nFrom September 2013 to February 2014, Indian philosopher Meena Dhanda led a project on ‘Caste in Britain’ for the UK Equality and Human Rights Commission (EHRC), which focused on the proposed inclusion of a provision in the Equality Act 2010 to protect British citizens against caste discrimination.\n\nSupporters of anti-caste legislation include Lord Avebury and Lady Thornton.\n\nSikhs in the United Kingdom are affected by caste. Gurdwaras such as those of the Ramgarhia Sikhs are organised along caste lines and most are controlled by a single caste. In most British towns and cities with a significant Sikh population, rival gurdwaras can be found with caste-specific management committees. The caste system and caste identity is entrenched and reinforced.\nDalit Sikhs have formed a network of lower caste temples throughout the UK. Caste tensions erupt between higher caste Jat Sikhs and lower caste Sikhs. Violence has erupted between the two communities over inter-caste marriages. In the city of Wolverhampton, there have been incidents of Jat Sikhs refusing to share water taps and avoiding physical contact with lower castes. At a sports competition in Birmingham in 1999, Jat Sikhs refused to eat food that had been cooked and prepared by the Chamar community.\n\nMany Jat Sikhs refer to lower-caste temples by name such as the \"Ramgharia Gurdwara\", \"Ghumaran Da Gurdwara\"or \"Chamar Gurdwara\". The majority of higher caste Sikhs would not eat in a Ravidassi house or in Ravidassi temples. Many Chamars stated that they are made to feel unwelcome in Sikh gurdwaras and Hindu temples. Many Sikhs do not wish to give Chamars equal status in their gurdwaras and communities. Sikh Chamars (Ramdassi Sikhs) united with fellow Chamars across religious boundaries to form Ravidassi temples.\n\nMazhabi Sikhs were subjected to the same forms of inequality and discrimination in gurdwaras from Upper caste Sikhs and unified with Hindu Churas to form Valmiki temples.\n\nSikh gurdwaras, which often are controlled by the older first generation immigrants, in Britain generally frown upon inter-caste marriages even though they are on the rise. More and more families are affected by inter-caste marriages.\n\nThe few gurdwaras that accept inter-caste marriages do so reluctantly. Gurdwaras may insist on the presence of \"Singh\" and \"Kaur\" in the names of the bridegroom and bride, or deny them access to gurdwara-based religious services and community centres.\n\nIt is estimated that in 1883, about one-third of the immigrants who arrived in the Caribbean were Dalits. The shared experience of being exploited in a foreign land gradually broke down caste barriers in the Caribbean Hindu communities.\n\nDalit literature forms a distinct part of Indian literature. One of the first Dalit writers was Madara Chennaiah, an 11th-century cobbler-saint who lived in the reign of Western Chalukyas and who is regarded by some scholars as the \"father of Vachana poetry\". Another early Dalit poet is Dohara Kakkaiah, a Dalit by birth, six of whose confessional poems survive. The Bharatiya Dalit Sahitya Akademi (Indian Dalit Literature Academy) was founded in 1984 by Babu Jagjivan Ram.\n\nNotable modern authors include Mahatma Phule and Ambedkar in Maharashtra, who focused on the issues of Dalits through their works and writings. This started a new trend in Dalit writing and inspired many Dalits to offer work in Marathi, Hindi, Tamil and Punjabi. There are novels, poems and even drama on Dalit issues. The Indian author Rajesh Talwar has written a play titled 'Gandhi, Ambedkar, and the Four Legged Scorpion' in which the personal experiences of Dr Ambedkar and the sufferings of the community have been highlighted.\n\nBaburao Bagul, Bandhu Madhav and Shankar Rao Kharat, worked in the 1960s. Later the Little magazine movement became popular. In Sri Lanka, writers such as K.Daniel and Dominic Jeeva gained mainstream popularity.\n\nUntil the 1980s, Dalits had little involvement in Bollywood or other film industries of India and the community were rarely depicted at the heart of storylines. Chirag Paswan (son of Dalit leader Ram Vilas Paswan) launched his career in Bollywood with his debut film \"Miley Naa Miley Hum\" in 2011. Despite political connections and the financial ability to struggle against ingrained prejudices, Chirag was not able to \"bag\" any other movie project in the following years. Chirag, in his early days, described Bollywood as his \"childhood dream\", but eventually entered politics instead. When the media tried to talk to him about \"Caste in Bollywood\", he refused to talk about the matter, and his silence speaks for itself. A recent Hindi film to portray a Dalit character in the leading role, although it was not acted by a Dalit, was \"\" (2007). The continued use of caste based references to Dalit sub-castes in South Indian films (typecast and pigeonholed in their main socio-economic sub-group) angers many Dalit fans.\n\nSeveral Dalit groups are rivals and sometimes communal tensions are evident. A study found more than 900 Dalit sub-castes throughout India, with internal divisions. Emphasising any one caste threatens what is claimed to be an emerging Dalit identity and fostering rivalry among SCs.\n\nA DLM (Dalit Liberation Movement) party leader said in the early 2000s that it is easier to organise Dalits on a caste basis than to fight caste prejudice itself.\n\nBalmikis and Pasis in the 1990s refused to support the BSP, claiming it was a Jatav party but over 80 per cent of dalits from all united Dalit castes voted BSP to power in 2007.\n\nMany converted Dalit Sikhs claim a superior status over the Hindu Raigars, Joatia Chamars and Ravidasis and sometimes refuse to intermarry with them. They are divided into gotras that regulate their marriage alliances. In Andhra Pradesh, Mala and Madiga were constantly in conflict with each other but as of 2015 Mala and Madiga students work for common dalit cause at University level.\n\nAlthough the Khateek (butchers) are generally viewed as a higher caste than Bhangis, the latter refuse to offer cleaning services to Khateeks, believing that their profession renders them unclean. They also consider the Balai, Dhobi, Dholi and Mogya as unclean and do not associate with them.\n\n\n\n"}
{"id": "29174", "url": "https://en.wikipedia.org/wiki?curid=29174", "title": "Social class", "text": "Social class\n\nA social class is a set of concepts in the social sciences and political theory centered on models of social stratification in which people are grouped into a set of hierarchical social categories, the most common being the upper, middle and lower classes.\n\n\"Class\" is a subject of analysis for sociologists, political scientists, anthropologists and social historians. However, there is not a consensus on a definition of \"class\" and the term has a wide range of sometimes conflicting meanings. Some people argue that due to social mobility, class boundaries do not exist. In common parlance, the term \"social class\" is usually synonymous with \"socio-economic class\", defined as \"people having the same social, economic, cultural, political or educational status\", e.g., \"the working class\"; \"an emerging professional class\". However, academics distinguish social class and socioeconomic status, with the former referring to one's relatively stable sociocultural background and the latter referring to one's current social and economic situation and consequently being more changeable over time.\n\nThe precise measurements of what determines social class in society have varied over time. Karl Marx thought \"class\" was defined by one's relationship to the means of production (their relations of production). His simple understanding of classes in modern capitalist society is the proletariat, those who work but do not own the means of production; and the bourgeoisie, those who invest and live off the surplus generated by the proletariat's operation of the means of production. This contrasts with the view of the sociologist Max Weber, who argued \"class\" is determined by economic position, in contrast to \"social status\" or \"Stand\" which is determined by social prestige rather than simply just relations of production. The term \"class\" is etymologically derived from the Latin \"classis\", which was used by census takers to categorize citizens by wealth in order to determine military service obligations.\n\nIn the late 18th century, the term \"class\" began to replace classifications such as estates, rank and orders as the primary means of organizing society into hierarchical divisions. This corresponded to a general decrease in significance ascribed to hereditary characteristics and increase in the significance of wealth and income as indicators of position in the social hierarchy.\n\nHistorically, social class and behavior were laid down in law. For example, permitted mode of dress in some times and places was strictly regulated, with sumptuous dressing only for the high ranks of society and aristocracy, whereas sumptuary laws stipulated the dress and jewelry appropriate for a person's social rank and station. In Europe, these laws became increasingly commonplace during the Middle Ages. However, these laws were prone to change due to societal changes, and in many cases, these distinctions may either almost disappear, such as the distinction between a patrician and a plebeian being almost erased during the late Roman Republic.\n\nJean-Jacques Rousseau had a large influence over political ideals of the French Revolution because of his views of inequality and classes. Rousseau saw humans as \"naturally pure and good,\" meaning that humans from birth were seen as innocent and any evilness was learned. He believed that social problems arise through the development of society and suppressing the innate pureness of humankind. He also believed that private property is the main reason for social issues in society because private property creates inequality through the property's value. Even though his theory predicted if there were no private property then there would be wide spread equality, Rousseau accepted that there will always be social inequality because of how society is viewed and run.\n\nLater Enlightenment thinkers viewed inequality as valuable and crucial to society's development and prosperity. They also acknowledged that private property will ultimately cause inequality because specific resources that are privately owned can be stored and the owners profit off of the deficit of the resource. This can create competition between the classes that was seen as necessary by these thinkers. This also creates stratification between the classes keeping a distinct difference between lower, poorer classes and the higher, wealthier classes.\n\nDefinitions of social classes reflect a number of sociological perspectives, informed by anthropology, economics, psychology and sociology. The major perspectives historically have been Marxism and structural functionalism. The common stratum model of class divides society into a simple hierarchy of working class, middle class and upper class. Within academia, two broad schools of definitions emerge: those aligned with 20th-century sociological stratum models of class society and those aligned with the 19th-century historical materialist economic models of the Marxists and anarchists.\n\nAnother distinction can be drawn between analytical concepts of social class, such as the Marxist and Weberian traditions, as well as the more empirical traditions such as socioeconomic status approach, which notes the correlation of income, education and wealth with social outcomes without necessarily implying a particular theory of social structure.\n\nFor Marx, class is a combination of objective and subjective factors. Objectively, a class shares a common relationship to the means of production. Subjectively, the members will necessarily have some perception (\"class consciousness\") of their similarity and common interest. Class consciousness is not simply an awareness of one's own class interest but is also a set of shared views regarding how society should be organized legally, culturally, socially and politically. These class relations are reproduced through time.\n\nIn Marxist theory, the class structure of the capitalist mode of production is characterized by the conflict between two main classes: the bourgeoisie, the capitalists who own the means of production and the much larger proletariat (or \"working class\") who must sell their own labour power (wage labour). This is the fundamental economic structure of work and property, a state of inequality that is normalized and reproduced through cultural ideology.\n\nFor Marxists, every person in the process of production has separate social relationships and issues. Along with this, every person is placed into different groups that have similar interests and values that can differ drastically from group to group. Class is special in that does not relate to specifically to a singular person, but to a specific role.\n\nMarxists explain the history of \"civilized\" societies in terms of a war of classes between those who control production and those who produce the goods or services in society. In the Marxist view of capitalism, this is a conflict between capitalists (bourgeoisie) and wage-workers (the proletariat). For Marxists, class antagonism is rooted in the situation that control over social production necessarily entails control over the class which produces goods—in capitalism this is the exploitation of workers by the bourgeoisie.\n\nFurthermore, \"in countries where modern civilisation has become fully developed, a new class of petty bourgeois has been formed\". \"An industrial army of workmen, under the command of a capitalist, requires, like a real army, officers (managers) and sergeants (foremen, over-lookers) who, while the work is being done, command in the name of the capitalist\".\n\nMarx makes the argument that, as the bourgeoisie reach a point of wealth accumulation, they hold enough power as the dominant class to shape political institutions and society according to their own interests. Marx then goes on to claim that the non-elite class, owing to their large numbers, have the power to overthrow the elite and create an equal society.\n\nIn \"The Communist Manifesto\", Marx himself argued that it was the goal of the proletariat itself to displace the capitalist system with socialism, changing the social relationships underpinning the class system and then developing into a future communist society in which: \"the free development of each is the condition for the free development of all\". This would mark the beginning of a classless society in which human needs rather than profit would be motive for production. In a society with democratic control and production for use, there would be no class, no state and no need for financial and banking institutions and money.\n\nThese theorists have taken this binary class system and expanded it to include contradictory class locations, the idea that a person can be employed in many different class locations that fall between the two classes of proletariat and bourgeoisie. Erik Olin Wright stated that class definitions are more diverse and elaborate through identifying with multiple classes, having familial ties with people in different a class, or having a temporary leadership role.\n\nMax Weber formulated a three-component theory of stratification that saw social class as emerging from an interplay between \"class\", \"status\" and \"power\". Weber believed that class position was determined by a person's relationship to the means of production, while status or \"Stand\" emerged from estimations of honor or prestige.\n\nWeber views class as a group of people who have common goals and opportunities that are available to them. This means that what separates each class from each other is their value in the marketplace through their own goods and services. This creates a divide between the classes through the assets that they have such as property and expertise.\n\nWeber derived many of his key concepts on social stratification by examining the social structure of many countries. He noted that contrary to Marx's theories, stratification was based on more than simply ownership of capital. Weber pointed out that some members of the aristocracy lack economic wealth yet might nevertheless have political power. Likewise in Europe, many wealthy Jewish families lacked prestige and honor because they were considered members of a \"pariah group\".\n\nOn April 2, 2013, the results of a survey conducted by BBC Lab UK developed in collaboration with academic experts and slated to be published in the journal \"Sociology\" were published online. The results released were based on a survey of 160,000 residents of the United Kingdom most of whom lived in England and described themselves as \"white\". Class was defined and measured according to the amount and kind of economic, cultural and social resources reported. Economic capital was defined as income and assets; cultural capital as amount and type of cultural interests and activities; and social capital as the quantity and social status of their friends, family and personal and business contacts. This theoretical framework was developed by Pierre Bourdieu who first published his theory of social distinction in 1979.\n\nToday, concepts of social class often assume three general economic categories: a very wealthy and powerful upper class that owns and controls the means of production; a middle class of professional workers, small business owners and low-level managers; and a lower class, who rely on low-paying wage jobs for their livelihood and often experience poverty.\n\nThe upper class is the social class composed of those who are rich, well-born, powerful, or a combination of those. They usually wield the greatest political power. In some countries, wealth alone is sufficient to allow entry into the upper class. In others, only people who are born or marry into certain aristocratic bloodlines are considered members of the upper class and those who gain great wealth through commercial activity are looked down upon by the aristocracy as \"nouveau riche\". In the United Kingdom, for example, the upper classes are the aristocracy and royalty, with wealth playing a less important role in class status. Many aristocratic peerages or titles have seats attached to them, with the holder of the title (e.g. Earl of Bristol) and his family being the custodians of the house, but not the owners. Many of these require high expenditures, so wealth is typically needed. Many aristocratic peerages and their homes are parts of estates, owned and run by the title holder with moneys generated by the land, rents or other sources of wealth. However, in the United States where there is no aristocracy or royalty, the upper class status belongs to the extremely wealthy, the so-called \"super-rich\", though there is some tendency even in the United States for those with old family wealth to look down on those who have earned their money in business, the struggle between new money and old money.\n\nThe upper class is the group of people located at the top of the socioeconomic chart. They are the highest you can be. Most of the time, the upper class is seen as the class who does not have to work. This is a myth. Most people in the upper class continue to work and increase their wealth. Although they have many luxuries, they live with a safety net of money. The upper class generally has a big influence on the world and nations economy. In the United States, they are considered \"the top one-percent\".\n\nThe upper class is generally contained within the richest one or two percent of the population. Members of the upper class are often born into it and are distinguished by immense wealth which is passed from generation to generation in the form of estates. Based on some new social and political theories upper class consists of the most wealthy decile group in society which holds nearly 87% of the whole society's wealth.\n\nThe middle class is the most contested of the three categories, the broad group of people in contemporary society who fall socio-economically between the lower and upper classes. One example of the contest of this term is that in the United States \"middle class\" is applied very broadly and includes people who would elsewhere be considered working class. Middle-class workers are sometimes called \"white-collar workers\".\n\nTheorists such as Ralf Dahrendorf have noted the tendency toward an enlarged middle class in modern Western societies, particularly in relation to the necessity of an educated work force in technological economies. Perspectives concerning globalization and neocolonialism, such as dependency theory, suggest this is due to the shift of low-level labour to developing nations and the Third World.\n\nMiddle class is the group of people with typical-everyday jobs that pay significantly more than the poverty line. Examples of these types of jobs are factory workers, salesperson, teacher, cooks and nurses. Typically, the middle class has white-collar jobs as opposed to hands-on jobs. Those with higher skills are rewarded with higher paying jobs or raises, while workers with lower level skills are not paid as much or simply not hired. There is a new trend by some scholars which assumes that the size of the middle class in every society is the same. For example, in paradox of interest theory, middle class are those who are in 6th-9th decile groups which hold nearly 12% of the whole society's wealth.\n\nLower class (occasionally described as working class) are those employed in low-paying wage jobs with very little economic security. The term \"lower class\" also refers to persons with low income.\n\nThe working class is sometimes separated into those who are employed but lacking financial security (the \"working poor\") and an underclass—those who are long-term unemployed and/or homeless, especially those receiving welfare from the state. The latter is analogous to the Marxist term \"lumpenproletariat\". Members of the working class are sometimes called blue-collar workers.\n\nA person's socioeconomic class has wide-ranging effects. It can impact the schools they are able to attend, their health, the jobs open to them, when they exit the labour market, who they may marry and their treatment by police and the courts.\n\nAngus Deaton and Anne Case have analyzed the mortality rates related to the group of white, middle-aged Americans between the ages of 45 and 54 and its relation to class. There has been a growing number of suicides and deaths by substance abuse in this particular group of middle-class Americans. This group also has been recorded to have an increase in reports of chronic pain and poor general health. Deaton and Case came to the conclusion from these observations that because of the constant stress that these white, middle aged Americans feel fighting poverty and wavering between the middle and lower classes, these strains have taken a toll on these people and affected their whole bodies.\n\nSocial classifications can also determine the sporting activities that such classes take part in. It is suggested that those of an upper social class are more likely to take part in sporting activities, whereas those of a lower social background are less likely to participate in sport. However, upper-class people tend to not take part in certain sports that have been commonly known to be linked with the lower class.\n\nA person's social class has a significant impact on their educational opportunities. Not only are upper-class parents able to send their children to exclusive schools that are perceived to be better, but in many places, state-supported schools for children of the upper class are of a much higher quality than those the state provides for children of the lower classes. This lack of good schools is one factor that perpetuates the class divide across generations.\n\nIn the UK, the educational consequences of class position have been discussed by scholars inspired by the cultural studies framework of the CCCS and/or, especially regarding working-class girls, feminist theory. On working-class boys, Paul Willis' 1977 book \"Learning to Labour: How Working Class Kids Get Working Class Jobs\" is seen within the British Cultural Studies field as a classic discussion of their antipathy to the acquisition of knowledge. Beverley Skeggs described \"Learning to Labour\" as a study on the \"irony\" of \"how the process of cultural and economic reproduction is made possible by 'the lads' ' celebration of the hard, macho world of work.\"\n\nA person's social class has a significant impact on their physical health, their ability to receive adequate medical care and nutrition and their life expectancy.\n\nLower-class people experience a wide array of health problems as a result of their economic status. They are unable to use health care as often and when they do it is of lower quality, even though they generally tend to experience a much higher rate of health issues. Lower-class families have higher rates of infant mortality, cancer, cardiovascular disease and disabling physical injuries. Additionally, poor people tend to work in much more hazardous conditions, yet generally have much less (if any) health insurance provided for them, as compared to middle- and upper-class workers.\n\nThe conditions at a person's job vary greatly depending on class. Those in the upper-middle class and middle class enjoy greater freedoms in their occupations. They are usually more respected, enjoy more diversity and are able to exhibit some authority. Those in lower classes tend to feel more alienated and have lower work satisfaction overall. The physical conditions of the workplace differ greatly between classes. While middle-class workers may \"suffer alienating conditions\" or \"lack of job satisfaction\", blue-collar workers are more apt to suffer alienating, often routine, work with obvious physical health hazards, injury and even death.\n\nIn the UK, a 2015 government study by the Social Mobility Commission suggested the existence of a \"glass floor\" in British society preventing those who are less able, but who come from wealthier backgrounds, from slipping down the social ladder. The report proposed a 35% greater likelihood of less able, better-off children becoming high earners than bright poor children.\n\nClass conflict, frequently referred to as \"class warfare\" or \"class struggle\", is the tension or antagonism which exists in society due to competing socioeconomic interests and desires between people of different classes.\n\nFor Marx, the history of class society was a history of class conflict. He pointed to the successful rise of the bourgeoisie and the necessity of revolutionary violence—a heightened form of class conflict—in securing the bourgeoisie rights that supported the capitalist economy.\n\nMarx believed that the exploitation and poverty inherent in capitalism were a pre-existing form of class conflict. Marx believed that wage labourers would need to revolt to bring about a more equitable distribution of wealth and political power.\n\n\"Classless society\" refers to a society in which no one is born into a social class. Distinctions of wealth, income, education, culture or social network might arise and would only be determined by individual experience and achievement in such a society.\n\nSince these distinctions are difficult to avoid, advocates of a classless society (such as anarchists and communists) propose various means to achieve and maintain it and attach varying degrees of importance to it as an end in their overall programs/philosophy.\n\nRace and other large-scale groupings can also influence class standing. The association of particular ethnic groups with class statuses is common in many societies. As a result of conquest or internal ethnic differentiation, a ruling class is often ethnically homogenous and particular races or ethnic groups in some societies are legally or customarily restricted to occupying particular class positions. Which ethnicities are considered as belonging to high or low classes varies from society to society.\n\nIn modern societies, strict legal links between ethnicity and class have been drawn, such as in apartheid, the caste system in Africa, the position of the Burakumin in Japanese society and the casta system in Latin America.\n\n60. Conley, D. (2017). You may ask yourself: An introduction to thinking like a sociologist (Core 5th ed.). New York: W.W. Norton.\n\n"}
{"id": "50405", "url": "https://en.wikipedia.org/wiki?curid=50405", "title": "Clergy", "text": "Clergy\n\nClergy are formal leaders within established religions. Their roles and functions vary in different religious traditions, but usually involve presiding over specific rituals and teaching their religion's doctrines and practices. Some of the terms used for individual clergy are \"clergyman\", \"clergywoman\", and \"churchman\". Less common terms are \"churchwoman\" and \"clergyperson\", while \"cleric\" and \"clerk in holy orders\" both have a long history but are rarely used. \n\nIn Christianity, the specific names and roles of the clergy vary by denomination and there is a wide range of formal and informal clergy positions, including deacons, elders, priests, bishops, preachers, pastors, ministers and the pope. \n\nIn Islam, a religious leader is often known formally or informally as an imam, qadi, mufti, mullah, or ayatollah. \n\nIn the Jewish tradition, a religious leader is often a rabbi (teacher) or hazzan (cantor).\n\nThe word \"Cleric\" comes from the ecclesiastical Latin \"Clericus\", for those belonging to the priestly class. In turn, the source of the Latin word is from the Ecclesiastical Greek \"Klerikos\" (κληρικός), meaning appertaining to an inheritance, in reference to the fact that the Levitical priests of the Old Testament had no inheritance except the Lord. \"Clergy\" is from two Old French words, \"clergié\" and \"clergie\", which refer to those with learning and derive from Medieval Latin \"clericatus\", from Late Latin \"clericus\" (the same word from which \"cleric\" is derived). \"Clerk\", which used to mean one ordained to the ministry, also derives from \"clericus\". In the Middle Ages, reading and writing were almost exclusively the domain of the priestly class, and this is the reason for the close relationship of these words. Within Christianity, especially in Eastern Christianity and formerly in Western Roman Catholicism, the term \"cleric\" refers to any individual who has been ordained, including deacons, priests, and bishops. In Latin Roman Catholicism, the tonsure was a prerequisite for receiving any of the minor orders or major orders before the tonsure, minor orders, and the subdiaconate were abolished following the Second Vatican Council. Now, the clerical state is tied to reception of the diaconate. Minor Orders are still given in the Eastern Catholic Churches, and those who receive those orders are 'minor clerics.'\n\nThe use of the word \"Cleric\" is also appropriate for Eastern Orthodox minor clergy who are tonsured in order not to trivialize orders such as those of Reader in the Eastern Church, or for those who are tonsured yet have no minor or major orders. It is in this sense that the word entered the Arabic language, most commonly in Lebanon from the French, as \"kleriki\" (or, alternatively, \"cleriki\") meaning \"seminarian.\" This is all in keeping with Eastern Orthodox concepts of clergy, which still include those who have not yet received, or do not plan to receive, the diaconate.\n\nA priesthood is a body of priests, shamans, or oracles who have special religious authority or function. The term priest is derived from the Greek presbyter (πρεσβύτερος, \"presbýteros\", elder or senior), but is often used in the sense of sacerdos in particular, i.e., for clergy performing ritual within the sphere of the sacred or numinous communicating with the gods on behalf of the community.\n\nBuddhist clergy are often collectively referred to as the Sangha, and consist of various orders of male and female monks (originally called bhikshus and bhikshunis respectively). This diversity of monastic orders and styles was originally one community founded by Gautama Buddha during the 5th century BC living under a common set of rules (called the Vinaya). According to scriptural records, these celibate monks and nuns in the time of the Buddha lived an austere life of meditation, living as wandering beggars for nine months out of the year and remaining in retreat during the rainy season (although such a unified condition of Pre-sectarian Buddhism is questioned by some scholars). However, as Buddhism spread geographically over time - encountering different cultures, responding to new social, political, and physical environments - this single form of Buddhist monasticism diversified. The interaction between Buddhism and Tibetan Bon led to a uniquely Tibetan Buddhism, within which various sects, based upon certain teacher-student lineages arose. Similarly, the interaction between Indian Buddhist monks (particularly of the Southern Madhyamika School) and Chinese Confucian and Taoist monks from c200-c900AD produced the distinctive Ch'an Buddhism. Ch'an, like the Tibetan style, further diversified into various sects based upon the transmission style of certain teachers (one of the most well known being the 'rapid enlightenment' style of Linji Yixuan), as well as in response to particular political developments such as the An Lushan Rebellion and the Buddhist persecutions of Emperor Wuzong. In these ways, manual labour was introduced to a practice where monks originally survived on alms; layers of garments were added where originally a single thin robe sufficed; etc. This adaptation of form and roles of Buddhist monastic practice continued after the transmission to Japan. For example, monks took on administrative functions for the Emperor in particular secular communities (registering births, marriages, deaths), thereby creating Buddhist 'priests'. Again, in response to various historic attempts to suppress Buddhism (most recently during the Meiji Era), the practice of celibacy was relaxed and Japanese monks allowed to marry. This form was then transmitted to Korea, during later Japanese occupation, where celibate and non-celibate monks today exist in the same sects. (Similar patterns can also be observed in Tibet during various historic periods multiple forms of monasticism have co-existed such as \"ngagpa\" lamas, and times at which celibacy was relaxed). As these varied styles of Buddhist monasticism are transmitted to Western cultures, still more new forms are being created.\n\nIn general, the Mahayana schools of Buddhism tend to be more culturally adaptive and innovative with forms, while Theravada schools (the form generally practised in Thailand, Burma, Cambodia and Sri Lanka) tend to take a much more conservative view of monastic life, and continue to observe precepts that forbid monks from touching women or working in certain secular roles. This broad difference in approach led to a major schism among Buddhist monastics in about the 4th century BCE, creating the Early Buddhist Schools.\n\nWhile female monastic (\"bhikkhuni\") lineages existed in most Buddhist countries at one time, the Theravada lineages of Southeast Asia died out during the 14th-15th Century AD. As there is some debate about whether the bhikkhuni lineage (in the more expansive Vinaya forms) was transmitted to Tibet, the status and future of female Buddhist clergy in this tradition is sometimes disputed by strict adherents to the Theravadan style. Some Mahayana sects, notably in the United States (such as San Francisco Zen Center) are working to reconstruct the female branches of what they consider a common, interwoven lineage.\n\nThe diversity of Buddhist traditions makes it difficult to generalize about Buddhist clergy. In the United States, Pure Land priests of the Japanese diaspora serve a role very similar to Protestant ministers of the Christian tradition. Meanwhile, reclusive Theravada forest monks in Thailand live a life devoted to meditation and the practice of austerities in small communities in rural Thailand- a very different life from even their city-dwelling counterparts, who may be involved primarily in teaching, the study of scripture, and the administration of the nationally organized (and government sponsored) Sangha. In the Zen traditions of China, Korea and Japan, manual labor is an important part of religious discipline; meanwhile, in the Theravada tradition, prohibitions against monks working as laborers and farmers continue to be generally observed.\n\nCurrently in North America, there are both celibate and non-celibate clergy in a variety of Buddhist traditions from around the world. In some cases they are forest dwelling monks of the Theravada tradition and in other cases they are married clergy of a Japanese Zen lineage and may work a secular job in addition to their role in the Buddhist community. There is also a growing realization that traditional training in ritual and meditation as well as philosophy may not be sufficient to meet the needs and expectations of American lay people. Some communities have begun exploring the need for training in counseling skills as well. Along these lines, at least two fully accredited Master of Divinity programs are currently available: one at Naropa University in Boulder, CO and one at the University of the West in Rosemead, CA.\n\nTitles for Buddhist clergy include:\n\nIn Theravada:\n\nIn Mahayana:\n\nIn Vajrayana:\n\nIn general, Christian clergy are ordained; that is, they are set apart for specific ministry in religious rites. Others who have definite roles in worship but who are not ordained (e.g. laypeople acting as acolytes) are generally not considered clergy, even though they may require some sort of official approval to exercise these ministries.\n\nTypes of clerics are distinguished from offices, even when the latter are commonly or exclusively occupied by clerics. A Roman Catholic cardinal, for instance, is almost without exception a cleric, but a cardinal is not a type of cleric. An archbishop is not a distinct type of cleric, but is simply a bishop who occupies a particular position with special authority. Conversely, a youth minister at a parish may or may not be a cleric. Different churches have different systems of clergy, though churches with similar polity have similar systems.\n\nIn Anglicanism, clergy consist of the orders of deacons, priests (presbyters) and bishops in ascending order of seniority. \"Canon\", \"archdeacon\", \"archbishop\" and the like are specific positions within these orders. Bishops are typically overseers, presiding over a diocese composed of many parishes, with an archbishop presiding over a province, which is a group of dioceses. A parish (generally a single church) is looked after by one or more priests, although one priest may be responsible for several parishes. New clergy are ordained deacons. Those seeking to become priests are usually ordained priest after a year. Since the 1960s some Anglican churches have reinstituted the permanent diaconate also in addition to the transitional, order of ministry focused on ministry that bridges the church and the world, especially ministry to those on the margins of society.\n\nFor the forms of address for Anglican clergy, see Forms of address in the United Kingdom.\n\nFor a short period of history before the ordination of women as deacons, priests and bishops began within Anglicanism they could be \"deaconesses\". Although they were usually considered having a ministry distinct from deacons they often had similar ministerial responsibilities.\n\nIn Anglican churches all clergy are permitted to marry. In most national churches women may become deacons or priests, but while fifteen out of 38 national churches allow for the consecration of women as bishops, only five have ordained any. Celebration of the Eucharist is reserved for priests and bishops.\n\nNational Anglican churches are presided over by one or more primates or metropolitans (archbishops or presiding bishops). The senior archbishop of the Anglican Communion is the Archbishop of Canterbury, who acts as leader of the Church of England and 'first among equals' of the primates of all Anglican churches.\n\nBeing a deacon, priest or bishop is considered a function of the person and not a job. When priests retire they are still priests even if they no longer have any active ministry. However, they only hold the basic rank after retirement. Thus a retired archbishop can only be considered a bishop (though it is possible to refer to 'Bishop John Smith, the former Archbishop of York'), a canon or archdeacon is a priest on retirement and does not hold any additional honorifics.\n\nOrdained clergy in the Roman Catholic Church are either deacons, priests, or bishops belonging to the diaconate, the presbyterate, or the episcopate, respectively. Among bishops, some are metropolitans, archbishops, or patriarchs. The pope is the bishop of Rome, the supreme and universal hierarch of the Church, and his authorization is now required for the ordination of all Catholic bishops. With rare exceptions, cardinals are bishops, although it was not always so; formerly, some cardinals were people who had received clerical tonsure, but not Holy Orders. Secular clergy are ministers, such as deacons and priests, who do not belong to a religious institute and live in the world at large, rather than a religious institute (\"saeculum\"). The Holy See supports the activity of its clergy by the Congregation for the Clergy (), a dicastery of Roman curia.\n\nCanon Law indicates (canon 207) that \"[b]y divine institution, there are among the Christian faithful in the Church sacred ministers who in law are also called clerics; the other members of the Christian faithful are called lay persons\". This distinction of a separate ministry was formed in the early times of Christianity; one early source reflecting this distinction, with the three ranks or orders of bishop, priest and deacon, is the writings of Saint Ignatius of Antioch.\n\nHoly Orders is one of the Seven Sacraments, enumerated at the Council of Trent, that the Magisterium considers to be of divine institution. In the Roman Catholic Church, only men are permitted to be clerics, although in antiquity women were ordained to the diaconate.\n\nIn the Latin Church before 1972, tonsure admitted someone to the clerical state, after which he could receive the four minor orders (ostiary, lectorate, order of exorcists, order of acolytes) and then the major orders of subdiaconate, diaconate, presbyterate, and finally the episcopate, which according to Roman Catholic doctrine is \"the fullness of Holy Orders\". Since 1972 the minor orders and the subdiaconate have been replaced by lay ministries and clerical tonsure no longer takes place, except in some Traditionalist Catholic groups, and the clerical state is acquired, even in those groups, by Holy Orders. In the Latin Church the initial level of the three ranks of Holy Orders is that of the diaconate. In addition to these three orders of clerics, some Eastern Catholic, or \"Uniate\", Churches have what are called \"minor clerics\".\n\nMembers of institutes of consecrated life and societies of apostolic life are clerics only if they have received Holy Orders. Thus, unordained monks, friars, nuns, and religious brothers and sisters are not part of the clergy.\n\nThe Code of Canon Law and the Code of Canons of the Eastern Churches prescribe that every cleric must be enrolled or \"incardinated\" in a diocese or its equivalent (an apostolic vicariate, territorial abbey, personal prelature, etc.) or in a religious institute, society of apostolic life or secular institute. The need for this requirement arose because of the trouble caused from the earliest years of the Church by unattached or vagrant clergy subject to no ecclesiastical authority and often causing scandal wherever they went.\n\nCurrent canon law prescribes that to be ordained a priest, an education is required of two years of philosophy and four of theology, including study of dogmatic and moral theology, the Holy Scriptures, and canon law have to be studied within a seminary or an ecclesiastical faculty at a university.\n\nRoman Catholicism mandates clerical celibacy for all clergy in the predominant Latin Rite, with the exception of deacons who do not intend to become priests. Exceptions are sometimes admitted for ordination to transitional diaconate and priesthood on a case-by-case basis for married clergymen of other churches or communities who become Catholics, but ordination of married men to the episcopacy is excluded (see personal ordinariate). Clerical marriage is not allowed and therefore, if those for whom in some particular Church celibacy is optional (such as permanent deacons in the Latin Church) wish to marry, they must do so before ordination. Eastern Catholic Churches either follow the same rules as the Latin Church or require celibacy only for bishops.\n\nIn the High Middle Ages, clergy in Western Europe had four privileges:\n\nThe Church of Jesus Christ of Latter-day Saints (LDS Church) has no dedicated clergy, and is governed instead by a system of lay priesthood leaders. Locally, unpaid and part-time priesthood holders lead the church; the worldwide church is supervised by full-time general authorities, some of whom receive modest living allowances. No formal theological training is required for any position. All leaders in the church are called by revelation and the laying on of hands by one who holds authority. Jesus Christ stands at the head of the church and leads the church through revelation given to the President of the Church, the First Presidency, and Twelve Apostles, all of whom are recognized as prophets, seers, and revelators and have lifetime tenure. Below these men in the hierarchy are quorums of seventy, which are assigned geographically over the areas of the church. Locally, the church is divided into stakes; each stake has a president, who is assisted by two counselors and a high council. The stake is made up of several individual congregations, which are called \"wards\" or \"branches.\" Wards are led by a bishop and his counselors and branches by a president and his counselors. Local leaders serve in their positions until released by their supervising authorities.\n\nGenerally, all worthy males age 12 and above receive the priesthood. Youth age 12 to 18 are ordained to the Aaronic priesthood as deacons, teachers, or priests, which authorizes them to perform certain ordinances and sacraments. Adult males are ordained to the Melchizedek priesthood, as elders, seventies, high priests, or patriarchs in that priesthood, which is concerned with spiritual leadership of the church. Although the term \"clergy\" is not typically used in the LDS Church, it would most appropriately apply to local bishops and stake presidents. Merely holding an office in the priesthood does not imply authority over other church members or agency to act on behalf of the entire church.\n\nThe Orthodox Church has three ranks of holy orders: bishop, priest, and deacon. These are the same offices identified in the New Testament and found in the Early Church, as testified by the writings of the Holy Fathers. Each of these ranks is ordained through the Sacred Mystery (sacrament) of the laying on of hands (called \"cheirotonia\") by bishops. Priests and deacons are ordained by their own diocesan bishop, while bishops are consecrated through the laying on of hands of at least three other bishops.\n\nWithin each of these three ranks there are found a number of titles. Bishops may have the title of archbishop, metropolitan, and patriarch, all of which are considered honorifics. Among the Orthodox, all bishops are considered equal, though an individual may have a place of higher or lower honor, and each has his place within the order of precedence. Priests (also called presbyters) may (or may not) have the title of archpriest, protopresbyter (also called \"protopriest\", or \"protopope\"), hieromonk (a monk who has been ordained to the priesthood) archimandrite (a senior hieromonk) and hegumen (abbot). Deacons may have the title of hierodeacon (a monk who has been ordained to the deaconate), archdeacon or protodeacon.\n\nThe lower clergy are not ordained through \"cheirotonia\" (laying on of hands) but through a blessing known as \"cheirothesia\" (setting-aside). These clerical ranks are subdeacon, reader and altar server (also known as taper-bearer). Some churches have a separate service for the blessing of a cantor.\n\nOrdination of a bishop, priest, deacon or subdeacon must be conferred during the Divine Liturgy (Eucharist)—though in some churches it is permitted to ordain up through deacon during the Liturgy of the Presanctified Gifts—and no more than a single individual can be ordained to the same rank in any one service. Numerous members of the lower clergy may be ordained at the same service, and their blessing usually takes place during the Little Hours prior to Liturgy, or may take place as a separate service. The blessing of readers and taper-bearers is usually combined into a single service. Subdeacons are ordained during the Little Hours, but the ceremonies surrounding his blessing continue through the Divine Liturgy, specifically during the Great Entrance.\n\nBishops are usually drawn from the ranks of the archimandrites, and are required to be celibate; however, a non-monastic priest may be ordained to the episcopate if he no longer lives with his wife (following Canon XII of the Quinisext Council of Trullo) In contemporary usage such a non-monastic priest is usually tonsured to the monastic state, and then elevated to archimandrite, at some point prior to his consecration to the episcopacy. Although not a formal or canonical prerequisite, at present bishops are often required to have earned a university degree, typically but not necessarily in theology.\n\nUsual titles are \"Your Holiness\" for a patriarch (with \"Your All-Holiness\" reserved for the Ecumenical Patriarch of Constantinople), \"Your Beatitude\" for an archbishop/metropolitan overseeing an autocephalous Church, \"Your Eminence\" for an archbishop/metropolitan generally, \"Master\" or \"Your Grace\" for a bishop and \"Father\" for priests, deacons and monks, although there are variations between the various Orthodox Churches. For instance, in Churches associated with the Greek tradition, while the Ecumenical Patriarch is addressed as \"Your All-Holiness,\" all other Patriarchs (and archbishops/metropolitans who oversee autocephalous Churches) are addressed as \"Your Beatitude.\"\n\nOrthodox priests, deacons, and subdeacons must be either married or celibate (preferably monastic) prior to ordination, but may not marry after ordination. \"Re\"marriage of clergy following divorce or widowhood is forbidden. Married clergy are considered as best-suited to staff parishes, as a priest with a family is thought better qualified to counsel his flock. It has been common practice in the Russian tradition for unmarried, non-monastic clergy to occupy academic posts.\n\nClergy in Protestantism fill a wide variety of roles and functions. In many denominations, such as Methodism, Presbyterianism, and Lutheranism, the roles of clergy are similar to Roman Catholic or Anglican clergy, in that they hold an ordained pastoral or priestly office, administer the sacraments, proclaim the word, lead a local church or parish, and so forth. The Baptist tradition only recognizes two ordained positions in the church as being the elders (pastors) and deacons as outlined in the third chapter of I Timothy in the Bible. The Presbyterian Church (U.S.A.) ordains two types of presbyters or elders, teaching (pastor) and ruling (leaders of the congregation which form a council with the pastors). Teaching elders are seminary trained and ordained as a presbyter and set aside on behalf of the whole denomination to the ministry of Word and Sacrament. Ordinarily, teaching elders are installed by a presbytery as pastor of a congregation. Ruling elders, after receiving training, may be commissioned by a presbytery to serve as a pastor of a congregation, as well as preach and administer sacraments.\n\nThe process of being designated as a member of the Protestant clergy, as well as that of being assigned to a particular office, varies with the denomination or faith group. Some Protestant denominations, such as Methodism, Presbyterianism, and Lutheranism, are hierarchical in nature; and ordination and assignment to individual pastorates or other ministries are made by the parent denominations. In other traditions, such as the Baptist and other Congregational groups, local churches are free to hire (and often ordain) their own clergy, although the parent denominations typically maintain lists of suitable candidates seeking appointment to local church ministries and encourage local churches to consider these individuals when filling available positions.\n\nSome Protestant denominations require that candidates for ordination be \"licensed\" to the ministry for a period of time (typically one to three years) prior to being ordained. This period typically is spent performing the duties of ministry under the guidance, supervision, and evaluation of a more senior, ordained minister. In some denominations, however, licensure is a permanent, rather than a transitional state for ministers assigned to certain specialized ministries, such as music ministry or youth ministry.\n\nMany Protestant denominations reject the idea that the clergy are a separate category of people, but rather stress the \"priesthood of all believers\". Based on this theological approach, most Protestants do not have a sacrament of ordination like the pre-Reformation churches. Protestant ordination, therefore, can be viewed more as a public statement by the ordaining body that an individual possesses the theological knowledge, moral fitness, and practical skills required for service in that faith group's ministry. Some Lutheran churches form an exception to this rule, as the Lutheran Book of Concord allows ordination to be received as a sacrament.\n\nSome Protestant denominations dislike the word \"clergy\" and do not use it of their own leaders. Often they refer to their leaders as pastors or ministers, titles that, if used, sometimes apply to the person only as long as he or she holds a particular office.\n\nIslam, like Judaism, has no clergy in the sacerdotal sense; there is no institution resembling the Christian priesthood. Islamic religious leaders do not \"serve as intermediaries between mankind and God\", have \"process of ordination\", nor \"sacramental functions\". They have been said to resemble more rabbis, serving as \"exemplars, teachers, judges, and community leaders,\" providing religious rules to the pious on \"even the most minor and private\" matters.\n\nThe title \"mullah\" (a Persian variation of the Arabic \"maula\", \"master\"), commonly translated \"cleric\" in the West and thought to be analogous to \"priest\" or \"rabbi\", is a title of address for any educated or respected figure, not even necessarily (though frequently) religious. The title \"sheikh\" (\"elder\") is used similarly.\n\nMost of the religious titles associated with Islam are scholastic or academic in nature: they recognize the holder's exemplary knowledge of the theory and practice of \"ad-dín\" (religion), and do not confer any particular spiritual or sacerdotal authority. The most general such title is \"`alim\" (pl. \"`ulamah\"), or \"scholar\". This word describes someone engaged in advanced study of the traditional Islamic sciences \"(`ulum)\" at an Islamic university or \"madrasah jami`ah\". A scholar's opinions may be valuable to others because of his/her knowledge in religious matters; but such opinions should not generally be considered binding, infallible, or absolute, as the individual Muslim is directly responsible to God for his or her own religious beliefs and practice.\n\nThe nearest analogue among Sunni Muslims to the parish priest or pastor, or to the \"pulpit rabbi\" of a synagogue, is called the \"imam khatib.\" This compound title is merely a common combination of two elementary offices: leader \"(imam)\" of the congregational prayer, which in most mosques is performed at the times of all daily prayers; and preacher \"(khatib)\" of the sermon or \"khutba\" of the obligatory congregational prayer at midday every Friday. Although either duty can be performed by anyone who is regarded as qualified by the congregation, at most well-established mosques \"imam khatib\" is a permanent part-time or full-time position. He may be elected by the local community, or appointed by an outside authority – e.g., the national government, or the waqf which sustains the mosque. There is no ordination as such; the only requirement for appointment as an \"imam khatib\" is recognition as someone of sufficient learning and virtue to perform both duties on a regular basis, and to instruct the congregation in the basics of Islam.\n\nThe title \"hafiz\" (lit. \"preserver\") is awarded to one who has memorized the entire Qur'an, often by attending a special course for the purpose; the \"imam khatib\" of a mosque is frequently (though not always) a \"hafiz.\"\n\nThere are several specialist offices pertaining to the study and administration of Islamic law or \"shari`ah.\" A scholar with a specialty in \"fiqh\" or jurisprudence is known as a \"faqih\". A \"qadi\" is a judge in an Islamic court. A \"mufti\" is a scholar who has completed an advanced course of study which qualifies him to issue judicial opinions or \"fatawah\".\n\nIn modern Shia Islam, scholars play a more prominent role in the daily lives of Muslims than in Sunni Islam; and there is a hierarchy of higher titles of scholastic authority, such as \"Ayatollah\". Since around the mid-19th century, a more complex title has been used in Twelver Shi`ism, namely \"marjaʿ at-taqlid\". \"Marjaʿ\" (pl. \"marajiʿ\") means \"source\", and \"taqlid\" refers to religious emulation or imitation. Lay Shi`ah must identify a specific \"marjaʿ\" whom they emulate, according to his legal opinions \"(fatawah)\" or other writings. On several occasions, the \"Marjaʿiyyat\" (community of all \"marajiʿ\") has been limited to a single individual, in which case his rulings have been applicable to all those living in the Twelver Shi'ah world. Of broader importance has been the role of the \"mujtahid\", a cleric of superior knowledge who has the authority to perform \"ijtihad\" (independent judgment). Mujtahids are few in number, but it is from their ranks that the \"marajiʿ at-taqlid\" are drawn.\n\nThe spiritual guidance function known in many Christian denominations as \"pastoral care\" is fulfilled for many Muslims by a \"murshid\" (\"guide\"), a master of the spiritual sciences and disciplines known as \"tasawuf\" or Sufism. Sufi guides are commonly styled \"Shaikh\" in both speaking and writing; in North Africa they are sometimes called \"marabouts\". They are traditionally appointed by their predecessors, in an unbroken teaching lineage reaching back to Muhammad. (The lineal succession of guides bears a superficial similarity to Christian ordination and apostolic succession, or to Buddhist dharma transmission; but a Sufi guide is regarded primarily as a specialized teacher and Islam denies the existence of an earthly hierarchy among believers.)\n\nMuslims who wish to learn Sufism dedicate themselves to a \"murshid\"&apos;s guidance by taking an oath called a \"bai'ah\". The aspirant is then known as a \"murid\" (\"disciple\" or \"follower\"). A \"murid\" who takes on special disciplines under the guide's instruction, ranging from an intensive spiritual retreat to voluntary poverty and homelessness, is sometimes known as a dervish.\n\nDuring the Islamic Golden Age, it was common for scholars to attain recognized mastery of both the \"exterior sciences\" \"(`ulum az-zahir)\" of the madrasahs as well as the \"interior sciences\" \"(`ulum al-batin)\" of Sufism. Al-Ghazali and Rumi are two notable examples.\n\nThe highest office an Ahmadi can hold is that of \"Khalifatu l-Masih\". Such a person may appoint amirs who manage regional areas. The consultative body for Ahmadiyya is called the \"Majlis-i-Shura\", which ranks second in importance to the \"Khalifatu l-Masih\". However, the Ahmadiyya community is declared as non-Muslims by many mainstream Muslims and they reject the messianic claims of Mirza Ghulam Ahmad.\n\nRabbinic Judaism does not have clergy as such, although according to the Torah there is a tribe of priests known as the Kohanim who were leaders of the religion up to the destruction of the Temple of Jerusalem in 70 AD when most Sadducees were wiped out; each member of the tribe, a Kohen had priestly duties, many of which centered around the sacrificial duties, atonement and blessings of the Israelite nation. Today, Jewish Kohanim know their status by family tradition, and still offer the priestly blessing during certain services in the synagogue and perform the \"Pidyon haben\" (redemption of the first-born son) ceremony.\n\nSince the time of the destruction of the Temple of Jerusalem, the religious leaders of Judaism have often been rabbis, who are technically scholars in Jewish law empowered to act as judges in a rabbinical court. All types of Judaism except Orthodox Judaism allow women as well as men to be ordained as rabbis and cantors. The leadership of a Jewish congregation is, in fact, in the hands of the laity: the president of a synagogue is its actual leader and any adult male Jew (or adult Jew in non-traditional congregations) can lead prayer services. The rabbi is not an occupation found in the Torah; the first time this word is mentioned is in the Mishnah. The modern form of the rabbi developed in the Talmudic era. Rabbis are given authority to make interpretations of Jewish law and custom. Traditionally, a man obtains one of three levels of Semicha (rabbinic ordination) after the completion of an arduous learning program in Torah, Tanakh (Hebrew Bible), Mishnah and Talmud, Midrash, Jewish ethics and lore, the codes of Jewish law and responsa, theology and philosophy.\nSince the early medieval era an additional communal role, the \"Hazzan\" (cantor) has existed as well. Cantors have sometimes been the only functionaries of a synagogue, empowered to undertake religio-civil functions like witnessing marriages. Cantors do provide leadership of actual services, primarily because of their training and expertise in the music and prayer rituals pertaining to them, rather than because of any spiritual or \"sacramental\" distinction between them and the laity. Cantors as much as rabbis have been recognized by civil authorities in the United States as clergy for legal purposes, mostly for awarding education degrees and their ability to perform weddings, and certify births and deaths.\n\nAdditionally, Jewish authorities license \"mohels\", people specially trained by experts in Jewish law and usually also by medical professionals to perform the ritual of circumcision. Traditional Orthodox Judaism does not license women as mohels, but other types of Judaism do. They are appropriately called \"mohelot\" (pl. of \"mohelet,\" f. of mohel)\n. As the Jewish News Weekly of Northern California states, \"...there is no halachic prescription against female mohels, [but] none exist in the Orthodox world, where the preference is that the task be undertaken by a Jewish man.\".\n\nIn many places, mohels are also licensed by civil authorities, as circumcision is technically a surgical procedure. Kohanim, who must avoid contact with dead human body parts (such as the removed foreskin) for ritual purity, cannot act as mohels, but some mohels are also either rabbis or cantors.\n\nAnother licensed cleric in Judaism is the \"shochet\", who are trained and licensed by religious authorities for kosher slaughter according to ritual law. A Kohen may be a shochet. Most shochetim are ordained rabbis. \n\nThen there is the \"mashgiach\". A \"mashgiach\" is someone who supervises the \"kashrut\" status of a kosher establishment. The \"mashgiach\" must know the Torah laws of \"kashrut\", and how they apply in the environment he is supervising. Obviously, this can vary. In many instances, the \"mashgiach\" is a rabbi. This helps, since rabbinical students learn the laws of kosher as part of their syllabus. However, not every \"mashgiach\" is a rabbi, and not every rabbi is qualified to be a \"mashgiach\".\n\nIn contemporary Orthodox Judaism, women are forbidden from becoming rabbis or cantors. Most Orthodox rabbinical seminaries or yeshivas also require dedication of many years to education, but few require a formal degree from a civil education institution that often define Christian clergy. Training is often focused on Jewish law, and some Orthodox Yeshivas forbid secular education.\n\nIn Hasidic Judaism, generally understood as a branch of Orthodox Judaism, there are dynastic spiritual leaders known as \"Rebbes\", often translated in English as \"Grand Rabbi\". The office of Rebbe is generally a hereditary one, but may also be passed from Rebbe to student or by recognition of a congregation conferring a sort of coronation to their new Rebbe. Although one does not need to be an ordained Rabbi to be a Rebbe, most Rebbes today are ordained Rabbis. Since one does not need to be an ordained rabbi to be a Rebbe, at some points in history there were female Rebbes as well, particularly the Maiden of Ludmir.\n\nIn Conservative Judaism, both men and women are ordained as rabbis and cantors. Conservative Judaism differs with Orthodoxy in that it sees Jewish Law as binding but also as subject to many interpretations, including more liberal interpretations. Academic requirements for becoming a rabbi are rigorous. First earn a bachelor's degree before entering rabbinical school. Studies are mandated in pastoral care and psychology, the historical development of Judaism and most importantly the academic study of Bible, Talmud and rabbinic literature, philosophy and theology, liturgy, Jewish history, and Hebrew literature of all periods.\n\nReconstructionist Judaism and Reform Judaism do not maintain the traditional requirements for study as rooted in Jewish Law and traditionalist text. Both men and women may be rabbis or cantors. The rabbinical seminaries of these movements hold that one must first earn a bachelor's degree before entering the rabbinate. In addition studies are mandated in pastoral care and psychology, the historical development of Judaism; and academic biblical criticism. Emphasis is placed not on Jewish law, but rather on sociology, modern Jewish philosophy, Theology and Pastoral Care.\n\nSikh clergy consists of five \"Jathedars\", one each from five \"takhts\" or sacred seats. The \"Jathedars\" are appointed by the Shiromani Gurdwara Parbandhak Committee (SGPC), an elected body of the Sikhs sometimes called the \"Parliament of Sikhs.\" The highest seat of the Sikh religion is called \"Akal Takht\" and the \"Jathedar\" of \"Akal Takht\" makes all the important decisions after consultations with the \"Jathedars\" of the other four \"takhts\" and the SGPC.\n\nHistorically traditional (or \"pagan\") religions typically combine religious authority and political power. What this means is that the sacred king or queen is therefore seen to combine both kingship and priesthood within his or her person, even though he or she is often aided by an actual high priest or priestess (see, for example, the Maya priesthood). When the functions of political ruler and religious leader are combined in this way, deification could be seen to be the next logical stage of his or her social advancement within his or her native environment, as is found in the case of the Egyptian Pharaohs. The Vedic priesthood of India is an early instance of a structured body of clergy organized as a separate and hereditary caste, one that occupied the highest social rung of its nation. A modern example of this phenomenon the priestly monarchs of the Yoruba holy city of Ile-Ife in Nigeria, whose reigning Onis have performed ritual ceremonies for centuries for the sustenance of the entire planet and its people.\n\nIn recent years, studies have suggested that American clergy in certain Protestant, Evangelical and Jewish traditions are more at risk than the general population of obesity, hypertension and depression. Their life expectancies have fallen in recent years and in the last decade their use of antidepressants has risen. Several religious bodies in the United States (Methodist, Episcopal, Baptist and Lutheran) have implemented measures to address the issue, through wellness campaigns, for example - but also by simply ensuring that clergy take more time off. It is unclear whether similar symptoms affect American Muslim clerics, although an anecdotal comment by one American imam suggested that leaders of mosques may also share these problems.\n\nOne exception to the findings of these studies is the case of American Catholic priests, who are required by canon law to take a spiritual retreat each year, and four weeks of vacation. Sociological studies at the University of Chicago have confirmed this exception; the studies also took the results of several earlier studies into consideration and included Roman Catholic priests nationwide. It remains unclear whether American clergy in other religious traditions experience the same symptoms, or whether clergy outside the United States are similarly affected.\n\n\n\n\n"}
{"id": "251534", "url": "https://en.wikipedia.org/wiki?curid=251534", "title": "Middle class", "text": "Middle class\n\nThe middle class is a class of people in the middle of a social hierarchy. It usage has often been vague whether defined in terms of occupation, income, education or social status. The definition by any one author is often chosen for political connotations. Writers on the left favor the lower-status \"working class.\" Modern social theorists—and especially economists—have defined and re-defined the term \"middle class\" in order to serve their particular social or political ends. \n\nWithin capitalism, \"middle-class\" initially referred to the \"bourgeoisie\"; later, with the further differentiation of classes as capitalist societies developed, the term came to be synonymous with the term \"petite bourgeoisie\".\n\nThe common measures of what constitutes middle class vary significantly among cultures. On the one hand, the term can be viewed primarily in terms of socioeconomic status. One of the narrowest definitions limits it to those in the middle fifth of the nation's income ladder. A wider characterization includes everyone but the poorest 20% and the wealthiest 20%. Some theories like \"Paradox of Interest\", use decile groups and wealth distribution data to determine the size and wealth share of the middle class.\n\nIn modern American vernacular, the term \"middle class\" is most often used as a self-description by those persons whom academics and Marxists would otherwise identify as the working class, which are below both the upper class and the true middle class, but above those in poverty. This leads to considerable ambiguity over the meaning of the term \"middle class\" in American usage. Sociologists such as Dennis Gilbert and Joseph Kahl see this American self-described \"middle class\" (working class) as the most populous class in the United States.\n\nIn 1977 Barbara Ehrenreich and her then husband John defined a new class in the United States as \"salaried menial workers who do not own the means of production and whose major function in the social division of labor ... [is] ... the reproduction of capitalist culture and capitalist class relations;\" the Ehrenreichs named this group the \"professional-managerial class.\"\n\nThere has been significant global middle-class growth over time. In February 2009, \"The Economist\" asserted that over half the world's population now belongs to the middle class, as a result of rapid growth in emerging countries. It characterized the middle class as having a reasonable amount of discretionary income, so that they do not live from hand-to-mouth as the poor do, and defined it as beginning at the point where people have roughly a third of their income left for discretionary spending after paying for basic food and shelter.\n\nThe term \"middle class\" was coined by British writer James Bradshaw in a 1745 pamphlet \"Scheme to prevent running Irish Wools to France.\" The term has had various, even contradictory, meanings. In medieval European feudal society (8th–12th centuries), a \"middle class\" composed primarily of peasants who formed a new \"bourgeoisie\" based on the success of their mercantile ventures, eventually overthrew the ruling monarchists of their society and ultimately led to the rise of capitalist societies.\n\nThe term \"middle class\" is first attested in James Bradshaw's 1745 pamphlet \"Scheme to prevent running Irish Wools to France.\" Another phrase used in Early modern Europe was \"the middling sort.\"\n\nThe term \"middle class\" has had several, sometimes contradictory, meanings. Friedrich Engels saw the category as an intermediate social class between the nobility and the peasantry of Europe in late-feudalist society. While the nobility owned much of the countryside, and the peasantry worked it, a new \"bourgeoisie\" (literally \"town-dwellers\") arose around mercantile functions in the city. In France, the middle classes helped drive the French Revolution. This \"middle class\" eventually overthrew the ruling monarchists of feudal society, thus becoming the new ruling class or bourgeoisie in the new capitalist-dominated societies.\n\nThe modern usage of the term \"middle-class\", however, dates to the 1913 UK Registrar-General's report, in which the statistician T.H.C. Stevenson identified the middle class as those falling between the upper-class and the working-class. The middle class includes: professionals, managers, and senior civil servants. The chief defining characteristic of membership in the middle-class is control of significant human capital while still being under the dominion of the elite upper class, who control much of the financial and legal capital in the world.\n\nWithin capitalism, \"middle-class\" initially referred to the \"bourgeoisie\"; later, with the further differentiation of classes as capitalist societies developed, the term came to be synonymous with the term \"petite bourgeoisie\". The boom-and-bust cycles of capitalist economies result in the periodic (and more or less temporary) impoverisation and proletarianisation of much of the \"petite bourgeois\" world, resulting in their moving back and forth between working-class and petite-bourgeois status. The typical modern definitions of \"middle class\" tend to ignore the fact that the classical petite-bourgeoisie is and has always been the owner of a small-to medium-sized business whose income is derived almost exclusively from the employment of workers; \"middle class\" came to refer to the combination of the labour aristocracy, professionals, and salaried, white-collar workers.\n\nThe size of the middle class depends on how it is defined, whether by education, wealth, environment of upbringing, social network, manners or values, etc. These are all related, but are far from deterministically dependent. The following factors are often ascribed in the literature on this topic to a \"middle class:\"\n\nIn the United States, by the end of the twentieth century, more people identified themselves as middle-class than as lower or \"working\" class (with insignificant numbers identifying themselves as upper-class). The Labour Party in the UK, which grew out of the organised labour movement and originally drew almost all of its support from the working-class, reinvented itself under Tony Blair in the 1990s as \"New Labour\", a party competing with the Conservative Party for the votes of the middle-class as well as those of the Labour Party's traditional group of voters – the working-class. By 2011 almost three-quarters of British people were found to identify themselves as middle-class.\n\nMarxism defines social classes according to their relationship with the means of production. The \"middle class\" is said to be the class below the ruling class and above the proletariat in the Marxist social schema and is synonymous with the term \"petite-\" or \"petty-bourgeoisie.\" Marxist writers have used the term in two distinct but related ways. In the first sense, it is used for the bourgeoisie (the urban merchant and professional class) that arose between the aristocracy and the proletariat in the waning years of feudalism in the Marxist model. V. I. Lenin stated that the \"peasantry ... in Russia constitute eight- or nine-tenths of the petty bourgeoisie\". However, in modern developed countries, Marxist writers define the \"petite bourgeoisie\" as primarily comprising (as the name implies) owners of small to medium-sized businesses, who derive their income from the exploitation of wage-laborers (and who are in turn exploited by the \"big\" bourgeoisie i.e. bankers, owners of large corporate trusts, etc.) as well as the highly educated professional class of doctors, engineers, architects, lawyers, university professors, salaried middle-management of capitalist enterprises of all sizes, etc. – as the \"middle class\" which stands between the ruling capitalist \"owners of the means of production\" and the working class (whose income is derived solely from hourly wages).\n\nPioneer 20th century American Marxist theoretician Louis C. Fraina (Lewis Corey) defined the middle class as \"the class of independent small enterprisers, owners of productive property from which a livelihood is derived.\" From Fraina's perspective, this social category included \"propertied farmers\" but not propertyless tenant farmers. Middle class also included salaried managerial and supervisory employees but not \"the masses of propertyless, dependent salaried employees. Fraina speculated that the entire category of salaried employees might be adequately described as a \"new middle class\" in economic terms, although this remained a social grouping in which \"most of whose members are a new proletariat.\"\n\nIn 1977 Barbara Ehrenreich and her then husband John defined a new class in the United States as \"salaried menial workers who do not own the means of production and whose major function in the social division of labor ... [is] ... the reproduction of capitalist culture and capitalist class relations;\" the Ehrenreichs named this group the \"professional-managerial class.\"\nThis group of middle-class professionals is distinguished from other social classes by their training and education (typically business qualifications and university degrees), with example occupations including academics and teachers, social workers, engineers, managers, nurses, and middle-level administrators. The Ehrenreichs developed their definition from studies by André Gorz, Serge Mallet, and others, of a \"new working class,\" which, despite education and a perception of themselves as middle class, were part of the working class because they did not own the means of production, and were wage earners paid to produce a piece of capital. The professional-managerial class seeks higher rank status and salary and tend to have incomes above the average for their country.\n\nIt is important to understand that modern definitions of the term \"middle class\" are often politically motivated and vary according to the exigencies of political purpose which they were conceived to serve in the first place as well as due to the multiplicity of more- or less-scientific methods used to measure and compare \"wealth\" between modern advanced industrial states (where poverty is relatively low and the distribution of wealth more egalitarian in a relative sense) and in developing countries (where poverty and a profoundly unequal distribution of wealth crush the vast majority of the population). Many of these methods of comparison have been harshly criticised; for example, economist Thomas Piketty, in his book \"Capital in the Twenty-First Century\", describes one of the most commonly used comparative measures of wealth across the globe – the Gini coefficient – as being an example of \"synthetic indices ... which mix very different things, such as inequality with respect to labor and capital, so that it is impossible to distinguish clearly among the multiple dimensions of inequality and the various mechanisms at work.\"\n\nIn February 2009, \"The Economist\" asserted that over half the world's population now belongs to the middle class, as a result of rapid growth in emerging countries. It characterized the middle class as having a reasonable amount of discretionary income, so that they do not live from hand-to-mouth as the poor do, and defined it as beginning at the point where people have roughly a third of their income left for discretionary spending after paying for basic food and shelter. This allows people to buy consumer goods, improve their health care, and provide for their children's education. Most of the emerging middle class consists of people who are middle class by the standards of the developing world but not the developed one, since their money incomes do not match developed country levels, but the percentage of it which is discretionary does. By this definition, the number of middle-class people in Asia exceeded that in the West sometime around 2007 or 2008.\n\n\"The Economist\" article pointed out that in many emerging countries the middle class has not grown incrementally but explosively. The point at which the poor start entering the middle class by the millions is alleged to be the time when poor countries get the maximum benefit from cheap labour through international trade, before they price themselves out of world markets for cheap goods. It is also a period of rapid urbanization, when subsistence farmers abandon marginal farms to work in factories, resulting in a several-fold increase in their economic productivity before their wages catch up to international levels. That stage was reached in China some time between 1990 and 2005, when the Chinese \"middle class\" grew from 15% to 62% of the population and is just being reached in India now.\n\n\"The Economist\" predicted that surge across the poverty line should continue for a couple of decades and the global middle class will grow exponentially between now and 2030. \nBased on the rapid growth, scholars expect the global middle class to be the driving force for sustainable development. This assumption, however, is contested (see below).\n\nAs the American middle class is estimated by some researchers to comprise approximately 45% of the population, \"The Economist\" article would put the size of the American middle class below the world average. This difference is due to the extreme difference in definitions between \"The Economist\" and many other models.\n\nIn 2010, a working paper by the OECD asserted that 1.8 billion people were now members of the global \"middle class.\" Credit Suisse's Global Wealth Report 2014, released in October 2014, estimated that one billion adults belonged to the \"middle class,\" with wealth anywhere between the range of $10,000–$100,000.\n\nAccording to a study carried out by the Pew Research Center, a combined 16% of the world's population in 2011 were \"upper-middle income\" and \"upper income.\"\n\nAn April 2019 OECD report said that the millennial generation is being pushed out of the middle class throughout the Western world.\n\nIn 2012, the \"middle class\" in Russia was estimated as 15% of the whole population. Due to sustainable growth, the pre-crisis level was exceeded. In 2015, research from the Russian Academy of Sciences estimated that around 15% of the Russian population are \"firmly middle class\", while around another 25% are \"on the periphery\".\n\nSince the beginning of the 21st century, China's middle class has grown by significant margins. According to the Center for Strategic and International Studies, by 2013, some 420 million people, or 31%, of the Chinese population qualified as middle class. Based on the World Bank definition of middle class as those having with daily spending between $10 to $50 per day, nearly 40% of the Chinese population were considered middle class as of 2017.\n\nEstimates vary widely on the number of middle-class people in India. According to \"The Economist\", 78 million of India's population are considered middle class as of 2017, if defined using the cutoff of those making more than $10 per day, a standard used by the India's National Council of Applied Economic Research. If including those with incomes $2 – $10 per day, the number increases to 604 million. This was termed by researchers as the \"new middle class\". Measures considered include geography, lifestyle, income, and education. The World Inequality Report in 2018 further concluded that elites (i.e. the top 10%) are accumulating wealth at a greater rate than the middle class, that rather than growing, India's middle class may be shrinking in size.\n\nAccording to a 2014 study by Standard Bank economist Simon Freemantle, a total of 15.3 million households in 11 surveyed African nations are middle-class. These include Angola, Ethiopia, Ghana, Kenya, Mozambique, Nigeria, South Sudan, Sudan, Tanzania, Uganda and Zambia. In South Africa, a report conducted by the Institute for Race Relations in 2015 estimated that between 10%–20% of South Africans are middle class, based on various criteria. An earlier study estimated that in 2008 21.3% of South Africans were members of the middle class.\n\nA study by EIU Canback indicates 90% of Africans fall below an income of $10 a day. The proportion of Africans in the $10–$20 middle class (excluding South Africa), rose from 4.4% to only 6.2% between 2004 and 2014. Over the same period, the proportion of \"upper middle\" income ($20–$50 a day) went from 1.4% to 2.3%.\n\nAccording to a 2014 study by the German Development Institute, the middle class of Sub-Saharan Africa rose from 14 million to 31 million people between 1990 and 2010.\n\nAccording to a study by the World Bank, the number of Latin Americans who are middle class rose from 103m to 152m between 2003 and 2009.\n\nThe numbers below reflect the middle, upper, and lower share of all adults by country by net wealth (not income). Middle class is defined here for the US as those adults with a net wealth of between US$50,000 and US$500,000 in mid 2015. Purchasing power parity is used to adjust these number for other countries. Unlike that of the upper class, wealth of the middle and lowest quintile consists substantially of non-financial assets, specifically home equity. Factors which explain differences in home equity include housing prices and home ownership rates. According to the OECD, the vast majority of financial assets in every country analysed is found in the top of the wealth distribution.\n\nThe American middle class is smaller than the middle classes across Western Europe, but its income is higher, according to a recent Pew Research Center analysis of the U.S. and 11 European nations.\n\nThe median disposable (after-tax) income of middle-class households in the U.S. was $60,884 in 2010. With the exception of Luxembourg – a virtual city-state where the median income was $71,799 – the disposable incomes of middle-class households in the other 10 Western European countries in the study trailed well behind the American middle class.\n\n\n\nOther:\n\n\n"}
{"id": "43349971", "url": "https://en.wikipedia.org/wiki?curid=43349971", "title": "Proletariat", "text": "Proletariat\n\nThe proletariat ( from Latin \"producing offspring\") is the class of wage-earners in an economic society whose only possession of significant material value is their labour-power (how much work they can do). A member of such a class is a proletarian.\n\nMarxist theory considers the proletariat to be oppressed by capitalism and the wage system. This oppression gives the proletariat common economic and political interests that transcend national boundaries. These common interests put the proletariat in a position to unite and take power away from the capitalist class (see dictatorship of the proletariat), in order to create a communist society free from class distinctions.\n\nThe constituted a social class of Roman citizens owning little or no property. The origin of the name is presumably linked with the census, which Roman authorities conducted every five years to produce a register of citizens and their property from which their military duties and voting privileges could be determined. For citizens with property valued 11,000 or less, which was below the lowest census for military service, their children— (from Latin , \"offspring\")—were listed instead of their property; hence, the name , \"the one who produces offspring\". The only contribution of a to the Roman society was seen in his ability to raise children, the future Roman citizens who can colonize new territories conquered by the Roman Republic and later by the Roman Empire. The citizens who had no property of significance were called because they were \"persons registered not as to their property...but simply as to their existence as living individuals, primarily as heads () of a family.\"\n\nAlthough included in one of the five support of the (English: Centuriate Assembly), were largely deprived of their voting rights due to their low social status caused by their lack of \"even the minimum property required for the lowest class\" and a class-based hierarchy of the . The late Roman historians, such as Livy, not without some uncertainty, understood the to be one of three forms of popular assembly of early Rome composed of , the voting units whose members represented a class of citizens according to the value of their property. This assembly, which usually met on the to discuss public policy issues, was also used as a means of designating military duties demanded of Roman citizens. One of reconstructions of the features 18 of cavalry, and 170 of infantry divided into five classes by wealth, plus 5 of support personnel called . The top infantry class assembled with full arms and armor; the next two classes brought arms and armor, but less and lesser; the fourth class only spears; the fifth slings. In voting, the cavalry and top infantry class were enough to decide an issue; as voting started at the top, an issue might be decided before the lower classes voted.\n\nAfter the closing of the Second Punic War (218–201 BC), a series of subsequent wars, including the Jugurthine War and various conflicts in Macedonia and Asia, resulted in a significant reduction in the number of Roman family farmers. The effect was therefore that the Roman Republic experienced a shortage of people whose property qualified them to perform the citizenry's military duty to Rome. As a result of the Marian reforms initiated in 107 BC by the Roman general Gaius Marius (157–86), which expanded the eligibility of military service to the urban poor, the became the backbone of the Roman army.\n\nIn the era of early 19th century, many Western European liberal scholars - who dealt with social sciences and economics - pointed out the socio-economic similarities of the modern rapidly growing industrial worker class and the classic ancient proletarians. One of the earliest analogies can be found in the 1807 paper of French philosopher and political scientist Hugues Felicité Robert de Lamennais. Later it was translated to English with the title: \"Modern Slavery\".\n\nSwiss liberal economist and historian , was the first who applied the proletariat term to the working class created under capitalism, and whose writings were frequently cited by Marx. Marx most likely encountered the Proletariat term while studying the works of Sismondi. \n\nKarl Marx, who studied Roman law at the Friedrich Wilhelm University of Berlin, used the term \"proletariat\" in his socio-political theory of Marxism to describe a working class unadulterated by private property and capable of a revolutionary action to topple capitalism in order to create classless society.\nIn Marxist theory, the proletariat is the social class that does not have ownership of the means of production and whose only means of subsistence is to sell their labor power for a wage or salary. Proletarians are wage-workers, while some refer to those who receive salaries as the \"salariat\". For Marx, however, wage labor may involve getting a salary rather than a wage \"per se\". Marxism sees the proletariat and bourgeoisie (capitalist class) as occupying conflicting positions, since workers automatically wish their wages to be as high as possible, while owners and their proxies wish for wages (costs) to be as low as possible.\nIn Marxist theory, the borders between the proletariat and some layers of the petite bourgeoisie, who rely primarily but not exclusively on self-employment at an income no different from an ordinary wage or below it – and the lumpenproletariat, who are not in legal employment – are not necessarily well defined. Intermediate positions are possible, where some wage-labor for an employer combines with self-employment. Marx makes a clear distinction between proletariat as salaried workers, which he sees as a progressive class, and Lumpenproletariat, \"rag-proletariat\", the poorest and outcasts of the society, such as beggars, tricksters, entertainers, buskers, criminals and prostitutes, which he considers a retrograde class. Socialist parties have often struggled over the question of whether they should seek to organize and represent all the lower classes, or just the wage-earning proletariat.\n\nAccording to Marxism, capitalism is a system based on the exploitation of the proletariat by the bourgeoisie. This exploitation takes place as follows: the workers, who own no means of production of their own, must use the means of production that are property of others in order to produce, and consequently earn, their living. Instead of hiring those means of production, they themselves get hired by capitalists and work for them, producing goods or services. These goods or services become the property of the capitalist, who sells them at the market.\n\nOne part of the wealth produced is used to pay the workers' wages (variable costs), another part to renew the means of production (constant costs) while the third part, surplus value is split between the capitalist's private takings (profit), and the money used to pay rents, taxes, interests, etc. Surplus value is the difference between the wealth that the proletariat produces through its work, and the wealth it consumes to survive and to provide labor to the capitalist companies. A part of the surplus value is used to renew or increase the means of production, either in quantity or quality (i.e., it is turned into capital), and is called capitalized surplus value. What remains is consumed by the capitalist class.\n\nThe commodities that proletarians produce and capitalists sell are valued for the amount of labor embodied in them. The same goes for the workers' labor power itself: it is valued, not for the amount of wealth it produces, but for the amount of labor necessary to produce and reproduce it. Thus the capitalists earn wealth from the labor of their employees, not as a function of their personal contribution to the productive process, which may even be null, but as a function of the juridical relation of property to the means of production. Marxists argue that new wealth is created through labor applied to natural resources.\n\nMarx argued that the proletariat would displace the capitalist system with the dictatorship of the proletariat, abolishing the social relationships underpinning the class system and then developing into a communist society in which \"the free development of each is the condition for the free development of all\".\n\nProle drift, short for proletarian drift, is the tendency in advanced industrialized societies for everything inexorably to become proletarianized, or to become commonplace and commodified. This trend is attributed to mass production, mass selling, mass communication and mass education. Examples include best-seller lists, films and music that must appeal to the masses, and shopping malls.\n\n\n"}
{"id": "18717981", "url": "https://en.wikipedia.org/wiki?curid=18717981", "title": "Sociology", "text": "Sociology\n\nSociology is a study of society, patterns of social relationships, social interaction and culture of everyday life. It is a social science that uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order, acceptance, and change or social evolution. Sociology is also defined as the general science of society. While some sociologists conduct research that may be applied directly to social policy and welfare, others focus primarily on refining the theoretical understanding of social processes. Subject matter ranges from the micro-sociology level of individual agency and interaction to the macro level of systems and the social structure.\n\nThe different traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects, such as health, medical, economy, military and penal institutions, the Internet, education, social capital, and the role of social activity in the development of scientific knowledge.\n\nThe range of social scientific methods has also expanded. Social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century led to increasingly interpretative, hermeneutic, and philosophic approaches towards the analysis of society. Conversely, the end of the 1990s and the beginning of the 2000s have seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis.\n\nSocial research informs politicians and policy makers, educators, planners, legislators, administrators, developers, business magnates, managers, social workers, non-governmental organizations, non-profit organizations, and people interested in resolving social issues in general. There is often a great deal of crossover between social research, market research, and other statistical fields.\n\nSociological reasoning predates the foundation of the discipline. Social analysis has origins in the common stock of Western knowledge and philosophy, and has been carried out from as far back as the time of ancient Greek philosopher Plato, if not before. The origin of the survey (the collection of information from a sample of individuals) can be traced back to at least the Domesday Book in 1086, while ancient philosophers such as Confucius wrote about the importance of social roles. There is evidence of early sociology in medieval Arab writings. Some sources consider Ibn Khaldun, a 14th-century Arab Islamic scholar from North Africa (Tunisia), to have been the first sociologist and the father of sociology (see Branches of the early Islamic philosophy); his \"Muqaddimah\" was perhaps the first work to advance social-scientific reasoning on social cohesion and social conflict.\n\nThe word \"sociology\" (or \"sociologie\") is derived from both Latin and Greek origins. The Latin word: \"socius\", \"companion\"; the suffix \"-logy\", \"the study of\" from Greek -λογία from λόγος, \"lógos\", \"word\", \"knowledge\". It was first coined in 1780 by the French essayist Emmanuel-Joseph Sieyès (1748–1836) in an unpublished manuscript. \"Sociology\" was later defined independently by the French philosopher of science, Auguste Comte (1798–1857) in 1838 as a new way of looking at society. Comte had earlier used the term \"social physics\", but that had subsequently been appropriated by others, most notably the Belgian statistician Adolphe Quetelet. Comte endeavoured to unify history, psychology, and economics through the scientific understanding of the social realm. Writing shortly after the malaise of the French Revolution, he proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in \"The Course in Positive Philosophy\" (1830–1842) and \"A General View of Positivism\" (1848). Comte believed a positivist stage would mark the final era, after conjectural theological and metaphysical phases, in the progression of human understanding. In observing the circular dependence of theory and observation in science, and having classified the sciences, Comte may be regarded as the first philosopher of science in the modern sense of the term.\n\nHerbert Spencer (27 April 1820 – 8 December 1903) was one of the most popular and influential 19th-century sociologists. It is estimated that he sold one million books in his lifetime, far more than any other sociologist at the time. So strong was his influence that many other 19th-century thinkers, including Émile Durkheim, defined their ideas in relation to his. Durkheim's \"Division of Labour in Society\" is to a large extent an extended debate with Spencer from whose sociology, many commentators now agree, Durkheim borrowed extensively. Also a notable biologist, Spencer coined the term \"survival of the fittest\". While Marxian ideas defined one strand of sociology, Spencer was a critic of socialism as well as strong advocate for a laissez-faire style of government. His ideas were closely observed by conservative political circles, especially in the United States and England.\n\nThe first formal Department of Sociology in the world was established by Albion Small – at the invitation of William Rainey Harper – at the University of Chicago in 1892, and the American Journal of Sociology was founded shortly thereafter in 1895 by Small as well. However, the institutionalization of sociology as an academic discipline was chiefly led by Émile Durkheim (1858–1917), who developed positivism as a foundation for practical social research. While Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method, maintaining that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisting that they may retain the same objectivity, rationalism, and approach to causality. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his \"Rules of the Sociological Method\" (1895). For Durkheim, sociology could be described as the \"science of institutions, their genesis and their functioning\".\n\nDurkheim's monograph \"Suicide\" (1897) is considered a seminal work in statistical analysis by contemporary sociologists. \"Suicide\" is a case study of variations in suicide rates among Catholic and Protestant populations, and served to distinguish sociological analysis from psychology or philosophy. It also marked a major contribution to the theoretical concept of structural functionalism. By carefully examining suicide statistics in different police districts, he attempted to demonstrate that Catholic communities have a lower suicide rate than that of Protestants, something he attributed to social (as opposed to individual or psychological) causes. He developed the notion of objective \"sui generis\" \"social facts\" to delineate a unique empirical object for the science of sociology to study. Through such studies he posited that sociology would be able to determine whether any given society is 'healthy' or 'pathological', and seek social reform to negate organic breakdown or \"social anomie\".\n\nSociology quickly evolved as an academic response to the perceived challenges of modernity, such as industrialization, urbanization, secularization, and the process of \"rationalization\". The field predominated in continental Europe, with British anthropology and statistics generally following on a separate trajectory. By the turn of the 20th century, however, many theorists were active in the English-speaking world. Few early sociologists were confined strictly to the subject, interacting also with economics, jurisprudence, psychology and philosophy, with theories being appropriated in a variety of different fields. Since its inception, sociological epistemology, methods, and frames of inquiry, have significantly expanded and diverged.\n\nDurkheim, Marx, and the German theorist Max Weber (1864–1920) are typically cited as the three principal architects of sociology. Herbert Spencer, William Graham Sumner, Lester F. Ward, W.E.B. Du Bois, Vilfredo Pareto, Alexis de Tocqueville, Werner Sombart, Thorstein Veblen, Ferdinand Tönnies, Georg Simmel, Jane Addams and Karl Mannheim are often included on academic curricula as founding theorists. Curricula also may include Charlotte Perkins Gilman, Marianne Weber and Friedrich Engels as founders of the feminist tradition in sociology. Each key figure is associated with a particular theoretical perspective and orientation.\n\nThe overarching methodological principle of positivism is to conduct sociology in broadly the same manner as natural science. An emphasis on empiricism and the scientific method is sought to provide a tested foundation for sociological research based on the assumption that the only authentic knowledge is scientific knowledge, and that such knowledge can only arrive by positive affirmation through scientific methodology.\n\nThe term has long since ceased to carry this meaning; there are no fewer than twelve distinct epistemologies that are referred to as positivism. Many of these approaches do not self-identify as \"positivist\", some because they themselves arose in opposition to older forms of positivism, and some because the label has over time become a pejorative term by being mistakenly linked with a theoretical empiricism. The extent of antipositivist criticism has also diverged, with many rejecting the scientific method and others only seeking to amend it to reflect 20th-century developments in the philosophy of science. However, positivism (broadly understood as a scientific approach to the study of society) remains dominant in contemporary sociology, especially in the United States.\n\nLoïc Wacquant distinguishes three major strains of positivism: Durkheimian, Logical, and Instrumental. None of these are the same as that set forth by Comte, who was unique in advocating such a rigid (and perhaps optimistic) version. While Émile Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method. Durkheim maintained that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisted that they should retain the same objectivity, rationalism, and approach to causality. He developed the notion of objective \"sui generis\" \"social facts\" to serve as unique empirical objects for the science of sociology to study.\n\nThe variety of positivism that remains dominant today is termed \"instrumental positivism\". This approach eschews epistemological and metaphysical concerns (such as the nature of social facts) in favour of methodological clarity, replicability, reliability and validity. This positivism is more or less synonymous with quantitative research, and so only resembles older positivism in practice. Since it carries no explicit philosophical commitment, its practitioners may not belong to any particular school of thought. Modern sociology of this type is often credited to Paul Lazarsfeld, who pioneered large-scale survey studies and developed statistical techniques for analysing them. This approach lends itself to what Robert K. Merton called middle-range theory: abstract statements that generalize from segregated hypotheses and empirical regularities rather than starting with an abstract idea of a social whole.\n\nReactions against social empiricism began when German philosopher Hegel voiced opposition to both empiricism, which he rejected as uncritical, and determinism, which he viewed as overly mechanistic. Karl Marx's methodology borrowed from Hegelian dialecticism but also a rejection of positivism in favour of critical analysis, seeking to supplement the empirical acquisition of \"facts\" with the elimination of illusions. He maintained that appearances need to be critiqued rather than simply documented. Early hermeneuticians such as Wilhelm Dilthey pioneered the distinction between natural and social science ('Geisteswissenschaft'). Various neo-Kantian philosophers, phenomenologists and human scientists further theorized how the analysis of the social world differs to that of the natural world due to the irreducibly complex aspects of human society, culture, and being.\n\nIn the Italian context of development of social sciences and of sociology in particular, there are oppositions to the first foundation of the discipline, sustained by speculative philosophy in accordance with the antiscientific tendencies matured by critique of positivism and evolutionism, so a tradition Progressist struggles to establish itself.\n\nAt the turn of the 20th century the first generation of German sociologists formally introduced methodological anti-positivism, proposing that research should concentrate on human cultural norms, values, symbols, and social processes viewed from a resolutely subjective perspective. Max Weber argued that sociology may be loosely described as a science as it is able to identify causal relationships of human \"social action\"—especially among \"ideal types\", or hypothetical simplifications of complex social phenomena. As a non-positivist, however, Weber sought relationships that are not as \"historical, invariant, or generalisable\" as those pursued by natural scientists. Fellow German sociologist, Ferdinand Tönnies, theorised on two crucial abstract concepts with his work on \"Gemeinschaft and Gesellschaft\" (lit. \"community\" and \"society\"). Tönnies marked a sharp line between the realm of concepts and the reality of social action: the first must be treated axiomatically and in a deductive way (\"pure sociology\"), whereas the second empirically and inductively (\"applied sociology\").\n\nBoth Weber and Georg Simmel pioneered the \"Verstehen\" (or 'interpretative') method in social science; a systematic process by which an outside observer attempts to relate to a particular cultural group, or indigenous people, on their own terms and from their own point of view. Through the work of Simmel, in particular, sociology acquired a possible character beyond positivist data-collection or grand, deterministic systems of structural law. Relatively isolated from the sociological academy throughout his lifetime, Simmel presented idiosyncratic analyses of modernity more reminiscent of the phenomenological and existential writers than of Comte or Durkheim, paying particular concern to the forms of, and possibilities for, social individuality. His sociology engaged in a neo-Kantian inquiry into the limits of perception, asking 'What is society?' in a direct allusion to Kant's question 'What is nature?'\n\nThe first college course entitled \"Sociology\" was taught in the United States at Yale in 1875 by William Graham Sumner. In 1883 Lester F. Ward, the first president of the American Sociological Association, published \"Dynamic Sociology—Or Applied social science as based upon statical sociology and the less complex sciences\" and attacked the laissez-faire sociology of Herbert Spencer and Sumner. Ward's 1200 page book was used as core material in many early American sociology courses. In 1890, the oldest continuing American course in the modern tradition began at the University of Kansas, lectured by Frank W. Blackmar. The Department of Sociology at the University of Chicago was established in 1892 by Albion Small, who also published the first sociology textbook: An introduction to the study of society 1894. George Herbert Mead and Charles Cooley, who had met at the University of Michigan in 1891 (along with John Dewey), would move to Chicago in 1894. Their influence gave rise to social psychology and the symbolic interactionism of the modern Chicago School. The \"American Journal of Sociology\" was founded in 1895, followed by the \"American Sociological Association\" (ASA) in 1905. The sociological \"canon of classics\" with Durkheim and Max Weber at the top owes in part to Talcott Parsons, who is largely credited with introducing both to American audiences. Parsons consolidated the sociological tradition and set the agenda for American sociology at the point of its fastest disciplinary growth. Sociology in the United States was less historically influenced by Marxism than its European counterpart, and to this day broadly remains more statistical in its approach.\n\nThe first sociology department to be established in the United Kingdom was at the London School of Economics and Political Science (home of the \"British Journal of Sociology\") in 1904. Leonard Trelawny Hobhouse and Edvard Westermarck became the lecturers in the discipline at the University of London in 1907. Harriet Martineau, an English translator of Comte, has been cited as the first female sociologist. In 1909 the \"Deutsche Gesellschaft für Soziologie\" (German Sociological Association) was founded by Ferdinand Tönnies and Max Weber, among others. Weber established the first department in Germany at the Ludwig Maximilian University of Munich in 1919, having presented an influential new antipositivist sociology. In 1920, Florian Znaniecki set up the first department in Poland. The \"Institute for Social Research\" at the University of Frankfurt (later to become the Frankfurt School of critical theory) was founded in 1923. International co-operation in sociology began in 1893, when René Worms founded the \"\", an institution later eclipsed by the much larger International Sociological Association (ISA), founded in 1949.\n\nThe contemporary discipline of sociology is theoretically multi-paradigmatic in line with the contentions of classical social theory. In Randall Collins' well-cited survey of sociological theory he retroactively labels various theorists as belonging to four theoretical traditions: Functionalism, Conflict, Symbolic Interactionism, and Utilitarianism. Modern sociological theory descends predominantly from functionalist (Durkheim) and conflict-centred (Marx and Weber) accounts of social structure, as well as the symbolic interactionist tradition consisting of micro-scale structural (Simmel) and pragmatist (Mead, Cooley) theories of social interaction. Utilitarianism, also known as Rational Choice or Social Exchange, although often associated with economics, is an established tradition within sociological theory. Lastly, as argued by Raewyn Connell, a tradition that is often forgotten is that of Social Darwinism, which brings the logic of Darwinian biological evolution and applies it to people and societies. This tradition often aligns with classical functionalism. It was the dominant theoretical stance in American sociology from around 1881 to 1915 and is associated with several founders of sociology, primarily Herbert Spencer, Lester F. Ward and William Graham Sumner. Contemporary sociological theory retains traces of each of these traditions and they are by no means mutually exclusive.\n\nA broad historical paradigm in both sociology and anthropology, functionalism addresses the social structure, referred to as social organization by the classical theorists, as a whole and with respect to the necessary function of its constituent elements. A common analogy (popularized by Herbert Spencer) is to regard norms and institutions as 'organs' that work towards the proper-functioning of the entire 'body' of society. The perspective was implicit in the original sociological positivism of Comte but was theorized in full by Durkheim, again with respect to observable, structural laws. Functionalism also has an anthropological basis in the work of theorists such as Marcel Mauss, Bronisław Malinowski and Radcliffe-Brown. It is in Radcliffe-Brown's specific usage that the prefix 'structural' emerged. Classical functionalist theory is generally united by its tendency towards biological analogy and notions of social evolutionism, in that the basic form of society would increase in complexity and those forms of social organization that promoted solidarity would eventually overcome social disorganization. As Giddens states: \"Functionalist thought, from Comte onwards, has looked particularly towards biology as the science providing the closest and most compatible model for social science. Biology has been taken to provide a guide to conceptualizing the structure and the function of social systems and to analysing processes of evolution via mechanisms of adaptation. functionalism strongly emphasizes the pre-eminence of the social world over its individual parts (i.e. its constituent actors, human subjects).\"\n\nFunctionalist theories emphasize \"cohesive systems\" and are often contrasted with \"conflict theories\", which critique the overarching socio-political system or emphasize the inequality between particular groups. The following quotes from Durkheim and Marx epitomize the political, as well as theoretical, disparities, between functionalist and conflict thought respectively:\n\nSymbolic interaction; often associated with Interactionism, Phenomenological sociology, Dramaturgy, Interpretivism, is a sociological tradition that places emphasis on subjective meanings and the empirical unfolding of social processes, generally accessed through micro-analysis. This tradition emerged in the \"Chicago School\" of the 1920s and 1930s, which prior to World War II \"had been \"the\" center of sociological research and graduate study\". The approach focuses on creating a framework for building a theory that sees society as the product of the everyday interactions of individuals. Society is nothing more than the shared reality that people construct as they interact with one another. This approach sees people interacting in countless settings using symbolic communications to accomplish the tasks at hand. Therefore, society is a complex, ever-changing mosaic of subjective meanings. Some critics of this approach argue that it only looks at what is happening in a particular social situation, and disregards the effects that culture, race or gender (i.e. social-historical structures) may have in that situation. Some important sociologists associated with this approach include Max Weber, George Herbert Mead, Erving Goffman, George Homans and Peter Blau. It is also in this tradition that the radical-empirical approach of Ethnomethodology emerges from the work of Harold Garfinkel.\n\nUtilitarianism is often referred to as exchange theory or rational choice theory in the context of sociology. This tradition tends to privilege the agency of individual rational actors and assumes that within interactions individuals always seek to maximize their own self-interest. As argued by Josh Whitford, rational actors are assumed to have four basic elements, the individual has (1) \"a knowledge of alternatives,\" (2) \"a knowledge of, or beliefs about the consequences of the various alternatives,\" (3) \"an ordering of preferences over outcomes,\" (4) \"A decision rule, to select among the possible alternatives\" Exchange theory is specifically attributed to the work of George C. Homans, Peter Blau and Richard Emerson. Organizational sociologists James G. March and Herbert A. Simon noted that an individual's rationality is bounded by the context or organizational setting. The utilitarian perspective in sociology was, most notably, revitalized in the late 20th century by the work of former ASA president James Coleman.\n\nFollowing the decline of theories of sociocultural evolution, in the United States, the interactionism of the Chicago School dominated American sociology. As Anselm Strauss describes, \"We didn't think symbolic interaction was a perspective in sociology; we thought it was sociology.\" After World War II, mainstream sociology shifted to the survey-research of Paul Lazarsfeld at Columbia University and the general theorizing of Pitirim Sorokin, followed by Talcott Parsons at Harvard University. Ultimately, \"the failure of the Chicago, Columbia, and Wisconsin [sociology] departments to produce a significant number of graduate students interested in and committed to general theory in the years 1936–45 was to the advantage of the Harvard department.\" As Parsons began to dominate general theory, his work primarily referenced European sociology—almost entirely omitting citations of both the American tradition of sociocultural-evolution as well as pragmatism. In addition to Parsons' revision of the sociological canon (which included Marshall, Pareto, Weber and Durkheim), the lack of theoretical challenges from other departments nurtured the rise of the Parsonian structural-functionalist movement, which reached its crescendo in the 1950s, but by the 1960s was in rapid decline.\n\nBy the 1980s, most functionalisms in Europe had broadly been replaced by conflict-oriented approaches and to many in the discipline, functionalism was considered \"as dead as a dodo.\" \"According to Giddens, the orthodox consensus terminated in the late 1960s and 1970s as the middle ground shared by otherwise competing perspectives gave way and was replaced by a baffling variety of competing perspectives. This third 'generation' of social theory includes phenomenologically inspired approaches, critical theory, ethnomethodology, symbolic interactionism, structuralism, post-structuralism, and theories written in the tradition of hermeneutics and ordinary language philosophy.\"\n\nWhile some conflict approaches also gained popularity in the United States, the mainstream of the discipline instead shifted to a variety of empirically oriented middle-range theories with no single overarching, or \"grand\", theoretical orientation. John Levi Martin refers to this \"golden age of methodological unity and theoretical calm\" as the \"Pax Wisconsana\", as it reflected the composition of the sociology department at the University of Wisconsin–Madison: numerous scholars working on separate projects with little contention. Omar Lizardo describes the \"Pax Wisconsana\" as: \"a Midwestern flavored, Mertonian resolution of the theory/method wars in which [sociologists] all agreed on at least two working hypotheses: (1) grand theory is a waste of time; (2) [and] good theory has to be good to think with or goes in the trash bin.\" Despite the aversion to grand theory in the latter half of the 20th century, several new traditions have emerged that propose various syntheses: structuralism, post-structuralism, cultural sociology and systems theory.\n\nThe structuralist movement originated primarily from the work of Durkheim as interpreted by two European anthropologists. Anthony Giddens' theory of structuration draws on the linguistic theory of Ferdinand de Saussure and the French anthropologist Claude Lévi-Strauss. In this context, 'structure' refers not to 'social structure' but to the semiotic understanding of human culture as a system of signs. One may delineate four central tenets of structuralism: First, structure is what determines the structure of a whole. Second, structuralists believe that every system has a structure. Third, structuralists are interested in 'structural' laws that deal with coexistence rather than changes. Finally, structures are the 'real things' beneath the surface or the appearance of meaning.\n\nThe second tradition of structuralist thought, contemporaneous with Giddens, emerges from the American school of social network analysis, spearheaded by the Harvard Department of Social Relations led by Harrison White and his students in the 1970s and 1980s. This tradition of structuralist thought argues that, rather than semiotics, social structure is networks of patterned social relations. And, rather than Levi-Strauss, this school of thought draws on the notions of structure as theorized by Levi-Strauss' contemporary anthropologist, Radcliffe-Brown. Some refer to this as \"network structuralism,\" and equate it to \"British structuralism\" as opposed to the \"French structuralism\" of Levi-Strauss.\n\nPost-structuralist thought has tended to reject 'humanist' assumptions in the construction of social theory. Michel Foucault provides an important critique in his \"Archaeology of the Human Sciences\", though Habermas and Rorty have both argued that Foucault merely replaces one such system of thought with another. The dialogue between these intellectuals highlights a trend in recent years for certain schools of sociology and philosophy to intersect. The anti-humanist position has been associated with \"postmodernism\", a term used in specific contexts to describe an \"era\" or \"phenomena\", but occasionally construed as a \"method\".\n\nOverall, there is a strong consensus regarding the central problems of sociological theory, which are largely inherited from the classical theoretical traditions. This consensus is: how to link, transcend or cope with the following \"big three\" dichotomies: subjectivity and objectivity, structure and agency, and synchrony and diachrony. The first deals with \"knowledge\", the second with \"action\", and the last with \"time\". Lastly, sociological theory often grapples with the problem of integrating or transcending the divide between micro, meso and macro-scale social phenomena, which is a subset of all three central problems.\n\nThe problem of subjectivity and objectivity can be divided into two parts: a concern over the general possibilities of social actions, and the specific problem of social scientific knowledge. In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is \"intersubjectivity\" achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Also, some qualitative methods take a radical approach to objective description in situ.\n\nThe latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain. Bourdieu puts this problem rather succinctly:\n\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context 'agency' refers to the capacity of individuals to act independently and make free choices, whereas 'structure' relates to factors that limit or affect the choices and actions of individuals (such as social class, religion, gender, ethnicity, and so on). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (\"What is the social world made of?\", \"What is a cause in the social world, and what is an effect?\"). A perennial question within this debate is that of \"social reproduction\": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals?\n\nSynchrony and diachrony, or statics and dynamics, within social theory are terms that refer to a distinction emerging out of the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. The former slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a \"language\", while diachrony would refer to unfolding processes like actual \"speech\". In Anthony Giddens' introduction to \"Central Problems in Social Theory\", he states that, \"in order to show the interdependence of action and structure ... we must grasp the time space relations inherent in the constitution of all social interaction.\" And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data.\n\nMany people divide sociological research methods into two broad categories, although many others see research methods as a continuum:\n\nSociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics.\" Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists.\n\nThe choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative \"and\" qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency.\n\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population. In quantitative research, statistics are used to draw inferences from this sample regarding the population as a whole. The process of selecting a sample is referred to as 'sampling'. While it is usually best to sample randomly, concern with differences between specific subpopulations sometimes calls for stratified sampling. Conversely, the impossibility of random sampling sometimes necessitates nonprobability sampling, such as convenience sampling or snowball sampling.\n\n\"The following list of research methods is neither exclusive nor exhaustive:\"\n\nSociologists increasingly draw upon computationally intensive methods to analyse and model social phenomena. Using computer simulations, artificial intelligence, text mining, complex statistical methods, and new analytic approaches like social network analysis and social sequence analysis, computational sociology develops and tests theories of complex social processes through bottom-up modelling of social interactions.\n\nAlthough the subject matter and methodologies in social science differ from those in natural science or computer science, several of the approaches used in contemporary social simulation originated from fields such as physics and artificial intelligence. By the same token, some of the approaches that originated in computational sociology have been imported into the natural sciences, such as measures of network centrality from the fields of social network analysis and network science. In relevant literature, computational sociology is often related to the study of social complexity. Social complexity concepts such as complex systems, non-linear interconnection among macro and micro process, and emergence, have entered the vocabulary of computational sociology. A practical and well-known example is the construction of a computational model in the form of an \"artificial society\", by which researchers can analyse the structure of a social system.\n\nSociologists' approach to culture can be divided into \"sociology of culture\" and \"cultural sociology\"—the terms are similar, though not entirely interchangeable. Sociology of culture is an older term, and considers some topics and objects as more or less \"cultural\" than others. Conversely, cultural sociology sees all social phenomena as inherently cultural. Sociology of culture often attempts to explain certain cultural phenomena as a product of social processes, while cultural sociology sees culture as a potential explanation of social phenomena.\n\nFor Simmel, culture referred to \"the cultivation of individuals through the agency of external forms which have been objectified in the course of history\". While early theorists such as Durkheim and Mauss were influential in cultural anthropology, sociologists of culture are generally distinguished by their concern for modern (rather than primitive or ancient) society. Cultural sociology often involves the hermeneutic analysis of words, artefacts and symbols, or ethnographic interviews. However, some sociologists employ historical-comparative or quantitative techniques in the analysis of culture, Weber and Bourdieu for instance. The subfield is sometimes allied with critical theory in the vein of Theodor W. Adorno, Walter Benjamin, and other members of the Frankfurt School. Loosely distinct from the sociology of culture is the field of cultural studies. Birmingham School theorists such as Richard Hoggart and Stuart Hall questioned the division between \"producers\" and \"consumers\" evident in earlier theory, emphasizing the reciprocity in the production of texts. Cultural Studies aims to examine its subject matter in terms of cultural practices and their relation to power. For example, a study of a subculture (such as white working class youth in London) would consider the social practices of the group as they relate to the dominant class. The \"cultural turn\" of the 1960s ultimately placed culture much higher on the sociological agenda.\n\nSociology of literature, film, and art is a subset of the sociology of culture. This field studies the social production of artistic objects and its social implications. A notable example is Pierre Bourdieu's 1992 \"Les Règles de L'Art: Genèse et Structure du Champ Littéraire\", translated by Susan Emanuel as \"Rules of Art: Genesis and Structure of the Literary Field\" (1996). None of the founding fathers of sociology produced a detailed study of art, but they did develop ideas that were subsequently applied to literature by others. Marx's theory of ideology was directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Weber's theory of modernity as cultural rationalization, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Adorno and Jürgen Habermas. Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's own work is clearly indebted to Marx, Weber and Durkheim.\n\nCriminologists analyse the nature, causes, and control of criminal activity, drawing upon methods across sociology, psychology, and the behavioural sciences. The sociology of deviance focuses on actions or behaviours that violate norms, including both infringements of formally enacted rules (e.g., crime) and informal violations of cultural norms. It is the remit of sociologists to study why these norms exist; how they change over time; and how they are enforced. The concept of social disorganization is when the broader social systems leads to violations of norms. For instance, Robert K. Merton produced a typology of deviance, which includes both individual and system level causal explanations of deviance.\n\nThe study of law played a significant role in the formation of classical sociology. Durkheim famously described law as the \"visible symbol\" of social solidarity. The sociology of law refers to both a sub-discipline of sociology and an approach within the field of legal studies. Sociology of law is a diverse field of study that examines the interaction of law with other aspects of society, such as the development of legal institutions and the effect of laws on social change and vice versa. For example, an influential recent work in the field relies on statistical analyses to argue that the increase in incarceration in the US over the last 30 years is due to changes in law and policing and not to an increase in crime; and that this increase has significantly contributed to the persistence of racial stratification.\n\nThe sociology of communications and information technologies includes \"the social aspects of computing, the Internet, new media, computer networks, and other communication and information technologies\".\n\nThe Internet is of interest to sociologists in various ways; most practically as a tool for research and as a discussion platform. The sociology of the Internet in the broad sense concerns the analysis of online communities (e.g. newsgroups, social networking sites) and virtual worlds, meaning that there is often overlap with community sociology. Online communities may be studied statistically through network analysis or interpreted qualitatively through virtual ethnography. Moreover, organizational change is catalysed through new media, thereby influencing social change at-large, perhaps forming the framework for a transformation from an industrial to an informational society. One notable text is Manuel Castells' \"The Internet Galaxy\"—the title of which forms an inter-textual reference to Marshall McLuhan's \"The Gutenberg Galaxy\". Closely related to the sociology of the Internet is digital sociology, which expands the scope of study to address not only the internet but also the impact of the other digital media and devices that have emerged since the first decade of the twenty-first century.\n\nAs with cultural studies, media study is a distinct discipline that owes to the convergence of sociology and other social sciences and humanities, in particular, literary criticism and critical theory. Though neither the production process nor the critique of aesthetic forms is in the remit of sociologists, analyses of socializing factors, such as ideological effects and audience reception, stem from sociological theory and method. Thus the 'sociology of the media' is not a subdiscipline \"per se\", but the media is a common and often indispensable topic.\n\nThe term \"economic sociology\" was first used by William Stanley Jevons in 1879, later to be coined in the works of Durkheim, Weber and Simmel between 1890 and 1920. Economic sociology arose as a new approach to the analysis of economic phenomena, emphasizing class relations and modernity as a philosophical concept. The relationship between capitalism and modernity is a salient issue, perhaps best demonstrated in Weber's \"The Protestant Ethic and the Spirit of Capitalism\" (1905) and Simmel's \"The Philosophy of Money\" (1900). The contemporary period of economic sociology, also known as \"new economic sociology\", was consolidated by the 1985 work of Mark Granovetter titled \"Economic Action and Social Structure: The Problem of Embeddedness\". This work elaborated the concept of embeddedness, which states that economic relations between individuals or firms take place within existing social relations (and are thus structured by these relations as well as the greater social structures of which those relations are a part). Social network analysis has been the primary methodology for studying this phenomenon. Granovetter's theory of the strength of weak ties and Ronald Burt's concept of structural holes are two of the best known theoretical contributions of this field.\n\nThe sociology of work, or industrial sociology, examines \"the direction and implications of trends in technological change, globalization, labour markets, work organization, managerial practices and employment relations to the extent to which these trends are intimately related to changing patterns of inequality in modern societies and to the changing experiences of individuals and families the ways in which workers challenge, resist and make their own contributions to the patterning of work and shaping of work institutions.\"\n\nThe sociology of education is the study of how educational institutions determine social structures, experiences, and other outcomes. It is particularly concerned with the schooling systems of modern industrial societies. A classic 1966 study in this field by James Coleman, known as the \"Coleman Report\", analysed the performance of over 150,000 students and found that student background and socioeconomic status are much more important in determining educational outcomes than are measured differences in school resources (\"i.e.\" per pupil spending). The controversy over \"school effects\" ignited by that study has continued to this day. The study also found that socially disadvantaged black students profited from schooling in racially mixed classrooms, and thus served as a catalyst for desegregation busing in American public schools.\n\nEnvironmental sociology is the study of human interactions with the natural environment, typically emphasizing human dimensions of environmental problems, social impacts of those problems, and efforts to resolve them. As with other sub-fields of sociology, scholarship in environmental sociology may be at one or multiple levels of analysis, from global (e.g. world-systems) to local, societal to individual. Attention is paid also to the processes by which environmental problems become \"defined\" and \"known\" to humans. As argued by notable environmental sociologist John Bellamy Foster, the predecessor to modern environmental sociology is Marx's analysis of the metabolic rift, which influenced contemporary thought on sustainability. Environmental sociology is often interdisciplinary and overlaps with the sociology of risk, rural sociology and the sociology of disaster.\n\nHuman ecology deals with interdisciplinary study of the relationship between humans and their natural, social, and built environments. In addition to Environmental sociology, this field overlaps with architectural sociology, urban sociology, and to some extent visual sociology. In turn, visual sociology—which is concerned with all visual dimensions of social life—overlaps with media studies in that it uses photography, film and other technologies of media.\n\nSocial pre-wiring deals with the study of fetal social behavior and social interactions in a multi-fetal environment. Specifically, social pre-wiring refers to the ontogeny of social interaction. Also informally referred to as, \"wired to be social.\" The theory questions whether there is a propensity to socially oriented action already present \"before\" birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social.\n\nCircumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be contributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics.\n\nPrincipal evidence of this theory is uncovered by examining Twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed.\n\nThe social pre-wiring hypothesis was proved correct, \"The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions.\".\n\nFamily, gender and sexuality form a broad area of inquiry studied in many sub-fields of sociology. A family is a group of people who are related by kinship ties :- Relations of blood / marriage / civil partnership or adoption. The family unit is one of the most important social institutions found in some form in nearly all known societies. It is the basic unit of social organization and plays a key role in socializing children into the culture of their society. The sociology of the family examines the family, as an institution and unit of socialization, with special concern for the comparatively modern historical emergence of the nuclear family and its distinct gender roles. The notion of \"childhood\" is also significant. As one of the more basic institutions to which one may apply sociological perspectives, the sociology of the family is a common component on introductory academic curricula. Feminist sociology, on the other hand, is a normative sub-field that observes and critiques the cultural categories of gender and sexuality, particularly with respect to power and inequality. The primary concern of feminist theory is the patriarchy and the systematic oppression of women apparent in many societies, both at the level of small-scale interaction and in terms of the broader social structure. Feminist sociology also analyses how gender interlocks with race and class to produce and perpetuate social inequalities. \"How to account for the differences in definitions of femininity and masculinity and in sex role across different societies and historical periods\" is also a concern. Social psychology of gender, on the other hand, uses experimental methods to uncover the microprocesses of gender stratification. For example, one recent study has shown that resume evaluators penalize women for motherhood while giving a boost to men for fatherhood.\n\nThe sociology of health and illness focuses on the social effects of, and public attitudes toward, illnesses, diseases, mental health and disabilities. This sub-field also overlaps with gerontology and the study of the ageing process. Medical sociology, by contrast, focuses on the inner-workings of medical organizations and clinical institutions. In Britain, sociology was introduced into the medical curriculum following the Goodenough Report (1944).\n\nThe sociology of the body and embodiment takes a broad perspective on the idea of \"the body\" and includes \"a wide range of embodied dynamics including human and non-human bodies, morphology, human reproduction, anatomy, body fluids, biotechnology, genetics. This often intersects with health and illness, but also theories of bodies as political, social, cultural, economic and ideological productions. The ISA maintains a Research Committee devoted to \"The Body in the Social Sciences\".\n\nA subfield of the sociology of health and illness that overlaps with cultural sociology is the study of death, dying and bereavement, sometimes referred to broadly as the sociology of death. This topic is exemplifed by the work of Douglas Davies and Michael C. Kearl.\n\nThe sociology of knowledge is the study of the relationship between human thought and the social context within which it arises, and of the effects prevailing ideas have on societies. The term first came into widespread use in the 1920s, when a number of German-speaking theorists, most notably Max Scheler, and Karl Mannheim, wrote extensively on it. With the dominance of functionalism through the middle years of the 20th century, the sociology of knowledge tended to remain on the periphery of mainstream sociological thought. It was largely reinvented and applied much more closely to everyday life in the 1960s, particularly by Peter L. Berger and Thomas Luckmann in \"The Social Construction of Reality\" (1966) and is still central for methods dealing with qualitative understanding of human society (compare \"socially constructed reality\"). The \"archaeological\" and \"genealogical\" studies of Michel Foucault are of considerable contemporary influence.\n\nThe sociology of science involves the study of science as a social activity, especially dealing \"with the social conditions and effects of science, and with the social structures and processes of scientific activity.\" Important theorists in the sociology of science include Robert K. Merton and Bruno Latour. These branches of sociology have contributed to the formation of science and technology studies. Both the ASA and the BSA have sections devoted to the subfield of Science, Knowledge and Technology. The ISA maintains a Research Committee on Science and Technology\n\nSociology of leisure is the study of how humans organize their free time. Leisure includes a broad array of activities, such as sport, tourism, and the playing of games. The sociology of leisure is closely tied to the sociology of work, as each explores a different side of the work–leisure relationship. More recent studies in the field move away from the work–leisure relationship and focus on the relation between leisure and culture. This area of sociology began with Thorstein Veblen's \"Theory of the Leisure Class\".\n\nThis subfield of sociology studies, broadly, the dynamics of war, conflict resolution, peace movements, war refugees, conflict resolution and military institutions. As a subset of this subfield, military sociology aims towards the systematic study of the military as a social group rather than as an organization. It is a highly specialized sub-field which examines issues related to service personnel as a distinct group with coerced collective action based on shared interests linked to survival in vocation and combat, with purposes and values that are more defined and narrow than within civil society. Military sociology also concerns civilian-military relations and interactions between other groups or governmental agencies. Topics include the dominant assumptions held by those in the military, changes in military members' willingness to fight, military unionization, military professionalism, the increased utilization of women, the military industrial-academic complex, the military's dependence on research, and the institutional and organizational structure of military.\n\nHistorically, political sociology concerned the relations between political organization and society. A typical research question in this area might be: \"Why do so few American citizens choose to vote?\" In this respect questions of political opinion formation brought about some of the pioneering uses of statistical survey research by Paul Lazarsfeld. A major subfield of political sociology developed in relation to such questions, which draws on comparative history to analyse socio-political trends. The field developed from the work of Max Weber and Moisey Ostrogorsky.\n\nContemporary political sociology includes these areas of research, but it has been opened up to wider questions of power and politics. Today political sociologists are as likely to be concerned with how identities are formed that contribute to structural domination by one group over another; the politics of who knows how and with what authority; and questions of how power is contested in social interactions in such a way as to bring about widespread cultural and social change. Such questions are more likely to be studied qualitatively. The study of social movements and their effects has been especially important in relation to these wider definitions of politics and power.\n\nPolitical sociology has also moved beyond methodological nationalism and analysed the role of non-governmental organizations, the diffusion of the nation-state throughout the Earth as a social construct, and the role of stateless entities in the modern world society. Contemporary political sociologists also study inter-state interactions and human rights.\n\nDemographers or sociologists of population study the size, composition and change over time of a given population. Demographers study how these characteristics impact, or are impacted by, various social, economic or political systems. The study of population is also closely related to human ecology and environmental sociology, which studies a populations relationship with the surrounding environment and often overlaps with urban or rural sociology. Researchers in this field may study the movement of populations: transportation, migrations, diaspora, etc., which falls into the subfield known as Mobilities studies and is closely related to human geography. Demographers may also study spread of disease within a given population or epidemiology.\n\nPublic sociology refers to an approach to the discipline which seeks to transcend the academy in order to engage with wider audiences. It is perhaps best understood as a style of sociology rather than a particular method, theory, or set of political values. This approach is primarily associated with Michael Burawoy who contrasted it with professional sociology, a form of academic sociology that is concerned primarily with addressing other professional sociologists. Public sociology is also part of the broader field of science communication or science journalism.\n\nThe sociology of race and of ethnic relations is the area of the discipline that studies the social, political, and economic relations between races and ethnicities at all levels of society. This area encompasses the study of racism, residential segregation, and other complex social processes between different racial and ethnic groups. This research frequently interacts with other areas of sociology such as stratification and social psychology, as well as with postcolonial theory. At the level of political policy, ethnic relations are discussed in terms of either assimilationism or multiculturalism. Anti-racism forms another style of policy, particularly popular in the 1960s and 1970s.\n\nThe sociology of religion concerns the practices, historical backgrounds, developments, universal themes and roles of religion in society. There is particular emphasis on the recurring role of religion in all societies and throughout recorded history. The sociology of religion is distinguished from the philosophy of religion in that sociologists do not set out to assess the validity of religious truth-claims, instead assuming what Peter L. Berger has described as a position of \"methodological atheism\". It may be said that the modern formal discipline of sociology \"began\" with the analysis of religion in Durkheim's 1897 study of suicide rates among Roman Catholic and Protestant populations. Max Weber published four major texts on religion in a context of economic sociology and social stratification: \"The Protestant Ethic and the Spirit of Capitalism\" (1905), \"\" (1915), \"\" (1915), and \"Ancient Judaism\" (1920). Contemporary debates often centre on topics such as secularization, civil religion, the intersection of religion and economics and the role of religion in a context of globalization and multiculturalism.\n\nThe sociology of change and development attempts to understand how societies develop and how they can be changed. This includes studying many different aspects of society, for example demographic trends, political or technological trends, or changes in culture. Within this field, sociologists often use macrosociological methods or historical-comparative methods. In contemporary studies of social change, there are overlaps with international development or community development. However, most of the founders of sociology had theories of social change based on their study of history. For instance, Marx contended that the material circumstances of society ultimately caused the ideal or cultural aspects of society, while Weber argued that it was in fact the cultural mores of Protestantism that ushered in a transformation of material circumstances. In contrast to both, Durkheim argued that societies moved from simple to complex through a process of sociocultural evolution. Sociologists in this field also study processes of globalization and imperialism. Most notably, Immanuel Wallerstein extends Marx's theoretical frame to include large spans of time and the entire globe in what is known as world systems theory. Development sociology is also heavily influenced by post-colonialism. In recent years, Raewyn Connell issued a critique of the bias in sociological research towards countries in the Global North. She argues that this bias blinds sociologists to the lived experiences of the Global South, specifically, so-called, \"Northern Theory\" lacks an adequate theory of imperialism and colonialism.\n\nThere are many organizations studying social change, including the Fernand Braudel Center for the Study of Economies, Historical Systems, and Civilizations, and the Global Social Change Research Project.\n\nA social network is a social structure composed of individuals (or organizations) called \"nodes\", which are tied (connected) by one or more specific types of interdependency, such as friendship, kinship, financial exchange, dislike, sexual relationships, or relationships of beliefs, knowledge or prestige. Social networks operate on many levels, from families up to the level of nations, and play a critical role in determining the way problems are solved, organizations are run, and the degree to which individuals succeed in achieving their goals. An underlying theoretical assumption of social network analysis is that groups are not necessarily the building blocks of society: the approach is open to studying less-bounded social systems, from non-local communities to networks of exchange. Drawing theoretically from relational sociology, social network analysis avoids treating individuals (persons, organizations, states) as discrete units of analysis, it focuses instead on how the structure of ties affects and constitutes individuals and their relationships. In contrast to analyses that assume that socialization into norms determines behaviour, network analysis looks to see the extent to which the structure and composition of ties affect norms. On the other hand, recent research by Omar Lizardo also demonstrates that network ties are shaped and created by previously existing cultural tastes. Social network theory is usually defined in formal mathematics and may include integration of geographical data into Sociomapping.\n\nSociological social psychology focuses on micro-scale social actions. This area may be described as adhering to \"sociological miniaturism\", examining whole societies through the study of individual thoughts and emotions as well as behaviour of small groups. Of special concern to psychological sociologists is how to explain a variety of demographic, social, and cultural facts in terms of human social interaction. Some of the major topics in this field are social inequality, group dynamics, prejudice, aggression, social perception, group behaviour, social change, non-verbal behaviour, socialization, conformity, leadership, and social identity. Social psychology may be taught with psychological emphasis. In sociology, researchers in this field are the most prominent users of the experimental method (however, unlike their psychological counterparts, they also frequently employ other methodologies). Social psychology looks at social influences, as well as social perception and social interaction.\n\nSocial stratification is the hierarchical arrangement of individuals into social classes, castes, and divisions within a society. Modern Western societies stratification traditionally relates to cultural and economic classes arranged in three main layers: upper class, middle class, and lower class, but each class may be further subdivided into smaller classes (e.g. occupational). Social stratification is interpreted in radically different ways within sociology. Proponents of structural functionalism suggest that, since the stratification of classes and castes is evident in all societies, hierarchy must be beneficial in stabilizing their existence. Conflict theorists, by contrast, critique the inaccessibility of resources and lack of social mobility in stratified societies.\n\nKarl Marx distinguished social classes by their connection to the means of production in the capitalist system: the bourgeoisie own the means, but this effectively includes the proletariat itself as the workers can only sell their own labour power (forming the material base of the cultural superstructure). Max Weber critiqued Marxist economic determinism, arguing that social stratification is not based purely on economic inequalities, but on other status and power differentials (e.g. patriarchy). According to Weber, stratification may occur among at least three complex variables: (1) Property (class): A person's economic position in a society, based on birth and individual achievement. Weber differs from Marx in that he does not see this as the supreme factor in stratification. Weber noted how managers of corporations or industries control firms they do not own; Marx would have placed such a person in the proletariat. (2) Prestige (status): A person's prestige, or popularity in a society. This could be determined by the kind of job this person does or wealth. and (3) Power (political party): A person's ability to get their way despite the resistance of others. For example, individuals in state jobs, such as an employee of the Federal Bureau of Investigation, or a member of the United States Congress, may hold little property or status but they still hold immense power Pierre Bourdieu provides a modern example in the concepts of cultural and symbolic capital. Theorists such as Ralf Dahrendorf have noted the tendency towards an enlarged middle-class in modern Western societies, particularly in relation to the necessity of an educated work force in technological or service-based economies. Perspectives concerning globalization, such as dependency theory, suggest this effect owes to the shift of workers to the developing countries.\n\nUrban sociology involves the analysis of social life and human interaction in metropolitan areas. It is a discipline seeking to provide advice for planning and policy making. After the industrial revolution, works such as Georg Simmel's \"The Metropolis and Mental Life\" (1903) focused on urbanization and the effect it had on alienation and anonymity. In the 1920s and 1930s The Chicago School produced a major body of theory on the nature of the city, important to both urban sociology and criminology, utilizing symbolic interactionism as a method of field research. Contemporary research is commonly placed in a context of globalization, for instance, in Saskia Sassen's study of the \"Global city\". Rural sociology, by contrast, is the analysis of non-metropolitan areas. As agriculture and wilderness tend to be a more prominent social fact in rural regions, rural sociologists often overlap with environmental sociologists.\n\nOften grouped with urban and rural sociology is that of community sociology or the sociology of community. Taking various communities—including online communities—as the unit of analysis, community sociologists study the origin and effects of different associations of people. For instance, German sociologist Ferdinand Tönnies distinguished between two types of human association: \"Gemeinschaft\" (usually translated as \"community\") and \"Gesellschaft\" (\"society\" or \"association\"). In his 1887 work, \"Gemeinschaft und Gesellschaft\", Tönnies argued that \"Gemeinschaft\" is perceived to be a tighter and more cohesive social entity, due to the presence of a \"unity of will\". The 'development' or 'health' of a community is also a central concern of community sociologists also engage in development sociology, exemplified by the literature surrounding the concept of social capital.\n\nSociology overlaps with a variety of disciplines that study society, in particular anthropology, political science, economics, social work and social philosophy. Many comparatively new fields such as communication studies, cultural studies, demography and literary theory, draw upon methods that originated in sociology. The terms \"social science\" and \"social research\" have both gained a degree of autonomy since their origination in classical sociology. The distinct field of social anthropology or anthroposociology is the dominant constituent of anthropology throughout the United Kingdom and Commonwealth and much of Europe (France in particular), where it is distinguished from cultural anthropology. In the United States, social anthropology is commonly subsumed within cultural anthropology (or under the relatively new designation of sociocultural anthropology).\n\nSociology and applied sociology are connected to the professional and academic discipline of social work. Both disciplines study social interactions, community and the effect of various systems (i.e. family, school, community, laws, political sphere) on the individual. However, social work is generally more focused on practical strategies to alleviate social dysfunctions; sociology in general provides a thorough examination of the root causes of these problems. For example, a sociologist might study \"why\" a community is plagued with poverty. The applied sociologist would be more focused on practical strategies on \"what\" needs to be done to alleviate this burden. The social worker would be focused on \"action\"; implementing theses strategies \"directly\" or \"indirectly\" by means of mental health therapy, counselling, advocacy, community organization or community mobilization.\n\nSocial anthropology is the branch of anthropology that studies how contemporary living human beings behave in social groups. Practitioners of social anthropology, like sociologists, investigate various facets of social organization. Traditionally, social anthropologists analysed non-industrial and non-Western societies, whereas sociologists focused on industrialized societies in the Western world. In recent years, however, social anthropology has expanded its focus to modern Western societies, meaning that the two disciplines increasingly converge.\n\nSociocultural anthropology, which include linguistic anthropology, is concerned with the problem of difference and similarity within and between human populations. The discipline arose concomitantly with the expansion of European colonial empires, and its practices and theories have been questioned and reformulated along with processes of decolonization. Such issues have re-emerged as transnational processes have challenged the centrality of the nation-state to theorizations about culture and power. New challenges have emerged as public debates about multiculturalism, and the increasing use of the culture concept outside of the academy and among peoples studied by anthropology. These times are not \"business-as-usual\" in the academy, in anthropology, or in the world, if ever there were such times.\n\nIrving Louis Horowitz, in his \"The Decomposition of Sociology\" (1994), has argued that the discipline, while arriving from a \"distinguished lineage and tradition\", is in decline due to deeply ideological theory and a lack of relevance to policy making: \"The decomposition of sociology began when this great tradition became subject to ideological thinking, and an inferior tradition surfaced in the wake of totalitarian triumphs.\" Furthermore: \"A problem yet unmentioned is that sociology's malaise has left all the social sciences vulnerable to pure positivism—to an empiricism lacking any theoretical basis. Talented individuals who might, in an earlier time, have gone into sociology are seeking intellectual stimulation in business, law, the natural sciences, and even creative writing; this drains sociology of much needed potential.\" Horowitz cites the lack of a 'core discipline' as exacerbating the problem. Randall Collins, the Dorothy Swaine Thomas Professor in Sociology at the University of Pennsylvania and a member of the Advisory Editors Council of the Social Evolution & History journal, has voiced similar sentiments: \"we have lost all coherence as a discipline, we are breaking up into a conglomerate of specialities, each going on its own way and with none too high regard for each other.\"\n\nIn 2007, \"The Times Higher Education Guide\" published a list of 'The most cited authors of books in the Humanities' (including philosophy and psychology). Seven of the top ten are listed as sociologists: Michel Foucault (1), Pierre Bourdieu (2), Anthony Giddens (5), Erving Goffman (6), Jürgen Habermas (7), Max Weber (8), and Bruno Latour (10).\n\nThe most highly ranked general journals which publish original research in the field of sociology are the \"American Journal of Sociology\" and the \"American Sociological Review\". The \"Annual Review of Sociology\", which publishes original review essays, is also highly ranked. Many other generalist and specialized journals exist.\n\n\n"}
{"id": "34154970", "url": "https://en.wikipedia.org/wiki?curid=34154970", "title": "Criminology", "text": "Criminology\n\nCriminology (from Latin , \"accusation\" originally derived from the Ancient Greek verb \"krino\" \"κρίνω\", and Ancient Greek , \"-logy|-logia\", from \"logos\" meaning: \"word,\" \"reason,\" or \"plan\") is the scientific study of the nature, extent, management, causes, control, consequences, and prevention of criminal behaviour, both on individual and social levels. Criminology is an interdisciplinary field in both the behavioural and social sciences, which draws primarily upon the research of sociologists, psychologists, philosophers, psychiatrists, biologists, social anthropologists, as well as scholars of law.\n\nThe term \"criminology\" was coined in 1885 by Italian law professor Raffaele Garofalo as \".\" Later, French anthropologist Paul Topinard used the analogous French term \".\" Paul Topinard's major work appeared in 1879. In the eighteenth and early nineteenth centuries, the emphasis of criminology was on the reform of criminal law and not on the causes of crime. Scholars such as Beccaria and Bentham, were more concerned with the humanitarian aspects in dealing with criminals and reforming several criminal laws. Great progress in criminology was made after the first quarter of the twentieth century. The first American textbook on criminology was written in 1920 by sociologist Maurice Parmalee under the title Criminology. Programmes were developed for the specific purpose of training students to be criminologists, but the development was rather slow.\n\nFrom 1900 through to 2000 the study underwent three significant phases in the United States: (1) Golden Age of Research (1900-1930)-which has been described as a multiple-factor approach, (2) Golden Age of Theory (1930-1960)-which shows that there was no systematic way of connecting criminological research to theory, and (3) a 1960-2000 period-which was seen as a significant turning point for criminology.\n\nCriminologists are the people working and researching all of the ins and outs of criminology. Criminologists often look for behavioral patterns of a possible criminal in hopes of finding a particular perpetrator. They also conduct research and investigations, developing theories, and composing results, and often more than not solve crimes.\n\nThe interests of criminologists include the study of nature of crime and criminals, origins of criminal law, etiology of crime, social reaction to crime, and the functioning of law enforcement agencies and the penal institutions. It can be broadly said that criminology directs its enquiries along three lines: first, it investigates the nature of criminal law and its administration and conditions under which it develops, second, it analyses the causation of crime and the personality of criminals; and third, it studies the control of crime and the rehabilitation of offenders. Thus, criminology includes within its scope the activities of legislative bodies, law-enforcement agencies, judicial institutions, correctional institutions and educational, private and public social agencies.\n\nIn the mid-18th century, criminology arose as social philosophers gave thought to crime and concepts of law. Over time, several schools of thought have developed. There were three main schools of thought in early criminological theory spanning the period from the mid-18th century to the mid-twentieth century: Classical, Positivist, and Chicago. These schools of thought were superseded by several contemporary paradigms of criminology, such as the sub-culture, control, strain, labeling, critical criminology, cultural criminology, postmodern criminology, feminist criminology and others discussed below.\n\nThe Classical school arose in the mid-18th century and has its basis in utilitarian philosophy. Cesare Beccaria, author of \"On Crimes and Punishments\" (1763–64), Jeremy Bentham (inventor of the \"panopticon\"), and other philosophers in this school argued:\nThis school developed during a major reform in penology when society began designing prisons for the sake of extreme punishment. This period also saw many legal reforms, the French Revolution, and the development of the legal system in the United States.\n\nThe Positivist school argues criminal behavior comes from internal and external factors out of the individual's control. Its key method of thought is that criminals are born as criminals not made into them; this school of thought also supports theory of nature in the debate between nature versus nurture. They also argue that criminal behavior is innate and within a person. Philosophers within this school applied the scientific method to study human behavior. Positivism comprises three segments: biological, psychological and social positivism.\n\nBiological Positivism is the belief that these criminals and their criminal behavior stem from \"chemical imbalances\" or \"abnormalities\" within the brain or the DNA due to basic internal \"defects\".\n\nPsychological Positivism is the concept that criminal acts or the people doing said crimes do them because of internal factors driving them. It differs from biological positivism in the thought that that school of thought says criminals are born criminals, whereas the psychological perspective recognizes the internal factors are results of external factor such as, but are not limited to, abusive parents, abusive relationships, drug problems, etc.\n\nSocial Positivism, which oftentimes referred to as Sociological Positivism, discusses the thought process that criminals are produced by society. This school claims that low income levels, high poverty/unemployment rates, and poor educational systems create and fuel criminals and crimes.\n\nThe notion of having a criminal personality is derived from the school of thought of psychological positivism. It essentially means that parts of a person's personality have traits that align with many traits possessed by criminals, such as neuroticism, anti-social tendencies, and aggressive behaviors. There is no evidence of causation between these personality traits and criminal actions, but there is a correlation.\n\nCesare Lombroso (1835–1909), an Italian sociologist working in the late 19th century, is often called \"the father of criminology.\" He was one of the key contributors to biological positivism and founded the Italian school of criminology. Lombroso took a scientific approach, insisting on empirical evidence for studying crime. He suggested physiological traits such as the measurements of cheekbones or hairline, or a cleft palate could indicate \"atavistic\" criminal tendencies. This approach, whose influence came via the theory of phrenology and by Charles Darwin's theory of evolution, has been superseded. Enrico Ferri, a student of Lombroso, believed social as well as biological factors played a role, and believed criminals should not be held responsible when factors causing their criminality were beyond their control. Criminologists have since rejected Lombroso's biological theories since control groups were not used in his studies.\n\nSociological positivism suggests societal factors such as poverty, membership of subcultures, or low levels of education can predispose people to crime. Adolphe Quetelet used data and statistical analysis to study the relationship between crime and sociological factors. He found age, gender, poverty, education, and alcohol consumption were important factors to crime. Lance Lochner performed three different research experiments, each one proving education reduces crime. Rawson W. Rawson used crime statistics to suggest a link between population density and crime rates, with crowded cities producing more crime. Joseph Fletcher and John Glyde read papers to the Statistical Society of London on their studies of crime and its distribution. Henry Mayhew used empirical methods and an ethnographic approach to address social questions and poverty, and gave his studies in \"London Labour and the London Poor\". Émile Durkheim viewed crime as an inevitable aspect of a society with uneven distribution of wealth and other differences among people.\n\nDifferential association (subcultural) posits that people learn crime through association. This theory was advocated by Edwin Sutherland, who focused on how \"a person becomes delinquent because of an excess of definitions favorable to violation of law over definitions unfavorable to violation of law.\" Associating with people who may condone criminal conduct, or justify crime under specific circumstances makes one more likely to take that view, under his theory. Interacting with this type of \"antisocial\" peer is a major cause of delinquency. Reinforcing criminal behavior makes it chronic. Where there are criminal subcultures, many individuals learn crime, and crime rates swell in those areas.\n\nThe Chicago school arose in the early twentieth century, through the work of Robert E. Park, Ernest Burgess, and other urban sociologists at the University of Chicago. In the 1920s, Park and Burgess identified five concentric zones that often exist as cities grow, including the \"zone of transition\", which was identified as the most volatile and subject to disorder. In the 1940s, Henry McKay and Clifford R. Shaw focused on juvenile delinquents, finding that they were concentrated in the zone of transition. The Chicago School was a school of thought developed that blames social structures for human behaviors. This thought can be associated or used within criminology, because it essentially takes the stance of defending criminals and criminal behaviors. The defense and argument lies in the thoughts that these people and their acts are not their faults but they are actually the result of society (i.e. unemployment, poverty, etc.), and these people are actually, in fact, behaving properly.\n\nChicago school sociologists adopted a social ecology approach to studying cities and postulated that urban neighborhoods with high levels of poverty often experience a breakdown in the social structure and institutions, such as family and schools. This results in social disorganization, which reduces the ability of these institutions to control behavior and creates an environment ripe for deviant behavior.\n\nOther researchers suggested an added social-psychological link. Edwin Sutherland suggested that people learn criminal behavior from older, more experienced criminals with whom they may associate.\n\nTheoretical perspectives used in criminology include psychoanalysis, functionalism, interactionism, Marxism, econometrics, systems theory, postmodernism, genetics, neuropsychology, evolutionary psychology, etc.\n\nThis theory is applied to a variety of approaches within the bases of criminology in particular and in sociology more generally as a conflict theory or structural conflict perspective in sociology and sociology of crime. As this perspective is itself broad enough, embracing as it does a diversity of positions.\n\nSocial disorganization theory is based on the work of Henry McKay and Clifford R. Shaw of the Chicago School. Social disorganization theory postulates that neighborhoods plagued with poverty and economic deprivation tend to experience high rates of population turnover. This theory suggests that crime and deviance is valued within groups in society, ‘subcultures’ or ‘gangs’. These groups have different values to the social norm. These neighborhoods also tend to have high population heterogeneity. With high turnover, informal social structure often fails to develop, which in turn makes it difficult to maintain social order in a community.\n\nSince the 1950s, social ecology studies have built on the social disorganization theories. Many studies have found that crime rates are associated with poverty, disorder, high numbers of abandoned buildings, and other signs of community deterioration. As working and middle-class people leave deteriorating neighborhoods, the most disadvantaged portions of the population may remain. William Julius Wilson suggested a poverty \"concentration effect\", which may cause neighborhoods to be isolated from the mainstream of society and become prone to violence.\n\nStrain theory, also known as Mertonian Anomie, advanced by American sociologist Robert Merton, suggests that mainstream culture, especially in the United States, is saturated with dreams of opportunity, freedom, and prosperity—as Merton put it, the \"American Dream\". Most people buy into this dream, and it becomes a powerful cultural and psychological motivator. Merton also used the term \"anomie\", but it meant something slightly different for him than it did for Durkheim. Merton saw the term as meaning a dichotomy between what society expected of its citizens and what those citizens could actually achieve. Therefore, if the social structure of opportunities is unequal and prevents the majority from realizing the dream, some of those dejected will turn to illegitimate means (crime) in order to realize it. Others will retreat or drop out into deviant subcultures (such as gang members, or what he calls \"hobos\"). Robert Agnew developed this theory further to include types of strain which were not derived from financial constraints. This is known as general strain theory\".\n\nThe Messner and Rosenfeld Institutional Theory stems from a pre-existing theory, one that was discovered by Merton, named Strain theory. Rosenberger took his findings from it and offered a different approach to the definition. It was based on the notion that everybody should already have \"the assumption that the 'American Dream' produces a society that is dominated by the economy and obsessed with the pursuit of success.\" The Messner and Rosenfeld Institutional Theory has many different views and definitions by many people, therefore, this becomes a very flexible theory.\n\nLarine Hughes found a correlation between economic pressure and how it links the \"American Dream\" and \"Individualism\" to a high crime. Hughes came to final decision that \"Therefore, individuals that do not have the drive to succeed and achieve this \"goal\" will fail, despite social and cultural pressures.\n\nSamantha Applin then took an approach relating genders and The Messner and Rosenfeld Institutional Theory. Stated in an article by S. Applin, \"males make up what is considered \"normal subjects\", and due to this, force woman to lie on a boundary that expectation is outside the dominant cultural frame.\" Therefore, setting women back in the beginning stages of Messner and Rosenfeld Institutional Theory.\n\nBrian Stults began to research studies on the violence between youth and the correlation that it has on Messner and Rosenfeld Institutional Anomie Theory. Stults stated, \"As well as the results showed graduating high school represents an important developmental stage in American society, as it ends the compulsory education, and is often followed by a significant decline in daily need and extra resources from your parents.\"\n\nFollowing the Chicago school and strain theory, and also drawing on Edwin Sutherland's idea of differential association, sub-cultural theorists focused on small cultural groups fragmenting away from the mainstream to form their own values and meanings about life.\n\nAlbert K. Cohen tied anomie theory with Sigmund Freud's reaction formation idea, suggesting that delinquency among lower-class youths is a reaction against the social norms of the middle class. Some youth, especially from poorer areas where opportunities are scarce, might adopt social norms specific to those places that may include \"toughness\" and disrespect for authority. Criminal acts may result when youths conform to norms of the deviant subculture.\n\nRichard Cloward and Lloyd Ohlin suggested that delinquency can result from a differential opportunity for lower class youth. Such youths may be tempted to take up criminal activities, choosing an illegitimate path that provides them more lucrative economic benefits than conventional, over legal options such as minimum wage-paying jobs available to them.\n\nDelinquency tends to occur among the lower-working-class males who have a lack of resources available to them and live in impoverished areas, as mentioned extensively by Albert Cohen (Cohen, 1965). Bias has been known to occur among law enforcement agencies, where officers tend to place a bias on minority groups, without knowing for sure if they had committed a crime or not. Delinquents may also commit crimes in order to secure funds for themselves or their loved ones, such as committing an armed robbery, as studied by many scholars (Briar & Piliavin).\n\nBritish sub-cultural theorists focused more heavily on the issue of class, where some criminal activities were seen as \"imaginary solutions\" to the problem of belonging to a subordinate class. A further study by the Chicago school looked at gangs and the influence of the interaction of gang leaders under the observation of adults.\n\nSociologists such as Raymond D. Gastil have explored the impact of a Southern culture of honor on violent crime rates.\n\nAnother approach is made by the social bond or social control theory. Instead of looking for factors that make people become criminal, these theories try to explain why people do \"not\" become criminal. Travis Hirschi identified four main characteristics: \"attachment to others\", \"belief in moral validity of rules\", \"commitment to achievement\", and \"involvement in conventional activities\". The more a person features those characteristics, the less likely he or she is to become deviant (or criminal). On the other hand, if these factors are not present, a person is more likely to become a criminal. Hirschi expanded on this theory with the idea that a person with low self-control is more likely to become criminal. As opposed to most criminology theories, these do not look at why people commit crime but rather why they do not commit crime.\n\nA simple example: Someone wants a big yacht but does not have the means to buy one. If the person cannot exert self-control, he or she might try to get the yacht (or the means for it) in an illegal way, whereas someone with high self-control will (more likely) either wait, deny themselves of what want or seek an intelligent intermediate solution, such as joining a yacht club to use a yacht by group consolidation of resources without violating social norms.\n\nSocial bonds, through peers, parents, and others can have a countering effect on one's low self-control. For families of low socio-economic status, a factor that distinguishes families with delinquent children, from those who are not delinquent, is the control exerted by parents or chaperonage. In addition, theorists such as David Matza and Gresham Sykes argued that criminals are able to temporarily neutralize internal moral and social-behavioral constraints through techniques of neutralization.\n\nsee main article Psychoanalytic criminology\n\nPsychoanalysis is a psychological theory (and therapy) which regards the unconscious mind, repressed memories and trauma, as the key drivers of behavior, especially deviant behavior. Sigmund Freud talks about how the unconscious desire for pain relates to psychoanalysis in his novel, \"Beyond the Pleasure Principle,\". Freud suggested that unconscious impulses such as ‘repetition compulsion’ and a ‘death drive’ can dominate a person's creativity, leading to self-destructive behavior. Phillida Rosnick, in the article \"Mental Pain and Social Trauma,\" posits a difference in the thoughts of individuals suffering traumatic unconscious pain which corresponds to them having thoughts and feelings which are not reflections of their true selves. There is enough correlation between this altered state of mind and criminality to suggest causation. Sander Gilman, in the article \"Freud and the Making of Psychoanalysis\", looks for evidence in the physical mechanisms of the human brain and the nervous system and suggests there is a direct link between an unconscious desire for pain or punishment and the impulse to commit crime or deviant acts.\n\nSymbolic interactionism draws on the phenomenology of Edmund Husserl and George Herbert Mead, as well as subcultural theory and conflict theory. This school of thought focused on the relationship between state, media, and conservative-ruling elite and other less powerful groups. The powerful groups had the ability to become the \"significant other\" in the less powerful groups' processes of generating meaning. The former could to some extent impose their meanings on the latter; therefore they were able to \"label\" minor delinquent youngsters as criminal. These youngsters would often take the label on board, indulge in crime more readily, and become actors in the \"self-fulfilling prophecy\" of the powerful groups. Later developments in this set of theories were by Howard Becker and Edwin Lemert, in the mid-20th century. Stanley Cohen developed the concept of \"moral panic\" describing the societal reaction to spectacular, alarming social phenomena (e.g. post-World War 2 youth cultures like the Mods and Rockers in the UK in 1964, AIDS epidemic and football hooliganism).\n\nLabeling theory refers to an individual who is labeled in a particular way and was studied in great detail by Becker. It arrives originally from sociology but is regularly used in criminological studies. It is said that when someone is given the label of a criminal they may reject or accept it and continue to commit crime. Even those who initially reject the label can eventually accept it as the label becomes more well known, particularly among their peers. This stigma can become even more profound when the labels are about deviancy, and it is thought that this stigmatization can lead to deviancy amplification. Malcolm Klein conducted a test which showed that labeling theory affected some youth offenders but not others.\n\nAt the other side of the spectrum, criminologist Lonnie Athens developed a theory about how a process of brutalization by parents or peers that usually occurs in childhood results in violent crimes in adulthood. Richard Rhodes' \"Why They Kill\" describes Athens' observations about domestic and societal violence in the criminals' backgrounds. Both Athens and Rhodes reject the genetic inheritance theories.\n\nRational choice theory is based on the utilitarian, classical school philosophies of Cesare Beccaria, which were popularized by Jeremy Bentham. They argued that punishment, if certain, swift, and proportionate to the crime, was a deterrent for crime, with risks outweighing possible benefits to the offender. In \"Dei delitti e delle pene\" (On Crimes and Punishments, 1763–1764), Beccaria advocated a rational penology. Beccaria conceived of punishment as the necessary application of the law for a crime; thus, the judge was simply to confirm his or her sentence to the law. Beccaria also distinguished between crime and sin, and advocated against the death penalty, as well as torture and inhumane treatments, as he did not consider them as rational deterrents.\n\nThis philosophy was replaced by the positivist and Chicago schools and was not revived until the 1970s with the writings of James Q. Wilson, Gary Becker's 1965 article \"Crime and Punishment\" and George Stigler's 1970 article \"The Optimum Enforcement of Laws\". Rational choice theory argues that criminals, like other people, weigh costs or risks and benefits when deciding whether to commit crime and think in economic terms. They will also try to minimize risks of crime by considering the time, place, and other situational factors.\n\nBecker, for example, acknowledged that many people operate under a high moral and ethical constraint but considered that criminals rationally see that the benefits of their crime outweigh the cost, such as the probability of apprehension and conviction, severity of punishment, as well as their current set of opportunities. From the public policy perspective, since the cost of increasing the fine is marginal to that of the cost of increasing surveillance, one can conclude that the best policy is to maximize the fine and minimize surveillance.\n\nWith this perspective, crime prevention or reduction measures can be devised to increase the effort required to commit the crime, such as target hardening. Rational choice theories also suggest that increasing risk and likelihood of being caught, through added surveillance, law enforcement presence, added street lighting, and other measures, are effective in reducing crime.\n\nOne of the main differences between this theory and Bentham's rational choice theory, which had been abandoned in criminology, is that if Bentham considered it possible to completely annihilate crime (through the panopticon), Becker's theory acknowledged that a society could not eradicate crime beneath a certain level. For example, if 25% of a supermarket's products were stolen, it would be very easy to reduce this rate to 15%, quite easy to reduce it until 5%, difficult to reduce it under 3% and nearly impossible to reduce it to zero (a feat which the measures required would cost the supermarket so much that it would outweigh the benefits). This reveals that the goals of utilitarianism and classical liberalism have to be tempered and reduced to more modest proposals to be practically applicable.\n\nSuch rational choice theories, linked to neoliberalism, have been at the basics of crime prevention through environmental design and underpin the Market Reduction Approach to theft by Mike Sutton, which is a systematic toolkit for those seeking to focus attention on \"crime facilitators\" by tackling the markets for stolen goods that provide motivation for thieves to supply them by theft.\n\nRoutine activity theory, developed by Marcus Felson and Lawrence Cohen, draws upon control theories and explains crime in terms of crime opportunities that occur in everyday life. A crime opportunity requires that elements converge in time and place including a motivated offender, suitable target or victim, and lack of a capable guardian. A guardian at a place, such as a street, could include security guards or even ordinary pedestrians who would witness the criminal act and possibly intervene or report it to law enforcement. Routine activity theory was expanded by John Eck, who added a fourth element of \"place manager\" such as rental property managers who can take nuisance abatement measures.\n\nBiosocial criminology is an interdisciplinary field that aims to explain crime and antisocial behavior by exploring both biological factors and environmental factors. While contemporary criminology has been dominated by sociological theories, biosocial criminology also recognizes the potential contributions of fields such as genetics, neuropsychology, and evolutionary psychology. See also: genetics of aggression.\n\nAggressive behavior has been associated with abnormalities in three principal regulatory systems in the body: serotonin systems, catecholamine systems, and the hypothalamic-pituitary-adrenocortical axis. Abnormalities in these systems also are known to be induced by stress, either severe, acute stress or chronic low-grade stress.\n\nIn 1968, young British sociologists formed the National Deviance Conference (NDC) group. The group was restricted to academics and consisted of 300 members. Ian Taylor, Paul Walton and Jock Young – members of the NDC – rejected previous explanations of crime and deviance. Thus, they decided to pursue a new Marxist criminological approach. In \"The New Criminology\", they argued against the biological \"positivism\" perspective represented by Lombroso, Hans Eysenck and Gordon Trasler.\n\nAccording to the Marxist perspective on crime, \"defiance is normal – the sense that men are now consciously involved [...] in assuring their human diversity.\" Thus Marxists criminologists argued in support of society in which the facts of human diversity, be it social or personal, would not be criminalized. They further attributed the processes of crime creation not to genetic or psychological facts, but rather to the material basis of a given society.\n\nConvict criminology is a school of thought in the realm of criminology. Convict criminologists have been directly affected by the criminal justice system, oftentimes having spent years inside the prison system. Researchers in the field of convict criminology such as John Irwin and Stephan Richards argue that traditional criminology can better be understood by those who lived in the walls of a prison. Martin Leyva argues that \"prisonization\" oftentimes begins before prison, in the home, community, and schools.\n\nAccording to Rod Earle, Convict Criminology started in the United States after the major expansion of prisons in the 1970s, and the U.S still remains the main focus for those who study convict criminology. \n\nQueer criminology is a field of study that focuses on LGBT individuals and their interactions with the criminal justice system. The goals of this field of study are as follows:\n\nLegitimacy of Queer criminology:\n\nThe value of pursuing criminology from a queer theorist perspective is contested; some believe that it is not worth researching and not relevant to the field as a whole, and as a result is a subject that lacks a wide berth of research available. On the other hand, it could be argued that this subject is highly valuable in highlighting how LGBT individuals are affected by the criminal justice system. This research also has the opportunity to \"queer\" the curriculum of criminology in educational institutions by shifting the focus from controlling and monitoring LGBT communities to liberating and protecting them.\n\nCultural criminology views crime and its control within the context of culture. Ferrell believes criminologists can examine the actions of criminals, control agents, media producers, and others to construct the meaning of crime. He discusses these actions as a means to show the dominant role of culture. Kane adds that cultural criminology has three tropes; village, city street, and media, in which males can be geographically influenced by society's views on what is broadcast and accepted as right or wrong. The village is where one engages in available social activities. Linking the history of an individual to a location can help determine social dynamics. The city street involves positioning oneself in the cultural area. This is full of those affected by poverty, poor health and crime, and large buildings that impact the city but not neighborhoods. Mass media gives an all-around account of the environment and the possible other subcultures that could exist beyond a specific geographical area.\n\nIt was later that Naegler and Salman introduced feminist theory to cultural criminology and discussed masculinity and femininity, sexual attraction and sexuality, and intersectional themes. Naegler and Salman believed that Ferrell's mold was limited and that they could add to the understanding of cultural criminology by studying women and those who do not fit Ferrell's mold. Hayward would later add that not only feminist theory, but green theory as well, played a role in the cultural criminology theory through the lens of adrenaline, the soft city, the transgressive subject, and the attentive gaze. The adrenaline lens deals with rational choice and what causes a person to have their own terms of availability, opportunity, and low levels of social control. The soft city lens deals with reality outside of the city and the imaginary sense of reality: the world where transgression occurs, where rigidity is slanted, and where rules are bent. The transgressive subject refers to a person who is attracted to rule-breaking and is attempting to be themselves in a world where everyone is against them. The attentive gaze is when someone, mainly an ethnographer, is immersed into the culture and interested in lifestyle(s) and the symbolic, aesthetic, and visual aspects. When examined, they are left with the knowledge that they are not all the same, but come to a settlement of living together in the same space. Through it all, sociological perspective on cultural criminology theory attempts to understand how the environment an individual is in determines their criminal behavior.\n\nRelative deprivation involves the process where an individual measures his or her own well-being and materialistic worth against that of other people and perceive that they are worse off in comparison. When humans fail to obtain what they believe they are owed, they can experience anger or jealousy over the notion that they have been wrongly disadvantaged.\n\nRelative deprivation was originally utilized in the field of sociology by Samuel A. Stouffer, who was a pioneer of this theory. Stouffer revealed that soldiers fighting in World War II measured their personal success by the experience in their units rather than by the standards set by the military. Relative deprivation can be made up of societal, political, economic, or personal factors which create a sense of injustice. It is not based on absolute poverty, a condition where one cannot meet a necessary level to maintain basic living standards. Rather, relative deprivation enforces the idea that even if a person is financially stable, he or she can still feel relatively deprived. The perception of being relatively deprived can result in criminal behavior and/or morally problematic decisions. Relative deprivation theory has increasingly been used to partially explain crime as rising living standards can result in rising crime levels. In criminology, the theory of relative deprivation explains that people who feel jealous and discontent of others might turn to crime to acquire the things that they can not afford.\n\nRural criminology is the study of crime trends outside of metropolitan and suburban areas. Rural criminologists have used social disorganization and routine activity theories. The FBI Uniform Crime Report shows that rural communities have significantly different crime trends as opposed to metropolitan and suburban areas. The crime in rural communities consists predominantly of narcotic related crimes such as the production, use, and trafficking of narcotics. Social disorganization theory is used to examine the trends involving narcotics. Social disorganization leads to narcotic use in rural areas because of low educational opportunities and high unemployment rates. Routine activity theory is used to examine all low level street crimes such as theft. Much of the crime in rural areas is explained through routine activity theory because there is often a lack of capable guardians in rural areas.\n\nPublic Criminology is a strand within criminology closely tied with \"public sociology,\" focused on disseminating criminological insights to a broader audience than academia. Advocates of public criminology argue that criminologists should be \"conducting and disseminating research on crime, law, and deviance in dialogue with affected communities.\" Its goal is for academics and researchers in criminology to provide their research to the public in order to inform public decisions and policymaking.\n\nBoth the positivist and classical schools take a consensus view of crime: that a crime is an act that violates the basic values and beliefs of society. Those values and beliefs are manifested as laws that society agrees upon. However, there are two types of laws:\n\nTherefore, definitions of crimes will vary from place to place, in accordance to the cultural norms and mores, but may be broadly classified as a blue-collar crime, corporate crime, organized crime, political crime, public order crime, state crime, state-corporate crime, and white-collar crime. However, there have been moves in contemporary criminological theory to move away from liberal pluralism, culturalism, and postmodernism by introducing the universal term \"harm\" into the criminological debate as a replacement for the legal term \"crime\".\n\nHomicide is the killing of another individual. Homicide can be broken up into many degrees and classifications. Murder is a specific classification of homicide. Specific legal definitions of homicide can vary from state to state, or country to country. However, there are still three basic categories of homicide: justifiable, excusable, and criminal.\n\nWithin the United States of America there are three general classifications and rankings of murder. The first is first-degree murder, which is committed with premeditation and deliberation, meaning that the offender thought about and planned. Second-degree murder refers to a murder that is not premeditated or deliberated. Felony murder refers to a murder that occurs in correspondence with another felony, such as a robbery.\n\nManslaughter refers to a murder where there was no deliberation, premeditation, or malice involved in the act. This category of homicide can be broken into two subsets. Voluntary manslaughter, sometimes referred to as a crime of passion, is murder based on immediate impulse in a moment of passion or emotion. Involuntary manslaughter refers to the murder of someone due to an offender's reckless behavior, such as drunk driving.\n\nHomicide classifications encompass all types of homicide.\n\n\nHomicide can either be expressive or instrumental\n\nExpressive homicide refers to murder that are unplanned and come from fits of rage or emotion. Instrumental homicide refers to murders that are used as a means to an end or are conducted to reach a goal. An example would be murder during a robbery.\n\nMulticides, or the murder of multiple people, can be broken down into three different classifications: mass murder, spree murder, and serial murder.\n\nMass murder is classified by the following characteristics:\n\n\nSpree murder is classified by the following characteristics:\n\n\nSpree murders are the least common of the three classifications of multicides.\n\nSerial murder is classified by the following characteristics:\n\n\nDomestic violence usually occurs between a pair or partner of people, in the form of either familial violence or Intimate Partner Violence (IPV). It can range across any type of violence, whether it's physical, financial, emotional, sexual, or verbal.\n\nRecording domestic violence is usually dependent on victim reporting, therefore, statistics and numbers are usually an underestimate of reality. Victims can report occurrences reporting systems, which show less accurate numbers, or surveying systems, which show more accurate numbers, although still not exact.\n\nUniform Crime Reports are dependent on victims reporting to the police. 50% of IPV altercations are reported to the police and there is no specific information recorded on the relationship between the victim and the offender.\n\nThis survey system interviews household members age 12 and above about domestic violence of all kinds.\n\nThis is a survey system through the Center for Disease Control. Members are 18 and older and are asked specific questions about their experiences and are not questioned as to who initiated altercations.\n\nStalking involves \"repeated visual or physical proximity, non-consensual communication, threats, or a combination thereof\" that are repeated on at least two occasions. There are seven specific tactics used by stalkers:\n\n\nRape and sexual assault are forced sexual action or coercion of sexual action onto a non-consenting or coerced person.\n\nAreas of study in criminology include:\n\n\n\n"}
{"id": "51512", "url": "https://en.wikipedia.org/wiki?curid=51512", "title": "Etiquette", "text": "Etiquette\n\nEtiquette ( and ; ) is the set of conventional rules of personal behaviour in polite society, usually in the form of an ethical code that delineates the expected and accepted social behaviors that accord with the conventions and norms observed by a society, a social class, or a social group. In modern English usage, the French word \"étiquette\" (ticket) dates from the year 1750.\n\nIn the 3rd millennium BC, the Ancient Egyptian vizier Ptahhotep wrote \"The Maxims of Ptahhotep\" (2375–2350 BC), a book of didactic precepts extolling civil virtues, such as truthfulness, self-control, and kindness towards other people. Recurrent thematic motifs in the maxims include learning by listening to other people, and that being mindful of the imperfection of human knowledge; thus, avoiding open conflict, wherever possible, should not be considered weakness.\n\nThat the pursuit of justice should be foremost, yet acknowledged that, in human affairs, the command of a god ultimately prevails in all matters. Some maxims indicate a person's correct behaviours in the presence of great personages (political, military, religious). Instructions on how to choose the right master and how to serve him. Other maxims teach the correct way to be a leader, through openness and kindness. That greed is the base of all evil, and should be guarded against, and that generosity towards family and friends is praiseworthy.\n\nConfucius (551–479 BC) was the Chinese intellectual and philosopher whose works emphasized personal and governmental morality, correctness of social relationships, the pursuit of justice in a personal dealings, and sincerity in all personal relations.\n\nBaldassare Castiglione (1478–1529), count of Casatico, was an Italian courtier and diplomat, soldier, and author of \"The Book of the Courtier\" (1528), an exemplar courtesy book dealing with questions of the etiquette and morality of the courtier during the Italian Renaissance.\n\nLouis XIV (1638–1718), King of France, used a codified etiquette to tame the French nobility and assert his supremacy as the absolute monarch of France. In consequence, the ceremonious royal court favourably impressed foreign dignitaries whom the king received at the seat of French government, the Palace of Versailles, to the south-west of Paris.\n\nIn the 18th century, during the Age of Enlightenment, the adoption of etiquette was a self-conscious process for acquiring the conventions of politeness and the normative behaviours (charm, manners, demeanour) which symbolically identified the person as a genteel member of the upper class. To identify with the social élite, the upwardly mobile middle class and the bourgeoisie adopted the behaviours and the artistic preferences of the upper class. To that end, socially ambitious people of the middle classes occupied themselves with learning, knowing, and practising the rules of social etiquette, such as the arts of elegant dress and gracious conversation, when to show emotion, and courtesy with and towards women.\n\nIn the early 18th century, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, wrote influential essays that defined \"politeness\" as the art of being pleasing in company; and discussed the function and nature of politeness in the social discourse of a commercial society:\n\nPeriodicals, such as \"The Spectator\", a daily publication founded in 1711 by Joseph Addison and Richard Steele, regularly advised their readers on the etiquette required of a gentleman, a man of good and courteous conduct; their stated editorial goal was \"to enliven morality with wit, and to temper wit with morality . . . to bring philosophy out of the closets and libraries, schools and colleges, to dwell in clubs and assemblies, at tea-tables and coffeehouses\"; to which end, the editors published articles written by educated authors, which provided topics for civil conversation, and advice on the requisite manners for carrying a polite conversation, and for managing social interactions.\n\nConceptually allied to etiquette is the notion of \"civility\" (social interaction characterised by sober and reasoned debate) which for socially ambitious men and women also became an important personal quality to possess for social advancement. In the event, gentlemen's clubs, such as Harrington's Rota Club, published an in-house etiquette that codified the civility expected of the members. Besides \"The Spectator\", other periodicals sought to infuse politeness into English coffeehouse conversation, the editors of \"The Tatler\" were explicit that their purpose was the reformation of English manners and morals; to those ends, etiquette was presented as the virtue of morality and a code of behaviour.\n\nIn the mid-18th century, the first, modern English usage of \"etiquette\" (the conventional rules of personal behaviour in polite society) was by Philip Stanhope, 4th Earl of Chesterfield, in the book \"Letters to His Son on the Art of Becoming a Man of the World and a Gentleman\" (1774), a correspondence of more than 400 letters written from 1737 until the death of his son, in 1768; most of the letters were instructive, concerning varied subjects that a worldly gentleman should know. The letters were first published in 1774, by Eugenia Stanhope, the widow of the diplomat Philip Stanhope, Chesterfield's bastard son. Throughout the correspondence, Chesterfield endeavoured to decouple the matter of social manners from conventional morality, with perceptive observations that pragmatically argue to Philip that mastery of etiquette was an important means for social advancement, for a man such as he. Chesterfield's elegant, literary style of writing epitomised the emotional restraint characteristic of polite social intercourse in 18th-century society:\n\nIn the 19th century, Victorian era (1837–1901) etiquette had developed into a complicated system of codified behaviours, which governed the range of manners in society — from the proper language, style, and method for writing letters, to correctly using cutlery at table, and to the minute regulation of social relations and personal interactions between men and women and among the social classes.\n\nIn a society, manners are described as either good manners or as bad manners to indicate to a person whether or not his or her behavior is acceptable to the cultural group. As such, manners enable \"ultrasociality\" and are integral to the functioning of the social norms and conventions that are informally enforced through personal self-regulation in public life and in private life. The perspectives of sociology indicate that manners are a means for men and women to display their social status, and a means of demarcating, observing, and maintaining the boundaries of social identity and of social class. \n\nIn \"The Civilizing Process\" (1939), the sociologist Norbert Elias said that manners arose as a product of group living, and persist as a way of maintaining social order. That manners proliferated during the Renaissance in response to the development of the ‘absolute state’ — the progression from small-group living to large-group living characterised by the centralized power of the State. That the rituals and manners associated with the royal court of England during that period were closely bound to a person's social status. That manners demonstrate a person's position within a social network, and that a person's manners are a means of negotiation from that social position.\n\nFrom the perspective of public health, in \"The Healthy Citizen\" (1996), Alana R. Petersen and Deborah Lupton said that manners assisted the diminishment of the social boundaries that existed between the public sphere and the private sphere of a person's life, and so gave rise to “a highly reflective self, a self who monitors his or her behavior with due regard for others with whom he or she interacts, socially”; and that “the public behavior of individuals came to signify their social standing; a means of presenting the self and of evaluating others, and thus the control of the outward self was vital.”\n\nMoreover, the sociologist Pierre Bourdieu applied the concept of \"habitus\" towards understanding the societal functions of manners. The \"habitus\" is the set of mental attitudes, personal habits, and skills that a person possesses, his and her \"dispositions\" of character that are neither self-determined, nor pre-determined by the external environment, but which are produced and reproduced by social interactions; and are “inculcated through experience and explicit teaching” — yet tend to function at the subconscious level. Therefore, manners are likely to be a central part of the \"dispositions\" that guide a person’s ability to decide upon socially-compliant behaviours.\n\nIn \"Purity and Danger: An Analysis of Concepts of Pollution and Taboo\" (2003) the anthropologist Mary Douglas said that the unique manners, social behaviors, and group rituals enable the local cosmology to remain ordered and free from those things that may pollute or defile the integrity of the culture. That ideas of pollution, defilement, and disgust are attached to the margins of socially acceptable behaviour in order to curtail unacceptable behaviour, and so maintain “the assumptions by which experience is controlled” within the culture.\n\nIn studying the expression of emotion by humans and animals, the naturalist Charles Darwin noted the universality of facial expressions of disgust and shame among infants and blind people, and concluded that the emotional responses of shame and disgust are innate bahaviours. The public health specialist Valerie Curtis said that the development of facial responses was concomitant with the development of manners, which are behaviours with an evolutionary role in preventing the transmission of diseases, thus, people who practise personal hygiene and politeness will most benefit from membership in their social group, and so stand the best chance of biological survival, by way of opportunities for reproduction.\n\nFrom the study of the evolutionary bases of prejudice, the social psychologists Catherine Cottrell and Steven Neuberg said that human behavioural responses to ‘otherness’ might enable the preservation of manners and social norms. That the feeling of \"foreignness\" — which people experience in their first social interaction with someone from another culture — might partly serve an evolutionary function: 'Group living surrounds one with individuals [who are] able to physically harm fellow group members, to spread contagious disease, or to \"free ride\" on their efforts'; therefore, a commitment to sociality is a risk: 'If threats, such as these, are left unchecked, the costs of sociality will quickly exceed its benefits. Thus, to maximize the returns on group \"living\", individual group members should be attuned to others’ features or behaviors.'\n\nTherefore, people who possess the social traits common to the cultural group are to be trusted, and people without the common social traits are to be distrusted as ‘others’, and thus treated with suspicion or excluded from the group. That pressure of social exclusivity, born from the shift towards communal living, excluded uncooperative people and persons with poor personal hygiene. The threat of social exclusion led people to avoid personal behaviours and that might embarrass the group or that might provoke revulsion among the group.\n\nTo demonstrate the transmission of social conformity, the anthropologists Joseph Henrich and Robert Boyd developed a behavioural model indicating that manners are a means of mitigating social differences, curbing undesirable personal behaviours, and fostering co-operation within the social group. That natural selection has favoured the acquisition of genetically-transmitted mechanisms for learning, thereby increasing a person's chances for acquiring locally adaptive behaviours: “Humans possess a reliably developing neural encoding that compels them both to punish individuals who violate group norms (common beliefs or practices) and [to] punish individuals who do not punish norm-violators.”\n\nSocial manners are in three categories: (i) manners of hygiene, (ii) manners of courtesy, and (iii) manners of cultural norm, each category accounts for an aspect of the functional role that manners play in a society. The categories of manners are based upon the social outcome of behaviour, rather than upon the personal motivation of the behaviour. As a means of social management, the rules of etiquette encompass most aspects of human social interaction; thus, a rule of etiquette reflects an underlying ethical code, and can reflect a person's fashion and social status.\n\n(i) Hygiene Manners — are the manners that concern avoiding the transmission of disease, and usually are taught by the parent to the child by way of parental discipline, positive behavioural enforcement of body-fluid continence (toilet training), and the avoidance of and removal of disease vectors that risk the health of children. To that effect, society expects that, by adulthood, the manners for personal hygiene have become a second-nature behaviour, the violations of which shall provoke physical and moral disgust.\n\n(ii) Courtesy Manners — are the manners of self-control and good-faith behaviour, by which a person gives priority to the interests of another person, and priority to the interests of a socio-cultural group, in order to be a trusted member of that group. Courtesy manners maximize the benefits of group-living, by regulating the nature of social interactions; however, the performance of courtesy manners occasionally interferes with the avoidance of communicable disease. Generally, parents teach courtesy manners in the same way they teach hygiene manners, but the child also learns manners directly (by observing the behaviour of other people in their social interactions) and by imagined social interactions (through the executive functions of the brain). A child usually learns courtesy manners at an older age than when he or she was toilet trained (taught hygiene manners), because learning the manners of courtesy requires that the child be self-aware and conscious of social position, which then facilitate understanding that violations (accidental or deliberate) of social courtesy will provoke peer disapproval within the social group.\n\n(iii) Cultural Norm Manners — are the manners of culture and society by which a person establishes his and her identity and membership in a given socio-cultural group. In observing and abiding the manners of cultural norm, a person demarcates socio-cultural identity and establishes social boundaries, which then identify whom to trust and whom to distrust as 'the other', who is not the self. Cultural norm manners are learnt through the enculturation with and the routinisation of ‘the familiar’, and through social exposure to the ‘cultural otherness’ of people identified as foreign to the group. Transgressions and flouting of the manners of cultural norm usually result in the social alienation of the transgressor. The nature of culture-norm manners allows a high level of between-group variability, but the manners usually are common to the people who identify with the given socio-cultural group.\n\n\"The Book of the Courtier\" (1528), by Baldassare Castiglione, identified the manners and the morals required by socially ambitious men and women for success in a royal court of the Italian Renaissance (14th–17th c.); as an etiquette text, \"The Courtier\" was an influential courtesy book in 16th-century Europe.\n\n\"On Good Manners for Boys\" (1530), by Erasmus of Rotterdam, instructs boys in the means of becoming a young man; how to walk and talk, speak and act in the company of adults. The practical advice for acquiring adult self-awareness includes explanations of the symbolic meanings — for adults — of a boy's body language when he is fidgeting and yawning, scratching and bickering. On completing Erasmus's curriculum of etiquette, the boy has learnt that civility is the point of good manners; the adult ability to 'readily ignore the faults of others, but avoid falling short, yourself,' in being civilised.\n\n\"Etiquette in Society, in Business, in Politics, and at Home\" (1922), by Emily Post documents the \"trivialities\" of desirable conduct in daily life, and provided pragmatic approaches to the practice good manners; the social conduct expected and appropriate for the events of life, such as a baptism, a wedding, and a funeral.\n\nAs didactic texts, books of etiquette (the conventional rules of personal behaviour in polite society) usually feature explanatory titles, such as \"The Ladies' Book of Etiquette, and Manual of Politeness: A Complete Hand Book for the Use of the Lady in Polite Society\" (1860), by Florence Hartley; \"Amy Vanderbilt’s Complete Book of Etiquette\" (1957), by Amy Vanderbilt; \"Miss Manners’ Guide to Excruciatingly Correct Behavior\" (1979), by Judith Martin; and \"Peas & Queues: The Minefield of Modern Manners\" (2013), by Sandi Toksvig, present ranges of civility; socially acceptable behaviours for their respective times; yet each author cautions the reader, that to be a well-mannered person he and she must practise good manners in their public and private lives. Moreover, the \"How Rude!\" comic-book series addresses and discusses adolescent perspectives and questions of etiquette, social manners, and civility.\n\nIn commerce, the purpose of etiquette is to facilitate the social relations necessary for realising the business transactions of buying and selling goods and services; in particular, the social interactions among the workers, and between labour and management. Business etiquette varies by culture, such as the Chinese and Australian approaches to conflict resolution. The Chinese business philosophy is based upon \"guanxi\" (personal connections), whereby person-to-person negotiation resolves difficult matters, whereas Australian business philosophy relies upon attorneys-at-law to resolve business conflicts through legal mediation; thus, adjusting to the etiquette and professional ethics of another culture is an element of culture shock for businesspeople.\n\nIn 2011, etiquette trainers formed the Institute of Image Training and Testing International (IITTI) a non-profit organisation to train personnel departments in measuring and developing and teaching social skills to employees, by way of education in the rules of personal and business etiquette, in order to produce business workers who possess standardised manners for successfully conducting business with people from other cultures. In the retail branch of commerce, the saying: \"The customer is always right.\" summarises the profit-orientation of good manners, between the buyer and the seller of goods and services:\n\nEtiquette and language\n\nEtiquette and letters\nEtiquette and society\n\nWorldwide etiquette\n\n\n"}
{"id": "24388", "url": "https://en.wikipedia.org/wiki?curid=24388", "title": "Political science", "text": "Political science\n\nPolitical science is a social science which deals with systems of governance, and the analysis of political activities, political thoughts, and political behavior.\n\nPolitical science—occasionally called politology—comprises numerous subfields, including comparative politics, political economy, international relations, political theory, public administration, public policy, and political methodology. Furthermore, political science is related to, and draws upon, the fields of economics, law, sociology, history, philosophy, geography, psychology/psychiatry, anthropology and neurosciences.\n\nComparative politics is the science of comparison and teaching of different types of constitutions, political actors, legislature and associated fields, all of them from an intrastate perspective. International relations deals with the interaction between nation-states as well as intergovernmental and transnational organizations. Political theory is more concerned with contributions of various classical and contemporary thinkers and philosophers.\n\nPolitical science is methodologically diverse and appropriates many methods originating in psychology, social research and cognitive neuroscience. Approaches include positivism, interpretivism, rational choice theory, behaviouralism, structuralism, post-structuralism, realism, institutionalism, and pluralism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents and official records, secondary sources such as scholarly journal articles, survey research, statistical analysis, case studies, experimental research, and model building.\n\nPolitical science is a social study concerning the allocation and transfer of power in decision making, the roles and systems of governance including governments and international organizations, political behaviour and public policies. They measure the success of governance and specific policies by examining many factors, including stability, justice, material wealth, peace and public health. Some political scientists seek to advance positive (attempt to describe how things are, as opposed to how they should be) theses by analysing politics. Others advance normative theses, by making specific policy recommendations.\n\nPolitical scientists provide the frameworks from which journalists, special interest groups, politicians, and the electorate analyse issues. According to Chaturvedy,\nIn the United States, political scientists known as \"Americanists\" look at a variety of data including constitutional development, elections, public opinion, and public policy such as Social Security reform, foreign policy, US Congressional committees, and the US Supreme Court — to name only a few issues.\n\nBecause political science is essentially a study of human behaviour, in all aspects of politics, observations in controlled environments are often challenging to reproduce or duplicate, though experimental methods are increasingly common (see experimental political science). Citing this difficulty, former American Political Science Association President Lawrence Lowell once said \"We are limited by the impossibility of experiment. Politics is an observational, not an experimental science.\" Because of this, political scientists have historically observed political elites, institutions, and individual or group behaviour in order to identify patterns, draw generalizations, and build theories of politics.\n\nLike all social sciences, political science faces the difficulty of observing human actors that can only be partially observed and who have the capacity for making conscious choices unlike other subjects such as non-human organisms in biology or inanimate objects as in physics. Despite the complexities, contemporary political science has progressed by adopting a variety of methods and theoretical approaches to understanding politics and methodological pluralism is a defining feature of contemporary political science.\n\nThe advent of political science as a university discipline was marked by the creation of university departments and chairs with the title of political science arising in the late 19th century. In fact, the designation \"political scientist\" is typically for those with a doctorate in the field, but can also apply to those with a master's in the subject. Integrating political studies of the past into a unified discipline is ongoing, and the history of political science has provided a rich field for the growth of both normative and positive political science, with each part of the discipline sharing some historical predecessors. The American Political Science Association and the \"American Political Science Review\" were founded in 1903 and 1906, respectively, in an effort to distinguish the study of politics from economics and other social phenomena.\n\nIn the 1950s and the 1960s, a behavioural revolution stressing the systematic and rigorously scientific study of individual and group behaviour swept the discipline. A focus on studying political behaviour, rather than institutions or interpretation of legal texts, characterized early behavioural political science, including work by Robert Dahl, Philip Converse, and in the collaboration between sociologist Paul Lazarsfeld and public opinion scholar Bernard Berelson.\n\nThe late 1960s and early 1970s witnessed a take off in the use of deductive, game theoretic formal modelling techniques aimed at generating a more analytical corpus of knowledge in the discipline. This period saw a surge of research that borrowed theory and methods from economics to study political institutions, such as the United States Congress, as well as political behaviour, such as voting. William H. Riker and his colleagues and students at the University of Rochester were the main proponents of this shift.\n\nDespite considerable research progress in the discipline based on all the kinds of scholarship discussed above, it has been observed that progress toward systematic theory has been modest and uneven.\n\nThe theory of political transitions, and the methods of their analysis and anticipating of crises, form an important part of political science. Several general indicators of crises and methods were proposed for anticipating critical transitions. Among them, a statistical indicator of crisis, simultaneous increase of variance and correlations in large groups, was proposed for crises anticipation and successfully used in various areas. Its applicability for early diagnosis of political crises was demonstrated by the analysis of the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years. A feature shared by certain major revolutions is that they were not predicted. The theory of apparent inevitability of crises and revolutions was also developed.\n\nIn the Soviet Union, political studies were carried out under the guise of some other disciplines like theory of state and law, area studies, international relations, studies of labor movement, \"critique of bourgeois theories\", etc. Soviet scholars were represented at the International Political Science Association (IPSA) since 1955 (since 1960 by the Soviet Association of Political and State Studies).\n\nIn 1979, the 11th World Congress of IPSA took place in Moscow. Until the late years of the Soviet Union, political science as a field was subjected to tight control of the Communist Party of the Soviet Union and was thus subjected to distrust. Anti-communists accused political scientists of being \"false\" scientists and of having served the old regime.\n\nAfter the fall of the Soviet Union, two of the major institutions dealing with political science, the Institute of Contemporary Social Theories and the Institute of International Affairs, were disbanded, and most of their members were left without jobs. These institutes were victims of the first wave of anticommunist opinion and ideological attacks. Today, the Russian Political Science Association unites professional political scientists from all around Russia.\n\nIn 2000, the Perestroika Movement in political science was introduced as a reaction against what supporters of the movement called the mathematicization of political science. Those who identified with the movement argued for a plurality of methodologies and approaches in political science and for more relevance of the discipline to those outside of it.\n\nSome evolutionary psychology theories argue that humans have evolved a highly developed set of psychological mechanisms for dealing with politics. However, these mechanisms evolved for dealing with the small group politics that characterized the ancestral environment and not the much larger political structures in today's world. This is argued to explain many important features and systematic cognitive biases of current politics.\n\nPolitical science, possibly like the social sciences as a whole, \"as a discipline lives on the fault line between the 'two cultures' in the academy, the sciences and the humanities.\" Thus, in some American colleges where there is no separate School or College of Arts and Sciences per se, political science may be a separate department housed as part of a division or school of Humanities or Liberal Arts. Whereas classical political philosophy is primarily defined by a concern for Hellenic and Enlightenment thought, political scientists are also marked by a great concern for \"modernity\" and the contemporary nation state, along with the study of classical thought, and as such share a greater deal of terminology with sociologists (e.g. structure and agency).\n\nMost United States colleges and universities offer B.A. programs in political science. M.A. or M.A.T. and Ph.D. or Ed.D. programs are common at larger universities. The term \"political science\" is more popular in North America than elsewhere; other institutions, especially those outside the United States, see political science as part of a broader discipline of \"political studies,\" \"politics,\" or \"government.\" While \"political science\" implies use of the scientific method, \"political studies\" implies a broader approach, although the naming of degree courses does not necessarily reflect their content. Separate degree granting programs in international relations and public policy are not uncommon at both the undergraduate and graduate levels. Master's level programs in political science are common when political scientists engage in public administration.\n\nThe national honor society for college and university students of government and politics in the United States is Pi Sigma Alpha.\n\nMost political scientists work broadly in one or more of the following five areas:\n\nSome political science departments also classify methodology as well as scholarship on the domestic politics of a particular country as distinct fields. In the United States, American politics is often treated as a separate subfield.\n\nIn contrast to this traditional classification, some academic departments organize scholarship into thematic categories, including political philosophy, political behaviour (including public opinion, collective action, and identity), and political institutions (including legislatures and international organizations). Political science conferences and journals often emphasize scholarship in more specific categories. The American Political Science Association, for example, has 42 organized sections that address various methods and topics of political inquiry.\n\nProgram evaluation is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs, particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While program evaluation first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.\n\nPolicy analysis is a technique used in public administration to enable civil servants, activists, and others to examine and evaluate the available options to implement the goals of laws and elected officials.\n\nAs a social political science, contemporary political science started to take shape in the latter half of the 19th century. At that time it began to separate itself from political philosophy, which traces its roots back to the works of Aristotle, and Plato which were written nearly 2,500 years ago. The term \"political science\" was not always distinguished from political philosophy, and the modern discipline has a clear set of antecedents including also moral philosophy, political economy, political theology, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal state.\n\n\n\n\n"}
{"id": "329948", "url": "https://en.wikipedia.org/wiki?curid=329948", "title": "Social change", "text": "Social change\n\nSocial change involves alteration of the social order of a society. It may include changes in social institutions, social behaviours or social relations.\n\nSocial change may refer to the notion of social progress or sociocultural evolution, the philosophical idea that society moves forward by evolutionary means. It may refer to a paradigmatic change in the socio-economic structure, for instance a shift away from feudalism and towards capitalism.\n\nSocial Development refers to how people develop social and emotional skills across the lifespan, with particular attention to childhood and adolescence. Healthy social development allows us to form positive relationships with family, friends, teachers, and other people in our lives.\n\nAccordingly, it may also refer to social revolution, such as the Socialist revolution presented in Marxism, or to other social movements, such as Women's suffrage or the Civil rights movement. Social change may be driven through cultural, religious, economic, scientific or technological force's.\n\nChange comes from two sources. One source is random or unique factors such as climate, weather, or the presence of specific groups of people. Another source is systematic factors. For example, successful development has the same general requirements, such as a stable and flexible government, enough free and available resources, and a diverse social organization of society. On the whole, social change is usually a combination of systematic factors along with some random or unique factors.\n\nThere are many theories of social change. Generally, a theory of change should include elements such as structural aspects of change (like population shifts), processes and mechanisms of social change, and directions of change.\n\n\nGrant suggests that individuals can have the largest personal impact by focusing on levels 2 and 3.\n\nOne of the most obvious changes currently occurring is the change in the relative global population distribution between countries. In recent decades, developing countries became a larger proportion of the world population, increasing from 68% in 1950 to 82% in 2010, while the population of the developed countries has declined from 32% of the total world population in 1950 to 18% in 2010. China and India continue to be the largest countries, followed by the US as a distant third. However, population growth throughout the world is slowing. Population growth among developed countries has been slowing since the 1950s and is now at 0.3% annual growth. Population growth among the less developed countries excluding the least developed has also been slowing, since 1960, and is now at 1.3% annual growth. Population growth among the least developed countries has slowed relatively little, and is the highest at 2.7% annual growth.\n\nIn much of the developed world, changes from distinct men's work and women's work to more gender equal patterns have been economically important since the mid-20th century. Both men and women are considered to be great contributors to social change worldwide.\n\n\n"}
{"id": "81682", "url": "https://en.wikipedia.org/wiki?curid=81682", "title": "Social control", "text": "Social control\n\nSocial control is a concept within the disciplines of the social sciences.\n\nThe term \"social control\" was first coined by Albion Woodbury Small and George Edgar Vincent in 1894; however, at the time sociologists only showed sporadic interest in the subject.\n\nSome social philosophers have played a role in the development of social control such as Thomas Hobbes in his work \"Leviathan\" that discusses social order and how the state exerts this using civil and military power; as well as Cesare Beccaria's \"On crimes and punishments\" that argues that people will avoid criminal behavior if their acts result in harsher punishment, stating that changes in punishment will act as a form of social control. Sociologist Èmile Durkheim also explored social control in the work \"The Division of Labor in Society\" and discusses the paradox of deviance, stating that social control is what makes us abide by laws in the first place.\n\nSociety uses certain sanctions to enforce a standard of behavior that is deemed socially acceptable. Individuals and institutions utilize social control to establish social norms and rules, which can be exercised by peers or friends, family, state and religious organizations, schools, and the workplace. The goal of social control is to maintain order in society and ensure conformity in those who are deemed as deviant or undesirable in society.\n\nSociologists identify two basic forms of social control:\n\nAs briefly defined above, the means to enforce social control can be either informal or formal. Sociologist Edward A. Ross argues that belief systems exert a greater control on human behavior than laws imposed by government, no matter what form the beliefs take.\n\nSocial control is considered one of the foundations of order within society.\n\nRoodenburg identifies the concept of social control as a classical concept.\n\nWhile the concept of social control has been around since the formation of organized sociology, the meaning has been altered over time. Originally, the concept simply referred to society's ability to regulate itself. However, in the 1930s, the term took on its more modern meaning of an individual's conversion to conformity. Academics began to study Social control theory as a separate field in the early 20th century.\n\nThe term social control as defined by Merriam Webster is known as certain rules and standards in society that keep individuals bound to conventional standards as well as the use of formalized mechanisms \nThe concept of social control is related to the notion of social order, a concept identified as existing in the following areas of society:\n\nThe term social control has also been linked to the term delinquency, defined as deviancy, which is the violation of established mores, social norms, and laws. More serious acts of delinquency are defined as consensus crimes and conflict crimes that are determined by society and the law to inhibit unwanted or negative behavior as a form of social control.\n\nSocial values are result of an individual internalizing certain norms and values. \nSocial values present in individuals are products of informal social control, exercised implicitly by a society through particular customs, norms, and mores. Individuals internalize the values of their society, whether conscious or not of the indoctrination. Traditional society relies mostly on informal social control embedded in its customary culture to socialize its members. The internalization of these values and norms is known as a process called socialization.\n\nInformal sanctions may include shame, ridicule, sarcasm, criticism, and disapproval, which can cause an individual to stray towards the social norms of the society. In extreme cases sanctions may include social discrimination and exclusion. Informal social control usually has more effect on individuals because the social values become internalized, thus becoming an aspect of the individual's personality.\n\nInformal sanctions check \"deviant\" behavior. An example of a negative sanction comes from a scene in the film \"Pink Floyd – The Wall\", whereby the young protagonist is ridiculed and verbally abused by a high school teacher for writing poetry in a mathematics class. Another example from the movie \"About a Boy\", when a young boy hesitates to jump from a high springboard and is ridiculed for his fear. Though he eventually jumps, his behavior is controlled by shame.\n\nInformal controls reward or punish acceptable or unacceptable behavior (i.e., deviance) and are varied from individual to individual, group to group, and society to society. For example, at a Women's Institute meeting, a disapproving look might convey the message that it is inappropriate to flirt with the minister. In a criminal gang, on the other hand, a stronger sanction applies in the case of someone threatening to inform to the police of illegal activity.\n\nSocial control by use of reward is known as positive reinforcement. In society and the laws and regulations implemented by the government tend to focus on punishment or the enforcing negative sanctions to act as a deterrent as means of social control.\n\nTheorists such as Noam Chomsky have argued that systemic bias exists in the modern media. The marketing, advertising, and public relations industries have thus been said to utilize mass communications to aid the interests of certain political and business elites. Powerful ideological, economic and religious lobbyists have often used school systems and centralized electronic communications to influence public opinion.\n\nFormal sanctions are usually imposed by the government and organizations in the form of laws to reward or punish behavior. Some formal sanctions include fines and incarceration in order to deter negative behavior. \nOther forms of formal social control can include other sanctions that are more severe depending on the behavior seen as negative such as censorship, expulsion, and limits on political freedom.\n\nExamples of this can be seen in law. If a person breaks a law set forth by the government and is caught, they will have to go to court and depending on the severity, will have to pay fines or face harsher consequences.\n\nAccording to a study done on crime in cities, in cities that have a higher incarceration rate and those that police make more arrests for public offenses, tend to have lower crime rates and incarceration rates.\n\nSocial control developed together with civilization, as a rational measure against the uncontrollable forces of nature which tribal organisations were at prey to within archaic tribal societies.\n\nCriminal persecutions first emerged around sixth century B.C. as a form of formal social control in Athens, Greece. The purpose of these persecutions were to check certain groups and protect them from malicious interests.\n\nRulers have used legitimized torture as a means of mind control, as well as murder, imprisonment and exile to remove from public space anyone the state authorities deemed undesirable.\n\nIn the Age of Enlightenment, harsh penalties for crimes and civil disobedience were criticized by philosophers such as Cesare Beccaria and Jeremy Bentham, whose work inspired reform movements. These movements eventually led to the Universal Declaration of Human Rights in 1948, which informs most western jurisdictions and the similar Cairo Declaration on Human Rights in Islam in 1990.\n\nThe word \"crime\" became part of the vocabulary of the English language via Old French, during the Middle Ages, and within the Middle English phase of the language.\n\nIn history, religion provided a moral influence on the community and each person, providing an internal locus of control oriented toward a morality, so that each person was empowered to have a degree of control over themselves within society.\nAs Auguste Comte instituted sociology (1830-1842), already certain thinkers predicted the discontinuation of a perceived \"false consciousness\" intrinsic to religious belief. Nevertheless, within the twentieth century, social scientists presumed that religion was still a principal factor of social control.\n\nComte, and those who preceded him, were breathing the air of a revolution in the latter part of the eighteenth century (French Revolution) to bring about a so-called enlightened way of being in society, which brought about a new liberty for the individual, without the constraints of an over-seeing aristocracy.\n\nIn the context social control through penal and correctional services, the \"rehabilitative ideal\" (Francis Allen 1964) is a key idea that formed within the 20th century—the first principle of which is that behavior is first caused by things that happened before (\"Human behaviour is a product of antecedent causes\"). The idea was later thought to have less relevancy to the philosophy and exaction or execution of correctional measures, at least according to a 2007 publication (and elsewhere).\n\nLaw is a technique used for the purposes of social control. For example, there are certain laws regarding appropriate sexual relationships and these are largely based on societal values. Historically, homosexuality has been criminalised. In modern times, this is no longer an offence and this is due to shifts in society's values. However, there are still laws regarding age of consent and incest, as these are still deemed as issues in society that require means of control.\n\nA mechanism of social control occurs through the use of selective incentives. Selective incentives are private goods, which are gifts or services, made available to people depending on whether they do or don't contribute to the good of a group, collective, or the common good. If people do contribute, they are rewarded, if they don't they are punished. Mancur Olson gave rise to the concept in its first instance (c.f. \"The Logic of Collective Action\").\n\nOberschall, in his work, identifies three elements to the pragmatics of social control as they exist in our current society. These are, confrontational control, such as riot control and crowd control, preventative measures to deter non-normal behaviors, which is legislation outlining expected boundaries for behavior, and measures complementary to preventative measures, which amount to punishment of criminal offences.\n\nCities can implement park exclusion orders (prohibiting individuals from frequenting some or all of the parks in a city for an extended period due to a previous infraction), trespass laws (privatizing areas generally thought of as public so police can choose which individuals to interrogate), and off-limit orders (Stay Out of Drug Areas (SODA) and Stay Out of Areas of Prostitution (SOAP) that obstruct access to these spaces). These are just a few of the new social control techniques cities use to displace certain individuals to the margins of society. Several common themes are apparent in each of these control mechanisms. The first is the ability to spatially constrain individuals in their own city. Defying any of the above statutes is a criminal offense resulting in possible incarceration. Though not all individuals subjected to an exclusion order obey it, these individuals are, at the very least, spatially hindered through decreased mobility and freedom throughout the city. This spatial constrain on individuals leads to disruption and interference in their lives. Homeless individuals generally frequent parks since the area provides benches for sleeping, public washrooms, occasional public services, and an overall sense of security by being near others in similar conditions. Privatizing areas such as libraries, public transportation systems, college campuses, and commercial establishments that are generally public gives the police permission to remove individuals as they see fit, even if the individual has ethical intent in the space. Off-limit orders attempting to keep drug addicts, prostitutes, and others out of concentrated areas of drug and sex crimes commonly restricts these individuals' ability to seek social services beneficial to rehabilitation, since these services are often located within the SODA and SOAP territories.\n\nIn the United States, early societies were able to easily expel individuals deemed undesirable from public space through vagrancy laws and other forms of banishment. In the 1960s and 1970s, however, these exclusion orders were denounced as unconstitutional in America and consequently were rejected by the US Supreme Court. The introduction of broken windows theory in the 1980s transformed the concepts cities used to form policies, to circumvent the previous issue of unconstitutionality. According to the theory, the environment of a particular space signals its health to the public, including to potential vandals. By maintaining an organized environment, individuals are dissuaded from causing disarray in that particular location. However, environments filled with disorder, such as broken windows or graffiti, indicate an inability for the neighborhood to supervise itself, therefore leading to an increase in criminal activity. Instead of focusing on the built environment, policies substantiated by the Broken Windows Theory overwhelmingly emphasize undesirable human behavior as the environmental disorder prompting further crime. The civility laws, originating in the late 1980s and early 1990s, provide an example of the usage of this latter aspect of the Broken Windows Theory as legitimization for discriminating against individuals considered disorderly in order to increase the sense of security in urban spaces. These civility laws effectively criminalize activities considered undesirable, such as sitting or lying on sidewalks, sleeping in parks, urinating or drinking in public, and begging, in an attempt to force the individuals doing these and other activities to relocate to the margins of society. Not surprisingly then, these restrictions disproportionally affect the homeless.\n\nIndividuals are deemed undesirable in urban space because they do not fit into social norms, which causes unease for many residents of certain neighborhoods. This fear has been deepened by the Broken Windows Theory and exploited in policies seeking to remove undesirables from visible areas of society. In the post-industrial city, concerned primarily with retail, tourism, and the service sector, the increasing pressure to create the image of a livable and orderly city has no doubt aided in the most recent forms of social control. These new techniques involve even more intense attempts to spatially expel certain individuals from urban space since the police are entrusted with considerably more power to investigate individuals, based on suspicion rather than on definite evidence of illicit actions.\n\nIn the decades leading up to the end of the 1980s, an increased prevalence of the individual as a feature within society led to many new therapies, suggesting the use of therapy as a means of social control (Conrad & Scheider, 1980: Mechanic 1989).\n\n"}
{"id": "191253", "url": "https://en.wikipedia.org/wiki?curid=191253", "title": "Social group", "text": "Social group\n\nIn the social sciences, a social group can be defined as two or more people who interact with one another, share similar characteristics, and collectively have a sense of unity. Other theorists disagree however, and are wary of definitions which stress the importance of interdependence or objective similarity. Instead, researchers within the social identity tradition generally define it as \"a group is defined in terms of those who identify themselves as members of the group\". Regardless, social groups come in a myriad of sizes and varieties. For example, a society can be viewed as a large social group.\n\nA social group exhibits some degree of social cohesion and is more than a simple collection or aggregate of individuals, such as people waiting at a bus stop, or people waiting in a line. Characteristics shared by members of a group may include interests, values, representations, ethnic or social background, and kinship ties. Kinship ties being a social bond based on common ancestry, marriage, or adoption. In a similar vein, some researchers consider the defining characteristic of a group as social interaction. According to Dunbar's number, on average, people cannot maintain stable social relationships with more than 150 individuals.\n\nSocial psychologist Muzafer Sherif proposed to define a social unit as a number of individuals interacting with each other with respect to:\n\nThis definition is long and complex, but it is also precise. It succeeds in providing the researcher with the tools required to answer three important questions:\n\n\nThe attention of those who use, participate in, or study groups has focused on functioning groups, on larger organizations, or on the decisions made in these organizations. Much less attention has been paid to the more ubiquitous and universal social behaviors that do not clearly demonstrate one or more of the five necessary elements described by Sherif.\n\nSome of the earliest efforts to understand these social units have been the extensive descriptions of urban street gangs in the 1920s and 1930s, continuing through the 1950s, which understood them to be largely reactions to the established authority. The primary goal of gang members was to defend gang territory, and to define and maintain the dominance structure within the gang. There remains in the popular media and urban law enforcement agencies an avid interest in gangs, reflected in daily headlines which emphasize the criminal aspects of gang behavior. However, these studies and the continued interest have not improved the capacity to influence gang behavior or to reduce gang related violence.\n\nThe relevant literature on animal social behaviors, such as work on territory and dominance, has been available since the 1950s. Also, they have been largely neglected by policy makers, sociologists and anthropologists. Indeed, vast literature on organization, property, law enforcement, ownership, religion, warfare, values, conflict resolution, authority, rights, and families have grown and evolved without any reference to any analogous social behaviors in animals. This disconnect may be the result of the belief that social behavior in humankind is radically different from the social behavior in animals because of the human capacity for language use and rationality. Of course, while this is true, it is equally likely that the study of the social (group) behaviors of other animals might shed light on the evolutionary roots of social behavior in people.\n\nTerritorial and dominance behaviors in humans are so universal and commonplace that they are simply taken for granted (though sometimes admired, as in home ownership, or deplored, as in violence). But these social behaviors and interactions between human individuals play a special role in the study of groups: \"they are necessarily prior to the formation of groups\". The psychological internalization of territorial and dominance experiences in conscious and unconscious memory are established through the formation of social identity, personal identity, body concept, or self concept. An adequately functioning individual identity is necessary before an individual can function in a division of labor (role), and hence, within a cohesive group. Coming to understand territorial and dominance behaviors may thus help to clarify the development, functioning, and productivity of groups.\n\nExplicitly contrasted against a social cohesion based definition for social groups is the social identity perspective, which draws on insights made in social identity theory. Here, rather than defining a social group based on expressions of cohesive social relationships between individuals, the social identity model assumes that \"psychological group membership has primarily a perceptual or cognitive basis\". It posits that the necessary and sufficient condition for individuals to act as group members is \"awareness of a common category membership\" and that a social group can be \"usefully conceptualized as a number of individuals who have internalized the same social category membership as a component of their self concept\". Stated otherwise, while the social cohesion approach expects group members to ask \"who am I attracted to?\", the social identity perspective expects group members to simply ask \"who am I?\"\n\nEmpirical support for the social identity perspective on groups was initially drawn from work using the minimal group paradigm. For example, it has been shown that the mere act of allocating individuals to explicitly random categories is sufficient to lead individuals to act in an ingroup favouring fashion (even where no individual self-interest is possible). Also problematic for the social cohesion account is recent research showing that seemingly meaningless categorization can be an antecedent of perceptions of interdependence with fellow category members.\n\nWhile the roots of this approach to social groups had its foundations in social identity theory, more concerted exploration of these ideas occurred later in the form of self-categorization theory. Whereas social identity theory was directed initially at the explanation of intergroup conflict in the absence of any conflict of interests, self-categorization theory was developed to explain how individuals come to perceive themselves as members of a group in the first place, and how this self-grouping process underlies and determines all problems subsequent aspects of group behaviour.\n\nIn his text, \"Group Dynamics,\" Forsyth (2010) discuses several common characteristics of groups that can help to define them.\n\nThis group component varies greatly, including verbal or non-verbal communication, social loafing, networking, forming bonds, etc. Research by Bales (cite, 1950, 1999) determine that there are two main types of interactions; relationship interactions and task interactions.\n\nMost groups have a reason for their existence, be it increasing the education and knowledge, receiving emotional support, or experiencing spirituality or religion. Groups can facilitate the achievement of these goals. The circumplex model of group tasks, by Joseph McGrath organizes group related tasks and goals. Groups may focus on several of these goals, or one area at a time. The model divides group goals into four main types, which are further sub-categorized\n\n“The state of being dependent, to some degree, on other people, as when one’s outcomes, actions, thoughts, feelings, and experiences are determined in whole or part by others.\" Some groups are more interdependent than others. For example, a sports team would have a relatively high level of interdependence as compared to a group of people watching a movie at the movie theater. Also, interdependence may be mutual (flowing back and forth between members) or more linear/unilateral. For example, some group members may be more dependent on their boss than the boss is on each of the individuals.\n\nGroup structure involves the emergence or regularities, norms, roles and relations that form within a group over time. Roles involve the expected performance and conduct of people within the group depending on their status or position within the group. Norms are the ideas adopted by the group pertaining to acceptable and unacceptable conduct by members. Group structure is a very important part of a group. If people fail to meet their expectations within to groups, and fulfil their roles, they may not accept the group, or be accepted by other group members.\n\nWhen viewed holistically, a group is greater than the sum of its individual parts. When people speak of groups, they speak of the group as a whole, or an entity, rather than speaking of it in terms of individuals. For example, it would be said that “The \"band\" played beautifully.” Several factors play a part in this image of unity, including group cohesiveness, and entitativity (appearance of cohesion by outsiders).\n\nAccording to Charles Horton Cooley (1864–1929), a primary group is a small social group whose members share personal and lasting relationships. People joined in primary relationships spend a great deal of time together, engage in a wide range of activities, and feel that they know one another well. In short, they show real concern for one another. In every society, the family is the most important primary group. Groups based on lasting friendships are also primary groups.\n\nSecondary groups, in contrast to primary groups, are large groups involving formal and institutional relationships. Secondary relationships involve weak emotional ties and little personal knowledge of one another. Most secondary groups are short term, beginning and ending without particular significance. They may last for years or may disband after a short time. The formation of primary groups happens within secondary groups.\n\nPrimary groups can be present in secondary settings. For example, attending a university exemplifies membership of a secondary group, while the friendships that are made there would be considered a primary group that you belong to. Likewise, some businesses care deeply about the well being of one another, while some immediate families have hostile relations within it.\n\nThe second type of group is based on the membership of the group. They are known as In-group and out-group, which are based on individual memberships and preferences. In-groups are those groups that an individual is a member of and out-groups are those groups those the individual is not a member.\n\nIndividuals almost universally have a bond toward what sociologists call reference groups. A reference group is a social group that serves as a point of reference in making evaluations and decisions.\n\nSome examples of types of groups include the following:\n\nGroups can also be categorized according to the number of people present within the group. This makes sense if the size of the group has consequences for the way group members relate with each other. In a small group, for example, \"each member receives some impression ... of each other member distinct enough so that he or she ... can give some reaction to each of the others as an individual person.\" This personal interaction is not possible in larger groups.\n\nThe social groups people are involved with in the workplace directly affect their health. No matter where you work or what the occupation is, feeling a sense of belonging in a peer group is a key to overall success. Part of this is the responsibility of the leader (manager, supervisor, etc.). If the leader helps everyone feel a sense of belonging within the group, it can help boost morale and productivity. According to Dr. Niklas Steffens \"Social identification contributes to both psychological and physiological health, but the health benefits are stronger for psychological health\". The social relationships people have can be linked to different health conditions. Lower quantity or quality social relationships have been connected to issues such as: development of cardiovascular disease, recurrent myocardial infarction, atherosclerosis, autonomic dysregulation, high blood pressure, cancer and delayed cancer recovery, and slower wound healing as well as inflammatory biomarkers and impaired immune function, factors associated with adverse health outcomes and mortality. The social relationship of marriage is the most studied of all, the marital history over the course of one's life can form differing health outcomes such as cardiovascular disease, chronic conditions, mobility limitations, self-rated health, and depressive symptoms. Social connectedness also plays a large part in overcoming mental afflictions such as drug, alcohol, or substance abuse. With these types of issues, a person's peer group play a big role in helping them stay sober. Conditions do not need to be life-threatening, one's social group can help deal with work anxiety as well. When people are more socially connected have access to more support. Some of the health issues people have may also stem from their uncertainty about just where they stand among their colleagues. It has been shown that being well socially connected has a significant impact on a person as they age, according to a 10-year study by the MacArthur Foundation, which was published in the book 'Successful Aging' the support, love, and care we feel through our social connections can help to counteract some of the health-related negatives of aging. Older people who were more active in social circles tended to be better off health-wise.\n\nSocial groups tend to form based on certain principles of attraction, that draw individuals to affiliate with each other, eventually forming a group.\n\n\nOther factors also influence the formation of a group. Extroverts may seek out groups more, as they find larger and more frequent interpersonal interactions stimulating and enjoyable (more than introverts). Similarly, groups may seek out extroverts more than introverts, perhaps because they find they connect with extroverts more readily. Those higher in relationality (attentiveness to their relations with other people) are also likelier to seek out and prize group membership. Relationality has also been associated with extroversion and agreeableness. Similarly, those with a high need for affiliation are more drawn to join groups, spend more time with groups and accept other group members more readily.\n\nPrevious experiences with groups (good and bad) inform people’s decisions to join prospective groups. Individuals will compare the rewards of the group (e.g. belonging, emotional support, informational support, instrumental support, spiritual support; see Uchino, 2004 for an overview) against potential costs (e.g. time, emotional energy). Those with negative or 'mixed' experiences with previous groups will likely be more deliberate in their assessment of potential groups to join, and with which groups they choose to join. (For more, see Minimax Principal, as part of Social Exchange Theory)\n\nOnce a group has begun to form, it can increase membership through a few ways. If the group is an open group, where membership boundaries are relatively permeable, group members can enter and leave the group as they see fit (often via at least one of the aforementioned Principles of Attraction). A closed group on the other hand, where membership boundaries are more rigid and closed, often engages in deliberate and/or explicit recruitment and socialization of new members.\n\nIf a group is highly cohesive, it will likely engage in processes that contribute to cohesion levels, especially when recruiting new members, who can add to a group's cohesion, or destabilize it. Classic examples of groups with high cohesion are fraternities, sororities, gangs, and cults, which are all noted for their recruitment process, especially their initiation or hazing. In all groups, formal and informal initiations add to a group's cohesion and strengthens the bond between the individual and group by demonstrating the exclusiveness of group membership as well as the recruit's dedication to the group. Initiations tend to be more formal in more cohesive groups. Initiation is also important for recruitment because it can mitigate any cognitive dissonance in potential group members.\n\nIn some instances, such as cults, recruitment can also be referred to as conversion. Kelman's Theory of Conversion identifies 3 stages of conversion: compliance (individual will comply or accept group's views, but not necessarily agree with them), identification (member begins to mimic group's actions, values, characteristics, etc.) and internalization (group beliefs and demands become congruent with member's personal beliefs, goals and values). This outlines the process of how new members can become deeply connected to the group.\n\nIf one brings a small collection of strangers together in a restricted space and environment, provides a common goal and maybe a few ground rules, then a highly probable course of events will follow. Interaction between individuals is the basic requirement. At first, individuals will differentially interact in sets of twos or threes while seeking to interact with those with whom they share something in common: i.e., interests, skills, and cultural background. Relationships will develop some stability in these small sets, in that individuals may temporarily change from one set to another, but will return to the same pairs or trios rather consistently and resist change. Particular twosomes and threesomes will stake out their special spots within the overall space.\n\nAgain depending on the common goal, eventually twosomes and threesomes will integrate into larger sets of six or eight, with corresponding revisions of territory, dominance-ranking, and further differentiation of roles. All of this seldom takes place without some conflict or disagreement: for example, fighting over the distribution of resources, the choices of means and different subgoals, the development of what are appropriate norms, rewards and punishments. Some of these conflicts will be territorial in nature: i.e., jealousy over roles, or locations, or favored relationships. But most will be involved with struggles for status, ranging from mild protests to serious verbal conflicts and even dangerous violence.\n\nBy analogy to animal behavior, sociologists may term these behaviors \"territorial behaviors\" and \"dominance behaviors\". Depending on the pressure of the common goal and on the various skills of individuals, differentiations of leadership, dominance, or authority will develop. Once these relationships solidify, with their defined roles, norms, and sanctions, a productive group will have been established.\n\nAggression is the mark of unsettled dominance order. Productive group cooperation requires that both dominance order and territorial arrangements (identity, self-concept) be settled with respect to the common goal and within the particular group. Some individuals may withdraw from interaction or be excluded from the developing group. Depending on the number of individuals in the original collection of strangers, and the number of \"hangers-on\" that are tolerated, one or more competing groups of ten or less may form, and the competition for territory and dominance will then also be manifested in the inter group transactions.\n\nTwo or more people in interacting situations will over time develop stable territorial relationships. As described above, these may or may not develop into groups. But stable groups can also break up in to several sets of territorial relationships. There are numerous reasons for stable groups to \"malfunction\" or to disperse, but essentially this is because of loss of compliance with one or more elements of the definition of group provided by Sherif. The two most common causes of a malfunctioning group are the addition of too many individuals, and the failure of the leader to enforce a common purpose, though malfunctions may occur due to a failure of any of the other elements (i.e., confusions status or of norms).\n\nIn a society, there is a need for more people to participate in cooperative endeavors than can be accommodated by a few separate groups. The military has been the best example as to how this is done in its hierarchical array of squads, platoons, companies, battalions, regiments, and divisions. Private companies, corporations, government agencies, clubs, and so on have all developed comparable (if less formal and standardized) systems when the number of members or employees exceeds the number that can be accommodated in an effective group. Not all larger social structures require the cohesion that may be found in the small group. Consider the neighborhood, the country club, or the megachurch, which are basically territorial organizations who support large social purposes. Any such large organizations may need only islands of cohesive leadership.\n\nFor a functioning group to attempt to add new members in a casual way is a certain prescription for failure, loss of efficiency, or disorganization. The number of functioning members in a group can be reasonably flexible between five and ten, and a long-standing cohesive group may be able to tolerate a few hangers on. The key concept is that the value and success of a group is obtained by each member maintaining a distinct, functioning identity in the minds of each of the members. The cognitive limit to this span of attention in individuals is often set at seven. Rapid shifting of attention can push the limit to about ten. After ten, subgroups will inevitably start to form with the attendant loss of purpose, dominance-order, and individuality, with confusion of roles and rules. The standard classroom with twenty to forty pupils and one teacher offers a rueful example of one supposed leader juggling a number of subgroups.\n\nWeakening of the common purpose once a group is well established can be attributed to: adding new members; unsettled conflicts of identities (i.e., territorial problems in individuals); weakening of a settled dominance-order; and weakening or failure of the leader to tend to the group. The actual loss of a leader is frequently fatal to a group, unless there was lengthy preparation for the transition. The loss of the leader tends to dissolve all dominance relationships, as well as weakening dedication to common purpose, differentiation of roles, and maintenance of norms. The most common symptoms of a troubled group are loss of efficiency, diminished participation, or weakening of purpose, as well as an increase in verbal aggression. Often, if a strong common purpose is still present, a simple reorganization with a new leader and a few new members will be sufficient to re-establish the group, which is somewhat easier than forming an entirely new group. This is the most common factor.\n"}
{"id": "26781", "url": "https://en.wikipedia.org/wiki?curid=26781", "title": "Social science", "text": "Social science\n\nSocial science is the branch of science devoted to the study of human societies and the relationships among individuals within those societies. The term was formerly used to refer to the field of sociology, the original \"science of society\", established in the 19th century. In addition to sociology, it now encompasses a wide array of academic disciplines, including anthropology, archaeology, economics, human geography, linguistics, management science, media studies, musicology, political science, psychology, and social history. (For a more detailed list of sub-disciplines within the social sciences see: Outline of social science.)\n\nPositivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term \"social research\" has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.\n\nThe history of the social sciences begins in the Age of Enlightenment after 1650, which saw a revolution within natural philosophy, changing the basic framework by which individuals understood what was \"scientific\". Social sciences came forth from the moral philosophy of the time and were influenced by the Age of Revolutions, such as the Industrial Revolution and the French Revolution. The \"social sciences\" developed from the sciences (experimental and applied), or the systematic knowledge-bases or prescriptive practices, relating to the social improvement of a group of interacting entities.\n\nThe beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Jean-Jacques Rousseau and other pioneers. The growth of the social sciences is also reflected in other specialized encyclopedias. The modern period saw \"social science\" first used as a distinct conceptual field. Social science was influenced by positivism, focusing on knowledge based on actual positive sense experience and avoiding the negative; metaphysical speculation was avoided. Auguste Comte used the term \"science sociale\" to describe the field, taken from the ideas of Charles Fourier; Comte also referred to the field as \"social physics\".\n\nFollowing this period, five paths of development sprang forth in the social sciences, influenced by Comte in other fields. One route that was taken was the rise of social research. Large statistical surveys were undertaken in various parts of the United States and Europe. Another route undertaken was initiated by Émile Durkheim, studying \"social facts\", and Vilfredo Pareto, opening metatheoretical ideas and individual theories. A third means developed, arising from the methodological dichotomy present, in which social phenomena were identified with and understood; this was championed by figures such as Max Weber. The fourth route taken, based in economics, was developed and furthered economic knowledge as a hard science. The last path was the correlation of knowledge and social values; the antipositivism and verstehen sociology of Max Weber firmly demanded this distinction. In this route, theory (description) and prescription were non-overlapping formal discussions of a subject.\n\nAround the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. The interdisciplinary and cross-disciplinary nature of scientific inquiry into human behaviour, social and environmental factors affecting it, made many of the natural sciences interested in some aspects of social science methodology. Examples of boundary blurring include emerging disciplines like social research of medicine, sociobiology, neuropsychology, bioeconomics and the history and sociology of science. Increasingly, quantitative research and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics. Statistical methods were used confidently.\n\nIn the contemporary period, Karl Popper and Talcott Parsons influenced the furtherance of the social sciences. Researchers continue to search for a unified consensus on what methodology might have the power and refinement to connect a proposed \"grand theory\" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks; for more, see consilience. The social sciences will for the foreseeable future be composed of different zones in the research of, and sometime distinct in approach toward, the field.\n\nThe term \"social science\" may refer either to the specific \"sciences of society\" established by thinkers such as Comte, Durkheim, Marx, and Weber, or more generally to all disciplines outside of \"noble science\" and arts. By the late 19th century, the academic social sciences were constituted of five fields: jurisprudence and amendment of the law, education, health, economy and trade, and art.\n\nAround the start of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.\n\nThe social science disciplines are branches of knowledge taught and researched at the college or university level. Social science disciplines are defined and recognized by the academic journals in which research is published, and the learned social science societies and academic departments or faculties to which their practitioners belong. Social science fields of study usually have several sub-disciplines or branches, and the distinguishing lines between these are often both arbitrary and ambiguous.\n\nAnthropology is the holistic \"science of man\", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities, and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The \"natural sciences\" seek to derive general laws through reproducible and verifiable experiments. The \"humanities\" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The \"social sciences\" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.\n\nThe anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word \"anthropos\" (ἄνθρωπος) in Ancient Greek means \"human being\" or \"person\". Eric Wolf described sociocultural anthropology as \"the most scientific of the humanities, and the most humanistic of the sciences\".\n\nThe goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called \"primitive\" in anthropological literature, but without any connotation of \"inferior\". Today, anthropologists use terms such as \"less complex\" societies or refer to specific modes of subsistence or production, such as \"pastoralist\" or \"forager\" or \"horticulturalist\" to refer to humans living in non-industrial, non-Western cultures, such people or folk (\"ethnos\") remaining of great interest within anthropology.\n\nThe quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.\n\nCommunication studies deals with processes of human communication, commonly defined as the sharing of symbols to create meaning. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, and social dimensions of their contexts. Communication is institutionalized under many different names at different universities, including \"communication\", \"communication studies\", \"speech communication\", \"rhetorical studies\", \"communication science\", \"media studies\", \"communication arts\", \"mass communication\", \"media ecology\", and \"communication and media science\".\n\nCommunication studies integrates aspects of both social sciences and the humanities. As a social science, the discipline often overlaps with sociology, psychology, anthropology, biology, political science, economics, and public policy, among others. From a humanities perspective, communication is concerned with rhetoric and persuasion (traditional graduate programs in communication studies trace their history to the rhetoricians of Ancient Greece). The field applies to outside disciplines as well, including engineering, architecture, mathematics, and information science.\n\nEconomics is a social science that seeks to analyze and describe the production, distribution, and consumption of wealth. The word \"economics\" is from the Ancient Greek \"oikos\", \"family, household, estate\", and νόμος \"nomos\", \"custom, law\", and hence means \"household management\" or \"management of the state\". An economist is a person using economic concepts and data in the course of employment, or someone who has earned a degree in the subject. The classic brief definition of economics, set out by Lionel Robbins in 1932, is \"the science which studies human behavior as a relation between scarce means having alternative uses\". Without scarcity and alternative uses, there is no economic problem. Briefer yet is \"the study of how people seek to satisfy needs and wants\" and \"the study of the financial aspects of human behavior\".\n\nEconomics has two broad branches: microeconomics, where the unit of analysis is the individual agent, such as a household or firm, and macroeconomics, where the unit of analysis is an economy as a whole. Another division of the subject distinguishes positive economics, which seeks to predict and explain economic phenomena, from normative economics, which orders choices and actions by some criterion; such orderings necessarily involve subjective value judgments. Since the early part of the 20th century, economics has focused largely on measurable quantities, employing both theoretical models and empirical analysis. Quantitative models, however, can be traced as far back as the physiocratic school. Economic reasoning has been increasingly applied in recent decades to other social situations such as politics, law, psychology, history, religion, marriage and family life, and other social interactions.\nThis paradigm crucially assumes (1) that resources are scarce because they are not sufficient to satisfy all wants, and (2) that \"economic value\" is willingness to pay as revealed for instance by market (arms' length) transactions. Rival heterodox schools of thought, such as institutional economics, green economics, Marxist economics, and economic sociology, make other grounding assumptions. For example, Marxist economics assumes that economics primarily deals with the investigation of exchange value, of which human labour is the source.\n\nThe expanding domain of economics in the social sciences has been described as economic imperialism.\n\nEducation encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgement and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). To educate means 'to draw out', from the Latin \"educare\", or to facilitate the realization of an individual's potential and talents. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology and anthropology.\n\nThe education of an individual human begins at birth and continues throughout life. (Some believe that education begins even before birth, as evidenced by some parents' playing music or reading to the baby in the womb in the hope it will influence the child's development.) For some, the struggles and triumphs of daily life provide far more instruction than does formal schooling (thus Mark Twain's admonition to \"never let school interfere with your education\").\n\nGeography as a discipline can be split broadly into two main sub fields: human geography and physical geography. The former focuses largely on the built environment and how space is created, viewed and managed by humans as well as the influence humans have on the space they occupy. This may involve cultural geography, transportation, health, military operations, and cities. The latter examines the natural environment and how the climate, vegetation and life, soil, oceans, water and landforms are produced and interact. Physical geography examines phenomena related to the measurement of earth. As a result of the two subfields using different approaches a third field has emerged, which is environmental geography. Environmental geography combines physical and human geography and looks at the interactions between the environment and humans. Other branches of geography include social geography, regional geography, and geomatics.\n\nGeographers attempt to understand the Earth in terms of physical and spatial relationships. The first geographers focused on the science of mapmaking and finding ways to precisely project the surface of the earth. In this sense, geography bridges some gaps between the natural sciences and social sciences. Historical geography is often taught in a college in a unified Department of Geography.\n\nModern geography is an all-encompassing discipline, closely related to GISc, that seeks to understand humanity and its natural environment. The fields of urban planning, regional science, and planetology are closely related to geography. Practitioners of geography use many technologies and methods to collect data such as GIS, remote sensing, aerial photography, statistics, and global positioning systems (GPS).\n\nHistory is the continuous, systematic narrative and research into past human events as interpreted through historiographical paradigms or theories.\n\nHistory has a base in both the social sciences and the humanities. In the United States the National Endowment for the Humanities includes history in its definition of humanities (as it does for applied linguistics). However, the National Research Council classifies history as a social science. The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The Social Science History Association, formed in 1976, brings together scholars from numerous disciplines interested in social history.\n\nThe social science of law, jurisprudence, in common parlance, means a rule that (unlike a rule of ethics) is capable of enforcement through institutions. However, many laws are based on norms accepted by a community and thus have an ethical foundation. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a \"system of rules\", as an \"interpretive concept\" to achieve justice, as an \"authority\" to mediate people's interests, and even as \"the command of a sovereign, backed by the threat of a sanction\". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on the distribution of wealth. The noun \"law\" derives from the late Old English \"lagu\", meaning something laid down or fixed and the adjective \"legal\" comes from the Latin word \"lex\".\n\nLinguistics investigates the cognitive and social aspects of human language. The field is divided into areas that focus on aspects of the linguistic signal, such as syntax (the study of the rules that govern the structure of sentences), semantics (the study of meaning), morphology (the study of the structure of words), phonetics (the study of speech sounds) and phonology (the study of the abstract sound system of a particular language); however, work in areas like evolutionary linguistics (the study of the origins and evolution of language) and psycholinguistics (the study of psychological factors in human language) cut across these divisions.\n\nThe overwhelming majority of modern research in linguistics takes a predominantly synchronic perspective (focusing on language at a particular point in time), and a great deal of it—partly owing to the influence of Noam Chomsky—aims at formulating theories of the cognitive processing of language. However, language does not exist in a vacuum, or only in the brain, and approaches like contact linguistics, creole studies, discourse analysis, social interactional linguistics, and sociolinguistics explore language in its social context. Sociolinguistics often makes use of traditional quantitative analysis and statistics in investigating the frequency of features, while some disciplines, like contact linguistics, focus on qualitative analysis. While certain areas of linguistics can thus be understood as clearly falling within the social sciences, other areas, like acoustic phonetics and neurolinguistics, draw on the natural sciences. Linguistics draws only secondarily on the humanities, which played a rather greater role in linguistic inquiry in the 19th and early 20th centuries. Ferdinand Saussure is considered the father of modern linguistics.\n\nPolitical science is an academic and research discipline that deals with the theory and practice of politics and the description and analysis of political systems and political behaviour. Fields and subfields of political science include political economy, political theory and philosophy, civics and comparative politics, theory of direct democracy, apolitical governance, participatory direct democracy, national systems, cross-national political analysis, political development, international relations, foreign policy, international law, politics, public administration, administrative behaviour, public law, judicial behaviour, and public policy. Political science also studies power in international relations and the theory of great powers and superpowers.\n\nPolitical science is methodologically diverse, although recent years have witnessed an upsurge in the use of the scientific method, that is, the proliferation of formal-deductive model building and quantitative hypothesis testing. Approaches to the discipline include rational choice, classical political philosophy, interpretivism, structuralism, and behaviouralism, realism, pluralism, and institutionalism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents, interviews, and official records, as well as secondary sources such as scholarly articles are used in building and testing theories. Empirical methods include survey research, statistical analysis or econometrics, case studies, experiments, and model building. Herbert Baxter Adams is credited with coining the phrase \"political science\" while teaching history at Johns Hopkins University.\n\nPsychology is an academic and applied field involving the study of behaviour and mental processes. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental illness. The word \"psychology\" comes from the Ancient Greek ψυχή \"psyche\" (\"soul\", \"mind\") and \"logy\" (\"study\").\n\nPsychology differs from anthropology, economics, political science, and sociology in seeking to capture explanatory generalizations about the mental function and overt behaviour of individuals, while the other disciplines focus on creating descriptive generalizations about the functioning of social groups or situation-specific human behaviour. In practice, however, there is quite a lot of cross-fertilization that takes place among the various fields. Psychology differs from biology and neuroscience in that it is primarily concerned with the interaction of mental processes and behaviour, and of the overall processes of a system, and not simply the biological or neural processes themselves, though the subfield of neuropsychology combines the study of the actual neural processes with the study of the mental effects they have subjectively produced.\nMany people associate psychology with clinical psychology, which focuses on assessment and treatment of problems in living and psychopathology. In reality, psychology has myriad specialties including social psychology, developmental psychology, cognitive psychology, educational psychology, industrial-organizational psychology, mathematical psychology, neuropsychology, and quantitative analysis of behaviour.\n\nPsychology is a very broad science that is rarely tackled as a whole, major block. Although some subfields encompass a natural science base and a social science application, others can be clearly distinguished as having little to do with the social sciences or having a lot to do with the social sciences. For example, biological psychology is considered a natural science with a social scientific application (as is clinical medicine), social and occupational psychology are, generally speaking, purely social sciences, whereas neuropsychology is a natural science that lacks application out of the scientific tradition entirely. In British universities, emphasis on what tenet of psychology a student has studied and/or concentrated is communicated through the degree conferred: B.Psy. indicates a balance between natural and social sciences, B.Sc. indicates a strong (or entire) scientific concentration, whereas a B.A. underlines a majority of social science credits. This is not always necessarily the case however, and in many UK institutions students studying the B.Psy, B.Sc, and B.A. follow the same curriculum as outlined by The British Psychological Society and have the same options of specialism open to them regardless of whether they choose a balance, a heavy science basis, or heavy social science basis to their degree. If they applied to read the B.A. for example, but specialized in heavily science-based modules, then they will still generally be awarded the B.A.\n\nSociology is the systematic study of society, individuals' relationship to their societies, the consequences of difference, and other aspects of human social action. The meaning of the word comes from the suffix \"-logy\", which means \"study of\", derived from Ancient Greek, and the stem \"soci-\", which is from the Latin word \"socius\", meaning \"companion\", or society in general.\n\nAuguste Comte (1798–1857) coined the term, Sociology, as a way to apply natural science principles and techniques to the social world in 1838. Comte endeavoured to unify history, psychology and economics through the descriptive understanding of the social realm. He proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in \"The Course in Positive Philosophy\" [1830–1842] and \"A General View of Positivism\" (1844). Though Comte is generally regarded as the \"Father of Sociology\", the discipline was formally established by another French thinker, Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his \"Rules of the Sociological Method\". In 1896, he established the journal \"L'Année Sociologique\". Durkheim's seminal monograph, \"Suicide\" (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.\n\nKarl Marx rejected Comte's positivism but nevertheless aimed to establish a \"science of society\" based on historical materialism, becoming recognized as a founding figure of sociology posthumously as the term gained broader meaning. Around the start of the 20th century, the first wave of German sociologists, including Max Weber and Georg Simmel, developed sociological antipositivism. The field may be broadly recognized as an amalgam of three modes of social thought in particular: Durkheimian positivism and structural functionalism; Marxist historical materialism and conflict theory; and Weberian antipositivism and verstehen analysis. American sociology broadly arose on a separate trajectory, with little Marxist influence, an emphasis on rigorous experimental methodology, and a closer association with pragmatism and social psychology. In the 1920s, the Chicago school developed symbolic interactionism. Meanwhile, in the 1930s, the Frankfurt School pioneered the idea of critical theory, an interdisciplinary form of Marxist sociology drawing upon thinkers as diverse as Sigmund Freud and Friedrich Nietzsche. Critical theory would take on something of a life of its own after World War II, influencing literary criticism and the Birmingham School establishment of cultural studies.\n\nSociology evolved as an academic response to the challenges of modernity, such as industrialization, urbanization, secularization, and a perceived process of enveloping rationalization. The field generally concerns the social rules and processes that bind and separate people not only as individuals, but as members of associations, groups, communities and institutions, and includes the examination of the organization and development of human social life. The sociological field of interest ranges from the analysis of short contacts between anonymous individuals on the street to the study of global social processes. In the terms of sociologists Peter L. Berger and Thomas Luckmann, social scientists seek an understanding of the \"Social Construction of Reality\". Most sociologists work in one or more subfields. One useful way to describe the discipline is as a cluster of sub-fields that examine different dimensions of society. For example, social stratification studies inequality and class structure; demography studies changes in a population size or type; criminology examines criminal behaviour and deviance; and political sociology studies the interaction between society and state.\n\nSince its inception, sociological epistemologies, methods, and frames of enquiry, have significantly expanded and diverged. Sociologists use a diversity of research methods, collect both quantitative and qualitative data, draw upon empirical techniques, and engage critical theory. Common modern methods include case studies, historical research, interviewing, participant observation, social network analysis, survey research, statistical analysis, and model building, among other approaches. Since the late 1970s, many sociologists have tried to make the discipline useful for purposes beyond the academy. The results of sociological research aid educators, lawmakers, administrators, developers, and others interested in resolving social problems and formulating public policy, through subdisciplinary areas such as evaluation research, methodological assessment, and public sociology.\n\nIn the early 1970s, women sociologists began to question sociological paradigms and the invisibility of women in sociological studies, analysis, and courses. In 1969, feminist sociologists challenged the discipline's androcentrism at the American Sociological Association's annual conference. This led to the founding of the organization Sociologists for Women in Society, and, eventually, a new sociology journal, Gender & Society. Today, the sociology of gender is considered to be one of the most prominent sub-fields in the discipline.\n\nNew sociological sub-fields continue to appear — such as community studies, computational sociology, environmental sociology, network analysis, actor-network theory, gender studies, and a growing list, many of which are cross-disciplinary in nature.\n\nAdditional applied or interdisciplinary fields related to the social sciences include:\n\nThe origin of the survey can be traced back at least early as the Domesday Book in 1086, while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's \"Natural and Political Observations upon the Bills of Mortality\". Social research began most intentionally, however, with the positivist philosophy of science in the 19th century.\n\nIn contemporary usage, \"social research\" is a relatively autonomous term, encompassing the work of practitioners from various disciplines that share in its aims and methods. Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena; from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. The methods originally rooted in classical sociology and statistical mathematics have formed the basis for research in other disciplines, such as political science, media studies, and marketing and market research.\n\nSocial research methods may be divided into two broad schools:\n\nSocial scientists will commonly combine quantitative and qualitative approaches as part of a multi-strategy design. Questionnaires, field-based data collection, archival database information and laboratory-based data collections are some of the measurement techniques used. It is noted the importance of measurement and analysis, focusing on the (difficult to achieve) goal of objective research or statistical hypothesis testing. A mathematical model uses mathematical language to describe a system. The process of developing a mathematical model is termed 'mathematical modelling' (also modeling). Eykhoff (1974) defined a \"mathematical model\" as 'a representation of the essential aspects of an existing system (or a system to be constructed) that presents knowledge of that system in usable form'. Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models.\n\nThese and other types of models can overlap, with a given model involving a variety of abstract structures. The system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. The concept of an \"integrated whole\" can also be stated in terms of a system embodying a set of relationships that are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. A dynamical system modeled as a mathematical formalization has a fixed \"rule\" that describes the time dependence of a point's position in its ambient space. Small changes in the state of the system correspond to small changes in the numbers. The \"evolution rule\" of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule is deterministic: for a given time interval only one future state follows from the current state.\n\nSocial scientists often conduct Program Evaluation, which is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs, particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While program evaluation first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.\n\nOther social scientists emphasize the subjective nature of research. These writers share social theory perspectives that include various types of the following:\n\nOther fringe social scientists delve in alternative nature of research. These writers share social theory perspectives that include various types of the following:\n\nMost universities offer degrees in social science fields. The Bachelor of Social Science is a degree targeted at the social sciences in particular. It is often more flexible and in-depth than other degrees that include social science subjects.\n\nIn the United States, a university may offer a student who studies a social sciences field a Bachelor of Arts degree, particularly if the field is within one of the traditional liberal arts such as history, or a BSc: Bachelor of Science degree such as those given by the London School of Economics, as the social sciences constitute one of the two main branches of science (the other being the natural sciences). In addition, some institutions have degrees for a particular social science, such as the Bachelor of Economics degree, though such specialized degrees are relatively rare in the United States.\n\nGraduate students may get a Master's degree (Master of Arts, Master of Science or a field-specific degree such as Master of Public Administration) or Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42400", "url": "https://en.wikipedia.org/wiki?curid=42400", "title": "Socialization", "text": "Socialization\n\nIn sociology, socialization is the process of internalizing the norms and ideologies of society. Socialization encompasses both learning and teaching and is thus \"the means by which social and cultural continuity are attained\".\n\nSocialization is strongly connected to developmental psychology. Humans need social experiences to learn their culture and to survive.\n\nSocialization essentially represents the whole process of learning throughout the life course and is a central influence on the behavior, beliefs, and actions of adults as well as of children.\n\nSocialization may lead to desirable outcomes—sometimes labeled \"moral\"—as regards the society where it occurs. Individual views are influenced by the society's consensus and usually tend toward what that society finds acceptable or \"normal\". Socialization provides only a partial explanation for human beliefs and behaviors, maintaining that agents are not blank slates predetermined by their environment; scientific research provides evidence that people are shaped by both social influences and genes.\n\nGenetic studies have shown that a person's environment interacts with his or her genotype to influence behavioral outcomes.\n\nNotions of society and the state of nature have existed for centuries. In its earliest usages, socialization was simply the act of socializing or another word for socialism. Socialization as a concept originated concurrently with sociology, as sociology was defined as the treatment of \"the specifically social, the process and forms of socialization, as such, in contrast to the interests and contents which find expression in socialization\". In particular, socialization consisted of the formation and development of social groups, and also the development of a social state of mind in the individuals who associate. Socialization is thus both a cause and an effect of association. The term was relatively uncommon before 1940, but became popular after World War II, appearing in dictionaries and scholarly works such as the theory of Talcott Parsons.\n\nLawrence Kohlberg studied moral reasoning and developed a theory of how individuals reason situations as right from wrong. The first stage is the pre-conventional stage, where a person (typically children) experience the world in terms of pain and pleasure, with their moral decisions solely reflecting this experience. Second, the conventional stage (typical for adolescents and adults) is characterized by an acceptance of society's conventions concerning right and wrong, even when there are no consequences for obedience or disobedience. Finally, the post-conventional stage (more rarely achieved) occurs if a person moves beyond society's norms to consider abstract ethical principles when making moral decisions.\n\nErik H. Erikson (1902–1994) explained the challenges throughout the life course. The first stage in the life course is infancy, where babies learn trust and mistrust. The second stage is toddlerhood where children around the age of two struggle with the challenge of autonomy versus doubt. In stage three, preschool, children struggle to understand the difference between initiative and guilt. Stage four, pre-adolescence, children learn about industriousness and inferiority. In the fifth stage called adolescence, teenagers experience the challenge of gaining identity versus confusion. The sixth stage, young adulthood, is when young people gain insight to life when dealing with the challenge of intimacy and isolation. In stage seven, or middle adulthood, people experience the challenge of trying to make a difference (versus self-absorption). In the final stage, stage eight or old age, people are still learning about the challenge of integrity and despair. This concept has been further developed by Klaus Hurrelmann and Gudrun Quenzel using the dynamic model of \"developmental tasks\".\n\nGeorge Herbert Mead (1863–1931) developed a theory of social behaviorism to explain how social experience develops an individual's self-concept. Mead's central concept is the self: It is composed of self-awareness and self-image. Mead claimed that the self is not there at birth, rather, it is developed with social experience. Since social experience is the exchange of symbols, people tend to find meaning in every action. Seeking meaning leads us to imagine the intention of others. Understanding intention requires imagining the situation from the others' point of view. In effect, others are a mirror in which we can see ourselves. Charles Horton Cooley (1902-1983) coined the term looking glass self, which means self-image based on how we think others see us. According to Mead the key to developing the self is learning to take the role of the other. With limited social experience, infants can only develop a sense of identity through imitation. Gradually children learn to take the roles of several others. The final stage is the generalized other, which refers to widespread cultural norms and values we use as a reference for evaluating others.\n\nBehaviorism makes claims that when infants are born they lack social experience or self. The social pre-wiring hypothesis, on the other hand, shows proof through a scientific study that social behavior is partly inherited and can influence infants and also even influence foetuses. Wired to be social means that infants are not taught that they are social beings, but they are born as prepared social beings.\n\nThe social pre-wiring hypothesis refers to the ontogeny of social interaction. Also informally referred to as, \"wired to be social\". The theory questions whether there is a propensity to socially oriented action already present \"before\" birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social.\n\nCircumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be contributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics.\n\nPrincipal evidence of this theory is uncovered by examining Twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed.\n\nThe social pre-wiring hypothesis was proved correct, \"The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions.\"\n\nPrimary socialization for a child is very important because it sets the ground work for all future socialization. Primary Socialization occurs when a child learns the attitudes, values, and actions appropriate to individuals as members of a particular culture. It is mainly influenced by the immediate family and friends. For example, if a child saw his/her mother expressing a discriminatory opinion about a minority group, then that child may think this behavior is acceptable and could continue to have this opinion about minority groups.\n\nSecondary socialization refers to the process of learning what is the appropriate behavior as a member of a smaller group within the larger society. Basically, is the behavioral patterns reinforced by socializing agents of society. Secondary socialization takes place outside the home. It is where children and adults learn how to act in a way that is appropriate for the situations they are in. Schools require very different behavior from the home, and children must act according to new rules. New teachers have to act in a way that is different from pupils and learn the new rules from people around them. Secondary socialization is usually associated with teenagers and adults, and involves smaller changes than those occurring in primary socialization. Such examples of secondary socialization are entering a new profession or relocating to a new environment or society.\n\nAnticipatory socialization refers to the processes of socialization in which a person \"rehearses\" for future positions, occupations, and social relationships. For example, a couple might move in together before getting married in order to try out, or anticipate, what living together will be like. Research by Kenneth J. Levine and Cynthia A. Hoffner suggests that parents are the main source of anticipatory socialization in regards to jobs and careers.\n\nResocialization refers to the process of discarding former behavior patterns and reflexes, accepting new ones as part of a transition in one's life. This occurs throughout the human life cycle. Resocialization can be an intense experience, with the individual experiencing a sharp break with his or her past, as well as a need to learn and be exposed to radically different norms and values. One common example involves resocialization through a total institution, or \"a setting in which people are isolated from the rest of society and manipulated by an administrative staff\". Resocialization via total institutions involves a two step process: 1) the staff work to root out a new inmate's individual identity & 2) the staff attempt to create for the inmate a new identity. Other examples of this are the experience of a young man or woman leaving home to join the military, or a religious convert internalizing the beliefs and rituals of a new faith. An extreme example would be the process by which a transsexual learns to function socially in a dramatically altered gender role.\n\nOrganizational socialization is the process whereby an employee learns the knowledge and skills necessary to assume his or her organizational role. As newcomers become socialized, they learn about the organization and its history, values, jargon, culture, and procedures. This acquired knowledge about new employees' future work environment affects the way they are able to apply their skills and abilities to their jobs. How actively engaged the employees are in pursuing knowledge affects their socialization process. They also learn about their work group, the specific people they work with on a daily basis, their own role in the organization, the skills needed to do their job, and both formal procedures and informal norms. Socialization functions as a control system in that newcomers learn to internalize and obey organizational values and practices.\n\nGroup socialization is the theory that an individual's peer groups, rather than parental figures, are the primary influence of personality and behavior in adulthood. Parental behavior and the home environment has either no effect on the social development of children, or the effect varies significantly between children. Adolescents spend more time with peers than with parents. Therefore, peer groups have stronger correlations with personality development than parental figures do. For example, twin brothers, whose genetic makeup are identical, will differ in personality because they have different groups of friends, not necessarily because their parents raised them differently. Behavioral genetics suggest that up to fifty percent of the variance in adult personality is due to genetic differences. The environment in which a child is raised accounts for only approximately ten percent in the variance of an adult's personality. As much as twenty percent of the variance is due to measurement error. This suggests that only a very small part of an adult's personality is influenced by factors parents control (i.e. the home environment). Harris claims that while it's true that siblings don't have identical experiences in the home environment (making it difficult to associate a definite figure to the variance of personality due to home environments), the variance found by current methods is so low that researchers should look elsewhere to try to account for the remaining variance. Harris also states that developing long-term personality characteristics away from the home environment would be evolutionarily beneficial because future success is more likely to depend on interactions with peers than interactions with parents and siblings. Also, because of already existing genetic similarities with parents, developing personalities outside of childhood home environments would further diversify individuals, increasing their evolutionary success.\n\nEntering high school is a crucial moment in many adolescent's lifespan involving the branching off from the restraints of their parents. When dealing with new life challenges, adolescents take comfort in discussing these issues within their peer groups instead of their parents. Peter Grier, staff writer of the Christian Science Monitor describes this occurrence as,\"Call it the benign side of peer pressure. Today's high-schoolers operate in groups that play the role of nag and nanny-in ways that are both beneficial and isolating.\"\n\nIndividuals and groups change their evaluations and commitments to each other over time. There is a predictable sequence of stages that occur in order for an individual to transition through a group; investigation, socialization, maintenance, resocialization, and remembrance. During each stage, the individual and the group evaluate each other which leads to an increase or decrease in commitment to socialization. This socialization pushes the individual from prospective, new, full, marginal, and ex member.\n\nStage 1: Investigation\nThis stage is marked by a cautious search for information. The individual compares groups in order to determine which one will fulfill their needs (\"reconnaissance\"), while the group estimates the value of the potential member (\"recruitment\"). The end of this stage is marked by entry to the group, whereby the group asks the individual to join and they accept the offer.\n\nStage 2: Socialization\nNow that the individual has moved from prospective member to new member, they must accept the group's culture. At this stage, the individual accepts the group's norms, values, and perspectives (\"assimilation\"), and the group adapts to fit the new member's needs (\"accommodation\"). The acceptance transition point is then reached and the individual becomes a full member. However, this transition can be delayed if the individual or the group reacts negatively. For example, the individual may react cautiously or misinterpret other members' reactions if they believe that they will be treated differently as a newcomer.\n\nStage 3: Maintenance\nDuring this stage, the individual and the group negotiate what contribution is expected of members (role negotiation). While many members remain in this stage until the end of their membership, some individuals are not satisfied with their role in the group or fail to meet the group's expectations (\"divergence\").\n\nStage 4: Resocialization\nIf the divergence point is reached, the former full member takes on the role of a marginal member and must be resocialized. There are two possible outcomes of resocialization: differences are resolved and the individual becomes a full member again (\"convergence\"), or the group expels the individual or the individual decides to leave (\"exit\").\n\nStage 5: Remembrance\nIn this stage, former members reminisce about their memories of the group, and make sense of their recent departure. If the group reaches a consensus on their reasons for departure, conclusions about the overall experience of the group become part of the group's \"tradition\".\n\nHenslin (1999:76) contends that \"an important part of socialization is the learning of culturally defined gender roles.\" Gender socialization refers to the learning of behavior and attitudes considered appropriate for a given sex. Boys learn to be boys and girls learn to be girls. This \"learning\" happens by way of many different agents of socialization. The behaviour that is seen to be appropriate for each gender is largely determined by societal, cultural and economic values in a given society. Gender socialization can therefore vary considerably among societies with different values. The family is certainly important in reinforcing gender roles, but so are groups including friends, peers, school, work and the mass media. Gender roles are reinforced through \"countless subtle and not so subtle ways\" (1999:76). In peer group activities, stereotypic gender roles may also be rejected, renegotiated or artfully exploited for a variety of purposes.\n\nCarol Gilligan compared the moral development of girls and boys in her theory of gender and moral development. She claimed (1982, 1990) that boys have a justice perspective meaning that they rely on formal rules to define right and wrong. Girls, on the other hand, have a care and responsibility perspective where personal relationships are considered when judging a situation. Gilligan also studied the effect of gender on self-esteem. She claimed that society's socialization of females is the reason why girls' self-esteem diminishes as they grow older. Girls struggle to regain their personal strength when moving through adolescence as they have fewer female teachers and most authority figures are men.\n\nAs parents are present in a child's life from the beginning, their influence in a child's early socialization is very important, especially in regards to gender roles. Sociologists have identified four ways in which parents socialize gender roles in their children: Shaping gender related attributes through toys and activities, differing their interaction with children based on the sex of the child, serving as primary gender models, and communicating gender ideals and expectations.\n\nSociologist of gender R.W. Connell contends that socialization theory is \"inadequate\" for explaining gender, because it presumes a largely consensual process except for a few \"deviants,\" when really most children revolt against pressures to be conventionally gendered; because it cannot explain contradictory \"scripts\" that come from different socialization agents in the same society, and because it does not account for conflict between the different levels of an individual's gender (and general) identity.\n\nRacial socialization has been defined as \"the developmental processes by which children acquire the behaviors, perceptions, values, and attitudes of an ethnic group, and come to see themselves and others as members of the group\". The existing literature conceptualizes racial socialization as having multiple dimensions. Researchers have identified five dimensions that commonly appear in the racial socialization literature: cultural socialization, preparation for bias, promotion of mistrust, egalitarianism, and other. Cultural socialization refers to parenting practices that teach children about their racial history or heritage and is sometimes referred to as pride development. Preparation for bias refers to parenting practices focused on preparing children to be aware of, and cope with, discrimination. Promotion of mistrust refers to the parenting practices of socializing children to be wary of people from other races. Egalitarianism refers to socializing children with the belief that all people are equal and should be treated with a common humanity.\n\nOppression socialization refers to the process by which \"individuals develop understandings of power and political structure, particularly as these inform perceptions of identity, power, and opportunity relative to gender, racialized group membership, and sexuality.\" This action is a form of political socialization in its relation to power and the persistent compliance of the disadvantaged with their oppression using limited \"overt coercion.\"\n\nBased on comparative research in different societies, focusing on the role of language in child development, linguistic anthropologists Elinor Ochs and Bambi Schieffelin have developed the theory of language socialization.\nThey discovered that the processes of enculturation and socialization do not occur apart from the process of language acquisition, but that children acquire language and culture together in what amounts to an integrated process. Members of all societies socialize children both \"to\" and \"through\" the use of language; acquiring competence in a language, the novice is by the same token socialized into the categories and norms of the culture, while the culture, in turn, provides the norms of the use of language.\n\nPlanned socialization occurs when other people take actions designed to teach or train others. This type of socialization can take on many forms and can occur at any point from infancy onward.\n\nNatural socialization occurs when infants and youngsters explore, play and discover the social world around them. Natural socialization is easily seen when looking at the young of almost any mammalian species (and some birds). Planned socialization is mostly a human phenomenon; all through history, people have been making plans for teaching or training others. Both natural and planned socialization can have good and bad qualities: it is useful to learn the best features of both natural and planned socialization in order to incorporate them into life in a meaningful way.\n\nPositive socialization is the type of social learning that is based on pleasurable and exciting experiences. We tend to like the people who fill our social learning processes with positive motivation, loving care, and rewarding opportunities. Positive socialization occurs when desirable behaviours are reinforced with a reward, encouraging the individual to continue exhibiting similar behaviours in the future.\n\nNegative socialization occurs when others use punishment, harsh criticisms or anger to try to \"teach us a lesson;\" and often we come to dislike both negative socialization and the people who impose it on us. There are all types of mixes of positive and negative socialization, and the more positive social learning experiences we have, the happier we tend to be—especially if we are able to learn useful information that helps us cope well with the challenges of life. A high ratio of negative to positive socialization can make a person unhappy, leading to defeated or pessimistic feelings about life.\n\nIn the social sciences, institutions are the structures and mechanisms of social order and cooperation governing the behavior of individuals within a given human collectivity. Institutions are identified with a social purpose and permanence, transcending individual human lives and intentions, and with the making and enforcing of rules governing cooperative human behavior.\n\nFrom the late 1980s, sociological and psychological theories have been connected with the term socialization. One example of this connection is the theory of Klaus Hurrelmann. In his book \"Social Structure and Personality Development\",<ref name= \"Hurrelmann 1989/2009\">Hurrelmann, Klaus (1989, reissued 2009). \"Social Structure and Personality Development\". Cambridge: Cambridge University Press</ref> he develops the model of \"productive processing of reality\". The core idea is that socialization refers to an individual's personality development. It is the result of the productive processing of interior and exterior realities. Bodily and mental qualities and traits constitute a person's inner reality; the circumstances of the social and physical environment embody the external reality. Reality processing is productive because human beings actively grapple with their lives and attempt to cope with the attendant developmental tasks. The success of such a process depends on the personal and social resources available. Incorporated within all developmental tasks is the necessity to reconcile personal individuation and social integration and so secure the \"I-dentity\". The process of productive processing of reality is an enduring process throughout the life course.\n\nThe problem of order or Hobbesian problem questions the existence of social orders and asks if it is possible to oppose them. Émile Durkheim viewed society as an external force controlling individuals through the imposition of sanctions and codes of law. However, constraints and sanctions also arise internally as feelings of guilt or anxiety. If conformity as an expression of the need for belonging, the process of socialization is not necessarily universal. Behavior may not be influenced by society at all, but instead be determined biologically. The behavioral sciences during the second half of the twentieth century were dominated by two contrasting models of human political behavior, homo economicus and cultural hegemony, collectively termed the standard social science model. The fields of sociobiology and evolutionary psychology developed in response notions such as dominance hierarchies, cultural group selection, and dual inheritance theory. Behavior is the result of a complex interaction between nature and nurture, or genes and culture. A focus on innate behavior at the expense of learning is termed undersocialization, while attributing behavior to learning when it is the result of evolution is termed oversocialization.\n\n"}
{"id": "150349", "url": "https://en.wikipedia.org/wiki?curid=150349", "title": "Subculture", "text": "Subculture\n\nA subculture is a group of people within a culture that differentiates itself from the parent culture to which it belongs, often maintaining some of its founding principles. Subcultures develop their own norms and values regarding cultural, political and sexual matters. Subcultures are part of society while keeping their specific characteristics intact. Examples of subcultures include hippies, goths and bikers. The concept of subcultures was developed in sociology and cultural studies. Subcultures differ from countercultures.\n\nWhile exact definitions vary, the \"Oxford English Dictionary\" defines a subculture as \"a cultural group within a larger culture, often having beliefs or interests at variance with those of the larger culture.\"\nAs early as 1950, David Riesman distinguished between a majority, \"which passively accepted commercially provided styles and meanings, and a 'subculture' which actively sought a minority style ... and interpreted it in accordance with subversive values\". In his 1979 book \"\", Dick Hebdige argued that a subculture is a subversion to normalcy. He wrote that subcultures can be perceived as negative due to their nature of criticism to the dominant societal standard. Hebdige argued that subcultures bring together like-minded individuals who feel neglected by societal standards and allow them to develop a sense of identity.\n\nIn 1995, Sarah Thornton, drawing on Pierre Bourdieu, described \"subcultural capital\" as the cultural knowledge and commodities acquired by members of a subculture, raising their status and helping differentiate themselves from members of other groups. In 2007, Ken Gelder proposed to distinguish subcultures from countercultures based on the level of immersion in society. Gelder further proposed six key ways in which subcultures can be identified through their:\n\nSociologists Gary Alan Fine and Sherryl Kleinman argued that their 1979 research showed that a subculture is a group that serves to motivate a potential member to adopt the artifacts, behaviors, norms, and values characteristic of the group.\n\nThe evolution of subcultural studies has three main steps:\n\nThe earliest subcultures studies came from the so-called Chicago School, who interpreted them as forms of deviance and delinquency. Starting with what they called Social Disorganization Theory, they claimed that subcultures emerged on one hand because of some population sectors’ lack of socialisation with the mainstream culture and, on the other, because of their adoption of alternative axiological and normative models. As Robert E. Park, Ernest Burgess and Louis Wirth suggested, by means of selection and segregation processes, there thus appear in society natural areas or moral regions where deviant models concentrate and are re-inforced; they do not accept objectives or means of action offered by the mainstream culture, proposing different ones in their place – thereby becoming, depending on circumstances, innovators, rebels or retreatists (Richard Cloward and Lloyd Ohlin). Subcultures, however, are not only the result of alternative action strategies but also of labelling processes on the basis of which, as Howard S. Becker explains, society defines them as outsiders. As Cohen clarifies, every subculture’s style, consisting of image, demeanour and language becomes its recognition trait. And an individual’s progressive adoption of a subcultural model will furnish him/her with growing status within this context but it will often, in tandem, deprive him/her of status in the broader social context outside where a different model prevails.\n\nIn the work of John Clarke, Stuart Hall, Tony Jefferson and Brian Roberts of the Birmingham CCCS (Centre for Contemporary Cultural Studies), subcultures are interpreted as forms of resistance. Society is seen as being divided into two fundamental classes, the working class and the middle class, each with its own class culture, and middle-class culture being dominant. Particularly in the working class, subcultures grow out of the presence of specific interests and affiliations around which cultural models spring up, in conflict with both their parent culture and mainstream culture. Facing a weakening of class identity, subcultures are then new forms of collective identification expressing what Cohen called symbolic resistance against the mainstream culture and developing imaginary solutions for structural problems. As Paul Willis and Dick Hebdige underline, identity and resistance are expressed through the development of a distinctive style which, by a re-signification and ‘bricolage’ operation, use cultural industry goods to communicate and express one’s own conflict. Yet the cultural industry is often capable of re-absorbing the components of such a style and once again transforming them into goods. At the same time the mass media, while they participate in building subcultures by broadcasting their images, also weaken them by depriving them of their subversive content or by spreading a stigmatized image of them.\n\nThe most recent interpretations see subcultures as forms of distinction. In an attempt to overcome the idea of subcultures as forms of deviance or resistance, they describe subcultures as collectivities which, on a cultural level, are sufficiently homogeneous internally and heterogeneous with respect to the outside world to be capable of developing, as Paul Hodkinson points out, consistent distinctiveness, identity, commitment and autonomy. Defined by Sarah Thornton as taste cultures, subcultures are endowed with elastic, porous borders, and are inserted into relationships of interaction and mingling, rather than independence and conflict, with the cultural industry and mass media, as Steve Redhead and David Muggleton emphasize. The very idea of a unique, internally homogeneous, dominant culture is explicitly criticized. Thus forms of individual involvement in subcultures are fluid and gradual, differentiated according to each actor’s investment, outside clear dichotomies. The ideas of different levels of subcultural capital (Sarah Thornton) possessed by each individual, of the supermarket of style (Ted Polhemus) and of style surfing (Martina Böse) replace that of the subculture’s insiders and outsiders – with the perspective of subcultures supplying resources for the construction of new identities going beyond strong, lasting identifications.\n\nThe study of subcultures often consists of the study of symbolism attached to clothing, music and other visible affectations by members of subcultures, and also of the ways in which these same symbols are interpreted by members of the dominant culture. Dick Hebdige writes that members of a subculture often signal their membership through a distinctive and symbolic use of style, which includes fashions, mannerisms and argot.\nSubcultures can exist at all levels of organizations, highlighting the fact that there are multiple cultures or value combinations usually evident in any one organization that can complement but also compete with the overall organisational culture. In some instances, subcultures have been legislated against, and their activities regulated or curtailed. British youth subcultures had been described as a moral problem that ought to be handled by the guardians of the dominant culture within the post-war consensus.\n\nIt may be difficult to identify certain subcultures because their style (particularly clothing and music) may be adopted by mass culture for commercial purposes. Businesses often seek to capitalize on the subversive allure of subcultures in search of \"Cool\", which remains valuable in the selling of any product. This process of cultural appropriation may often result in the death or evolution of the subculture, as its members adopt new styles that appear alien to mainstream society.\n\nMusic-based subcultures are particularly vulnerable to this process; what may be considered subcultures at one stage in their historiessuch as jazz, goth, punk, hip hop and rave culturesmay represent mainstream taste within a short period. Some subcultures reject or modify the importance of style, stressing membership through the adoption of an ideology which may be much more resistant to commercial exploitation. The punk subculture's distinctive (and initially shocking) style of clothing was adopted by mass-market fashion companies once the subculture became a media interest. Dick Hebdige argues that the punk subculture shares the same \"radical aesthetic practices\" as Dada and surrealism:\n\nLike Duchamp's 'ready mades' - manufactured objects which qualified as art because he chose to call them such, the most unremarkable and inappropriate items - a pin, a plastic clothes peg, a television component, a razor blade, a tampon - could be brought within the province of punk (un)fashion ... Objects borrowed from the most sordid of contexts found a place in punks' ensembles; lavatory chains were draped in graceful arcs across chests in plastic bin liners. Safety pins were taken out of their domestic 'utility' context and worn as gruesome ornaments through the cheek, ear or lip ... fragments of school uniform (white bri-nylon shirts, school ties) were symbolically defiled (the shirts covered in graffiti, or fake blood; the ties left undone) and juxtaposed against leather drains or shocking pink mohair tops.\n\nIn 1985, French sociologist Michel Maffesoli coined the term \"urban tribe\". It gained widespread use after the publication of his \"Le temps des tribus: le déclin de l'individualisme dans les sociétés postmodernes\" (1988). Eight years later, this book was published in the United Kingdom as \"The Time of the Tribes: The Decline of Individualism in Mass Society\".\n\nAccording to Maffesoli, urban tribes are microgroups of people who share common interests in urban areas. The members of these relatively small groups tend to have similar worldviews, dress styles and behavioral patterns. Their social interactions are largely informal and emotionally laden, different from late capitalism's corporate-bourgeoisie cultures, based on dispassionate logic. Maffesoli claims that punks are a typical example of an \"urban tribe\".\n\nFive years after the first English translation of \"Le temps des tribus\", writer Ethan Watters claims to have coined the same neologism in a \"New York Times Magazine\" article. This was later expanded upon the idea in his book \"Urban Tribes: A Generation Redefines Friendship, Family, and Commitment\". According to Watters, urban tribes are groups of never-marrieds between the ages of 25 and 45 who gather in common-interest groups and enjoy an urban lifestyle, which offers an alternative to traditional family structures.\n\nThe sexual revolution of the 1960s led to a countercultural rejection of the established sexual and gender norms, particularly in the urban areas of Europe, North and South America, Australia, and white South Africa. A more permissive social environment in these areas led to a proliferation of \"sexual subcultures\"—cultural expressions of non-normative sexuality. As with other subcultures, sexual subcultures adopted certain styles of fashion and gestures to distinguish them from the mainstream.\n\nHomosexuals expressed themselves through the gay culture, considered the largest sexual subculture of the 20th century. With the ever-increasing acceptance of homosexuality in the early 21st century, including its expressions in fashion, music, and design, the gay culture can no longer be considered a subculture in many parts of the world, although some aspects of gay culture like leathermen, bears, and feeders are considered subcultures within the gay movement itself. The butch and femme identities or roles among some lesbians also engender their own subculture with stereotypical attire, for instance drag kings. A late 1980s development, the queer movement can be considered a subculture broadly encompassing those that reject normativity in sexual behavior, and who celebrate visibility and activism. The wider movement coincided with growing academic interests in queer studies and queer theory. Aspects of sexual subcultures can vary along other cultural lines. For instance, in the United States, down-low refers to African-American men who do not identify themselves with the gay or queer cultures, but who practice gay cruising, and adopt a specific hip-hop attire during this activity.\n\nIn a 2011 study, Brady Robards and Andy Bennett said that online identity expression has been interpreted as exhibiting subcultural qualities. However, they argue it is more in line with neotribalism than with what is often classified as subculture. Social networking websites are quickly becoming the most used form of communication and means to distribute information and news. They offer a way for people with similar backgrounds, lifestyles, professions or hobbies to connect. According to a co-founder and executive creative strategist for RE-UP, as technology becomes a \"life force,\" subcultures become the main bone of contention for brands as networks rise through cultural mash-ups and phenomenons. Where social media is concerned, there seems to be a growing interest among media producers to use subcultures for branding. This is seen most actively on social network sites with user-generated content, such as YouTube.\n\nSocial media expert Scott Huntington cites one of the ways in which subcultures have been and can be successfully targeted to generate revenue: \"It’s common to assume that subcultures aren’t a major market for most companies. Online apps for shopping, however, have made significant strides. Take Etsy, for example. It only allow vendors to sell handmade or vintage items, both of which can be considered a rather \"hipster\" subculture. However, retailers on the site made almost $900 million in sales.\"\n\n\n"}
{"id": "24702", "url": "https://en.wikipedia.org/wiki?curid=24702", "title": "Peace", "text": "Peace\n\nPeace is a concept of societal friendship and harmony in the absence of hostility and violence. In a social sense, peace is commonly used to mean a lack of conflict (such as war) and freedom from fear of violence between individuals or groups. Throughout history leaders have used peacemaking and diplomacy to establish a certain type of behavioral restraint that has resulted in the establishment of regional peace or economic growth through various forms of agreements or peace treaties. Such behavioral restraint has often resulted in the reduction of conflicts, greater economic interactivity, and consequently substantial prosperity.\n\n\"Psychological peace\" (such as a peaceful thinking and emotions) is perhaps less well defined yet often a necessary precursor to establishing \"behavioral peace.\" Peaceful behavior sometimes results from a \"peaceful inner disposition.\" Some have expressed the belief that peace can be initiated with a certain quality of inner tranquility that does not depend upon the uncertainties of daily life for its existence. The acquisition of such a \"peaceful internal disposition\" for oneself and others can contribute to resolving of otherwise seemingly irreconcilable competing interests.\n\nThe Anglo-French term \"Pes\" itself comes from the Latin \"pax\", meaning \"peace, compact, agreement, treaty of peace, tranquility, absence of hostility, harmony.\" The English word came into use in various personal greetings from c.1300 as a translation of the Hebrew word shalom, which, according to Jewish theology, comes from a Hebrew verb meaning 'to be complete, whole'. Although 'peace' is the usual translation, however, it is an incomplete one, because 'shalom,' which is also cognate with the Arabic \"salaam\", has multiple other meanings in addition to peace, including justice, good health, safety, well-being, prosperity, equity, security, good fortune, and friendliness, as well as simply the greetings, \"hello\" and \"goodbye\". At a personal level, peaceful behaviors are kind, considerate, respectful, just, and tolerant of others' beliefs and behaviors – tending to manifest goodwill. The term-'peace' originates most recently from the Anglo-French \"pes,\" and the Old French \"pais\", meaning \"peace, reconciliation, silence, agreement\" (11th century).\n\nThis latter understanding of peace can also pertain to an individual's introspective sense or concept of her/himself, as in being \"at peace\" in one's own mind, as found in European references from c.1200. The early English term is also used in the sense of \"quiet\", reflecting calm, serene, and meditative approaches to family or group relationships that avoid quarreling and seek tranquility — an absence of disturbance or agitation.\n\nIn many languages, the word for peace is also used as a greeting or a farewell, for example the Hawaiian word aloha, as well as the Arabic word \"salaam\". In English the word peace is occasionally used as a farewell, especially for the dead, as in the phrase \"rest in peace\".\n\nWolfgang Dietrich in his research project which led to the book \"The Palgrave International Handbook of Peace Studies\" (2011) maps the different meanings of peace in different languages and from different regions across the world. Later, in his \"Interpretations of Peace in History and Culture\" (2012), he groups the different meanings of peace into five peace families: Energetic/Harmony, Moral/Justice, Modern/Security, Postmodern/Truth, and Transrational, a synthesis of the positive sides of the four previous families and the society.\n\nIn ancient times and more recently, peaceful alliances between different nations were codified through royal marriages. Two examples, Hermodike I c.800BC and Hermodike II c.600BC were Greek princesses from the house of Agamemnon who married kings from what is now Central Turkey. The union of Phrygia / Lydia with Aeolian Greeks resulted in regional peace, which facilitated the transfer of ground-breaking technological skills into Ancient Greece; respectively, the phonetic written script and the minting of coinage (to use a token currency, where the value is guaranteed by the state). Both inventions were rapidly adopted by surrounding nations through further trade and cooperation and have been of fundamental benefit to the progress of civilization.\n\nSince classical times, it has been noted that peace has sometimes been achieved by the victor over the vanquished by the imposition of ruthless measures. In his book \"Agricola\" the Roman historian Tacitus includes eloquent and vicious polemics against the rapacity and greed of Rome. One, that Tacitus says is by the Caledonian chieftain Calgacus, ends \"Auferre trucidare rapere falsis nominibus imperium, atque ubi solitudinem faciunt, pacem appellant.\" (To ravage, to slaughter, to usurp under false titles, they call empire; and where they make a desert, they call it peace. — Oxford Revised Translation).\n\nDiscussion of peace is therefore at the same time a discussion on the form of such peace. Is it simple absence of mass organized killing (war) or does peace require a particular morality and justice? (\"just peace\").\nA peace must be seen at least in two forms:\n\nMore recently, advocates for radical reform in justice systems have called for a public policy adoption of non-punitive, non-violent Restorative Justice methods, and many of those studying the success of these methods, including a United Nations working group on Restorative Justice, have attempted to re-define justice in terms related to peace. From the late 2000s on, a Theory of Active Peace has been proposed which conceptually integrates justice into a larger peace theory.\n\nThe United Nations (UN) is an international organization whose stated aims are to facilitate cooperation in international law, international security, economic development, social progress, human rights, and achieving world peace. The UN was founded in 1945 after World War II to replace the League of Nations, to stop wars between countries, and to provide a platform for dialogue.\n\nThe UN, after approval by the Security Council, sends peacekeepers to regions where armed conflict has recently ceased or paused to enforce the terms of peace agreements and to discourage combatants from resuming hostilities. Since the UN does not maintain its own military, peacekeeping forces are voluntarily provided by member states of the UN. The forces, also called the \"Blue Helmets\", who enforce UN accords are awarded United Nations Medals, which are considered international decorations instead of military decorations. The peacekeeping force as a whole received the Nobel Peace Prize in 1988.\n\nThe obligation of the state to provide for domestic peace within its borders in usually charged to the police and other general domestic policing activities. The police are a constituted body of persons empowered by a state to enforce the law, to protect the lives, liberty and possessions of citizens, and to prevent crime and civil disorder. Their powers include the power of arrest and the legitimized use of force. The term is most commonly associated with the police forces of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from the military and other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing. Police forces are usually public sector services, funded through taxes.\n\nIt is the obligation of national security to provide for peace and security in a nation against foreign threats and foreign aggression. Potential causes of national insecurity include actions by other states (e.g. military or cyber attack), violent non-state actors (e.g. terrorist attack), organised criminal groups such as narcotic cartels, and also the effects of natural disasters (e.g. flooding, earthquakes). Systemic drivers of insecurity, which may be transnational, include climate change, economic inequality and marginalisation, political exclusion, and militarisation. In view of the wide range of risks, the preservation of peace and the security of a nation state have several dimensions, including economic security, energy security, physical security, environmental security, food security, border security, and cyber security. These dimensions correlate closely with elements of national power.\n\nThe principal forerunner of the United Nations was the League of Nations. It was created at the Paris Peace Conference of 1919, and emerged from the advocacy of Woodrow Wilson and other idealists during World War I. The Covenant of the League of Nations was included in the Treaty of Versailles in 1919, and the League was based in Geneva until its dissolution as a result of World War II and replacement by the United Nations. The high hopes widely held for the League in the 1920s, for example amongst members of the League of Nations Union, gave way to widespread disillusion in the 1930s as the League struggled to respond to challenges from Nazi Germany, Fascist Italy, and Japan. \n\nOne of the most important scholars of the League of Nations was Sir Alfred Zimmern. Like many of the other British enthusiasts for the League, such as Gilbert Murray and Florence Stawell – the so-called \"Greece and peace\" set – he came to this from the study of the classics.\n\nThe creation of the League of Nations, and the hope for informed public opinion on international issues (expressed for example by the Union for Democratic Control during World War I), also saw the creation after World War I of bodies dedicated to understanding international affairs, such as the Council on Foreign Relations in New York and the Royal Institute of International Affairs at Chatham House in London. At the same time, the academic study of international relations started to professionalize, with the creation of the first professorship of international politics, named for Woodrow Wilson, at Aberystwyth, Wales, in 1919.\n\nThe late 19th century idealist advocacy of peace which led to the creation of the Nobel Peace Prize, the Rhodes Scholarships, the Carnegie Endowment for International Peace, and ultimately the League of Nations, also saw the re-emergence of the ancient Olympic ideal. Led by Pierre de Coubertin, this culminated in the holding in 1896 of the first of the modern Olympic Games.\n\nThe highest honour awarded to peace maker is the Nobel Prize in Peace, awarded since 1901 by the Norwegian Nobel Committee. It is awarded annually to internationally notable persons following the prize's creation in the will of Alfred Nobel. According to Nobel's will, the Peace Prize shall be awarded to the person who \"...shall have done the most or the best work for fraternity between nations, for the abolition or reduction of standing armies and for the holding and promotion of peace congresses.\"\n\nIn creating the Rhodes Scholarships for outstanding students from the United States, Germany and much of the British Empire, Cecil Rhodes wrote in 1901 that 'the object is that an understanding between the three great powers will render war impossible and educational relations make the strongest tie'. This peace purpose of the Rhodes Scholarships was very prominent in the first half of the 20th century, and became prominent again in recent years under Warden of the Rhodes House Donald Markwell, a historian of thought about the causes of war and peace. This vision greatly influenced Senator J. William Fulbright in the goal of the Fulbright fellowships to promote international understanding and peace, and has guided many other international fellowship programs, including the Schwarzman Scholars to China created by Stephen A. Schwarzman in 2013.\n\nThe International Gandhi Peace Prize, named after Mahatma Gandhi, is awarded annually by the Government of India. It is launched as a tribute to the ideals espoused by Gandhi in 1995 on the occasion of the 125th anniversary of his birth. This is an annual award given to individuals and institutions for their contributions towards social, economic and political transformation through non-violence and other Gandhian methods. The award carries Rs. 10 million in cash, convertible in any currency in the world, a plaque and a citation. It is open to all persons regardless of nationality, race, creed or sex.\n\nThe Student Peace Prize is awarded biennially to a student or a student organization that has made a significant contribution to promoting peace and human rights.\n\nThe Culture of Peace News Network, otherwise known simply as CPNN, is a UN authorized interactive online news network, committed to supporting the global movement for a culture of peace.\n\nEvery year in the first week of November, the Sydney Peace Foundation presents the Sydney Peace Prize. The Sydney Peace Prize is awarded to an organization or an individual whose life and work has demonstrated significant contributions to:\nThe achievement of peace with justice locally, nationally or internationally\nThe promotion and attainment of human rights\nThe philosophy, language and practice of non-violence\n\nA peace museum is a museum that documents historical peace initiatives. Many peace museums also provide advocacy programs for nonviolent conflict resolution. This may include conflicts at the personal, regional or international level.\n\nSmaller institutions:\n\nReligious beliefs often seek to identify and address the basic problems of human life, including the conflicts between, among, and within persons and societies. In ancient Greek-speaking areas the virtue of peace was personified as the goddess Eirene, and in Latin-speaking areas as the goddess Pax. Her image was typically represented by ancient sculptors as that of a full-grown woman, usually with a horn of plenty and scepter and sometimes with a torch or olive leaves.\n\nChristians, who believe Jesus of Nazareth to be the Jewish Messiah called Christ (meaning Anointed One), interpret Isaiah 9:6 as a messianic prophecy of Jesus in which he is called the \"Prince of Peace.\" In the Gospel of Luke, Zechariah celebrates his son John: And you, child, will be called prophet of the Most High, for you will go before the Lord to prepare his ways, to give his people knowledge of salvation through the forgiveness of their sins, because of the tender mercy of our God by which the daybreak from on high will visit us to shine on those who sit in darkness and death's shadow, to guide our feet into the path of peace.\n\nNumerous pontifical documents on the Holy Rosary document a continuity of views of the Popes to have confidence in the Holy Rosary as a means to foster peace. Subsequently, to the Encyclical Mense maio,1965, in which he urged the practice of the Holy Rosary, \"the prayer so dear to the Virgin and so much recommended by the Supreme Pontiffs,\" and as reaffirmed in the encyclical Christi Matri, 1966, to implore peace, Pope Paul VI stated in the apostolic Recurrens mensis, October 1969, that the Rosary is a prayer that favors the great gift of peace.\n\nIslam derived from the root word salam which literally means peace. Muslims are called followers of Islam. Quran clearly stated \"Those who have believed and whose hearts are assured by the remembrance of Allah. Unquestionably, by the remembrance of Allah, hearts are assured\" and stated \"O you who have believed, when you are told, \"Space yourselves\" in assemblies, then make space; Allah will make space for you. And when you are told, \"Arise,\" then arise; Allah will raise those who have believed among you and those who were given knowledge, by degrees. And Allah is Acquainted with what you do.\"\n\nBuddhists believe that peace can be attained once all suffering ends. They regard all suffering as stemming from cravings (in the extreme, greed), aversions (fears), or delusions. To eliminate such suffering and achieve personal peace, followers in the path of the Buddha adhere to a set of teachings called the Four Noble Truths — a central tenet in Buddhist philosophy.\n\nHindu texts contain the following passages:\n\nPacifism is the categorical opposition to the behaviors of war or violence as a means of settling disputes or of gaining advantage. Pacifism covers a spectrum of views ranging from the belief that international disputes can and should all be resolved via peaceful behaviors; to calls for the abolition of various organizations which tend to institutionalize aggressive behaviors, such as the military, or arms manufacturers; to opposition to any organization of society that might rely in any way upon governmental force. Such groups which sometimes oppose the governmental use of force include anarchists and libertarians. Absolute pacifism opposes violent behavior under all circumstance, including defense of self and others.\n\nPacifism may be based on moral principles (a deontological view) or pragmatism (a consequentialist view). Principled pacifism holds that all forms of violent behavior are inappropriate responses to conflict, and are morally wrong. Pragmatic pacifism holds that the costs of war and inter-personal violence are so substantial that better ways of resolving disputes must be found.\n\nPsychological or inner peace (i.e. peace of mind) refers to a state of being internally or spiritually at peace, with sufficient knowledge and understanding to keep oneself calm in the face of apparent discord or stress. Being internally \"at peace\" is considered by many to be a healthy mental state, or homeostasis and to be the opposite of feeling stressful, mentally anxious, or emotionally unstable. Within the meditative traditions, the psychological or inward achievement of \"peace of mind\" is often associated with bliss and happiness.\n\nPeace of mind, serenity, and calmness are descriptions of a disposition free from the effects of stress. In some meditative traditions, inner peace is believed to be a state of consciousness or enlightenment that may be cultivated by various types of meditation, prayer, t'ai chi ch'uan (太极拳, tàijíquán), yoga, or other various types of mental or physical disciplines. Many such practices refer to this peace as an experience of knowing oneself. An emphasis on finding one's inner peace is often associated with traditions such as Buddhism, Hinduism, and some traditional Christian contemplative practices such as monasticism, as well as with the New Age movement.\n\nSatyagraha is a philosophy and practice of nonviolent resistance developed by Mohandas Karamchand Gandhi. He deployed satyagraha techniques in campaigns for Indian independence and also during his earlier struggles in South Africa.\n\nThe word \"satyagraha\" itself was coined through a public contest that Gandhi sponsored through the newspaper he published in South Africa, 'Indian Opinion', when he realized that neither the common, contemporary Hindu language nor the English language contained a word which fully expressed his own meanings and intentions when he talked about his nonviolent approaches to conflict. According to Gandhi's autobiography, the contest winner was Maganlal Gandhi (presumably no relation), who submitted the entry 'sadagraha', which Gandhi then modified to 'satyagraha'. Etymologically, this Hindic word means 'truth-firmness', and is commonly translated as 'steadfastness in the truth' or 'truth-force'.\n\nSatyagraha theory also influenced Martin Luther King Jr. during the campaigns he led during the civil rights movement in the United States. The theory of satyagraha sees means and ends as inseparable. Therefore, it is contradictory to try to use violence to obtain peace. As Gandhi wrote: \"They say, 'means are, after all, means'. I would say, 'means are, after all, everything'. As the means so the end...\" A contemporary quote sometimes attributed to Gandhi, but also to A. J. Muste, sums it up: 'There is no way to peace; peace is the way.'\n\nThe following are monuments to peace:\n\nMany different theories of \"peace\" exist in the world of peace studies, which involves the study of de-escalation, conflict transformation, disarmament, and cessation of violence. The definition of \"peace\" can vary with religion, culture, or subject of study.\n\nThe classical \"realist\" position is that the key to promoting order between states, and so of increasing the chances of peace, is the maintenance of a balance of power between states – a situation where no state is so dominant that it can \"lay down the law to the rest\". Exponents of this view have included Metternich, Bismarck, Hans Morgenthau, and Henry Kissinger. A related approach – more in the tradition of Hugo Grotius than Thomas Hobbes – was articulated by the so-called \"English school of international relations theory\" such as Martin Wight in his book \"Power Politics\" (1946, 1978) and Hedley Bull in \"The Anarchical Society\" (1977).\n\nAs the maintenance of a balance of power could in some circumstances require a willingness to go to war, some critics saw the idea of a balance of power as promoting war rather than promoting peace. This was a radical critique of those supporters of the Allied and Associated Powers who justified entry into World War I on the grounds that it was necessary to preserve the balance of power in Europe from a German bid for hegemony.\n\nIn the second half of the 20th century, and especially during the cold war, a particular form of balance of power – mutual nuclear deterrence – emerged as a widely held doctrine on the key to peace between the great powers. Critics argued that the development of nuclear stockpiles increased the chances of war rather than peace, and that the \"nuclear umbrella\" made it \"safe\" for smaller wars (e.g. the Vietnam war and the Soviet invasion of Czechoslovakia to end the Prague Spring), so making such wars more likely.\n\nIt was a central tenet of classical liberalism, for example among English liberal thinkers of the late 19th and early 20th century, that free trade promoted peace. For example, the Cambridge economist John Maynard Keynes (1883–1946) said that he was \"brought up\" on this idea and held it unquestioned until at least the 1920s. During the economic globalization in the decades leading up to World War I, writers such as Norman Angell argued that the growth of economic interdependence between the great powers made war between them futile and therefore unlikely. He made this argument in 1913. A year later Europe's economically interconnected states were embroiled in what would later become known as the First World War.\n\nThese ideas have again come to prominence among liberal internationalists during the globalization of the late 20th and early 21st century. These ideas have seen capitalism as consistent with, even conducive to, peace.\n\nThe \"Peace & War Game\" is an approach in game theory to understand the relationship between peace and conflicts.\n\nThe iterated game hypotheses was originally used by academic groups and computer simulations to study possible strategies of cooperation and aggression.\n\nAs peace makers became richer over time, it became clear that making war had greater costs than initially anticipated. One of the well studied strategies that acquired wealth more rapidly was based on Genghis Khan, i.e. a constant aggressor making war continually to gain resources. This led, in contrast, to the development of what's known as the \"provokable nice guy strategy\", a peace-maker until attacked, improved upon merely to win by occasional forgiveness even when attacked. By adding the results of all pairwise games for each player, one sees that multiple players gain wealth cooperating with each other while bleeding a constantly aggressive player.\n\nSocialist, communist, and left-wing liberal writers of the 19th and 20th centuries (e.g., Lenin, J.A. Hobson, John Strachey) argued that capitalism caused war (e.g. through promoting imperial or other economic rivalries that lead to international conflict). This led some to argue that international socialism was the key to peace.\n\nHowever, in response to such writers in the 1930s who argued that capitalism caused war, the economist John Maynard Keynes (1883–1946) argued that managed capitalism could promote peace. This involved international coordination of fiscal/monetary policies, an international monetary system that did not pit the interests of countries against each other, and a high degree of freedom of trade. These ideas underlay Keynes's work during World War II that led to the creation of the International Monetary Fund and the World Bank at Bretton Woods in 1944, and later of the General Agreement on Tariffs and Trade (subsequently the World Trade Organization).\n\nBorrowing from the teachings of Norwegian theorist Johan Galtung, one of the pioneers of the field of Peace Research, on 'Positive Peace', and on the writings of Maine Quaker Gray Cox, a consortium of theorists, activists, and practitioners in the experimental John Woolman College initiative have arrived at a theory of \"active peace\". This theory posits in part that peace is part of a triad, which also includes justice and wholeness (or well-being), an interpretation consonant with scriptural scholarly interpretations of the meaning of the early Hebrew word \"shalom\". Furthermore, the consortium have integrated Galtung's teaching of the meanings of the terms peacemaking, peacekeeping, and peacebuilding, to also fit into a triadic and interdependent formulation or structure. Vermont Quaker John V. Wilmerding posits five stages of growth applicable to individuals, communities, and societies, whereby one transcends first the 'surface' awareness that most people have of these kinds of issues, emerging successively into acquiescence, pacifism, passive resistance, active resistance, and finally into \"active peace\", dedicating themselves to peacemaking, peacekeeping or peace building.\n\nOne of the most influential theories of peace, especially since Woodrow Wilson led the creation of the League of Nations at the Paris Peace Conference of 1919, is that peace will be advanced if the intentional anarchy of states is replaced through the growth of international law promoted and enforced through international organizations such as the League of Nations, the United Nations, and other functional international organizations. One of the most important early exponents of this view was Sir Alfred Zimmern, for example in his 1936 book \"The League of Nations and the Rule of Law\".\n\nMany \"idealist\" thinkers about international relations – e.g. in the traditions of Kant and Karl Marx – have argued that the key to peace is the growth of some form of solidarity between peoples (or classes of people) spanning the lines of cleavage between nations or states that lead to war.\n\nOne version of this is the idea of promoting international understanding between nations through the international mobility of students – an idea most powerfully advanced by Cecil Rhodes in the creation of the Rhodes Scholarships, and his successors such as J. William Fulbright.\n\nAnother theory is that peace can be developed among countries on the basis of active management of water resources.\n\nFollowing Wolfgang Dietrich, Wolfgang Sützl and the Innsbruck School of Peace Studies, some peace thinkers have abandoned any single and all-encompassing definition of peace. Rather, they promote the idea of \"many peaces\". They argue that since no singular, correct definition of peace can exist, peace should be perceived as a plurality. This post-modern understanding of peace(s) was based on the philosophy of Jean Francois Lyotard. It served as a fundament for the more recent concept of trans-rational peace(s) and elicitive conflict transformation.\n\nIn 2008 Dietrich enlarged his approach of the \"many peaces\" to the so-called \"five families\" of peace interpretations: the energetic, moral, modern, post-modern and trans-rational approach. Trans-rationality unites the rational and mechanistic understanding of modern peace in a relational and culture-based manner with spiritual narratives and energetic interpretations. The systemic understanding of trans-rational peaces advocates a client-centred method of conflict transformation, the so-called elicitive approach.\n\n\n\"Peace and conflict studies\" is an academic field which identifies and analyses violent and nonviolent behaviours, as well as the structural mechanisms attending violent and non-violent social conflicts. This is to better understand the processes leading to a more desirable human condition. One variation,\n\"Peace studies\" (irenology), is an interdisciplinary effort aiming at the prevention, de-escalation, and solution of conflicts. This contrasts with war studies (polemology), directed at the efficient attainment of victory in conflicts. Disciplines involved may include political science, geography, economics, psychology, sociology, international relations, history, anthropology, religious studies, and gender studies, as well as a variety of other disciplines.\n\nAlthough peace is widely perceived as something intangible, various organizations have been making efforts to quantify and measure it. The Global Peace Index produced by the Institute for Economics and Peace is a known effort to evaluate peacefulness in countries based on 23 indicators of the absence of violence and absence of the fear of violence.\n\nThe last edition of the Index ranks 163 countries on their internal and external levels of peace. According to the 2017 Global Peace Index, Iceland is the most peaceful country in the world while Syria is the least peaceful one. Fragile States Index (formerly known as the Failed States Index) created by the Fund for Peace focuses on risk for instability or violence in 178 nations. This index measures how fragile a state is by 12 indicators and subindicators that evaluate aspects of politics, social economy, and military facets in countries. The 2015 Failed State Index reports that the most fragile nation is South Sudan, and the least fragile one is Finland. University of Maryland publishes the Peace and Conflict Instability Ledger in order to measure peace. It grades 163 countries with 5 indicators, and pays the most attention to risk of political instability or armed conflict over a three-year period. The most recent ledger shows that the most peaceful country is Slovenia on the contrary Afghanistan is the most conflicted nation. Besides indicated above reports from the Institute for Economics and Peace, Fund for Peace, and University of Maryland, other organizations including George Mason University release indexes that rank countries in terms of peacefulness.\n\nThe longest continuing period of neutrality among currently existing states is observed in Switzerland, which has had an official policy of neutrality since 1815. This was made possible partly by the periods of relative peace in Europe and the world known as Pax Britannica (1815–1914), Pax Europaea/Pax Americana (since 1950s), and Pax Atomica (also since the 1950s).\n\nOther examples of long periods of peace are:\n\n"}
{"id": "33158", "url": "https://en.wikipedia.org/wiki?curid=33158", "title": "War", "text": "War\n\nWar is intense armed conflict between states, governments, societies, or paramilitary groups such as mercenaries, insurgents and militias. It is generally characterized by extreme violence, aggression, destruction, and mortality, using regular or irregular military forces. Warfare refers to the common activities and characteristics of types of war, or of wars in general. Total war is warfare that is not restricted to purely legitimate military targets, and can result in massive civilian or other non-combatant suffering and casualties.\n\nThe scholarly study of war is sometimes called polemology ( ), from the Greek \"polemos\", meaning \"war\", and \"-logy\", meaning \"the study of\".\n\nWhile some scholars see war as a universal and ancestral aspect of human nature, others argue it is a result of specific socio-cultural, economic or ecological circumstances.\n\nThe English word \"war\" derives from the 11th-century Old English words \"wyrre\" and \"werre\", from Old French \"werre\" (also \"guerre\" as in modern French), in turn from the Frankish *\"werra\", ultimately deriving from the Proto-Germanic *\"werzō\" 'mixture, confusion'. The word is related to the Old Saxon \"werran\", Old High German \"werran\", and the German \"verwirren\", meaning “to confuse”, “to perplex”, and “to bring into confusion”.\n\n\nThe earliest evidence for prehistoric warfare belongs to the Mesolithic cemetery Site 117, which has been determined to be approximately 14,000 years old. About forty-five percent of the skeletons there displayed signs of violent death. Since the rise of the state some 5,000 years ago, military activity has occurred over much of the globe. The advent of gunpowder and the acceleration of technological advances led to modern warfare. According to Conway W. Henderson, \"One source claims that 14,500 wars have taken place between 3500 BC and the late 20th century, costing 3.5 billion lives, leaving only 300 years of peace (Beer 1981: 20).\" An unfavorable review of this estimate mentions the following regarding one of the proponents of this estimate: \"In addition, perhaps feeling that the war casualties figure was improbably high, he changed \"approximately 3,640,000,000 human beings have been killed by war or the diseases produced by war\" to \"approximately 1,240,000,000 human beings...&c.\"\" The lower figure is more plausible, but could also be on the high side, considering that the 100 deadliest acts of mass violence between 480 BC and 2002 AD (wars and other man-made disasters with at least 300,000 and up to 66 million victims) claimed about 455 million human lives in total. Primitive warfare is estimated to have accounted for 15.1 % of deaths and claimed 400 million victims. Added to the aforementioned (and perhaps too high) figure of 1,240 million between 3500 BC and the late 20th century, this would mean a total of 1,640,000,000 people killed by war (including deaths from famine and disease caused by war) throughout the history and pre-history of mankind. For comparison, an estimated 1,680,000,000 people died from infectious diseases in the 20th century. Nuclear warfare breaking out in August 1988, when nuclear arsenals were at peak level, and the aftermath thereof, could have reduced human population from 5,150,000,000 by 1,850,000,000 to 3,300,000,000 within a period of about one year, according to a projection that did not consider \"the most severe predictions concerning nuclear winter\". This would have been a proportional reduction of the world’s population exceeding the reduction caused in the 14th century by the Black Death, and comparable in proportional terms with the plague’s impact on Europe's population in 1346–53.\n\nIn \"War Before Civilization\", Lawrence H. Keeley, a professor at the University of Illinois, says approximately 90–95% of known societies throughout history engaged in at least occasional warfare, and many fought constantly.\n\nKeeley describes several styles of primitive combat such as small raids, large raids, and massacres. All of these forms of warfare were used by primitive societies, a finding supported by other researchers. Keeley explains that early war raids were not well organized, as the participants did not have any formal training. Scarcity of resources meant defensive works were not a cost-effective way to protect the society against enemy raids.\n\nWilliam Rubinstein wrote \"Pre-literate societies, even those organised in a relatively advanced way, were renowned for their studied cruelty...'archaeology yields evidence of prehistoric massacres more severe than any recounted in ethnography [i.e., after the coming of the Europeans].'\"\nIn Western Europe, since the late 18th century, more than 150 conflicts and about 600 battles have taken place. During the 20th century, war resulted in a dramatic intensification of the pace of social changes, and was a crucial catalyst for the emergence of the Left as a force to be reckoned with.\nIn 1947, in view of the rapidly increasingly destructive consequences of modern warfare, and with a particular concern for the consequences and costs of the newly developed atom bomb, Albert Einstein famously stated, \"I know not with what weapons World War III will be fought, but World War IV will be fought with sticks and stones.\"\n\nMao Zedong urged the socialist camp not to fear nuclear war with the United States since, even if \"half of mankind died, the other half would remain while imperialism would be razed to the ground and the whole world would become socialist.\"\n\nA distinctive feature since 1945 is the absence of wars between major powers--indeed the near absence of any traditional wars between established countries. The major exceptions were the Indo-Pakistani War of 1971, the Iran–Iraq War 1980-1988, and the Gulf War of 1990-91. Instead actual fighting has largely been a matter of civil wars and insurgencies.\n\nThe Human Security Report 2005 documented a significant decline in the number and severity of armed conflicts since the end of the Cold War in the early 1990s. However, the evidence examined in the 2008 edition of the Center for International Development and Conflict Management's \"Peace and Conflict\" study indicated the overall decline in conflicts had stalled.\n\nThroughout the course of human history, the average number of people dying from war has fluctuated relatively little, being about 1 to 10 people dying per 100,000. However, major wars over shorter periods have resulted in much higher casualty rates, with 100-200 casualties per 100,000 over a few years. While conventional wisdom holds that casualties have increased in recent times due to technological improvements in warfare, this is not generally true. For instance, the Thirty Years' War (1618-1648) had about the same number of casualties per capita as World War I, although it was higher during World War II (WWII). That said, overall the number of casualties from war has not significantly increased in recent times. Quite to the contrary, on a global scale the time since WWII has been unusually peaceful.\n\nThe deadliest war in history, in terms of the cumulative number of deaths since its start, is World War II, from 1939 to 1945, with 60–85 million deaths, followed by the Mongol conquests at up to 60 million. As concerns a belligerent's losses in proportion to its prewar population, the most destructive war in modern history may have been the Paraguayan War (see Paraguayan War casualties). In 2013 war resulted in 31,000 deaths, down from 72,000 deaths in 1990. In 2003, Richard Smalley identified war as the sixth biggest problem (out of ten) facing humanity for the next fifty years. War usually results in significant deterioration of infrastructure and the ecosystem, a decrease in social spending, famine, large-scale emigration from the war zone, and often the mistreatment of prisoners of war or civilians. For instance, of the nine million people who were on the territory of the Byelorussian SSR in 1941, some 1.6 million were killed by the Germans in actions away from battlefields, including about 700,000 prisoners of war, 500,000 Jews, and 320,000 people counted as partisans (the vast majority of whom were unarmed civilians). Another byproduct of some wars is the prevalence of propaganda by some or all parties in the conflict, and increased revenues by weapons manufacturers.\n\nThree of the ten most costly wars, in terms of loss of life, have been waged in the last century. These are the two World Wars, followed by the Second Sino-Japanese War (which is sometimes considered part of World War II, or as overlapping). Most of the others involved China or neighboring peoples. The death toll of World War II, being over 60 million, surpasses all other war-death-tolls.\nMilitary personnel subject to combat in war often suffer mental and physical injuries, including depression, posttraumatic stress disorder, disease, injury, and death.\n\nDuring World War II, research conducted by US Army Brigadier General S.L.A. Marshall found, on average, 15% to 20% of American riflemen in WWII combat fired at the enemy. In Civil War Collector’s Encyclopedia, F.A. Lord notes that of the 27,574 discarded muskets found on the Gettysburg battlefield, nearly 90% were loaded, with 12,000 loaded more than once and 6,000 loaded 3 to 10 times. These studies suggest most military personnel resist firing their weapons in combat, that – as some theorists argue – human beings have an inherent resistance to killing their fellow human beings. Swank and Marchand’s WWII study found that after sixty days of continuous combat, 98% of all surviving military personnel will become psychiatric casualties. Psychiatric casualties manifest themselves in fatigue cases, confusional states, conversion hysteria, anxiety, obsessional and compulsive states, and character disorders.\n\nAdditionally, it has been estimated anywhere from 18% to 54% of Vietnam war veterans suffered from posttraumatic stress disorder.\n\nBased on 1860 census figures, 8% of all white American males aged 13 to 43 died in the American Civil War, including about 6% in the North and approximately 18% in the South. The war remains the deadliest conflict in American history, resulting in the deaths of 620,000 military personnel. United States military casualties of war since 1775 have totaled over two million. Of the 60 million European military personnel who were mobilized in World War I, 8 million were killed, 7 million were permanently disabled, and 15 million were seriously injured.\nDuring Napoleon's retreat from Moscow, more French military personnel died of typhus than were killed by the Russians. Of the 450,000 soldiers who crossed the Neman on 25 June 1812, less than 40,000 returned. More military personnel were killed from 1500–1914 by typhus than from military action. In addition, if it were not for modern medical advances there would be thousands more dead from disease and infection. For instance, during the Seven Years' War, the Royal Navy reported it conscripted 184,899 sailors, of whom 133,708 died of disease or were 'missing'.\n\nIt is estimated that between 1985 and 1994, 378,000 people per year died due to war.\n\nMost wars have resulted in significant loss of life, along with destruction of infrastructure and resources (which may lead to famine, disease, and death in the civilian population). During the Thirty Years' War in Europe, the population of the Holy Roman Empire was reduced by 15 to 40 percent. Civilians in war zones may also be subject to war atrocities such as genocide, while survivors may suffer the psychological aftereffects of witnessing the destruction of war.\n\nMost estimates of World War II casualties indicate around 60 million people died, 40 million of which were civilians. Deaths in the Soviet Union were around 27 million. Since a high proportion of those killed were young men who had not yet fathered any children, population growth in the postwar Soviet Union was much lower than it otherwise would have been.\n\nOnce a war has ended, losing nations are sometimes required to pay war reparations to the victorious nations. In certain cases, land is ceded to the victorious nations. For example, the territory of Alsace-Lorraine has been traded between France and Germany on three different occasions.\n\nTypically, war becomes intertwined with the economy and many wars are partially or entirely based on economic reasons. Some economists believe war can stimulate a country's economy (high government spending for World War II is often credited with bringing the U.S. out of the Great Depression by most Keynesian economists) but in many cases, such as the wars of Louis XIV, the Franco-Prussian War, and World War I, warfare primarily results in damage to the economy of the countries involved. For example, Russia's involvement in World War I took such a toll on the Russian economy that it almost collapsed and greatly contributed to the start of the Russian Revolution of 1917.\n\nWorld War II was the most financially costly conflict in history; its belligerents cumulatively spent about a trillion U.S. dollars on the war effort (as adjusted to 1940 prices).\nThe Great Depression of the 1930s ended as nations increased their production of war materials.\n\nBy the end of the war, 70% of European industrial infrastructure was destroyed. Property damage in the Soviet Union inflicted by the Axis invasion was estimated at a value of 679 billion rubles. The combined damage consisted of complete or partial destruction of 1,710 cities and towns, 70,000 villages/hamlets, 2,508 church buildings, 31,850 industrial establishments, of railroad, 4100 railroad stations, 40,000 hospitals, 84,000 schools, and 43,000 public libraries.\n\nWar leads to forced migration causing potentially large displacements of population. Among forced migrants there are usually relatively large shares of artists and other types of creative people, causing so the war effects to be particularly harmful for the country’s creative potential in the long-run. War also has a negative effect on an artists’ individual life-cycle output.\n\nIn war, cultural institutions, such as libraries, can become \"targets in themselves; their elimination was a way to denigrate and demoralize the enemy population.\" The impact such destruction can have on a society is important because \"in an era in which competing ideologies fuel internal and international conflict, the destruction of libraries and other items of cultural significance is neither random nor irrelevant. Preserving the world’s repositories of knowledge is crucial to ensuring that the darkest moments of history do not endlessly repeat themselves.\"\n\nEntities deliberately contemplating going to war and entities considering whether to end a war may formulate \"war aims\" as an evaluation/propaganda tool. War aims may stand as a proxy for national-military resolve.\n\nFried defines war aims as \"the desired territorial, economic, military or other benefits expected following successful conclusion of a war\".\n\nTangible/intangible aims:\n\nExplicit/implicit aims:\n\nPositive/negative aims:\n\nWar aims can change in the course of conflict and may eventually morph into \"peace conditions\" – the minimal conditions under which a state may cease to wage a particular war.\n\nReligious groups have long formally opposed or sought to limit war as in the Second Vatican Council document \"Gaudiem et Spes\": \"Any act of war aimed indiscriminately at the destruction of entire cities of extensive areas along with their population is a crime against God and man himself. It merits unequivocal and unhesitating condemnation.\"\n\nAnti-war movements have existed for every major war in the 20th century, including, most prominently, World War I, World War II, and the Vietnam War. In the 21st century, worldwide anti-war movements occurred in response to the United States invasion of Afghanistan and Iraq. Protests opposing the War in Afghanistan occurred in Europe, Asia, and the United States. Organizations like Stop the War Coalition, based in the United Kingdom, worked on campaigning against the war.\n\nThe Mexican Drug War, with estimated casualties of 40,000 since December 2006, has recently faced fundamental opposition. In 2011, the movement for peace and justice has started a popular middle-class movement against the war. It won the recognition of President Calderon, who began the war.\n\nThere are many theories about the motivations for war, but no consensus about which are most common. Carl von Clausewitz said, 'Every age has its own kind of war, its own limiting conditions, and its own peculiar preconceptions.'\n\nDutch psychoanalyst Joost Meerloo held that, \"War is often...a mass discharge of accumulated internal rage (where)...the inner fears of mankind are discharged in mass destruction.\" \n\nOther psychoanalysts such as E.F.M. Durban and John Bowlby have argued human beings are inherently violent. This aggressiveness is fueled by displacement and projection where a person transfers his or her grievances into bias and hatred against other races, religions, nations or ideologies. By this theory, the nation state preserves order in the local society while creating an outlet for aggression through warfare.\n\nThe Italian psychoanalyst Franco Fornari, a follower of Melanie Klein, thought war was the paranoid or projective “elaboration” of mourning. Fornari thought war and violence develop out of our “love need”: our wish to preserve and defend the sacred object to which we are attached, namely our early mother and our fusion with her. For the adult, nations are the sacred objects that generate warfare. Fornari focused upon sacrifice as the essence of war: the astonishing willingness of human beings to die for their country, to give over their bodies to their nation.\n\nDespite Fornari's theory that man's altruistic desire for self-sacrifice for a noble cause is a contributing factor towards war, few wars have originated from a desire for war among the general populace. Far more often the general population has been reluctantly drawn into war by its rulers. One psychological theory that looks at the leaders is advanced by Maurice Walsh. He argues the general populace is more neutral towards war and wars occur when leaders with a psychologically abnormal disregard for human life are placed into power. War is caused by leaders who seek war such as Napoleon and Hitler. Such leaders most often come to power in times of crisis when the populace opts for a decisive leader, who then leads the nation to war.\n\nSeveral theories concern the evolutionary origins of warfare. There are two main schools: One sees organized warfare as emerging in or after the Mesolithic as a result of complex social organization and greater population density and competition over resources; the other sees human warfare as a more ancient practice derived from common animal tendencies, such as territoriality and sexual competition.\n\nThe latter school argues that since warlike behavior patterns are found in many primate species such as chimpanzees, as well as in many ant species, group conflict may be a general feature of animal social behavior. Some proponents of the idea argue that war, while innate, has been intensified greatly by developments of technology and social organization such as weaponry and states.\n\nPsychologist and linguist Steven Pinker argued that war-related behaviors may have been naturally selected in the ancestral environment due to the benefits of victory. He also argued that in order to have credible deterrence against other groups (as well as on an individual level), it was important to have a reputation for retaliation, causing humans to develop instincts for revenge as well as for protecting a group's (or an individual's) reputation (\"honor\").\nCrofoot and Wrangham have argued that warfare, if defined as group interactions in which \"coalitions attempt to aggressively dominate or kill members of other groups\", is a characteristic of most human societies. Those in which it has been lacking \"tend to be societies that were politically dominated by their neighbors\".\n\nAshley Montagu strongly denied universalistic instinctual arguments, arguing that social factors and childhood socialization are important in determining the nature and presence of warfare. Thus, he argues, warfare is not a universal human occurrence and appears to have been a historical invention, associated with certain types of human societies. Montagu's argument is supported by ethnographic research conducted in societies where the concept of aggression seems to be entirely absent, e.g. the Chewong and Semai of the Malay peninsula. Bobbi S. Low has observed correlation between warfare and education, noting societies where warfare is commonplace encourage their children to be more aggressive.\n\nWar can be seen as a growth of economic competition in a competitive international system. In this view wars begin as a pursuit of markets for natural resources and for wealth. War has also been linked to economic development by economic historians and development economists studying state-building and fiscal capacity. While this theory has been applied to many conflicts, such counter arguments become less valid as the increasing mobility of capital and information level the distributions of wealth worldwide, or when considering that it is relative, not absolute, wealth differences that may fuel wars. There are those on the extreme right of the political spectrum who provide support, fascists in particular, by asserting a natural right of a strong nation to whatever the weak cannot hold by force. Some centrist, capitalist, world leaders, including Presidents of the United States and U.S. Generals, expressed support for an economic view of war.\n\nThe Marxist theory of war is quasi-economic in that it states all modern wars are caused by competition for resources and markets between great (imperialist) powers, claiming these wars are a natural result of the free market and class system. Part of the theory is that war will disappear once a world revolution, over-throwing free markets and class systems, has occurred. Marxist philosopher Rosa Luxemburg theorized that imperialism was the result of capitalist countries needing new markets. Expansion of the means of production is only possible if there is a corresponding growth in consumer demand. Since the workers in a capitalist economy would be unable to fill the demand, producers must expand into non-capitalist markets to find consumers for their goods, hence driving imperialism.\n\nDemographic theories can be grouped into two classes, Malthusian and youth bulge theories:\n\nMalthusian theories see expanding population and scarce resources as a source of violent conflict.\n\nPope Urban II in 1095, on the eve of the First Crusade, spoke:\nThis is one of the earliest expressions of what has come to be called the Malthusian theory of war, in which wars are caused by expanding populations and limited resources. Thomas Malthus (1766–1834) wrote that populations always increase until they are limited by war, disease, or famine.\n\nThe violent herder–farmer conflicts in Nigeria, Mali, Sudan and other countries in the Sahel region have been exacerbated by land degradation and population growth.\n\nAccording to Heinsohn, who proposed youth bulge theory in its most generalized form, a youth bulge occurs when 30 to 40 percent of the males of a nation belong to the \"fighting age\" cohorts from 15 to 29 years of age. It will follow periods with total fertility rates as high as 4–8 children per woman with a 15–29-year delay.\n\nHeinsohn saw both past \"Christianist\" European colonialism and imperialism, as well as today's Islamist civil unrest and terrorism as results of high birth rates producing youth bulges. Among prominent historical events that have been attributed to youth bulges are the role played by the historically large youth cohorts in the rebellion and revolution waves of early modern Europe, including the French Revolution of 1789, and the effect of economic depression upon the largest German youth cohorts ever in explaining the rise of Nazism in Germany in the 1930s. The 1994 Rwandan genocide has also been analyzed as following a massive youth bulge.\n\nYouth bulge theory has been subjected to statistical analysis by the World Bank, Population Action International, and the Berlin Institute for Population and Development. Youth bulge theories have been criticized as leading to racial, gender and age discrimination.\n\nRationalism is an international relations theory or framework. Rationalism (and Neorealism (international relations)) operate under the assumption that states or international actors are rational, seek the best possible outcomes for themselves, and desire to avoid the costs of war. Under one game theory approach, rationalist theories posit all actors can bargain, would be better off if war did not occur, and likewise seek to understand why war nonetheless reoccurs. Under another rationalist game theory without bargaining, the peace war game, optimal strategies can still be found that depend upon number of iterations played. In \"Rationalist Explanations for War\", James Fearon examined three rationalist explanations for why some countries engage in war:\n\n\"Issue indivisibility\" occurs when the two parties cannot avoid war by bargaining, because the thing over which they are fighting cannot be shared between them, but only owned entirely by one side or the other.\n\"Information asymmetry with incentives to misrepresent\" occurs when two countries have secrets about their individual capabilities, and do not agree on either: who would win a war between them, or the magnitude of state's victory or loss. For instance, Geoffrey Blainey argues that war is a result of miscalculation of strength. He cites historical examples of war and demonstrates, \"war is usually the outcome of a diplomatic crisis which cannot be solved because both sides have conflicting estimates of their bargaining power.\" Thirdly, bargaining may fail due to the states' inability to make credible commitments.\n\nWithin the rationalist tradition, some theorists have suggested that individuals engaged in war suffer a normal level of cognitive bias, but are still \"as rational as you and me\". According to philosopher Iain King, \"Most instigators of conflict overrate their chances of success, while most participants underrate their chances of injury...\" King asserts that \"Most catastrophic military decisions are rooted in GroupThink\" which is faulty, but still rational.\n\nThe rationalist theory focused around bargaining is currently under debate. The Iraq War proved to be an anomaly that undercuts the validity of applying rationalist theory to some wars.\n\nThe statistical analysis of war was pioneered by Lewis Fry Richardson following World War I. More recent databases of wars and armed conflict have been assembled by the Correlates of War Project, Peter Brecke and the Uppsala Conflict Data Program.\n\nThe following subsections consider causes of war from system, societal, and individual levels of analysis. This kind of division was first proposed by Kenneth Waltz in \"Man, the State, and War\" and has been often used by political scientists since then.\n\nThere are several different international relations theory schools. Supporters of realism in international relations argue that the motivation of states is the quest for security, and conflicts can arise from the inability to distinguish defense from offense, which is called the security dilemma.\n\nWithin the realist school as represented by scholars such as Henry Kissinger and Hans Morgenthau, and the neorealist school represented by scholars such as Kenneth Waltz and John Mearsheimer, two main sub-theories are:\nThe two theories are not mutually exclusive and may be used to explain disparate events according to the circumstance.\n\nLiberalism as it relates to international relations emphasizes factors such as trade, and its role in disincentivizing conflict which will damage economic relations. Realists respond that military force may sometimes be at least as effective as trade at achieving economic benefits, especially historically if not as much today. Furthermore, trade relations which result in a high level of dependency may escalate tensions and lead to conflict. Empirical data on the relationship of trade to peace are mixed, and moreover, some evidence suggests countries at war don't necessarily trade less with each other.\n\n\nThese theories suggest differences in people's personalities, decision-making, emotions, belief systems, and biases are important in determining whether conflicts get out of hand. For instance, it has been proposed that conflict is modulated by bounded rationality and various cognitive biases, such as prospect theory.\n\nThe morality of war has been the subject of debate for thousands of years.\n\nThe two principal aspects of ethics in war, according to the just war theory, are \"jus ad bellum\" and \"Jus in bello\".\n\n\"Jus ad bellum\" (right to war), dictates which unfriendly acts and circumstances justify a proper authority in declaring war on another nation. There are six main criteria for the declaration of a just war: first, any just war must be declared by a lawful authority; second, it must be a just and righteous cause, with sufficient gravity to merit large-scale violence; third, the just belligerent must have rightful intentions – namely, that they seek to advance good and curtail evil; fourth, a just belligerent must have a reasonable chance of success; fifth, the war must be a last resort; and sixth, the ends being sought must be proportional to means being used.\n\n\"Jus in bello\" (right in war), is the set of ethical rules when conducting war. The two main principles are proportionality and discrimination. Proportionality regards how much force is necessary and morally appropriate to the ends being sought and the injustice suffered. The principle of discrimination determines who are the legitimate targets in a war, and specifically makes a separation between combatants, who it is permissible to kill, and non-combatants, who it is not. Failure to follow these rules can result in the loss of legitimacy for the just-war-belligerent.\n\nFascism, and the ideals it encompasses, such as Pragmatism, racism, and social Darwinism, hold that violence is good. Pragmatism holds that war and violence can be good if it serves the ends of the people, without regard for universal morality. Racism holds that violence is good so that a master race can be established, or to purge an inferior race from the earth, or both. Social Darwinism asserts that violence is sometimes necessary to weed the unfit from society so civilization can flourish. These are broad archetypes for the general position that the ends justify the means. Lewis Coser, U.S. conflict theorist and sociologist, argued conflict provides a function and a process whereby a succession of new equilibriums are created. Thus, the struggle of opposing forces, rather than being disruptive, may be a means of balancing and maintaining a social structure or society.\n\n\nGeneral reference\nWar-related lists\n"}
{"id": "3008938", "url": "https://en.wikipedia.org/wiki?curid=3008938", "title": "Casualty (person)", "text": "Casualty (person)\n\nA casualty, as a term in military usage, is a person in military service, combatant or non-combatant, who becomes unavailable for duty due to several circumstances, including death, injury, illness, capture or desertion.\n\nIn civilian usage, a casualty is a person who is killed, wounded or incapacitated by some event; the term is usually used to describe multiple deaths and injuries due to violent incidents or disasters. It is sometimes misunderstood to mean \"fatalities\", but non-fatal injuries are also casualties.\n\nIn military usage, a \"casualty\" is a person in service killed in action, killed by disease, disabled by injuries, disabled by psychological trauma, captured, deserted, or missing, but not someone who sustains injuries which do not prevent them from fighting. Any casualty is no longer available for the immediate battle or campaign, the major consideration in combat; the number of casualties is simply the number of members of a unit who are not available for duty. The word has been used in a military context since at least 1513.\n\n\"Civilian casualties\" are civilians killed or injured by military personnel or combatants, sometimes instead referred to by the euphemistic expression \"collateral damage\".\n\nThe military organisation NATO uses the following definitions:\n\nIn relation to personnel, any person who is lost to his organization by reason of being declared dead, wounded, diseased, detained, captured or missing.\n\nAny casualty incurred as the direct result of hostile action, sustained in combat or relating thereto, or sustained going to or returning from a combat mission.\n\nA person who is not a battle casualty, but who is lost to his organization by reason of disease or injury, including persons dying from disease or injury, or by reason of being missing where the absence does not appear to be voluntary or due to enemy action or to being interned.\n\nThese definitions are popular among military historians.\nIn relation to personnel, any person killed in action, missing in action or who died of wounds or diseases before being evacuated to a medical installation.\n\nIn relation to personnel, any person incapacitated by wounds sustained or diseases contracted in a combat zone, as well as any person admitted to a medical installation for treatment or recuperation for more than a day. There is a distinction between combat medical casualty and non-combat medical casualty. The former refers to a medical casualty that is a direct result of combat action; the latter refers to a medical casualty that is not a direct result of combat action.\n\nA casualty classification generally used to describe any person killed by means of the action of hostile forces.\n\nA casualty classification generally used to describe any person reported missing during combat operations. They may have deserted, or may have been killed, wounded, or taken prisoner.\n\nA casualty classification generally used to describe any person who has incurred an injury by means of action of hostile forces.\n\nA casualty classification generally used to describe any person captured and held in custody by hostile forces.\n\nWhile the word \"casualty\" has been used since 1844 in civilian life, it is a less important concept; the number of deaths on the one hand and serious injuries on the other are separately of major importance, and immediate availability for service is not. These numbers are usually cited together with or instead of total casualties.\n\nAccording to WHO \"World health report 2004\", deaths from \"intentional\" injuries (including war, violence, and suicide) were estimated to be 2.8% of all deaths. In the same report, \"unintentional\" injury was estimated to be responsible for 6.2% of all deaths.\n\n\n"}
{"id": "261939", "url": "https://en.wikipedia.org/wiki?curid=261939", "title": "Declaration of war", "text": "Declaration of war\n\nA declaration of war is a formal act by which one state goes to war against another. The declaration is a performative speech act (or the signing of a document) by an authorized party of a national government, in order to create a state of war between two or more states.\n\nThe legality of who is competent to declare war varies between nations and forms of government. In many nations, that power is given to the head of state or sovereign. In other cases, something short of a full declaration of war, such as a letter of marque or a covert operation, may authorise war-like acts by privateers or mercenaries. The official international protocol for declaring war was defined in the Hague Convention (III) of 1907 on the Opening of Hostilities.\n\nSince 1945, developments in international law such as the United Nations Charter, which prohibits both the threat and the use of force in international conflicts, have made declarations of war largely obsolete in international relations. The UN Security Council, under powers granted in articles 24 and 25, and Chapter VII of the Charter, may authorize collective action to maintain or enforce international peace and security. Article 51 of the United Nations (UN) Charter also states that: \"Nothing in the present Charter shall impair the inherent right to individual or collective self-defence if an armed attack occurs against a state.\"\n\nFew nations have formally declared war upon another since then. In addition to this, non-state or terrorist organizations may claim to or be described as \"declaring war\" when engaging in violent acts. These declarations may have no legal standing in themselves, but they may still act as a call to arms for supporters of these organizations.\n\nA definition of the three ways of thinking about a declaration of war was developed by Saikrishna Prakash. He argues that a declaration of war can be seen from three perspectives:\n\nThe practice of declaring war has a long history. The ancient Sumerian Epic of Gilgamesh gives an account of it, as does the Old Testament.\n\nHowever, the practice of declaring war was not always strictly followed. In his study \"Hostilities without Declaration of War\" (1883), the British scholar John Frederick Maurice showed that between 1700 and 1870 war was declared in only 10 cases, while in another 107 cases war was waged without such declaration (these figures include only wars waged in Europe and between European states and the United States, not including colonial wars in Africa and Asia).\n\nIn modern public international law, a declaration of war entails the recognition between countries of a state of hostilities between these countries, and such declaration has acted to regulate the conduct between the military engagements between the forces of the respective countries. The primary multilateral treaties governing such declarations are the Hague Conventions.\n\nThe League of Nations, formed in 1919 in the wake of the First World War, and the General Treaty for the Renunciation of War of 1928 signed in Paris, France, demonstrated that world powers were seriously seeking a means to prevent the carnage of another world war. Nevertheless, these powers were unable to stop the outbreak of the Second World War, so the United Nations (UN) was established following that war in a renewed attempt to prevent international aggression through declarations of war.\n\nIn classical times, Thucydides condemned the Thebans, allies of Sparta, for launching a surprise attack without a declaration of war against Plataea, Athens' ally – an event that began the Peloponnesian War.\n\nThe utility of formal declarations of war has always been questioned, either as sentimental remnants of a long-gone age of chivalry or as imprudent warnings to the enemy. For example, writing in 1737, Cornelius van Bynkershoek judged that \"nations and princes endowed with some pride are not generally willing to wage war without a previous declaration, for they wish by an open attack to render victory more honourable and glorious.\" Writing in 1880, William Edward Hall judged that \"any sort of previous declaration therefore is an empty formality unless the enemy must be given time and opportunity to put himself in a state of defence, and it is needless to say that no one asserts such a quixotism to be obligatory.\"\n\nIn the first Hague Convention of 1899, the signatory states agreed that at least one other nation be used to mediate disputes between states before engaging in hostilities:\n\nIn case of serious disagreement or conflict, before an appeal to arms, the signatory Powers agree to have recourse, as far as circumstances allow, to the good offices or mediation of one or more friendly Powers.\n\nThe Hague Convention (III) of 1907 called \"Convention Relative to the Opening of Hostilities\" gives the international actions a country should perform when opening hostilities. The first two Articles say:\n\nIn 1989, Panama declared itself to be in a state of war with the United States.\nOn 13 May 1998, at the outbreak of the Eritrean–Ethiopian War, Ethiopia, in what Eritrean radio described as a \"total war\" policy, mobilized its forces for a full assault against Eritrea. The Claims Commission found that this was in essence an affirmation of the existence of a state of war between belligerents, not a declaration of war, and that Ethiopia also notified the United Nations Security Council, as required under Article 51 of the UN Charter.\n\nIn December 2005, the government of Chad declared that a state of war existed with Sudan, after Sudan hosted Chadian rebel groups that were behind fatal cross border raids.\n\nIn 2008, after armed clashes broke out during the Djiboutian–Eritrean border conflict, Djibouti's President Guelleh, when asked if his country was at war with Eritrea, replied with \"absolutely\".\n\nOn 11 April 2012, Sudan declared war on South Sudan after weeks of border clashes.\n\nDeclarations of war, while uncommon in the traditional sense, have mainly been limited to the conflict areas of the Western Asia and East Africa since 1945. Additionally, some small states have unilaterally declared war on major world powers such as the United States, United Kingdom, or Russia when faced with a hostile invasion and/or occupation.\n\nThis is a list of declarations of war (or the existence of war) by one sovereign state against another since the end of World War II in 1945. Only declarations that occurred in the context of a direct military conflict are included.\n\nThe United Nations Charter is the foundation of modern international law. The UN Charter is a treaty ratified by members of the UN, which are therefore legally bound by its terms. Article 2(4) of the UN Charter generally bans the use of force by states except when carefully circumscribed conditions are met, stating:\n\nAll members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the purposes of the United Nations.\nThis rule was \"enshrined in the United Nations Charter in 1945 for a good reason: to prevent states from using force as they felt so inclined\", said Louise Doswald-Beck, Secretary-General International Commission of Jurists.\n\nTherefore, in the absence of an armed attack against a country or its allies, any legal use of force, or any legal threat of the use of force, has to be supported by a United Nations Security Council resolution authorizing member states to use force.\n\nIn an effort to force nations to resolve issues without warfare, framers of the United Nations Charter attempted to commit member nations to using warfare only under limited circumstances, particularly for defensive purposes.\n\nThe UN became a combatant itself after North Korea invaded South Korea on 25 June 1950, which begun the Korean War. The UN Security Council condemned the North Korean action by a 9–0 resolution (with the Soviet Union absent) and called upon its member nations to come to the aid of South Korea. The United States and 15 other nations formed a \"UN force\" to pursue this action. In a press conference on 29 June 1950, U.S. President Harry S. Truman characterized these hostilities as not being a \"war\" but a \"police action\".\n\nThe United Nations has issued Security Council Resolutions that declared some wars to be legal actions under international law, most notably Resolution 678, authorizing the 1991 Gulf War which was triggered by Iraq's invasion of Kuwait. UN Resolutions authorise the use of \"force\" or \"all necessary means\".\n\nThroughout the Commonwealth realms (the UK, Australia, Canada, \"et al\".) the formal right to declare war rests with the monarch, currently Elizabeth II, or their representative (the governor-general), as part of the royal prerogative and exercised by the Prime Minister (for example in the UK) or that realm's written constitution. In the United Kingdom parliamentary approval is often sought to deploy combat forces overseas, for example in the Iraq War and airstrikes on Daesh (ISIL), but this is not a legal requirement.\n\nAccording to article 84 of Brazilian constitution the President of Brazil has the power to declare war, in the event of foreign aggression, when authorized by the National Congress or, upon its ratification if the aggression occurs between legislative sessions, and decree full or partial national mobilization under the same conditions.\n\nAccording to article 93 of the Finnish constitution, the President of Finland may declare war, or declare peace, with permission from the Parliament of Finland.\n\nAccording to Article 35 of the French constitution, the French Parliament has the authority to declare war.\nArticle 115a says that unless attacked by an opposing military force, Germany must vote a two-thirds majority vote in the Bundestag if the federal republic is under the threat of war.\n\nArticle 28.3.1° of the Constitution of Ireland states that \"war shall not be declared and the State shall not participate in any war save with the assent of Dáil Éireann.\"\nIreland has taken a policy of non-alignment (what many confuse with neutrality see: Irish Neutrality) in military terms and is thus not a member of NATO.\n\nAccording to the 11° article of the Italian Constitution, Italy rejects war as an instrument of aggression. Parliament has the power to declare war if it is necessary to create an order that ensures peace and justice among Nations; the most reliable authors exclude that among the circumstances in which it can be declared the state of war under Article 78 of the Constitution may be included also the state of internal civil war.\n\nAccording to Article 89 § VIII of the Mexican Constitution the President may declare war in the name of the United Mexican States after the correspondent law is enacted by the Congress of the Union.\n\nAccording to the Spanish constitution of 1978, Art. 63, the King, with prior authorization by the Parliament, has the power to declare war and make peace.\n\nAccording to 2010:1408 15 kap. 14 § entitled \"Krigsförklaring\" (declaration of war) the Swedish cabinet (regeringen) may not declare Sweden to be at war without the parliaments (riksdagen) consent unless Sweden is first attacked.\n\nIn the United States, Congress, which makes the rules for the military, has the power under the constitution to \"declare war\". However neither the U.S. Constitution nor any Act of Congress stipulate what format a declaration of war must take. War declarations have the force of law and are intended to be executed by the President as \"commander in chief\" of the armed forces. The last time Congress passed joint resolutions saying that a \"state of war\" existed was on June 5, 1942, when the U.S. declared war on Bulgaria, Hungary, and Romania. Since then, the US has used the term \"authorization to use military force,\" as in the case against Iraq in 2003.\n\nSometimes decisions for military engagements were made by US presidents, without formal approval by Congress, based on UN Security Council resolutions that do not expressly declare the UN or its members to be at war. Part of the justification for the United States invasion of Panama was to capture Manuel Noriega (as a prisoner of war) because he was declared a criminal rather than a belligerent.\n\nIn response to the September 11 attacks, the United States Congress passed the joint resolution Authorization for Use of Military Force Against Terrorists on September 14, 2001, which authorized the US President to fight the War on Terror.\n\n\n"}
{"id": "139114", "url": "https://en.wikipedia.org/wiki?curid=139114", "title": "Defensive wall", "text": "Defensive wall\n\nA defensive wall is a fortification usually used to protect a city, town or other settlement from potential aggressors. The walls can be simple palisades or earthworks to extensive military fortifications with towers, bastions and gates to access to the city. In ancient to modern times, they were used to enclose settlements. Generally, these are referred to as city walls or town walls, although there were also walls, such as the Great Wall of China, Walls of Benin, Hadrian's Wall, Anastasian Wall, the Cyclopean Wall Rajgir and the metaphorical Atlantic Wall, which extended far beyond the borders of a city and were used to enclose regions or mark territorial boundaries. In mountainous terrain, defensive walls such as \"letzis\" were used in combination with castles to seal valleys from potential attack. Beyond their defensive utility, many walls also had important symbolic functions representing the status and independence of the communities they embraced.\n\nExisting ancient walls are almost always masonry structures, although brick and timber-built variants are also known. Depending on the topography of the area surrounding the city or the settlement the wall is intended to protect, elements of the terrain such as rivers or coastlines may be incorporated in order to make the wall more effective.\n\nWalls may only be crossed by entering the appropriate city gate and are often supplemented with towers. The practice of building these massive walls, though having its origins in prehistory, was refined during the rise of city-states, and energetic wall-building continued into the medieval period and beyond in certain parts of Europe.\n\nSimpler defensive walls of earth or stone, thrown up around hillforts, ringworks, early castles and the like, tend to be referred to as ramparts or banks.\n\nFrom very early history to modern times, walls have been a near necessity for every city. Uruk in ancient Sumer (Mesopotamia) is one of the world's oldest known walled cities. Before that, the proto-city of Jericho in the West Bank had a wall surrounding it as early as the 8th millenniumBC.The earliest known town wall in Europe is of Solnitsata, built in the 6th or 5th millennium BC.\n\nThe Assyrians deployed large labour forces to build new palaces, temples and defensive walls.\n\nSome settlements in the Indus Valley Civilization were also fortified. By about 3500BC, hundreds of small farming villages dotted the Indus floodplain. Many of these settlements had fortifications and planned streets. The stone and mud brick houses of Kot Diji were clustered behind massive stone flood dykes and defensive walls, for neighboring communities quarreled constantly about the control of prime agricultural land. Mundigak (c. 2500BC) in present-day south-east Afghanistan has defensive walls and square bastions of sun dried bricks.\n\nBabylon was one of the most famous cities of the ancient world, especially as a result of the building program of Nebuchadnezzar, who expanded the walls and built the Ishtar Gate.\n\nExceptions were few, but neither ancient Sparta nor ancient Rome had walls for a long time, choosing to rely on their militaries for defense instead. Initially, these fortifications were simple constructions of wood and earth, which were later replaced by mixed constructions of stones piled on top of each other without mortar.\n\nIn Central Europe, the Celts built large fortified settlements which the Romans called oppida, whose walls seem partially influenced by those built in the Mediterranean. The fortifications were continuously expanded and improved.\n\nIn ancient Greece, large stone walls had been built in Mycenaean Greece, such as the ancient site of Mycenae (famous for the huge stone blocks of its 'cyclopean' walls). In classical era Greece, the city of Athens built a long set of parallel stone walls called the Long Walls that reached their guarded seaport at Piraeus.\n\nLarge rammed earth walls were built in ancient China since the Shang Dynasty (c. 1600–1050BC), as the capital at ancient Ao had enormous walls built in this fashion (see siege for more info). Although stone walls were built in China during the Warring States (481–221BC), mass conversion to stone architecture did not begin in earnest until the Tang Dynasty (618–907 AD). Sections of the Great Wall had been built prior to the Qin Dynasty (221–207BC) and subsequently connected and fortified during the Qin dynasty, although its present form was mostly an engineering feat and remodeling of the Ming Dynasty (1368–1644AD). The large walls of Pingyao serve as one example. Likewise, the walls of the Forbidden City in Beijing were established in the early 15th century by the Yongle Emperor.\n\nThe Romans fortified their cities with massive, mortar-bound stone walls. Among these are the largely extant Aurelian Walls of Rome and the Theodosian Walls of Constantinople, together with partial remains elsewhere. These are mostly city gates, like the Porta Nigra in Trier or Newport Arch in Lincoln.\n\nThe Persians built defensive walls to protect their territories, notably the Derbent Wall and the Great Wall of Gorgan built on the either sides of the Caspian Sea against nomadic nations.\n\nApart from these, the early Middle Ages also saw the creation of some towns built around castles. These cities were only rarely protected by simple stone walls and more usually by a combination of both walls and ditches. From the 12th century AD hundreds of settlements of all sizes were founded all across Europe, which very often obtained the right of fortification soon afterwards.\n\nThe founding of urban centers was an important means of territorial expansion and many cities, especially in central and eastern Europe, were founded for this purpose during the period of Eastern settlement. These cities are easy to recognise due to their regular layout and large market spaces. The fortifications of these settlements were continuously improved to reflect the current level of military development.\n\nDuring the Renaissance era, the Venetians raised great walls around cities threatened by the Ottoman Empire. Examples include the walled cities of Nicosia and Famagusta in Cyprus and the fortifications of Candia and Chania in Crete, which still stand.\n\nAt its simplest, a defensive wall consists of a wall enclosure and its gates. For the most part, the top of the walls were accessible, with the outside of the walls having tall parapets with embrasures or merlons. North of the Alps, this passageway at the top of the walls occasionally had a roof.\n\nIn addition to this, many different enhancements were made over the course of the centuries:\n\n\nThe defensive towers of west and south European fortifications in the Middle Ages were often very regularly and uniformly constructed (cf. Ávila, Provins), whereas Central European city walls tend to show a variety of different styles. In these cases the gate and wall towers often reach up to considerable heights, and gates equipped with two towers on either side are much rarer. Apart from having a purely military and defensive purpose, towers also played a representative and artistic role in the conception of a fortified complex. The architecture of the city thus competed with that of the castle of the noblemen and city walls were often a manifestation of the pride of a particular city.\n\nUrban areas outside the city walls, so-called Vorstädte, were often enclosed by their own set of walls and integrated into the defense of the city. These areas were often inhabited by the poorer population and held the \"noxious trades\". In many cities, a new wall was built once the city had grown outside of the old wall. This can often still be seen in the layout of the city, for example in Nördlingen, and sometimes even a few of the old gate towers are preserved, such as the \"white tower\" in Nuremberg. Additional constructions prevented the circumvention of the city, through which many important trade routes passed, thus ensuring that tolls were paid when the caravans passed through the city gates, and that the local market was visited by the trade caravans.\nFurthermore, additional signaling and observation towers were frequently built outside the city, and were sometimes fortified in a castle-like fashion. The border of the area of influence of the city was often partially or fully defended by elaborate ditches, walls and hedges. The crossing points were usually guarded by gates or gate houses. These defenses were regularly checked by riders, who often also served as the gate keepers. Long stretches of these defenses can still be seen to this day, and even some gates are still intact. To further protect their territory, rich cities also established castles in their area of influence. An example of this practice is the Romanian Bran Castle, which was intended to protect nearby Kronstadt (today's Braşov).\n\nThe city walls were often connected to the fortifications of hill castles via additional walls. Thus the defenses were made up of city and castle fortifications taken together. Several examples of this are preserved, for example in Germany Hirschhorn on the Neckar, Königsberg and Pappenheim, Franken, Burghausen in Oberbayern and many more.\nA few castles were more directly incorporated into the defensive strategy of the city (e.g. Nuremberg, Zons, Carcassonne), or the cities were directly outside the castle as a sort of \"pre-castle\" (Coucy-le-Chateau, Conwy and others). Larger cities often had multiple stewards for example Augsburg was divided into a Reichstadt and a clerical city. These different parts were often separated by their own fortifications.\n\nWith the development of firearms came the necessity to expand the existing installation, which occurred in multiples stages. Firstly, additional, half-circular towers were added in the interstices between the walls and pre-walls in which a handful of cannons could be placed. Soon after, reinforcing structures or \"bastions\" were added in strategically relevant positions, such as at the gates or corners. A well-preserved example of this is the \"Spitalbastei\" in Rothenburg or the bastions built as part of the 17th-century walls surrounding Derry, a city in Northern Ireland; however, at this stage the cities were still only protected by relatively thin walls which could offer little resistance to the cannons of the time. Therefore, new, star forts with numerous cannons and thick earth walls reinforced by stone were built. These could resist cannon fire for prolonged periods of time. However, these massive fortifications severely limited the growth of the cities, as it was much more difficult to move them as compared to the simple walls previously employed to make matters worse, it was forbidden to build \"outside the city gates\" for strategic reasons and the cities became more and more densely populated as a result.\n\nIn the wake of city growth and the ensuing change of defensive strategy, focusing more on the defense of forts around cities, many city walls were demolished. Also, the invention of gunpowder rendered walls less effective, as siege cannons could then be used to blast through walls, allowing armies to simply march through. Today, the presence of former city fortifications can often only be deduced from the presence of ditches, ring roads or parks.\n\nFurthermore, some street names hint at the presence of fortifications in times past, for example when words such as \"wall\" or \"glacis\" occur. Wall Street in New York City, itself a metonym for the entire United States financial system, is one example.\n\nIn the 19th century, less emphasis was placed on preserving the fortifications for the sake of their architectural or historical value on the one hand, complete fortifications were restored (Carcassonne), on the other hand many structures were demolished in an effort to modernize the cities. One exception to this is the \"monument preservation\" law by the Bavarian King Ludwig I of Bavaria, which led to the nearly complete preservation of many monuments such as the Rothenburg ob der Tauber, Nördlingen and Dinkelsbühl. The countless small fortified towns in the Franconia region were also preserved as a consequence of this edict.\n\nWalls and fortified wall structures were still built in the modern era. They did not, however, have the original purpose of being a structure able to resist a prolonged siege or bombardment. Modern examples of defensive walls include:\n\n\nAdditionally, in some countries, different embassies may be grouped together in a single \"embassy district,\" enclosed by a fortified complex with walls and towers this usually occurs in regions where the embassies run a high risk of being target of attacks. An early example of such a compound was the Legation Quarter in Beijing in the late 19th and early 20th centuries.\n\nMost of these modern city walls are made of steel and concrete. Vertical concrete plates are put together so as to allow the least space in between them, and are rooted firmly in the ground. The top of the wall is often protruding and beset with barbed wire in order to make climbing them more difficult. These walls are usually built in straight lines and covered by watchtowers at the corners. Double walls with an interstitial \"zone of fire\", as had the former Berlin Wall, are now rare.\n\nIn September 2014, Ukraine announced the construction of the \"European Rampart\" alongside its border with Russia to be able to successfully apply for a visa-free movement with the European Union.\n\nSeoul(Hanyang doseong)\nHwaseong(Suwon city)\n\n\n"}
