{"id": "191145", "url": "https://en.wikipedia.org/wiki?curid=191145", "title": "Autonomy", "text": "Autonomy\n\nIn developmental psychology and moral, political, and bioethical philosophy, autonomy is the capacity to make an informed, uncoerced decision. Autonomous organizations or institutions are independent or self-governing. Autonomy can also be defined from a human resources perspective, where it denotes a (relatively high) level of discretion granted to an employee in his or her work. In such cases, autonomy is known to generally increase job satisfaction. Autonomy is a term that is also widely used in the field of medicine — personal autonomy is greatly recognized and valued in health care.\n\nIn the sociology of knowledge, a controversy over the boundaries of autonomy inhibited analysis of any concept beyond relative autonomy, until a typology of autonomy was created and developed within science and technology studies. According to it, the institution of science's existing autonomy is “reflexive autonomy”: actors and structures within the scientific field are able to translate or to reflect diverse themes presented by social and political fields, as well as influence them regarding the thematic choices on research projects.\n\nInstitutional autonomy is having the capacities as a legislator to be able to implant and pursue official goals. Autonomous institutions are responsible for finding sufficient resources or modifying their plans, programs, courses, responsibilities, and services accordingly. But in doing so, they must contend with any obstacles that can occur, such as social pressure against cut-backs or socioeconomic difficulties. From a legislator's point of view, to increase institutional autonomy, conditions of self-management and institutional self-governance must be put in place. An increase in leadership and a redistribution of decision-making responsibilities would be beneficial to the research of resources. \nInstitutional autonomy was often seen as a synonym for self-determination, and many governments feared that it would lead institutions to an irredentist or secessionist region. But autonomy should be seen as a solution to self-determination struggles. Self-determination is a movement toward independence, whereas autonomy is a way to accommodate the distinct regions/groups within a country. Institutional autonomy can diffuse conflicts regarding minorities and ethnic groups in a society. Allowing more autonomy to groups and institutions helps create diplomatic relationships between them and the central government.\n\nIn governmental parlance, autonomy refers to self-governance. An example of an autonomous jurisdiction was the former United States governance of the Philippine Islands. The \"Philippine Autonomy Act of 1916\" provided the framework for the creation of an autonomous government under which the Filipino people had broader domestic autonomy than previously, although it reserved certain privileges to the United States to protect its sovereign rights and interests. Other examples include Kosovo (as the Socialist Autonomous Province of Kosovo) under the former Yugoslav government of Marshal Tito and Puntland Autonomous Region within Federal Republic of Somalia.\n\nAutonomy is a key concept that has a broad impact on different fields of philosophy. In metaphysical philosophy, the concept of autonomy is referenced in discussions about free will, fatalism, determinism, and agency. In \"How to Make Good Decisions and Be Right All the Time\", philosopher Iain King developed an 'Autonomy Principle', which he defines as \"Let people choose for themselves, unless we know their interests better than they can.\" King argues it is not enough to know someone else's interests better than that person; their autonomy should only be infringed if that person is \"unable\" to know their own interests on a particular matter. In moral philosophy, autonomy refers to subjecting oneself to objective moral law.\n\nImmanuel Kant (1724–1804) defined autonomy by three themes regarding contemporary ethics. Firstly, autonomy as the right for one to make their own decisions excluding any interference from others. Secondly, autonomy as the capacity to make such decisions through one's own independence of mind and after personal reflection. Thirdly, as an ideal way of living life autonomously. In summary, autonomy is the moral right one possesses, or the capacity we have in order to think and make decisions for oneself providing some degree of control or power over the events that unfold within one's everyday life.\n\nThe context in which Kant addresses autonomy is in regards to moral theory, asking both foundational and abstract questions. He believed that in order for there to be morality, there must be autonomy. He breaks down autonomy into two distinct components. \"Auto\" can be defined as the negative form of independence, or to be free in a negative sense. This is the aspect where decisions are made on your own. Whereas, \"nomos\" is the positive sense, a freedom or lawfulness, where you are choosing a law to follow. Kantian autonomy also provides a sense of rational autonomy, simply meaning one rationally possesses the motivation to govern their own life. Rational autonomy entails making your own decisions but it cannot be done solely in isolation. Cooperative rational interactions are required to both develop and exercise our ability to live in a world with others.\n\nKant argued that morality presupposes this autonomy () in moral agents, since moral requirements are expressed in categorical imperatives. An imperative is categorical if it issues a valid command independent of personal desires or interests that would provide a reason for obeying the command. It is hypothetical if the validity of its command, if the reason why one can be expected to obey it, is the fact that one desires or is interested in something further that obedience to the command would entail. \"Don't speed on the freeway if you don't want to be stopped by the police\" is a hypothetical imperative. \"It is wrong to break the law, so don't speed on the freeway\" is a categorical imperative. The hypothetical command not to speed on the freeway is not valid for you if you do not care whether you are stopped by the police. The categorical command is valid for you either way. Autonomous moral agents can be expected to obey the command of a categorical imperative even if they lack a personal desire or interest in doing so. It remains an open question whether they will, however.\n\nThe Kantian concept of autonomy is often misconstrued, leaving out the important point about the autonomous agent's self-subjection to the moral law. It is thought that autonomy is fully explained as the ability to obey a categorical command independently of a personal desire or interest in doing so—or worse, that autonomy is \"obeying\" a categorical command independently of a natural desire or interest; and that heteronomy, its opposite, is acting instead on personal motives of the kind referenced in hypothetical imperatives.\n\nIn his \"Groundwork of the Metaphysic of Morals\", Kant applied the concept of autonomy also to define the concept of personhood and human dignity. Autonomy, along with rationality, are seen by Kant as the two criteria for a meaningful life. Kant would consider a life lived without these not worth living; it would be a life of value equal to that of a plant or insect. According to Kant autonomy is part of the reason that we hold others morally accountable for their actions. Human actions are morally praise- or blame-worthy in virtue of our autonomy. Non- autonomous beings such as plants or animals are not blameworthy due to their actions being non-autonomous. Kant's position on crime and punishment is influenced by his views on autonomy. Brainwashing or drugging criminals into being law-abiding citizens would be immoral as it would not be respecting their autonomy. Rehabilitation must be sought in a way that respects their autonomy and dignity as human beings.\n\nFriedrich Nietzsche wrote about autonomy and the moral fight. Autonomy in this sense is referred to as the free self and entails several aspects of the self, including self-respect and even self-love. This can be interpreted as influenced by Kant (self-respect) and Aristotle (self-love). For Nietzsche, valuing ethical autonomy can dissolve the conflict between love (self-love) and law (self-respect) which can then translate into reality through experiences of being self-responsible. Because Nietzsche defines having a sense of freedom with being responsible for one's own life, freedom and self-responsibility can be very much linked to autonomy.\n\nThe Swiss philosopher Jean Piaget (1896-1980) believed that autonomy comes from within and results from a \"free decision\". It is of intrinsic value and the morality of autonomy is not only accepted but obligatory. When an attempt at social interchange occurs, it is reciprocal, ideal and natural for there to be autonomy regardless of why the collaboration with others has taken place. For Piaget, the term autonomous can be used to explain the idea that rules are self-chosen. By choosing which rules to follow or not, we are in turn determining our own behaviour.\n\nPiaget studied the cognitive development of children by analyzing them during their games and through interviews, establishing (among other principles) that the children's moral maturation process occurred in two phases, the first of heteronomy and the second of autonomy:\nRules are objective and unchanging. They must be literal because the authority are ordering it and do not fit exceptions or discussions. The base of the rule is the superior authority (parents, adults, the State), that it should not give reason for the rules imposed or fulfilled them in any case. Duties provided are conceived as given from oneself. Any moral motivation and sentiments are possible through what one believes to be right. \nRules are the product of an agreement and, therefore, are modifiable. They can be subject to interpretation and fit exceptions and objections. The base of the rule is its own acceptance, and its meaning has to be explained. Sanctions must be proportionate to the absence, assuming that sometimes offenses can go unpunished, so that collective punishment is unacceptable if it is not the guilty. The circumstances may not punish a guilty. Duties provided are conceived as given from the outside. One follows rules mechanically as it is simply a rule, or as a way to avoid a form of punishment.\n\nThe American psychologist Lawrence Kohlberg (1927-1987) continues the studies of Piaget. His studies collected information from different latitudes to eliminate the cultural variability, and focused on the moral reasoning, and not so much in the behavior or its consequences. Through interviews with adolescent and teenage boys, who were to try and solve \"moral dilemmas,\" Kohlberg went on to further develop the stages of moral development. The answers they provided could be one of two things. Either they choose to obey a given law, authority figure or rule of some sort or they chose to take actions that would serve a human need but in turn break this given rule or command.\n\nThe most popular moral dilemma asked involved the wife of a man approaching death due to a special type of cancer. Because the drug was too expensive to obtain on his own, and because the pharmacist who discovered and sold the drug had no compassion for him and only wanted profits, he stole it. Kohlberg asks these adolescent and teenage boys (10-, 13- and 16-year-olds) if they think that is what the husband should have done or not. Therefore, depending on their decisions, they provided answers to Kohlberg about deeper rationales and thoughts and determined what they value as important. This value then determined the \"structure\" of their moral reasoning.\n\nKohlberg established three stages of morality, each of which is subdivided into two levels. They are read in progressive sense, that is, higher levels indicate greater autonomy.\n\nAutonomy in childhood and adolescence is when one strives to gain a sense of oneself as a separate, self-governing individual. Between ages 1–3, during the second stage of Erikson's and Freud's stages of development, the psychosocial crisis that occurs is autonomy versus shame and doubt. The significant event that occurs during this stage is that children must learn to be autonomous, and failure to do so may lead to the child doubting their own abilities and feel ashamed. When a child becomes autonomous it allows them to explore and acquire new skills. Autonomy has two vital aspects wherein there is an emotional component where one relies more on themselves rather than their parents and a behavioural component where one makes decisions independently by using their judgement. The styles of child rearing affect the development of a child's autonomy. Authoritative child rearing is the most successful approach, where the parents engage in autonomy granting appropriate to their age and abilities. Autonomy in adolescence is closely related to their quest for identity. In adolescence parents and peers act as agents of influence. Peer influence in early adolescence may help the process of an adolescent to gradually become more autonomous by being less susceptible to parental or peer influence as they get older. In adolescence the most important developmental task is to develop a healthy sense of autonomy.\n\nIn Christianity, autonomy is manifested as a partial self-governance on various levels of church administration. During the history of Christianity, there were two basic types of autonomy. Some important parishes and monasteries have been given special autonomous rights and privileges, and the best known example of monastic autonomy is the famous Eastern Orthodox monastic community on Mount Athos in Greece. On the other hand, administrative autonomy of entire ecclesiastical provinces has throughout history included various degrees of internal self-governance.\n\nIn ecclesiology of Eastern Orthodox Churches, there is a clear distinction between autonomy and autocephaly, since autocephalous churches have full self-governance and independence, while every autonomous church is subject to some autocephalous church, having a certain degree of internal self-governance. Since every autonomous church had its own historical path to ecclesiastical autonomy, there are significant differences between various autonomous churches in respect of their particular degrees of self-governance. For example, churches that are autonomous can have their highest-ranking bishops, such as an archbishop or metropolitan, appointed or confirmed by the patriarch of the mother church from which it was granted its autonomy, but generally they remain self-governing in many other respects.\n\nIn the history of Western Christianity the question of ecclesiastical autonomy was also one of the most important questions, especially during the first centuries of Christianity, since various archbishops and metropolitans in Western Europe have often opposed centralizing tendencies of the Church of Rome. , the Catholic Church comprises 24 autonomous (\"sui iuris\") Churches in communion with the Holy See. Various denominations of Protestant churches usually have more decentralized power, and churches may be autonomous, thus having their own rules or laws of government, at the national, local, or even individual level.\n\nSartre brings the concept of the Cartesian god being totally free and autonomous. He states that existence precedes essence with god being the creator of the essences, eternal truths and divine will. This pure freedom of god relates to human freedom and autonomy; where a human is not subjected to pre-existing ideas and values.\n\nAccording to the first amendment, In the United States of America, the federal government is restricted in building a national church. This is due to the first amendment's recognizing people's freedom's to worship their faith according to their own belief's. For example, the American government has removed the church from their \"sphere of authority\" due to the churches' historical impact on politics and their authority on the public. This was the beginning of the disestablishment process. The Protestant churches in the United States had a significant impact on American culture in the nineteenth century, when they organized the establishment of schools, hospitals, orphanages, colleges, magazines, and so forth. This has brought up the famous, however, misinterpreted term of the separation of church and state. These churches lost the legislative and financial support from the state.\n\nThe first disestablishment began with the introduction of the bill of rights. In the twentieth century, due to the great depression of the 1930s and the completion of the second world war, the American churches were revived. Specifically the Protestant churches. This was the beginning of the second disestablishment when churches had become popular again but held no legislative power. One of the reasons why the churches gained attendance and popularity was due to the baby boom, when soldiers came back from the second world war and started their families. The large influx of newborns gave the churches a new wave of followers. However, these followers did not hold the same beliefs as their parents and brought about the political, and religious revolutions of the 1960s.\n\nDuring the 1960s, the collapse of religious and cultural middle brought upon the third disestablishment. Religion became more important to the individual and less so to the community. The changes brought from these revolutions significantly increased the personal autonomy of individuals due to the lack of structural restraints giving them added freedom of choice. This concept is known as \"new voluntarism\" where individuals have free choice on how to be religious and the free choice whether to be religious or not.\n\nIn a medical context, respect for a patient's personal autonomy is considered one of many fundamental ethical principles in medicine. Autonomy can be defined as the ability of the person to make his or her own decisions. This faith in autonomy is the central premise of the concept of informed consent and shared decision making. This idea, while considered essential to today's practice of medicine, was developed in the last 50 years. According to Tom Beauchamp and James Childress (in \"Principles of Biomedical Ethics\"), the Nuremberg trials detailed accounts of horrifyingly exploitative medical \"experiments\" which violated the subjects' physical integrity and personal autonomy. These incidences prompted calls for safeguards in medical research, such as the Nuremberg Code which stressed the importance of voluntary participation in medical research. It is believed that the Nuremberg Code served as the premise for many current documents regarding research ethics.\n\nRespect for autonomy became incorporated in health care and patients could be allowed to make personal decisions about the health care services that they receive. Notably, autonomy has several aspects as well as challenges that affect health care operations. The manner in which a patient is handled may undermine or support the autonomy of a patient and for this reason, the way a patient is communicated to becomes very crucial. A good relationship between a patient and a health care practitioner needs to be well defined to ensure that autonomy of a patient is respected. Just like in any other life situation, a patient would not like to be under the control of another person. The move to emphasize respect for patient's autonomy rose from the vulnerabilities that were pointed out in regards to autonomy.\n\nHowever, autonomy does not only apply in a research context. Users of the health care system have the right to be treated with respect for their autonomy, instead of being dominated by the physician. This is referred to as paternalism. While paternalism is meant to be overall good for the patient, this can very easily interfere with autonomy. Through the therapeutic relationship, a thoughtful dialogue between the client and the physician may lead to better outcomes for the client, as he or she is more of a participant in decision-making.\n\nThere are many different definitions of autonomy, many of which place the individual in a social context. See also: relational autonomy, which suggests that a person is defined through their relationships with others, and \"supported autonomy\" which suggests that in specific circumstances it may be necessary to temporarily compromise the autonomy of the person in the short term in order to preserve their autonomy in the long-term. Other definitions of the autonomy imagine the person as a contained and self-sufficient being whose rights should not be compromised under any circumstance.\n\nThere are also differing views with regard to whether modern health care systems should be shifting to greater patient autonomy or a more paternalistic approach. For example, there are such arguments that suggest the current patient autonomy practiced is plagued by flaws such as misconceptions of treatment and cultural differences, and that health care systems should be shifting to greater paternalism on the part of the physician given their expertise.  On the other hand, other approaches suggest that there simply needs to be an increase in relational understanding between patients and health practitioners to improve patient autonomy.\n\nOne argument in favor of greater patient autonomy and its benefits is by Dave deBronkart, who believes that in the technological advancement age, patients are capable of doing a lot of their research on medical issues from their home. According to deBronkart, this helps to promote better discussions between patients and physicians during hospital visits, ultimately easing up the workload of physicians. deBronkart argues that this leads to greater patient empowerment and a more educative health care system. In opposition to this view, technological advancements can sometimes be viewed as an unfavorable way of promoting patient autonomy. For example, self-testing medical procedures which have become increasingly common are argued by Greaney et. al. to increase patient autonomy, however, may not be promoting what is best for the patient. In this argument, contrary to deBronkart, the current perceptions of patient autonomy are excessively over-selling the benefits of individual autonomy, and is not the most suitable way to go about treating patients. Instead, a more inclusive form of autonomy should be implemented, relational autonomy, which factors into consideration those close to the patient as well as the physician. These different concepts of autonomy can be troublesome as the acting physician is faced with deciding which concept he/she will implement into their clinical practice.\n\nAutonomy varies and some patients find it overwhelming especially the minors when faced with emergency situations. Issues arise in emergency room situations where there may not be time to consider the principle of patient autonomy. Various ethical challenges are faced in these situations when time is critical, and patient consciousness may be limited. However, in such settings where informed consent may be compromised, the working physician evaluates each individual case to make the most professional and ethically sound decision. For example, it is believed that neurosurgeons in such situations, should generally do everything they can to respect patient autonomy. In the situation in which a patient is unable to make an autonomous decision, the neurosurgeon should discuss with the surrogate decision maker in order to aid in the decision making process. Performing surgery on a patient without informed consent is in general thought to only be ethically justified when the neurosurgeon and his/her team render the patient to not have the capacity to make autonomous decisions. If the patient is capable of making an autonomous decision, these situations are generally less ethically strenuous as the decision is typically respected.\n\nIt is important to note that not every patient is capable of making an autonomous decision. For example, a commonly proposed question is at what age children should be partaking in treatment decisions. This question arises as children develop differently, therefore making it difficult to establish a standard age at which children should become more autonomous. Those who are unable to make the decisions prompt a challenge to medical practitioners since it becomes difficult to determine the ability of a patient to make a decision. To some extent, it has been said that emphasis of autonomy in health care has undermined the practice of health care practitioners to improve the health of their patient as necessary. The scenario has led to tension in the relationship between a patient and a health care practitioner. This is because as much as a physician wants to prevent a patient from suffering, he or she still has to respect autonomy. Beneficence is a principle allowing physicians to act responsibly in their practice and in the best interests of their patients, which may involve overlooking autonomy. However, the gap between a patient and a physician has led to problems because in other cases, the patients have complained of not being adequately informed.\n\nThe seven elements of informed consent (as defined by Beauchamp and Childress) include threshold elements (competence and voluntariness), information elements (disclosure, recommendation, and understanding) and consent elements (decision and authorization). Some philosophers such as Harry Frankfurt consider Beauchamp and Childress criteria insufficient. They claim that an action can only be considered autonomous if it involves the exercise of the capacity to form higher-order values about desires when acting intentionally. What this means is that patients may understand their situation and choices but would not be autonomous unless the patient is able to form value judgements about their reasons for choosing treatment options they would not be acting autonomously.\n\nIn certain unique circumstances, government may have the right to temporarily override the right to bodily integrity in order to preserve the life and well-being of the person. Such action can be described using the principle of \"supported autonomy\", a concept that was developed to describe unique situations in mental health (examples include the forced feeding of a person dying from the eating disorder anorexia nervosa, or the temporary treatment of a person living with a psychotic disorder with antipsychotic medication). While controversial, the principle of supported autonomy aligns with the role of government to protect the life and liberty of its citizens. Terrence F. Ackerman has highlighted problems with these situations, he claims that by undertaking this course of action physician or governments run the risk of misinterpreting a conflict of values as a constraining effect of illness on a patient's autonomy.\n\nSince the 1960s, there have been attempts to increase patient autonomy including the requirement that physician's take bioethics courses during their time in medical school. Despite large-scale commitment to promoting patient autonomy, public mistrust of medicine in developed countries has remained. Onora O'Neill has ascribed this lack of trust to medical institutions and professionals introducing measures that benefit themselves, not the patient. O'Neill claims that this focus on autonomy promotion has been at the expense of issues like distribution of healthcare resources and public health. k\n\nOne proposal to increase patient autonomy is through the use of support staff. The use of support staff including medical assistants, physician assistants, nurse practitioners, nurses, and other staff that can promote patient interests and better patient care. Nurses especially can learn about patient beliefs and values in order to increase informed consent and possibly persuade the patient through logic and reason to entertain a certain treatment plan. This would promote both autonomy and beneficence, while keeping the physician's integrity intact. Furthermore, Humphreys asserts that nurses should have professional autonomy within their scope of practice (35-37). Humphreys argues that if nurses exercise their professional autonomy more, then there will be an increase in patient autonomy (35-37).\n\nAfter the Second World War there was a push for international human rights that came in many waves. Autonomy as a basic human right started the building block in the beginning of these layers alongside with liberty. The Universal declarations of Human rights of 1948 has made mention of autonomy or the legal protected right to individual self-determination in article 22.\n\nDocuments such as the United Nations Declaration on the Rights of Indigenous Peoples reconfirm international law in the aspect of human rights because those laws were already there, but it is also responsible for making sure that the laws highlighted when it comes to autonomy, cultural and integrity and land rights are made within an indigenous context by taking special attention to their historical and contemporary events\n\nThe United Nations Declaration on the Rights of Indigenous Peoples article 3 also through international law provides Human rights for Indigenous individuals through its third article by giving them a right to self-determination meaning they have all the liberties to choose their political status, and are capable to go and improve their economics social, and cultural statuses in society by developing it. Another example of this is article 4 of the same document which gives them autonomous rights when it comes to their internal or local affairs and how they can fund themselves in order to be able to self govern themselves.\n\nMinorities in countries are also protected as well by international law; the 27th article of the United Nations International covenant on Civil and Political rights or the ICCPR does so by allowing these individuals to be able to enjoy their own culture or use their language. Minorities in that manner are people from ethnic religious or linguistic groups according to the document.\n\nThe European Court of Human rights, is an international court that has been created on behalf of the European Conventions of Human rights. However, when it comes to autonomy they did not explicitly state it when it comes to the rights that individuals have. The current article 8 has remedied to that when the case of \"Pretty v the United Nations which was a case in 2002 involving assisted suicide\" where autonomy was used as a legal right in law. It was where Autonomy was distinguished and its reach into law was marked as well making it the foundations for legal precedent in making case law originating from the European Court of Human rights\n\nThe Yogyakarta Principles, a document with no binding effect in international human rights law, contend that \"self-determination\" used as meaning of autonomy on one's own matters including informed consent or sexual and reproductive rights, is integral for one's self-defined or gender identity and refused any medical procedures as a requirement for legal recognition of the gender identity of transgender. If eventually accepted by the international community in a treaty, this would make these ideas human rights in the law. The Convention on the Rights of Persons with Disabilities also defines autonomy as principles of rights of a person with disability including \"the freedom to make one's own choices, and independence of persons\".\n\nA study conducted by David C. Giles and John Maltby conveyed that after age effecting factors were removed a high emotional autonomy was a significant predictor of celebrity interest, as well as high attachment to peers with a low attachment to parents. Patterns of intense personal interest in celebrities was found to be conjunction with low levels of closeness and security. Furthermore, the results suggested that adults with a secondary group of pseudo-friends during development from parental attachment, usually focus solely on one particular celebrity, which could be due to difficulties in making this transition.\n\n\nAutonomy can be limited. For instance, by disabilities, civil society organizations may achieve a degree of autonomy albeit nested within––and relative to––formal bureaucratic and administrative regimes. Community partners can therefore assume a hybridity of capture and autonomy––or a mutuality––that is rather nuanced.\n\nThe term \"semi-autonomy\" (coined with prefix semi- / \"half\") designates partial or limited autonomy. As a relative term, it is usually applied to various semi-autonomous entities or processes that are substantially or functionally limited, in comparison to other fully autonomous entities or processes.\n\nThe term \"quasi-autonomy\" (coined with prefix quasi- / \"resembling\" or \"appearing\") designates formally acquired or proclaimed, but functionally limited or constrained autonomy. As a descriptive term, it is usually applied to various quasi-autonomous entities or processes that are formally designated or labeled as autonomous, but in reality remain functionally dependent or influenced by some other entity or process. An example for such use of the term can be seen in common designation for quasi-autonomous non-governmental organizations.\n\n\n"}
{"id": "196649", "url": "https://en.wikipedia.org/wiki?curid=196649", "title": "E-government", "text": "E-government\n\nE-government (short for electronic government) is the use of technological communications devices, such as computers and the Internet to provide public services to citizens and other persons in a country or region. E-government offers new opportunities for more direct and convenient citizen access to government, and for government provision of services directly to citizens. The term consists of the digital interactions between a citizen and their government (C2G), between governments and other government agencies (G2G), between government and citizens (G2C), between government and employees (G2E), and between government and businesses/commerces (G2B). E-government delivery models can be broken down into the following categories: This interaction consists of citizens communicating with all levels of government (city, state/province, national, and international), facilitating citizen involvement in governance using information and communication technology (ICT) (such as computers and websites) and business process re-engineering (BPR). Brabham and Guth (2017) interviewed the third party designers of e-government tools in North America about the ideals of user interaction that they build into their technologies, which include progressive values, ubiquitous participation, geolocation, and education of the public.\n\nOther definitions stray from the idea that technology is an object and defines e-government simply as facilitators or instruments and focus on specific changes in Public Administration issues. The internal transformation of a Government is the definition that established the specialist technologist Mauro D. Ríos. In his paper \"In search of a definition of Electronic Government\", he says: \"Digital Government is a new way of organization and management of public affairs, introducing positive transformational processes in management and the structure itself of the organization chart, adding value to the procedures and services provided, all through the introduction and continued appropriation of information and communication technologies as a facilitator of these transformations.\"\nE-government is also known as e-gov, electronic government, Internet governance, digital government, online government, connected government. As of 2014 the OECD still uses the term digital government, and distinguishes it from e-government in the recommendation produced there for the Network on E-Government of the Public Governance Committee. Several governments have started to use the term digital government to a wide range of services involving contemporary technology, such as big data, automation or predictive analytics.\n\n\"E-gov strategies\" (or \"digital government\") is defined as \"The employment of the Internet and the world-wide-web for delivering government information and services to the citizens.\" (United Nations, 2006; AOEMA, 2005). \"Electronic government\" (or \"e-government\") essentially refers to \"utilization of Information Technology (IT), Information and Communication Technologies (ICT s), and other web-based telecommunication technologies to improve and/or enhance on the efficiency and effectiveness of service delivery in the public sector.\". E-government promotes and improves broad stakeholders contribution to national and community development, as well as deepen the governance process.\n\nIn electronic government systems, government operations are supported by web-based services. It involves the use of information technology, specifically the Internet, to facilitate the communication between the government and its citizens.\n\nThe Division of a Public Administration and Development Management (DPAPM) of the United Nations Department of Economic and Social Affairs (UN-DESA) conducts a bi-annual e-government survey which includes a section titled \"e-Government Development Index (EGDI)\". It is a comparative ranking of 193 countries of the world according to three primary indicators: i) the OSI - Online Service Index that measures the online presence of the government in terms of service delivery; ii) the TII - Telecommunication Infrastructure Index iii) HCI -Human Capital Index. Constructing a model for the measurement of digitized services, the Survey assesses the 193 member states of the UN according to a quantitative composite index of e-government readiness based on website assessment; telecommunication infrastructure and human resource endowment.\n\nA diverse group of 100 researchers online volunteers from across the globe engaged with the United Nations Department of Economic Affairs (UN DESA) to process 386 research surveys carried out across 193 UN Member States for the 2016 UN E-Government Survey. The diversity of nationalities and languages of the online volunteers—more than 65 languages, 15 nationalities, of which half are from developing countries—mirrors perfectly the mission of the survey.\n\nThe survey has been criticized not including an index of digital inclusion levels.\n\nE-government should enable anyone visiting a city website to communicate and interact with city employees via the Internet with graphical user interfaces (GUI), instant-messaging (IM), learn about government issues through audio/video presentations, and in any way more sophisticated than a simple email letter to the address provided at the site\"\n\nThe essence of e-governance is \"The enhanced value for stakeholders through transformation\" and \"the use of technology to enhance the access to and delivery of government services to benefit citizens, business partners and employees\". The focus should be on:\nWhilst e-government has traditionally been understood as being centered around the operations of government, e-governance is understood to extend the scope by including citizen engagement and participation in governance. As such, following in line with the OECD definition of e-government, e-governance can be defined as the use of ICTs as a tool to achieve better governance.\n\nThe primary delivery models of e-government can be divided into:\n\nWithin each of these interaction domains, four kinds of activities take place:\n\nWhile e-government is often thought of as \"online government\" or \"Internet-based government,\" many non-Internet \"electronic government\" technologies can be used in this context. Some non-Internet forms include telephone, fax, PDA, SMS text messaging, MMS, wireless networks and services, Bluetooth, CCTV, tracking systems, RFID, biometric identification, road traffic management and regulatory enforcement, identity cards, smart cards and other near field communication applications; polling station technology (where non-online e-voting is being considered), TV and radio-based delivery of government services (e.g., CSMW), email, online community facilities, newsgroups and electronic mailing lists, online chat, and instant messaging technologies.\n\nThe main disadvantages concerning e-government are the lack of equality in public access to computers and the internet (the \"digital divide\", a reference to the fact that people who have low incomes, who are homeless and/or who live in remote regions may have little or no access to the Internet), reliability of information on the web, and issues that could influence and bias public opinions. There are many considerations and potential implications of implementing and designing e-government, including disintermediation of the government and its citizens, impacts on economic, social, and political factors, vulnerability to cyber attacks, and disturbances to the \"status quo\" in these areas. See also Electronic leviathan.\n\nThe political nature of public sector forms are also cited as disadvantages to e-government systems.\n\nTrust in e-governance is very highly dependent on its performance and execution, which can be measured through the effectiveness of current actions. This is much riskier and prone to fluctuation than a system of trust that is based on reputation because performance does not consider past actions.\n\nBecause E-government is in the early stages of development in many countries and jurisdictions, it is hard to be applied to forms of government that have been institutionalized. Age-old bureaucratic practices being delivered in new mediums or using new technologies can lead to problems of miscommunication\n\nIncreased electronic contact and data exchange between government and its citizens goes both ways. Once e-government technologies become more sophisticated, citizens will be likely be encouraged to interact electronically with the government for more transactions, as e-services are much less costly than bricks and mortar service offices (physical buildings) staffed by civil servants. This could potentially lead to a decrease in privacy for civilians as the government obtains more and more information about their activities. Without safeguards, government agencies might share information on citizens. In a worst-case scenario, with so much information being passed electronically between government and civilians, a totalitarian-like system could develop. When the government has easy access to countless information on its citizens, personal privacy is lost.\n\nAlthough \"a prodigious amount of money has been spent\" on the development and implementation of e-government, some say it has yielded only a mediocre result. The outcomes and effects of trial Internet-based government services are often difficult to gauge or users seem them unsatisfactory.\nAccording to Gartner, Worldwide IT spending is estimated to total $3.6 trillion in 2011 which is 5.1% increase from the year 2010 ($3.4 trillion).\n\nAn e-government website that provides government services often does not offer the \"potential to reach many users including those who live in remote areas [without Internet access], are homebound, have low literacy levels, exist on poverty line incomes.\" Homeless people, people in poverty and elderly people may not have access.\n\nOpponents of e-government argue that online governmental transparency is dubious because it is maintained by the governments themselves. Information can be added or removed from the public eye. To this day, very few organizations monitor and provide accountability for these modifications. Those that do so, like the United States’ OMBWatch and Government Accountability Project, are often nonprofit volunteers. Even the governments themselves do not always keep track of the information they insert and delete.\n\nThe ultimate goal of the e-government is to be able to offer an increased portfolio of public services to citizens in an efficient and cost-effective manner. E-government allows for government transparency. Government transparency is important because it allows the public to be informed about what the government is working on as well as the policies they are trying to implement.\nSimple tasks may be easier to perform through electronic government access. Many changes, such as marital status or address changes can be a long process and take a lot of paperwork for citizens. E-government allows these tasks to be performed efficiently with more convenience to individuals.\nE-government is an easy way for the public to be more involved in political campaigns. It could increase voter awareness, which could lead to an increase in citizen participation in elections.\nIt is convenient and cost-effective for businesses, and the public benefits by getting easy access to the most current information available without having to spend time, energy and money to get it.\n\nE-government helps simplify processes and makes government information more easily accessible for public sector agencies and citizens. For example, the Indiana Bureau of Motor Vehicles simplified the process of certifying driver records to be admitted in county court proceedings. Indiana became the first state to allow government records to be digitally signed, legally certified and delivered electronically by using Electronic Postmark technology. In addition to its simplicity, e-democracy services can reduce costs. Alabama Department of Conservation & Natural Resources, Wal-Mart and NIC developed an online hunting and fishing license service utilizing an existing computer to automate the licensing process. More than 140,000 licenses were purchased at Wal-Mart stores during the first hunting season and the agency estimates it will save $200,000 annually from service.\n\nThe anticipated benefits of e-government include efficiency, improved services, better accessibility of public services, sustainable community development and more transparency and accountability.\n\nOne goal of some e-government initiatives is greater citizen participation. Through the Internet's Web 2.0 interactive features, people from all over the country can provide input to politicians or public servants and make their voices heard. Blogging and interactive surveys allow politicians or public servants to see the views of the people on any issue. Chat rooms can place citizens in real-time contact with elected officials or their office staff or provide them with the means to interact directly with public servants, allowing voters to have a direct impact and influence in their government. These technologies can create a more transparent government, allowing voters to immediately see how and why their representatives in the capital are voting the way they are. This helps voters decide whom to vote for in the future or how to help the public servants become more productive.\n\nA government could theoretically move more towards a true democracy with the proper application of e-government. Government transparency will give insight to the public on how decisions are made and hold elected officials or public servants accountable for their actions. The public could become a direct and prominent influence in government legislature to some degree.\n\nProponents of e-government argue that online government services would lessen the need for hard copy paper forms. Due to recent pressures from environmentalist groups, the media, and the public, some governments and organizations have turned to the Internet to reduce paper use. The United States government utilizes the website http://www.forms.gov to provide \"internal government forms for federal employees\" and thus \"produce significant savings in paper. As well, if citizens can apply for government services or permits online, they may not need to drive into a government office, which could lead to less air pollution from gas and diesel-fuelled vehicles.\n\nE-government allows citizens to interact with computers to achieve objectives at any time and any location and eliminates the necessity for physical travel to government agents sitting behind desks and windows. Many e-government services are available to citizens with computers and Internet access 24 hours a day and seven days a week, in contrast to bricks and mortar government offices, which tend to be only open during business hours (notable exceptions are police stations and hospitals, which are usually open 24 hours a day so that staff can deal with emergencies).\n\nImproved accounting and record keeping can be noted through computerization, and information and forms can be easily accessed by citizens with computers and Internet access, which may enable quicker processing time for applications and find information. On the administrative side, access to help find or retrieve files and linked information can now be stored in electronic databases versus hard copies (paper copies) stored in various locations. Individuals with disabilities or conditions that affect their mobility no longer have to be mobile to be active in government and can access public services in the comfort of their own homes (as long as they have a computer and Internet and any accessibility equipment they may need).\n\nRecent trials of e-government have been met with acceptance and eagerness from the public. Citizens participate in online discussions of political issues with increasing frequency, and young people, who traditionally display minimal interest in government affairs, are drawn to electronic voting procedures.\n\nAlthough Internet-based governmental programs have been criticized for lack of reliable privacy policies, studies have shown that people value prosecution of offenders over personal confidentiality. Ninety percent of United States adults approve of Internet tracking systems of criminals, and 57% are willing to forgo some of their personal internet privacy if it leads to the prosecution of criminals or terrorists.\n\nThere are also some technology-specific sub-categories of e-government, such as m-government (mobile government), ubiquitous government), and g-government (GIS/GPS applications for e-government).\n\nThe previous concern about developments in E-government concerning technology are due to the limited use of online platforms for political reasons by citizens in local political participations.\n\nThe primary delivery models of e-government are classified depending on who benefits. In the development of the public sector or private sector portals and platforms, a system is created that benefits all constituents. Citizens needing to renew their vehicle registration have a convenient way to accomplish it while already engaged in meeting the regulatory inspection requirement. On behalf of a government partner, the business provides what has traditionally, and solely, managed by the government and can use this service to generate profit or attract new customers. Government agencies are relieved of the cost and complexity of having to process the transactions.\n\nTo develop these public sector portals or platforms, governments have the choice to internally develop and manage, outsource, or sign a self-funding contract. The self-funding model creates portals that pay for themselves through convenience fees for certain e-government transactions, known as self-funding portals.\n\nSocial networking services and websites are an emerging area for e-democracy. The social networking entry point is within the citizens’ environment and the engagement is on the citizens’ terms. Proponents of e-government perceive the government's use of social networking as a medium to help the government act more like the public it serves. Examples can be found at almost every state government portal through Facebook, Twitter, and YouTube widgets.\n\nGovernment and its agents also have the opportunity to follow citizens to monitor satisfaction with services they receive. Through ListServs, RSS feeds, mobile messaging, micro-blogging services and blogs, government and its agencies can share information to citizens who share common interests and concerns. Government is also beginning to Twitter. In the state of Rhode Island, Treasurer Frank T. Caprio is offering daily tweets of the state's cash flow. For a full list of state agencies with Twitter feeds, visit NIC. For more information, visit transparent-gov.com.\n\nGovernment 2.0 or Gov 2.0 refers to government policies that aim to harness collaborative technologies and interactive Internet tools to create an open-source computing platform in which government, citizens, and innovative companies can improve transparency and efficiency. Put simply, Gov 2.0 is about \"putting government in the hands of citizens\". Gov 2.0 combines interactive Web 2.0 fundamentals with e-government and increases citizen participation by using open-source platforms, which allow development of innovative apps, websites, and widgets. The government's role is to provide open data, web services, and platforms as an infrastructure.\n\nFollowing the transition from the longstanding Kenya African National Union government to the National Rainbow Coalition government in December 2002, in January 2004 a Directorate of e-government was established after an executive (cabinet) session. The newly created department had the duty to draw the plan of action for future ICT implementations.\n\nLike many other African nations, Kenya has embraced the high mobile penetration rate within its population. Even people living in remote areas that did not have access to traditional telecommunications' networks can now communicate with ease. The fact of the same has, and continues to have, a great impact on the governments' strategies in reaching out to its citizens.\nGiven that about 70% of the population owns mobile phones, leading mobile network operators like Safaricom have taken a great step in offering services that meet citizens' demands. Such services include Kipokezi (which allows subscribers to do online chatting and also exchange electronic mails via standard mobile phones), and M-Pesa (which allows the subscribers to send and receive electronic cash). Such services have even appealed to the majority of Kenyans, as they support the branchless members of the society too, in undertaking normal and secure businesses via M-Pesa. The recent IMF report reveals that MPESA transactions in Kenya exceeded those carried out by the Western Union worldwide.\n\nWebsite: Open Kenya | Transparent Africa\n\nThe eGovernment web portal has been developed to provide more convenient access to various government services and information through one window. Services can now be delivered to people at their convenience, and more importantly, now have a lot more weight on transparency and accountability of public services.\n\nThe E-Governance initiatives and programs in India are undertaken by the Ministry of Electronics and Information Technology (MeitY www.meity.gov.in ).\nThe current umbrella program for e-governance of Government of India is known by the title \"DIGITAL INDIA\" (www.digitalindia.gov.in)\n\nIndian government has launched many e-governance initiatives, including a portal for public grievance,<ref name=\"http://pgportal.gov.in/\"></ref> MCA21 Mission Mode Project,<ref name=\"http://www.mca.gov.in/MCA21/\"></ref> e-Filing of income tax,<ref name=\"https://incometaxindiaefiling.gov.in/\"></ref> e-gazette,<ref name=\"http://www.egazette.nic.in/\"></ref> Project Nemmadi, and their overall digital India policy.<ref name=\"http://deity.gov.in/sites/upload_files/dit/files/Digital%20India.pdf\"></ref>\n\nE-government in Indonesia is developing, especially in central and regional/local government offices. E-government was officially introduced to public administration by Presidential Directive No 6/2001 on Telematics, which states that the government of Indonesia has to use telematics technology to support good governance. Furthermore, e-government should have been introduced for different purposes in government offices. As one of the ISO member countries, Indonesia gives more attention to facilitating the activities of standardization. Among of the facilities provided are building the National information system of standardization (SISTANAS) and Indonesia Standardization Information Network (INSTANET). As of 2017, ministries, institutions and local governments of Indonesia used to run separate e-government systems, which is now integrated into a centrally based system. In 2017, the government has also undertaken programs for digitization of SMEs and the informal sector. Many of the cities across Indonesia including Jakarta, Bandung, Surabaya,and Makassar are implementing the concept of Smart City, consisting of e-government, e-health, e-education, e-logistics and e-procurement as priority areas.\n\nIn 2002, Iran published a detailed report named TAKFA (Barnameye Tose-e va Karborde Fanavaie Etela’at) in which it was predicted that most of the government bodies would try to virtualize their services as soon as possible. However, based on the reports by UN bodies, Iran has failed in recent years to meet the average standards of e-government. In 2008, the Supreme Council of Information released a report which criticized the government for its poor advancement in employing new communication technologies for administration purposes.\n\nIn 2016, Iran launched the National Information Network and improved the quality and speed of internet access. In 2017 Iran introduced phase one of e-government including E-Tax, E-Customs, E-Visa, E-Government Portal, and a mobile application to modernize Iran's government services.\n\nThe Iranian government plans to introduce other phases of E-gov soon.\n\nThe Iraqi E-government citizen program was established to \"eliminate bribery and favoritism and end the citizens' suffering in going back repeatedly to directories\", the interface lets the citizen send requests and complaints, it can also be used for issuing identity cards, driving licenses and passports.\n\nIn Malaysia, the e-government efforts are undertaken by the Malaysian government, under the umbrella of Multimedia Super Corridor (MSC) and e-government flagships, which was launched in mid-1996, by Dr Mahathir Mohamad (1981-2003), by the then Prime Minister of Malaysia (Jeong & Nor Fadzlina, 2007).\n\nElectronic government is an initiative aimed at reinventing how the government works. It seeks to improve how the government operates, as well as how it delivers services to the people (Ibrahim Ariff & Goh Chen Chuan, 2000).\n\nThe Yangon City Development Committee (Burmese- ရန်ကုန်မြို့တော်စည်ပင်သာယာရေးကော်မတီ) (YCDC) is the administrative body of Yangon, and Yangon is the largest city and former capital of Myanmar (Burma). The Yangon City Development Committee consists of 20 departments. Its headquarters was on the Yangon City Hall. The committee's chairman is also the city's mayor.\n\nIn 2003, YCDC was organized to provide e-Government for Yangon City. The main purposes of the city's e-Government program are to provide easy access between the government and the city's citizens via the Internet, to reduce paper usage, to reduce the city budget, to build the city's fiber ring, to provide timely public information, to store public data and to develop and expand G2G, G2C, G2B, and G2E programs.\n\nIn January 2013 responsibility for e-Government was divided between the e-Government Administration Committee and the e-Government Processing Committee. The e-Government Administration Committee includes the Mayor of Yangon City as Chief, the General Secretary of Yangon City as Sub-Chief, and the other 20 head of department officers as chairmen. The e-Government Processing Committee includes the Head of Public Relation and Information Department as Chief and the other 20 deputy head of department officers as chairmen.\n\nThe official web-portal is www.ycdc.gov.mm.\n\nMandalay is the second-largest city and the last royal capital of Myanmar (Burma).\nIn 2014, Mandalay Region Government developed www.mdyregion.gov.mm to know about regional government and their activities to people.\n\nMandalay Region Government organized the e-Government Steering Committee on 23 June 2016.\nThat committee chairman was U Sai Kyaw Zaw, Minister, Ministry of Ethnic Affairs.\n\nOn 21 July 2017 www.emandalay.gov.mm web portal was opened by Dr. Zaw Myint Maung, Prime Minister of Mandalay Region Government.\nThat portal includes 2 e-services, 199 topics from 70 agencies.\nThe committee develops a Regional Data Center too. That Datacenter will be opened in 2018.\n\nThe e-Government planning and conceptual framework has been presented to Nepal in extensive support from the Government of Korea (KIPA). E-government Vision is ‘The Value Networking Nepal’ through:\nNepal's E-government mission statement is \"Improve the quality of people’s lives without any discrimination, transcending regional and racial differences, and realize socio-economic development by building a transparent government and providing value-added quality services through ICT.\"\n\nThe e-Government practice has been slow both in adoption and practice in Nepal. However, local government bodies now have dedicated team of ICT Volunteers working towards implementing e-Government in the country through an extensive ICT for Local Bodies initiatives.\n\nIn 2014, the Government of Pakistan created the National Information Technology Board under the Ministry of Information Technology & Telecom to enable a digital eco-system for Government services to the citizens of Pakistan. NITB was formed as a result of a merger between Pakistan Computer Bureau (PCB) and Electronic Government Directorate (EGD). \n\nThe key functions identified by the NITB are:\n\nNITB rolled out an e-Office Suite across various ministries in the Government of Pakistan. While it clearly pursued efficiency gains and improved transparency, it also hoped to deliver \"efficient and cost-effective public services to citizens of Pakistan.\" The suite primarily included five modules or applications across all the ministries. Description of each module listed are: \nNITB released a high-level diagram that describes the process of transforming Federal Government agencies and ministries to e-Office environments.\n\nCriticism: NITB's rollout of the e-Office suite across almost all federal agencies is not only overly ambitious but also likely to fail. It seems to put together a lot of lofty organizational efficiency goals with a set of delivery or citizen-facing targets. In fact, most of the services NITB has provided have been largely conceptual and not sufficient concrete. The process outlined in the adoption process diagram seems devoid of any user-centric design or value proposition formulation. Instead of creating lots of MVPs (Minimum Viable Products) and taking advantage of an iterative and validated learning the process, the e-Office Suite seems to incorporate all the features and functions that various ministries and divisions may need or use. It seems to focus more on the needs of the bureaucrats and government agencies rather than the needs of the end-user (citizens of Pakistan) and what services would they need that a ministry or division can provide.\n\nSri Lanka have taken some initiative actions to provide the benefits of e-government to the citizens.\n\nTo implement the principles of e-government, the Ministry of Information and Telecommunication Technologies of Thailand developed a plan for creating a modern e-services system during 2009-2014.\n\nThe next stage was the five-year project of the digital government, which began in 2016 and will be completed in 2021. This project assumes that within five years, more than 80% of Thai government agencies will use electronic documents for identification.\n\nThere is the Unified State Portal of e-Government of Thailand, developed by the Ministry of Information and Telecommunications Technology in 2008.\n\nIn 2018, Thailand ranks 73rd in the UN e-government ranking.\n\nJordan has established its e-government program since 2002. many governmental services are provisioned online.\n\nThe e-Government was also established in Saudi Arabia, and it offers online government services and transactions.\n\nIn the United Arab Emirates, the Emirates eGovernment is designed for e-government operations.\n\nThe \"E-Government\" framework was established in accordance with the \"National Strategy on Information-Communication Technologies in the Development of the Republic of Azerbaijan (2003-2012)\" and implemented in the framework of the \"E-Azerbaijan\" Program. The project is aimed to increase the convenience and efficiency of the activity of state agencies, simplify interactions between population, businesses, and government agencies, contribute to creating new citizen-official relations framework and ensure transparency and free flow of information.\n\nThe main components of the e-government infrastructure are integrated network infrastructure for state bodies, E-government portal, E-government gateway, State register of information resources and systems, e-signature, e-document circulation and e-government data center (under preparation).\n\nState portal www.e-gov.az was established to facilitate citizens in benefiting from e-services provided by government agencies on a ‘single window’ principle with the combination of services. Through e-government portal, citizens can use more than 140 e-services of 27 state agencies. Besides, a gateway between government agencies was established to ensure the mutual exchange of information, and most state agencies are connected to this infrastructure. The gateway allows users to efficiently use the existing government information systems and safe contact between them, issuing requests and rendering e-services, liberates citizens from providing same information or documents which are already available in information databases.\n\nOn 14 March 2018, it was launched E-government Development Center. It is a public legal entity that is subordinated to State Agency for Public Service and Social Innovations under the President of the Republic of Azerbaijan. The service tries to utilize digital technologies, establish e-government to make state services operate more efficiently, ensure public services availability, and improve the living standards of the citizens of the country. It is government-to-citizen type of the e-governance.\n\nThe e-government portal egov.kz was launched in 2012 as part of Kazakhstan's effort to modernize how citizens access government services and information. The egov.kz mobile app was recognized as best app in the GovTechioneers competition at the 2017 World Government Summit in Dubai.\n\nArmenian e-government was established in 2004. E-government brings together all tools and databases created by Armenian state agencies and provides a user-friendly online environment for users. It includes more than twenty services and tools. Under this initiative, \"Interactive Budget\" and \"State Non-Commercial Organisations' Financing\" sections are available for the first time. There are also twenty other tools, including search engines, allowing to find the Government's and the Prime Minister's decisions, the agenda of the next cabinet sitting, information on the state purchases, the electronic tax reporting system, the online application system of the Intellectual Property Agency, the information search system of the Intellectual Property Agency, as well as the Electronic Signature and Electronic Visa (e-visa) sections. It is worth mentioning that the Electronic Signature is used in several other services when a user wants to submit an application or receive information. The Electronic Signature is universal system and is used both by the state officials and by citizens, legal entities.\n\n1) E-License\n\nThis system allows companies to submit an application for obtaining or terminating licenses regarding various activities (pharmaceuticals, banking, construction, transport etc.) It also provides other services in respect of already obtained license.\n\n2) System of reports on licensed activities\n\nThe Report Acceptance System for licensed persons enables to submit any report (annually, monthly or quarterly) on licensed activities.\n\n3) E-Payments\n\nElectronic Payment System effectively processes online payments. This application is designed specifically for charging the state fees, local fees, the administrative penalties or services provided by state and local governmental bodies. Payments can be made by Visa, Mastercard, PayPal and local Arca or Mobidram systems.\n\n4) E-Cadastre\n\nThe system enables to submit an application to the property cadastre and receives information on landowners, the surface of a plot of land, legal status of any property. The state electronic payment system is integrated into this tool. Online applications for registration of rights and restrictions and related documents may be submitted by users who have a digital signature.\n\n5) E-Draft\n\nIn 2016 the Ministry of Justice of Armenia developed Legal Drafts' Database. It is designed particularly for publication any draft initiated by the Government or Member of Parliaments. The database can be accessed through a website which provides the possibility of presenting the legal acts’ drafts to the public, organizing online discussions, and as a consequence - the active participation of representatives of civil society in the law-making process. The website enables them to search legal drafts, follow their further progress, and become familiar with the presented suggestions. The registered users can present suggestions, get informed with the \"summary paper\" of the suggestions to the draft, the adopted suggestions or the reasoning concerning the not adopted ones.\n\n6) E-Register\n\nThe system enables registration of legal entities, such as limited liability companies, joint-stock companies, foundations, and self-employed entrepreneurs. On average it takes twenty minutes to register a company depending on the entity's type. State fee can be paid through E-Payments system. The system also allows users to track the submitted applications and search existing companies as well as purchase full information about any company, including information about shareholders.\n\n7) Datalex\n\nThis system allows users to find cases, search for laws of Armenia, as well as to follow the schedule of court hearings.\n\n8) E-Announcement\n\nThe system is designed for public announcements. The state authorities are obliged to make public announcements under certain circumstances stipulated by law.\n\n10) E-Tax\n\nThis tool simplifies the tax declaration process for both taxpayers and tax authorities. Any natural person or legal entity can submit tax declaration verifying it by electronic signature.\n\n11) E-IP\n\nOnline submission of patent and trademark applications using electronic signature.\n\n12) E-Visa This application enables the process of obtaining a visa through an electronic application. Visas are issued within two days.\n\n13) E-Signature\n\nThe system allows users to verify the identity of the user and protect the submitted application. Any resident of Armenia, either a natural person or legal entity, can obtain an electronic signature and use it while applying E-Government systems.\n\nOn the Federal Law \"On providing state and municipal services\" (2010), the strategy on development of Information Society in the Russian Federation, approved by the President (2008), the Federal target programme \"Electronic Russia\" (2002 – 2010 years), approved by the Government (2002), the State Programme \"Information Society\" (2010), the Procedure on development and approval of administrative regulations execution of public functions (public services), approved by the Government (2005), the concept of administrative reform in the Russian Federation in 2006 - 2010 respectively, approved by the Government (2005), on other orders, resolutions and acts in the Russian Federation was created electronic government (or e-government).\n\nThe main target on creating e-government lies in the field of providing equal opportunities for all the Russians in spite of their living place and their incomes and make a more effective system of public administration. So e-government is created for reaching the useful system of public management accommodating the individual interests of every citizen by participation through ICTs in public policy-making.\n\nNowadays Russian e-government includes such systems as:\n\n1. The United interagency Interacting system using for providing of state and municipal services, exchange of information and data between participants of interagency interacting, quick approval of state and municipal decisions, etc.\n\n2. The United system for authentication and authorization providing evidence of the rights of all participants of e-government.\n\n3. United portal of state and municipal services and functions which are the \"single window\" for all information and services assured by government and municipals.\n\nThe portal of public services is one of the key elements of the project to create an \"electronic government\" in the country. The portal provides a single point of access to all references on state and municipal services through the Internet and provides citizens and organizations the opportunity to receive these services electronically. Monthly visits by users of the public services portal range between 200,000 and 700,000. For example, citizens are now able to get or exchange a driver's license through this portal.\n\n4. Head system providing utilization of electronic signature.\n\nOther systems located on cloud services.\n\nToday Russian e-government elements are demanded in the spheres of e-governance, e-services (e-health, e-education, e-library, etc.), e-commerce, e-democracy (web-election, Russian public initiative). By the United Nations E-Government Survey 2012: E-Government for the People Russia became one of the 7 emerging leaders in e-government development, took 9th place in rating of e-government development in largest population countries, took 8th rank in Top e-participation leaders, after Norway, Sweden and Chile, Advancing 32 positions in the world rankings, the Russian Federation became the leader of e-government in Eastern Europe. Evolution of ICT in the Russian Federation provided the raising of Russia in e-government development index to the 27 places.\n\ne-government shows significant advancement in Europe.\n\nThe current Clerk of the Privy Council – the head of the federal public service has made workplace renewal a pillar of overall public service renewal. The key to workplace renewal is the adoption of collaborative networked tools. An example of such a tool is GCPEDIA – a wiki platform for federal public servants. Other tools include GCconnex, a social networking tool, and GCforums, a discussion board system.\n\nThe election of Barack Obama as President of the United States became associated with the effective use of Internet technologies during his campaign and in the implementation of his new administration in 2009.\nOn January 21, 2009, the President signed one of his first memorandums – the Memorandum for the Heads of Executive Departments and Agencies on Transparency and Open Government. The memo called for an unprecedented level of openness in Government, asking agencies to \"ensure the public trust and establish a system of transparency, public participation, and collaboration.\" The memo further \"directs the Chief Technology Officer, in coordination with the Director of the Office of Management and Budget (OMB) and the Administrator of General Services (GSA), to coordinate the development by appropriate executive departments and agencies [and] to take specific actions implementing the principles set forth in the memorandum.\"\n\nPresident Obama's memorandum centered around the idea of increasing transparency throughout various different federal departments and agencies. By enabling public websites like recovery.gov and data.gov to distribute more information to the American population, the administration believes that it will gain greater citizen participation.\n\nIn 2009 the U.S. federal government launched Data.gov to make more government data available to the public. With data from Data.Gov, the public can build apps, websites, and mashups. Although \"Gov 2.0\", as a concept and as a term, had been in existence since the mid-2000s, it was the launch of Data.gov that made it \"go viral\".\n\nIn August 2009 the City of San Francisco launched DataSF.org with more than a hundred datasets. Just weeks after the DataSF.org launch, new apps and websites were developed. Using data feeds available on DataSF.org, civic-minded developers built programs to display public transportation arrival and departure times, where to recycle hazardous materials, and crime patterns. Since the launch of DataSF.org there have been more than seventy apps created with San Francisco's data.\n\nIn March 2009, former San Francisco Mayor Gavin Newsom was at Twitter headquarters for a conversation about technology in government. During the town hall, Newsom received a tweet about a pothole. He turned to Twitter co-founders Biz Stone and Evan Williams and said let's find a way for people to tweet their service requests directly to San Francisco's 311 customer service center. Three months later, San Francisco launched the first Twitter 311 service, called @SF311, allowing residents to tweet, text, and send photos of potholes and other requests directly to the city. Working with Twitter and using the open-source platform, CoTweet turned @SF311 into reality. The software procurement process for something like this would normally have taken months, but in this case, it took less than three months. The @SF311 is saving the city money in call center costs.\n\n"}
{"id": "201973", "url": "https://en.wikipedia.org/wiki?curid=201973", "title": "National Convention", "text": "National Convention\n\nThe National Convention () was the first government of the French Revolution, following the two-year National Constituent Assembly and the one-year Legislative Assembly. Created after the great insurrection of 10 August 1792, it was the first French government organized as a republic, abandoning the monarchy altogether. The Convention sat as a single-chamber assembly from 20 September 1792 to 26 October 1795 (4 Brumaire IV under the Convention's adopted calendar).\n\nThe Convention came about when the Legislative Assembly decreed the provisional suspension of King Louis XVI and the convocation of a National Convention to draw up a new constitution with no monarchy. The other major innovation was to decree that deputies to that Convention should be elected by all Frenchmen twenty-five years old or more, domiciled for a year and living by the product of their labor. The National Convention was, therefore, the first French assembly elected by a suffrage without distinctions of class.\n\nAlthough the Convention lasted until 1795, power was effectively stripped from the elected deputies and concentrated in the small Committee of Public Safety from April 1793. The eight months from the fall of 1793 to the spring of 1794, when Maximilien Robespierre and his allies dominated the Committee of Public Safety, represent the most radical and bloodiest phase of the French Revolution, known as the Reign of Terror. After the fall of Robespierre, the Convention lasted for another year until a new constitution was written, ushering in the French Directory.\n\nThe indirect election took place from 2 to 10 September 1792 after the election of the electoral colleges by primary assemblies on 26 August. Despite the introducation of universal male suffrage, the turn-out was low, though election saw an increased in comparison to the 1791 elections - in 1792 11.9% of a greatly increased electorate votes, compared to 10.2% of a much smaller electorate in the 1791.\nThe low turn-out was partly to a fear of victimization; in Paris, Robespierre presided over the elections and, in concert with the radical press, managed to exclude any candidate of royalist sympathies. In the whole of France, only eleven primary assemblies wanted to retain the monarchy. Of the electoral assemblies, all tacitly voted for a republic – though only Paris used the word. The elections returned the same sort of men that the active citizens had chosen in 1791.\n\nOn 20 September the Convention held its first session in the \"Salle des Cent-Suisses\", the next day it moved to the Salle du Manège, with little room for the public and a bad acoustic. From 10 May 1793 it met in the Salle des Machines, an immense hall in which the deputies were loosely scattered. The Salle des Machines had galleries for the public who often influenced the debates with interruptions or applause. \n\nThe members of the Convention came from all classes of society, but the most numerous were lawyers. 75 members had sat in the National Constituent Assembly, 183 in the Legislative Assembly. The full number of deputies was 749, not counting 33 from the French colonies, of whom only some arrived in Paris in time. Thomas Paine and Anacharsis Cloots were appointed in the Convention by Girondins. Besides these, however, the newly formed annexed to France from 1782 to 1789 were allowed to send deputations.\n\nAccording to its own ruling, the Convention elected its President every fortnight, and the outgoing President was eligible for re-election after the lapse of a fortnight. Ordinarily, the sessions were held in the morning, but evening sessions also occurred frequently, often extending late into the night. Sometimes in exceptional circumstances, the Convention declared itself in permanent session and sat for several days without interruption. For both legislative and administrative the Convention used committees, with powers more or less widely extended and regulated by successive laws. The most famous of these committees included the Committee of Public Safety and the Committee of General Security.\n\nThe Convention held legislative and executive powers during the first years of the French First Republic and had three distinct periods: Girondin, Montagnard or Jacobin, and Thermidorian.\n\nThe first session was held on 20 September 1792. The following day, the assembly agreed the proposition \"That royalty be abolished in France\" and was carried with cheers. On the 22nd came the news of the Battle of Valmy. On the same day it was decreed that \"in future the acts of the assembly shall be dated \"First Year of the French Republic\"\". Three days later the corollary that \"the French republic is one and indivisible\" was added to guard against federalism. A republic had been proclaimed, but it remained to enact a republican government. The country was little more republican in feeling or practice than it had been before at any time since Varennes. But now it had to become a republic, because it no longer had a king.\n\nWhen the Convention met the military situation was undergoing an extraordinary transformation that seemed to confirm the Girondin prophecies of easy victory. After Valmy the Prussians withdrew to the frontier, and in November French troops occupied the left bank of the Rhine. The Austrians, who had besieged Lille in October, were defeated by Dumouriez at the Battle of Jemappes on 6 November and evacuated the Austrian Netherlands. Nice was occupied and Savoy proclaimed its union with France. These successes made it safe to quarrel at home.\n\nMost historians divide the National Convention into two main factions: the Girondins and the Montagnards. The Girondins were the more radical democratic faction at the Convention, as opposed to the Montagnards, who were authoritarian populists. They drew their name from the Gironde, a region of France from which many of the deputies of this faction were elected (although many \"Girondins\" were actually Parisian by origin). They were also known as the Brissotins after their most prominent speaker, Jaques Pierre Brissot.\n\nThe Montagnards, who drew their support from the less educated segments of the Parisian population, drew their name from the high bleachers that they sat on while the Convention was in session. The Montagnards dominated the Convention through the threat of physical violence.\n\nThree questions dominated the first months of the Convention: revolutionary violence; the trial of the king; and Parisian dominance of politics.\n\nAntagonism between Paris and the provinces created friction among the people that served as a propaganda tool and combat weapon for the two groups. The departments resisted the idea of centralization. They saw this idea being symbolized by the desire to reduce the capital of the Revolution to its one-eighty-third share of influence. Much of the Gironde wished to remove the Assembly from a city dominated by \"agitators and flatterers of the people\": it did not at the time encourage an aggressive federalism that would have run counter to its political ambitions.\n\nThe Plain was a third faction during the Convention. Though some historians consider these men to be closely associated with the Girondins, the Plain was much more centrist in their ideals. The Plain held the largest group of deputies and derived their name from their place on the floor of the Convention. During the start of the Convention, they sided with the Girondins, however, as it progressed and the Montagnards began to push for the execution of Louis, the Plain began to side with them.\n\nThe Convention's unanimous declaration of a French Republic on 21 September 1792 left open the fate of the former king. A commission was therefore established to examine the evidence against him while the Convention's Legislation Committee considered legal aspects of any future trial. Most Montagnards favoured judgment and execution, while the Girondins were divided concerning Louis's fate, with some arguing for royal inviolability, others for clemency, and some advocating lesser punishment or banishment. On 13 November Robespierre stated in the Convention that a Constitution which Louis had violated himself, and which declared his inviolability, could not now be used in his defence. Robespierre had been taken ill and had done little other than support Saint-Just, who gave his first major speech, in his argument against the king's inviolability. On 20 November, opinion turned sharply against Louis following the discovery of a secret cache of 726 documents consisting of Louis's personal communications with bankers and ministers. At his trial, he claimed not to recognize documents clearly signed by himself.\n\nThe trial began on 10 December. The Montagnards put the debate on the ideological level. Louis XVI was classified as an enemy, alien to the body of the nation and as a \"usurper\". Balloting began on 14 January 1793. Each deputy explained his vote at the rostrum. The vote against the king was unanimous. There was to be no popular referendum as Girondins hoped. The fatal vote started on 16 January and continued until the next day. Of the 721 deputies present, 387 declared themselves for the death penalty, while 334 were opposed. 26 deputies voted for death on condition that he was reprieved. On 18 January the question of reprieve was put to a vote: 380 votes were cast against; 310 for. Each time the Girondins had split.\n\nOn the morning of 21 January the Convention ordered the entire National Guard to line both sides of the route to the scaffold. Louis was beheaded at the Place de la Revolution. Within the nation, \"voters\" and \"appellants\", those that were against the execution of Louis, swore undying hatred of each other. The rest of Europe, fearing the outcome of the French Revolution in their own countries, decreed a war of extermination against regicides.\n\nThe Assembly began harmoniously, but within a few days the Girondins launched a bitter attack on their Montagnard opponents. Conflict continued without interruption until the expulsion of the Girondin leaders from the Convention on 2 June 1793. The Girondins had relied on votes from the majority of the deputies, many of whom were alarmed as well as scandalized by the September massacres, but their insistence on monopolizing all positions of authority during the Convention, and their attacks on the Montagnard leaders, soon irritated them, causing them to regard the party as a faction. One by one, able deputies such as Couthon, Cambon, Carnot, Lindet and Barère began to gravitate towards the Montagnards, while the majority – the Plain, as it was called – held itself aloof from both sides.\n\nGirondins were convinced that their opponents aspired to a bloody dictatorship, while the Montagnards believed that the Girondins were ready for any compromise with conservatives, and even royalists, that would guarantee their remaining in power. The bitter enmity soon reduced the Convention to a state of limbo. Debate after debate degenerated into verbal brawling from which no decision could emerged. The political deadlock, which had repercussions all over France, eventually drove men to accept dangerous allies, royalists in the case of Girondins, sans-culottes in that of the Montagnards.\nThus the struggle within the Convention continued without results. The decision was to come from outside. Since the king's trial, the sans-culottes had been constantly assailing the \"appealers\" (\"appelants\"), and quickly came to desire their expulsion from the Convention. They demanded establishing a Revolutionary Tribunal to deal with supposed aristocratic plots. Military setbacks from the First Coalition, Dumouriez's defection to the enemy and the war in the Vendée, which began in March 1793, were all used as arguments by Montagnards and \"sans-culottes\" to portray Girondins as soft. They demanded that the Girondins take measures, but the Girondins were reluctant to adopt the proposed measures. The Girondins were forced to accept the Montagnards creation of the Revolutionary Tribunal and a Committee of Public Safety. Social and economic difficulties exacerbated the tensions between the groups. \nThe final showdown was precipitated by Jean-Paul Marat's trial and the arrest of sectional activists. On 25 May the Paris Commune marched to the Convention to demand the release of these activists. In reply, Maximin Isnard, who was presiding over the Convention, launched into a diatribe reminiscent of the Brunswick Manifesto: \"If any attack made on the persons of the representatives of the nation, then I declare to you in the name of the whole country that Paris would be destroyed\". On the next day the Jacobins declared themselves in a state of insurrection. On 28 May the \"Cité\" section called the other sections to a meeting in order to organize the insurrection. On 29 May the delegates representing 33 of the sections formed an insurrectionary committee of nine members. On 2 June, 80,000 armed \"sans-culottes\" surrounded the Convention. After an attempt of deputies to leave was stopped with guns, the deputies resigned themselves to declare the arrest of 29 leading Girondins. In this way the Gironde ceased to be a political force.\n\nScarcely had the Gironde been eliminated when the Convention, now under Montagnard leadership, found itself caught between two threats. While the federalist revolt gained strength, the popular movement, roused to fury by high prices, was increasing the pressure it exercised on the government. Meanwhile, the Government was proving incapable of controlling the situation. In July 1793 the nation appeared to be on the point of falling apart.\n\nDuring the month of June the Montagnards played for time. Yet the Convention did not overlook the peasants. It was to these latter that the revolution of 31 May (like those of 14 July and 10 August) brought a substantial benefit. On 3 June the sale of the property of emigrants, in small parcels and payable in ten years, was decreed; on the 10th, the optional division of common lands by head; and on 17 July, the abolition, without compensation, of all that remained of manorial rights.\n\nThe Montagnards attempted to reassure the middle classes by rejecting any idea of terror, by protecting property rights, and by restricting the popular movement to very narrowly circumscribed limits. It was a delicate balance to achieve, a balance that was destroyed in July by the worsening of the crisis. The Convention rapidly approved the new constitution, hoping to clear itself of the charge of dictatorship and calm the anxieties of the departments\".\n\nThe Declaration of Rights which precedes the text of the Constitution solemnly reaffirmed the nation's indivisibility and the great principles of freedom of the press, equality and resistance to oppression. It went far beyond the Declaration of 1789, adding to it the right to public assistance, work, education and insurrection. No man could impose his will on others. All political and social tyranny was abolished. Although the \"montagnards\" had refused to be led further down the road to democracy, the Constitution became the bible of all democrats.\n\nThe chief aim of the Constitution was to ensure the major role of the deputies in the Convention, which was seen as being the essential basis for political democracy. The Legislative Assembly was to be elected by direct vote cast for a single member; deputies were elected on receiving a simple majority of the votes cast, and the assembly would sit for one year. The executive council of 24 members was chosen by the Legislative Assembly from among the 83 candidates chosen by the departments on the basis of universal male suffrage, and in this way ministers were made responsible to the representatives of the nation. The exercise of national sovereignty was widened through the institution of the referendum – the Constitution was to be ratified by the people, as were laws in certain precisely defined circumstances.\n\nThe Constitution was submitted for popular ratification and adopted by a huge margin of more than 1,801,918 in favour to some 17,610 against. The results of the plebiscite were made public on 10 August 1793, but the application of the Constitution, the text of which was placed in the sacred ark and laid in the debating-chamber of the Convention, was postponed until peace had been made.\n\nIndeed, the Montagnards faced dramatic circumstances – federalist insurrection, war in the Vendée, military failures, and a worsening economic situation. Despite everything, a new civil war could not be avoided. By the middle of June, about sixty departments were in more or less open rebellion. However, the frontier departments had remained faithful to the Convention. The rising was widespread rather than deep. It was essentially the work of the departmental and district administrations. The communes, which were more popular in composition, showed themselves in general lukewarm or hostile; and federalist leaders soon became divided among themselves. Sincere republicans among them could not fail to be uneasy about the foreign invasion and the Vendée. Those who were seeing themselves rejected by the people, sought support from the moderates, the Feuillants and even from the aristocrats.\n\nJuly and August were bad months on the frontiers. Within three weeks Mainz, the symbol of previous successes, capitulated to the Prussians, and the Austrians captured the fortresses of Condé and Valenciennes and invaded northern France. Spanish troops crossed the Pyrenees and began advancing on Perpignan. The Piedmontese took advantage of the diversion of republican forces at Lyons in order to invade France from the East. In Corsica, Paoli's revolt expelled the French from the island with British support. British troops opened the siege of Dunkirk in August and in October the Allies invaded Alsace. The military situation had become desperate.\n\nIn addition there were other incidents which compounded the fury of the revolutionaries and convinced them that their opponents had abandoned all restraint of civilized behavior. On 13 July, Charlotte Corday murdered the \"sans-culotte\" idol Jean-Paul Marat. She had been in touch with Girondin rebels in Normandy and they were believed to have used her as their agent.\n\nThe lack of forethought displayed by the Convention during the first few days was redeemed by its vigor and skill in organizing measures of repression. Warrants were issued for the arrest of the rebellious Girondin leaders; the members of the revolting departmental administration were deprived of their office.\n\nThe regions in which the revolt was dangerous were precisely those in which a large number of royalists had remained. There was no room for a third party between the Mountain, which was identified with the Republic, and royalism, which was the ally of the enemy. The royalist insurrection in the Vendée had already led the Convention to take a long step in the direction of the Terror – that is to say, the dictatorship of central power and the suppression of liberties. The Girondin insurrection now prompted it to take a decisive step in the same direction.\n\nThe Constituent Assembly had legislated through its commissions. The Convention governed by means of its committees. Two of them were of essential importance: Public Safety and General Security. The second, which had formidable powers, is less well known than the first, which was the true executive authority and was armed with immense prerogatives. It dated from April, but its composition was thoroughly reshuffled during the summer of 1793.\n\nThe summer of 1793 saw \"sans-culotte\" disturbances reach a peak under a double banner: price-fixing and terror. On top of this came the news of unprecedented treason: Toulon and its squadron had been handed over to the enemy. In the name of the wretched poverty of the people, the leaders of the Enragés, with Jacques Roux at their head, called for a planned economy from a Convention which had no liking for the idea. But the revolutionary logic of the mobilization of resources by national dictatorship was infinitely more powerful than economic doctrine. In August, a series of decrees gave the authorities discretionary powers over the production and circulation of grain, as well as ferocious punishments for fraud. \"Granaries of plenty\" were prepared, to stock corn requisitioned by authorities in each district. On 23 August the decree on the \"levée en masse\" turned able-bodied civilians into soldiers.\n\nOn 5 September, Parisians tried to repeat the revolt of 2 June. Armed \"sections\" again encircled the Convention to demand the setting up of an internal revolutionary army, the arrest of suspects and a purge of the committees. It was probably the key day in the formation of the revolutionary government: the convention yielded, but kept control of events. It put Terror on the agenda on 5 September, on 6th elected Collot d'Herbois and Billaud-Varenne to the Committee of Public Safety, on the 9th created the revolutionary army, on the 11th decreed the Maximum for grain and fodder (general controls for prices, and wages on the 29th), on the 14th reorganized the Revolutionary Tribunal, on the 17th voted in the law on suspects, and on the 20th gave the local revolutionary committees the task of drawing up lists of them.\n\nThe dictatorship of the Convention and the committees, simultaneously supported and controlled by the Parisian sections, representing the sovereign people in permanent session, lasted from June to September. It governed through a network of institutions set up haphazardly since spring in March, the Revolutionary Tribunal and representatives on missions in the departments; was followed the next month by the Convention's representatives to the armies, also armed with unlimited powers; and enforced acceptance of \"assignat\" as the sole legal tender, price controls for grain and the forced loan of a billion livres from the rich.\n\nAt last France saw a government take shape. Danton resigned from it on 10 July. Couthon, Saint-Just, Jeanbon Saint-Andre, and Prieur of the Marne formed a nucleus of resolute Montagnards who rallied Barère and Lindet, then successfully added Robespierre on 27 July, Carnot and Prieur of Cote-d'Ore on 14 August, and Collot d'Herbois and Billaud-Varenne on 6 September. They had a few clear ideas to which they clung: to command, to fight, and to conquer. Their work in common, the danger, the taste of and pride in power created solidarity that made the Committee an autonomous organism.\n\nThe committee was always managed collegially, despite the specific nature of the tasks of each director: the division into \"politicians\" and \"technicians\" was a Thermidorian invention, intended to lay the corpses of the Terror at the door of the Robespierrists alone. Many things, however, set the twelve committee members at loggerheads; Barère was more a man of the Convention than of the committee and was a link with the Plaine. Robert Lindet had qualms about the Terror which, by contrast, was the outstanding theme of Collot d'Herbois and Billaud-Varenne, latecomers to the committee, forced on it by the \"sans-culottes\" in September; unlike Robespierre and his friends, Lazare Carnot had given his support only provisionally and for reasons of state to a policy concession to the people. But the situation which united them in the summer of 1793 was stronger than those differences of opinion. The Committee had to set itself above all, and choose those popular demands which were most suitable for achieving the Assembly's aims: to crush the enemies of the Republic and dash the last hopes of the aristocracy. To govern in the name of the Convention, at the same time controlling it, and to restrain the people without quenching their enthusiasm — this was a gamble.\n\nThe ensemble of institutions, measures and procedures which constituted it was codified in a decree of 14 Frimaire (4 December) which set the seal on what had been the gradual development of centralized dictatorship founded on the Terror. In the center was the Convention, whose secular arm was the Committee of Public Safety, vested with immense powers: it interpreted the Convention's decrees and settled their methods of application; under its immediate authority it had all state bodies and all civil servants (even ministers would disappear in April 1794); it directed military and diplomatic activity, appointed generals and members of other committees, subject to ratification by the Convention. It held responsibility for conducting war, public order and the provisioning of the population. The Commune of Paris, a famous \"sans-culotte\" bastion, was neutralized by coming under its control.\n\nAdministrative and economic centralization went hand in hand. The state of siege forced France into autarky; to save the Republic the government mobilized all the nation's productive forces and reluctantly accepted the need for a controlled economy, which it introduced extemporaneously, as the emergency required. It was necessary to develop war production, revive foreign trade, and find new resources in France itself; and time was short. Circumstances gradually compelled it to assume the economic government of the country. Along with organization of the army, this was the most original feature of its work.\n\nAll material resources were subjected to requisitioning. Farmers surrendered their grain, fodder, wool, flax, and hemp. Artisans and merchants gave up their manufactured products. Raw materials were carefully sought out – metal of all kinds, church bells, old paper, rags and parchments, grasses, brushwood, and even household ashes for manufacturing of potassium salts, and chestnuts for distilling. All businesses were placed at the disposal of the nation – forests, mines, quarries, furnaces, forges, tanneries, paper mills, large cloth factories and shoe making workshops. The labor of men and the value of things were subject to price controls. No one had a right to speculate at the cost of \"Patrie\" while it was in danger. Armaments caused more concern. As early as September 1793 efforts were made to create a large factory in Paris for rifles and sidearms. A special appeal was made to scientists. Monge, Vandermonde, Berthollet, Darcet, Fourcroy perfected metallurgy and manufacture of arms.\n\nOnly to the wage earners did the Maximum seem thoroughly advantageous. It increased wages by one-half in relation to 1790, and commodities by only one-third. But since the Committee did not ensure that it was respected (except for bread), they would have been duped had they not been benefiting from the favorable conditions that a great war always offers the labor force. Still Paris became calmer, because the \"sans-culottes\" were gradually finding ways to subsist; the \"levée en masse\" and the formation of the revolutionary army were thinning their ranks; many now were working in arms and equipment shops, or in the offices of the committees and ministries, which were expanded enormously.\n\nDuring the summer the requisition of the levy was completed and by July the total strength of the army reached 650,000. The difficulties were tremendous. The war production just started in September. The army was in the middle of the purge. In the spring of 1794 the amalgamation was undertaken. Two battalions of volunteers joined one battalion of regulars to constitute a demi-brigade, or regiment. At the same time the command was reconstituted. The purge ended with most of the nobles excluded. The new generation reached the highest ranks, and the War College (\"Ecole de Mars\") received six young men from each district to improve the staff. Army commanders were to be appointed by the Convention.\n\nWhat gradually emerged was a military command unequaled in quality: Marceau, Hoche, Kleber, Massena, Jourdan, and a host of others, backed by officers who were sound both in their abilities as soldiers and in their sense of civic responsibility. \n\nFor the first time since antiquity a truly national army marched to war, and for the first time, too, a nation succeeded in arming and feeding great numbers of soldiers – these are the novel characteristics of the army of the Year II. The technical innovations resulted chiefly from its sheer size as well the strategy that developed from it. The old system of cordons lost its prestige. Moving between the armies of the Coalition, the French could maneuver along interior lines, deploy part of their troops along the frontiers, and take advantage of the inaction of any one of their enemies to beat the others. Acting \"en masse\", and overwhelming the foe by sheer numbers – such were Carnot's principles. They were still untried, and not until Bonaparte appeared did they enjoy any great success.\n\nAs late as September 1793, there were two distinct wings among the revolutionaries. Firstly those who were later called Hébertists – although Hébert himself was never the official leader of a party – advocated war to the death and adopted the program of the \"Enragés\", ostensibly because the \"sans-culottes\" approved it. The Hebertists preferred to side with the Montagnards, so long as they could hope to control the Convention through them. They dominated the Cordeliers Club, filled Bouchotte's offices, and could generally carry the Commune with them. The other wing was that of the Dantonists, which formed in response to the increasing centralization of the Revolutionary Government and the dictatorship of the Committees. The Dantonists were led predominately by deputies of the Convention (rather than the sans-culottes), including Danton, Delacroix, and Desmoulins.\n\nPutting the needs of national defense above all other considerations, the Committee of Public Safety had no intentions of giving in to the demands of either the popular movement or the moderates. In order to balance the contradictory demands of these two factions, the Revolutionary Government attempted to maintain a position halfway between the moderate Dantonists (\"citras\") and the extremist Hebertists (\"ultras\").\n\nBut at the end of the winter of 1793–4, the shortage of food took a sharp turn for the worse. The Hebertists incited \"sans-culottes\" to demand stringent measures, and at first the Committee did prove conciliatory. The Convention voted 10 million for relief, on 3 Ventose, Barère presented a new general Maximum, and on the 8th Saint-Just obtained a decree confiscating the property of suspects and distributing it to the needy (Ventose decrees). The Hebertists felt that if they increased the pressure, they would triumph once and for all. Although the call appeared like one for insurrection it was probably just for a new demonstration, like the one in September. But the Committee of Public Safety decided on 22 Ventose Year II (12 March 1794) that the Hebertists posed too serious a threat. The Committee linked Hebert, Ronsin, Vincent, and Momoro to the emigres Proli, Cloots and Pereira, so as to present the Hebertists as parties to the \"foreign plot\". All were executed on 4 Germinal (24 March). This move largely silenced the Hebertists, now without their leadership. Having succeeded in stifling dissent on the left, the Committee then turned on the Dantonists, several members of which were implicated in financial corruption. The Committee forced the Convention to lift the parliamentary immunity of nine Dantonist deputies, allowing them to be put on trial. On 5 April Dantonist leaders Danton, Delacroix, Desmoulins, and Philippeaux were executed.\n\nThe execution of the leaderships of both rival factions caused some to become disillusioned. Many \"sans-culottes\" were stunned by the Hebertists' execution. All the positions of influence traditionally held by the sans-culottes were eliminated: the Revolutionary Army was disbanded, the inspectors of food-hoarding were dismissed, Bouchotte lost the War Office, the Cordeliers Club was forced to self-censor and the Government pressure brought about closing 39 popular societies. The Paris Commune, controlled by sans-culottes, was purged and filled with Committee nominees. With the execution of the Dantonists, many of the members of the National Convention lost trust in the Committee, and even began to fear for their personal safety.\n\nUltimately, the Committee had undermined its own support by eliminating the Dantonists and Hebertists, both of which had backed the Committee. By compelling the Convention to allow the arrests of the Girondins and Dantonists, the Committee believed it had destroyed its major opposition. However, the trials demonstrated the Committee's lack of respect for members of the Convention (several of whom had been executed). Many Convention members who had sided with the Committee in the past by mid-1794 no longer supported it. The Committee had acted as mediator between the Convention and the \"sans-culottes\" from which they both had acquired their strength. By executing the Hebertists and alienating the \"sans-culottes\", the Committee became unnecessary to the Assembly.\n\nThough the Terror was organized in September 1793, it was not introduced until October. It had resulted from a popular movement. A new chapter of the Revolutionary Tribunal was opened after 5 September, divided into four sections: the Committees of Public Safety and General Security were to propose the names of judges and jurymen; Fouquier-Tinville stayed as public prosecutor, and Herman was nominated president. The Terror was meant to discourage support for the enemies of the Revolution by condemning outspoken critics of the Montagnards.\n\nThe great political trials began in October. The queen was guillotined on 16 October. A special decree stifled the defense of 21 Girondins, including Vergniaud and Brissot, and they perished on the 31st.\n\nAt the summit of the apparatus of the Terror sat the Committee of General Security, the state's second organization. It consisted of twelve members elected each month by the Convention, and vested with security, surveillance and police functions, including over civil and military authorities. It employed a large staff, headed the gradually constituted network of local revolutionary committees, and applied the law on suspects by sifting through the thousands of local denunciations and arrests which it then had to try.\n\nIt struck down the enemies of the Republic whoever and wherever they were. It was socially indiscriminate and politically perspicacious. Its victims belonged to the classes which hated the Revolution or lived in the regions where rebellion was most serious. \"The severity of repressive measures in the provinces,\" wrote Mathiez, \"was in direct proportion to danger of revolt.\" Many outspoken members of the community were tried and executed for claims of treason: Camille Desmoulins and Georges Danton were two of the more notable men executed for their \"threats\" against the Revolution.\n\nDeputies sent as \"representatives on mission\" by the Committee of Public Safety, armed with full powers, reacted according to both the local situation and their own temperaments: Lindet pacified the Girondin west in July without a single death sentence; in Lyon, some months later, Collot d'Herbois and Joseph Fouche relied on frequent summary executions by shooting because the guillotine was not working swiftly enough.\n\nThe monarchy made a distinction between French soil on the mainland and soil under French control such as the colonies. This distinction allowed for slavery to be illegal in France but continue in the colonies. Colonists in Saint Domingue wanted to have representation, 21 members due to their population size and contribution to the economy. This was shot down by the National Convention as the majority of their population were slaves and thus had no rights as citizens and contributed nothing to representative population. The in France originally did oppose slavery during the 1780s, however much of this opposition was ignored as a result of the French Revolution breaking out. The French showed a much greater willingness to act on the issue of slavery when the threat of a war with Spain seemed imminent. In 1792 the National Convention agreed to delegate 3 commissaries for Saint Domingue. Two of the commissaires, Léger-Félicité Sonthonax and Étienne Polverel, implemented rights for free men of color that were equal to their white counterparts. On May 5, 1793, Sonthonax and Polverel, first attacked the plantation system and forced the owners to treat the slaves better and care more for their well-being. Sonthonax then attacked slavery itself by freeing any slave Huzards, Latin for hazards, who had been armed by their masters since they could not return to peaceful plantation life. Polverel issued a proclamation in Cap Francais on June 21, 1793 which freed all slaves who agreed to fight for the French Republic from both internal and external threats. The commissaires then ruled that the Republic would pay an indemnity to the owners of female slaves marrying free men and that all children of that union would be free. The National Convention eventually allowed for 6 representative members for the colony. When pressured by the to end the slave trade in the colonies, the National Convention refused on the grounds of slavery being too core to the French economic wealth. The committee felt “six million French people relied on the colonies to survive” and continued to stand by this argument. On October 12, 1790 the National Convention declared the only body of power who could control the status of people in the colonies were committees in the colonies themselves—this meant although free blacks met the requirement for active citizenship the white colonists would not allow it. This was done in an attempt to please the white colonists and convince them not to join forces with the British. This also gave the colonies the power to control their own laws regarding slavery and allowed for the National Convention to wash their hands of the issue. Three deputies from Saint Domingue traveled to France to attempt to persuade the National Convention to abolish slavery. The National Convention abolished slavery after hearing speeches from the deputies on February 4, 1794. However, the Committee of Public Safety delayed sending the proclamation to the colonies for two months. This was due to the apparent opposition of Robespierre to the abolition of slavery. The issue was eventually resolved following the Committee circumventing Robespierre and ordering the abolition decree to be sent to Saint Domingue. However, Napoleon's attempt to return to slavery in 1801 removed France's state of being the first to abolish slavery and led to the loss of the most prosperous French colony.\n\nThe Jacobin dictatorship could only hope to remain in power so long as it was dealing successfully with a national emergency. As soon as its political opponents had been destroyed, and its foreign enemies defeated, it would lose the chief force that kept it together. The Jacobin fall happened more rapidly than expected because of issues within the party.\n\nSo long as it remained united, the Committee was virtually invulnerable, but it had scarcely attained the apogee of its power before signs of internal conflict appeared. The Committee of Public Safety had never been a homogeneous body. It was a coalition cabinet. Its members were kept together less by comradeship or common ideals than by calculation and routine. The press of business which at first prevented personal quarrels also produced tired nerves. Trifling differences were exaggerated into the issues of life and death. Small disputes estranged them from one another. Carnot, in particular, was irritated by the criticisms directed at his plans by Robespierre and Saint-Just, Dispute followed dispute. Bickering broke out on the Committee of Public Safety, with Carnot describing Robespierre and Saint-Just as \"ridiculous dictators\" and Collot making veiled attacks on the \"Incorruptible\". From the end of June until 23 July Robespierre ceased to attend the Committee.\n\nRealizing the danger of fragmentation, they attempted a reconciliation. Saint-Just and Couthon favored it, but Robespierre doubted sincerity of his enemies. It was he who brought about the fatal intervention of the Convention. On 8 Thermidor, Year II (26 July 1794), he denounced his opponents, and demanded that \"unity of government\" be realized. When called upon to name those whom he was accusing, however, he refused. This failure destroyed him, for it was assumed that he was demanding a blank cheque. This night an uneasy alliance was formed from threatened deputies and members of The Plain. On the next day, 9 Thermidor, Robespierre and his friends were not allowed to speak, and their indictment was decreed. The men of the extreme left played the leading roles: Billaud-Varenne, who attacked, and Collot d'Herbois, who presided.\n\nOn hearing the news the Paris Commune, loyal to the man who had inspired it, called for an insurrection and released the arrested deputies in the evening and mobilized two or three thousand militants. The night of 9–10 Thermidor was one of great confusion in Paris, as Commune and Assembly competed for the support of the sections and their troops. The Convention proclaimed that the rebels were henceforth outlaws; Barras was given the task of mustering an armed force, and the moderate sections gave this their support. The National Guardsmen and artillerymen assembled outside the \"Hotel de Ville\" were left without instructions and little by little they dispersed and left the square deserted. Around two o'clock in the morning a column from Gravilliers section led by Léonard Bourdon burst in the \"Hotel de Ville\" and arrested insurgents.\n\nOn the evening of 10 Thermidor (28 July 1794), Robespierre, Saint-Just, Couthon and nineteen of their political allies were executed without trial. On the following day it was the turn of a large batch of 71 men, the largest mass execution in the entire course of the Revolution.\n\nWhatever reasons the conspirators had behind 9 Thermidor, the events afterwards went beyond their intentions. Evidently the remaining members on the Committees counted on staying in office and currying the favour of the Jacobin dictatorship, as though nothing more had happened than a party purge.\n\nThey were speedily disabused of this notion. Robespierrists might go out and Dantonists come in; the Convention had recovered its initiative and would put an end, once and for all, to the dictatorial committees government which had ousted it from power. It was decreed that no member of governing committees should hold office for more than four months. Three days later the Prairial Law was repealed and the Revolutionary Tribunal shorn of its abnormal powers. The Commune was replaced with a Commission of Civil Administrators (\"commission des administrateurs civils\") from the ranks of the Conventions. In November the Jacobin club was closed. Not merely anti-Robespierrist but anti-Jacobin reaction was in full flood. At the beginning of September Billaud, Collot and Barère left the Committee of Public Safety; by the end of the year they were in prison.\n\nThe stability of the government was weakening. Next came the concentration of power, another revolutionary principle. The identification of the Committee of Public Safety with the executive was ended on 7 Fructidor (24 August), restricting it to its former domain of war and diplomacy. The Committee of General Security kept its control over the police. There was now to be a total of sixteen committees. \"Conventionnels\", while aware of the dangers of fragmentation, were even more worried by its experience of monopoly of powers. In a few weeks the revolutionary government was dismantled.\n\nThese measures affected, finally, the instruments of the Terror and opened numerous breaches in the apparatus of repression. The law of 22 Prairial was repealed, the prisons were opened and \"suspects\" were released: 500 in Paris in a single week. A few public trials were staged — including those of Carrier, held responsible for the mass-drowning at Nantes, and Fouquier-Tinville, notorious as the public prosecutor of the Great Terror of the late spring and summer of 1794 – after which the Revolutionary Tribunal was quietly put aside.\n\nThe destruction of the system of revolutionary government eventually brought about the end of the 'Economic Terror'. \"Maximum\" was relaxed even before 9 Thermidor. Now virtually nobody believed in price controls any longer. Because the black market was plentifully supplied, the idea took hold that price controls equaled scarcity and that free trade, therefore, would bring back abundance. It was generally supposed by the free trade minded Physiocrat economists within France that prices would at first rise but that then they would fall as a result of competition. This illusion, however, was to be shattered in the winter of 1794–1795. Formally, the National Convention had put the end to the \"maximum\" as the season had started on the Christmas Eve of 4 Nivose Year III (24 December 1794).\n\nThat winter, the abandonment of the controlled economy provoked a frightful catastrophe. Prices soared and the rate of exchange fell. The Republic was condemned to massive inflation and its currency was ruined. In Thermidor, Year III, \"assignats\" were worth less than 3% of their face value. Neither peasants nor merchants would accept anything but cash. The debacle was so swift that economic life seemed to come to standstill.\n\nThe crisis was greatly aggravated by famine. Peasants, finally, stopped bringing any produce to market, because they did not wish to accept \"assignats\". The government continued to provision Paris, but was unable to supply the promised rations. In provinces local municipalities resorted to some sort of regulations, provided not direct coercion in obtaining provisions. The misery of rural day laborers, abandoned by everyone, was often appalling. Inflation ruined creditors to the advantage of debtors. It unleashed an unprecedented speculation.\n\nAt the beginning of spring in March–April 1795, scarcity was such that more unrest appeared almost everywhere. The City of Paris was 'active' once again.\n\nDiscontent increased along with the shortages. On 17 March a delegation from \"faubourgs Saint-Marceau and Saint-Jacques\" complained that \"We are on the verge of regretting all the sacrifices that we have made for the Revolution.\" Police law was passed which lay down the death penalty for use of seditious language. Arms were distributed to the \"good citizens\", the faithful nucleus of the National Guard. The trial of strength was approaching.\nOn 10 Germinal (30 March) all the sections called their general assemblies. The political geography of Paris emerged clearly from this. Convention debate was centered on two issues: the fate of Barère, Collot, Billaud and Vadier, and the implementation of the constitution of 1793. While in the sections of the center and the west formal addresses called for the punishment of the \"Four\" and passed over the food shortages, the sections of the east and the \"faubourgs\" demanded measures to deal with the grain crisis, the implementation of the constitution of 1793, the reopening of the popular societies and the release of the imprisoned patriots.\n\nOn the morning of 12 Germinal (1 April) crowds gathered on the \"Ile de la Cité\" and, pushing aside the palace guards, burst into the chamber where the Convention met. Amidst the uproar, spokesmen of the sections outlined the people's grievances. Reliable battalions of National Guard were called and demonstrators, lacking arms and leaders, were forced to withdraw. For the most people it was the constitution of 1793 – seen as a liberating utopia – which represented the solution to all evils. There were others who openly regretted the passing of \"the reign of Robespierre\".\nBut it was not the end. A new explosion was on the horizon. Insurrection was being openly prepared. On 1 Prairial (20 May 1795) the alarm bells sounded in the \"faubourgs\" Saint-Antoine and Marceau. The armed battalions arrived at Place du Carousel and entered the sitting chamber. After an hour of uproar, \"The Insurrection of the People\" (\"L'Insurection du Peuple\") was read. In the chaos, none of the ringleaders thought of implementing the key item of the program: the overthrow of the government.\n\nThe remainder of the Montagnards, The Crest (\"la Crête de la Montagne\"), managed to obtain the passage of decrees favorable to the rebels. But at 11:30 p.m. two armed columns entered the chamber and cleared out the rioters. The next day insurgents repeated the same mistakes and after receiving promises from the deputies to take speedy measures against the famine, returned to the sections.\n\nOn 3 Prairial the government assembled loyal troops, chasseurs and dragoons, national guardsmen, selected from those \"who had fortune to preserve\" — 20,000 men in all. Faubourg Saint-Antoine was surrounded and on 4 Prairial surrendered and was disarmed. Uncertainty about how to react, hesitancy in action, and lack of revolutionary leadership had doomed the popular movement to throw away its last chance in battle.\n\n4 Prairial Year III is one of the crucial dates of the revolutionary period. The people had ceased to be a political force, participants in history. They were now no more than victims or spectators.\n\nThe victors now could set up a new constitution, the task the National Convention was originally elected for. The Commission of Eleven (the most notable members of which were Daunou, Lanjuinais, Boissy d'Anglas, Thibaudeau and La Révellière) drafted a text which would reflect the new balance of power. It was presented on 5 Messidor (23 June) and passed on 22 August 1795 (5 Fructidor of the Year III).\n\nThe new constitution went back to the constitution of 1791 as to the dominant ideology of the country. Equality was certainly confirmed, but within the limits of civil equality. Numerous democratic rights of the constitution 1793 – the right to work, to relief, to education – were omitted. The Convention wanted to define rights and simultaneously reject both the privilege of the old order and social leveling.\n\nThe constitution went back to the distinction between active and passive citizens. Only citizens over twenty-five years old, disposing of an income of two hundred days of work, were eligible to be electors. This electoral body, which held the real power, included 30,000 people, half as many as in 1791. Guided by recent experience, institutions were set up to protect the Republic from two dangers: the omnipotence of an assembly and dictatorship.\n\nBicameral legislature as a precaution against sudden political fluctuations was proposed: the Council of Five Hundred with rights to propose laws and Council of the Ancients, 250 deputies, with powers to accept or reject proposed laws. Executive power was to be shared between five Directors chosen by the Ancients from the list drawn by Five Hundred. One of the Directors would be renewed each year with re-election after five years. As one of the practical precautions, no military were allowed within 60 miles of the sitting assembly and it could relocate in case of danger. The Directory still retained great power, including emergency powers to curb freedom of the press and freedom of association.\n\nThe Constitution generally was accepted favorably, even by those on the right, who were hopeful for the upcoming elections and even more happy to get rid of the legislative body so hated by them.\n\nBut how to make sure that the new elected body would not overturn the constitution as it was before with Legislative Assembly? Thermidorians attempted this on 5 Fructidor (22 August) by voting for a decree on \"formation of a new legislative body\". Article II stipulated: \"All members presently active in the Convention are re-eligible. Election assemblies may not take fewer than two-thirds of them to form the legislative body\". This was known as the Law of the Two-Thirds.\n\nOn 23 September the results were announced: the constitution was accepted by 1,057,390 votes, with 49,978 against. The Two-Thirds decrees obtained only 205,498 votes in favor and 108,754 against.\n\nBut the Convention had not taken into account those Paris sections who were against Two-Thirds decrees and failed to provide precise vote figures: 47 Parisian sections had rejected the decrees. Eighteen of the Paris sections contested the result. The \"Lepeletier\" section issued a call to insurrection. By 11 Vendemiaire seven sections were in state of revolt, sections which were the base of the Convention since 9 Thermidor and now won by the far right if not royalists. The Convention declared itself permanent. The \"conventionnels\" knew the score. They knew the art of insurrection by heart and to bring down \"muscadins\" was easier than the \"sans-culottes\". Five members including Barras were appointed to deal with the crisis. A decree of 12 Vendemiaire (4 October) repealed the former disarmament of the former terrorists and an appeal to \"sans-culottes\" was issued.\n\nDuring the nights of Vendemiaire 12–13 (October 4–5), General Jacques de Menou de Boussay was tasked with putting down the royalist rebels and keep them from attacking the Convention. He recruited other generals to help aid in quelling the insurrection such as, Napoleon Bonaparte. The rebels outnumbered the Army by the thousands, but because of their preparations the night before, Bonaparte and the armies were able to line the road into Paris with cannons from Sablons Camp. Without a way into Paris, the rebels surrendered to the Convention on Vendemiaire 13. Barras and the Convention gave the armies permission to kill. Within 45 minutes over 300 royalist rebels were dead in front of the Church of Saint Roch. The rest had scattered and fled.\n\nModerate repression ensued and the White Terror in the south was stopped. On 4 Brumaire Year IV, just before breaking up, the Convention voted a general amnesty for \"deeds exclusively connected with the Revolution\".\n\nThe article on the Convention in the 1911 Encyclopædia Britannica concludes, \"The work of the Convention was immense in all branches of public affairs. To appreciate it without prejudice, one should recall that this assembly saved France from a civil war and invasion, that it founded the system of public education (\"Museum\", , , , ), created institutions of capital importance, like that of the , and definitely established the social and political gains of the Revolution.\" By a decree of 4 February 1794 (16 pluviôse) it also ratified and expanded to the whole French colonial empire the 1793 abolition of slavery on Saint-Domingue by civil commissioners Sonthonax and Polverel, though this did not affect Martinique or Guadeloupe and was abolished by the law of 20 May 1802.\n\n\n\n"}
{"id": "9932845", "url": "https://en.wikipedia.org/wiki?curid=9932845", "title": "Juridical person", "text": "Juridical person\n\nA juridical person is a non-human legal entity, in other words any organization that is not a single natural person but is authorized by law with duties and rights and is recognized as a legal person and as having a distinct identity. This includes any incorporated organizations including corporations, government agencies, and NGOs. Also known as artificial person, juridical entity, juridic person, juristic person, or legal person.\n\nThe rights and responsibilities of a juridical person are distinct from those of the natural persons constituting it.\n\n\n"}
{"id": "233404", "url": "https://en.wikipedia.org/wiki?curid=233404", "title": "Foreign policy", "text": "Foreign policy\n\nA country's foreign policy, also called foreign relations or foreign affairs policy, consists of self-interest strategies chosen by the state to safeguard its national interests and to achieve goals within its international relations milieu. The approaches are strategically employed to interact with other countries. The study of such strategies is called foreign policy analysis. In recent decades, due to the deepening level of globalization and transnational activities, states also must interact with non-state actors. These interactions evaluated and monitored in seeking the benefits of bilateral and multilateral international cooperation.\n\nSince the national interests are paramount, governments design their foreign policies through high-level decision-making processes. National interests may be accomplished as a result of peaceful cooperation with other nations, or through exploitation. Usually, creating foreign policy is the job of the head of government and the foreign minister (or equivalent). In some countries, the legislature also has considerable effects. \n\nForeign policies of countries have varying rates of change and scopes of intent, which can be affected by factors that change the perceived national interests or even affect the stability of the country itself. The foreign policy of a country can have a profound and lasting impact on other countries and on the course of international relations as a whole, such as the Monroe Doctrine conflicting with the mercantilism policies of 19th-century European countries and the goals of independence of newly formed Central American and South American countries.\n\nThe ancient Greek philosopher Aristotle described humans as social animals, and friendships and relations have existed between humans as long as humans have existed. As organization developed in human affairs, relations between people also became organized. Foreign policy thus goes back to primitive times. Before writing, most of these relations were carried out by word of mouth and left little direct archaeological evidence. Literature from ancient times, the Bible, the Homeric poems, the histories of Herodotus and Thucydides, and many others, show an accumulation of experience in dealing with foreigners. Ancient Chinese writings give much evidence of thought concerned with the management of relations between peoples in the form of diplomatic correspondence between rulers and officials of different states and within systems of multi-tiered political relations such as the Han dynasty and its subordinate kings, the more powerful of which conducted their own limited foreign relations as long as those did not interfere with their obligations to the central government, There are treatises by Chanakya and other ancient Indian scholars, and the preserved text of ancient treaties, as well as frequent references by known ancient writers to other, even older sources which have since been lost or remain in fragmentary form only.\n\nGlobal wars were fought two times in the twentieth century. Consequently, international relations became a public concern as well as an important field of study and research.\nAfter the Second World War and during the 1960s, many researchers in the U.S. particularly, and from other countries in common, brought forth a wealth of research work and theory. This work was done for international relations and not for foreign policy as such. Gradually, various theories began to grow around international relations, international systems, and international politics, but the need for a theory of foreign policy (that is, the starting point in each sovereign state) continued to receive negligible attention. The reason was that the states used to keep their foreign policies under official secrecy, and unlike today, it was not considered appropriate for the public to know about these policies. This iron-bound secrecy is an essential part for the framework of foreign policy formulation.\n\nWorld War II and its devastation posed a great threat and challenge for humanity which revealed to everyone the importance of international relations. Though foreign policy formulation continued to remain a closely guarded process at the national level, wider access to governmental records and greater public interest provided more data from which academic work placed international relations in a structured framework of political science. Graduate and post-graduate courses developed. The research was encouraged, and gradually, international relations became an academic discipline in universities throughout the world.\n\nThe subject of whether or not constructive attempts at involvement by citizens benefits the disciplines of the \"art,\" or whether or not such disciplines as intercultural and interpersonal communications and others may play a significant part in the future of international relations could be a subject for further study by interested individuals/groups and is encouraged at the educational level.\n\nWriters researching foreign policy in the 20th century were unaware of whether or not agencies who most closely dealt with foreign policy kept logs of statistical experience not unlike the actuarial statistics kept by organizations of the insurance industry assessing the risk and danger involved (e.g., when situation \"C\" happened before, and subject included instances of \"E\" and \"L\", how was it handled and what was the result? When were peaceful and amicable results leading to better relations ever obtained through considered action and what was that action?).\n\nThe writers who worked with foreign policy can be divided into two groups:\n(The second group restricts to foreign policymaking.)\n\nThe works of the second group come closer to the theory of foreign policy, but there is no attempt to formulate a basic theory of foreign policy. Hans Morgenthau’s works on principal elements of foreign policy seem to have covered the most ground.\n\nBen Shapiro, in his comparative study of the foreign policy of different countries, felt that the lack of a basic theory of foreign policy was particularly disabling and pointed out the harmful effect of the absence of a general theory of foreign policy on foreign policy literature.\n\nThe most fundamental question that arises here is: why do we lack theories of foreign policy? Or why do we need it?\nThe absence of a general theory in this field leads to some serious consequences. Without theory:\n\n\nA theoretical framework of foreign policy is needed to analyze the day-to-day interactions in international relations and to compare individual foreign policies. The focus is primarily on the policies of state actors with defined territories and jurisdictional boundaries, and less so on non-state actors, except in the context of how they impact national government decisions and policies. The formal field of study of international relations is itself recent, and a specific subset of international relations such as foreign policy analysis does not receive wide attention as a field of scientific study. Rather, terms like \"foreign policy\" and \"foreign policy expert\" are often used in news media and general discussions about government, when such experts may have more extensive backgrounds in fields other than foreign policy analysis. The organization Foreign Policy Interrupted recognized the gender disparity in foreign policy expert representation and is amplifying the number of female voices in foreign policy media coverage. Government officials involved in making foreign policy often perceive risk in giving away information about their policy-making processes and do not discuss the subject since control of information is itself often a part of foreign policy.\n\nThe vast record of empirical data and research is given academic attention to fit it into the framwork of a general theory of foreign policy.\n\nThe second group of writers has made contributions to its development in many ways:\n\n\n\n"}
{"id": "269637", "url": "https://en.wikipedia.org/wiki?curid=269637", "title": "The Internationale", "text": "The Internationale\n\n\"The Internationale\" () is a left-wing anthem. It has been a standard of the socialist movement since the late nineteenth century, when the Second International adopted it as its official anthem. The title arises from the \"First International\", an alliance of workers which held a congress in 1864. The author of the anthem's lyrics, Eugène Pottier, an anarchist, attended this congress.\n\nThe original French refrain of the song is \"C'est la lutte finale / Groupons-nous et demain / L'Internationale / Sera le genre humain.\" (English: \"This is the final struggle / Let us group together and tomorrow / The Internationale / Will be the human race.\"). \"The Internationale\" has been translated into many languages.\n\n\"The Internationale\" has been celebrated by anarchists, communists, socialists, democratic socialists, and social democrats.\n\nThe original French words were written in June 1871 by Eugène Pottier (1816–1887, previously a member of the Paris Commune) and were originally intended to be sung to the tune of \"La Marseillaise\". In 1888 Pierre De Geyter (1848–1932) set the earlier lyrics to a new melody, composed especially for Pottier's lyrics. De Geyter's melody was first publicly performed in July 1888, and soon thereafter Pottier's lyrics became closely associated with, and widely used with, De Geyter's new melody. Thus \"The Internationale\" gained an identity that was entirely distinct, and no longer in any way directly tied to the French national anthem, the Marseillaise.\n\nIn a successful attempt to save Pierre De Geyter's job as a woodcarver, the 6,000 leaflets printed by Lille printer Bolboduc only mentioned the French version of his family name (Degeyter). In 1904, Pierre's brother Adolphe was induced by the Lille mayor to claim copyright, so that the income of the song would continue to go to Delory's French Socialist Party. Pierre De Geyter lost the first copyright case in 1914, but after his brother committed suicide and left a note explaining the fraud, Pierre was declared the copyright owner by a court of appeal in 1922.\n\nIn 1972 \"Montana Edition\", owned by , bought the rights to the song for 5,000 Deutschmark, first for the territory of the former West Germany, then in the former East Germany, then worldwide. East Germany paid Montana Edition 20,000 DM every year for its rights to play the music. Pierre De Geyter died in 1932, causing the copyrights to expire in 2002. Luckhardt's German text is public domain since 1984.\n\nAs the \"Internationale\" music was published before 1 July 1909 outside the United States of America, it is in the public domain in the United States. As of 2013, Pierre De Geyter's music is also in the public domain in countries and areas whose copyright durations are authors' lifetime plus 80 years or less. Due to France's wartime copyright extensions (\"prorogations de guerre\"), SACEM claimed that the music was still copyrighted in France until October 2014.\n\nAs Eugène Pottier died in 1887, his original French lyrics are in the public domain. Gustave Delory once acquired the copyright of his lyrics through the songwriter G B Clement having bought it from Pottier's widow.\n\nThe German version, \"\", was used by East German anti-Stalinists in the failed 1953 uprising and again during the 1989 protests which nonviolently toppled Communism in East Germany. When numerous East Germans were arrested for protesting the 40th anniversary celebrations for the GDR, many of them sang the Internationale in police custody to imply that they, rather than their captors, were the real revolutionaries.\n\nLuckhardt's version, the standard German translation, of the final line of the chorus tellingly reads: \"Die Internationale erkämpft das Menschenrecht\". (The Internationale fights for human rights.) In 1989, this was coupled with the chant: \"Volkspolizei, steh dem Volke bei\" (People's police, stand with the people!)\n\nHere follows the most known version, the 1910 translation by Emil Luckhardt:\n\nThe Russian version was initially translated by Arkady Kots in 1902 and printed in London in \"Zhizn\", a Russian émigré magazine. The first Russian version consisted of three stanzas (as opposed to six stanzas in the original French lyrics, and based on stanzas 1, 2 and 6) and the refrain. After the Bolshevik Revolution in Russia, the text was slightly re-worded to get rid of \"now useless\" future tenses – particularly the refrain was reworded (the future tense was replaced by the present, and the first person plural possessive pronoun was introduced). In 1918, the chief-editor of \"Izvestia\", Yuri Steklov, appealed to Russian writers to translate the other three stanzas and in the end, the song was expanded into six stanzas. On 15 March 1944, the Soviet Union adopted the \"Hymn of the Soviet Union\" as its national anthem. Prior to that time, \"The Internationale\" served as the principal musical expression of allegiance to the ideals of the October Revolution and the Soviet Union (the \"Internationale\" continued to be recognized as the official song of the Communist Party of the Soviet Union, and the post-1919 Soviet version is still used by the Communist Party of the Russian Federation). The full song is as follows:\n\nThe traditional British version of \"The Internationale\" is usually sung in three verses, while the American version, written by Charles Hope Kerr with five verses, is usually sung in two. The American version is sometimes sung with the phrase \"the internationale\", \"the international soviet\", or \"the international union\" in place of \"the international working class\". In English renditions, \"Internationale\" is sometimes sung as rather than the French pronunciation of .\n\nBilly Bragg was asked by Pete Seeger to sing \"The Internationale\" with him at the Vancouver Folk Festival in 1989. Bragg thought the traditional English lyrics were archaic and unsingable (Scottish musician Dick Gaughan and former Labour MP Tony Benn disagreed), and composed a new set of lyrics. The recording was released on his album \"The Internationale\" along with reworkings of other socialist songs.\n\n\"The Internationale\" in Chinese (simplified Chinese: 国际歌; traditional Chinese: 國際歌; pinyin: Guójìgē), literally the \"International Song\", was first translated on 15 June 1923 from the French original by Qu Qiubai (Chinese: 瞿秋白), a leading member of the Communist Party of China in the late 1920s. His translation has transliterated \"The Internationale\" as Yīngdénàxióngnà'ěr () when singing the phrase in Standard Chinese. As he was executed by the Kuomintang in 1935, his Chinese translation is in the public domain wherever the duration of copyright is an author's lifetime plus up to 70 years, including Chinese-speaking Mainland China, Hong Kong, Macau, Taiwan (lifetime plus 50 years in these places), and Singapore (lifetime plus 70 years). The three stanzas of this version roughly correspond to the three stanzas of Arkady Kots' Russian version and the first, second, and sixth French lyrics by Eugène Pottier. The third, fourth and fifth stanzas of the French original are not used in this version.\n\nThis version was translated from the Russian version in 1923 by the poet Xiao San, friend of Mao Zedong. This version is the most common and is also the anthem of the Communist Party of China. Xiao's version was a revision of Qu's translation, which did not see widespread use due to it being written in Classical Chinese, although the phrase \"The Internationale\" was similarly transliterated as Yīngtènàxióngnài'ěr (). When the Chinese Soviet Republic was established in 1931, it was decided to be its national anthem. The version was officially revised in 1962 by China National Radio and Chinese Musicians' Association.\n\nThe song was a rallying anthem of the demonstrators at the Tiananmen Square protests of 1989, and was repeatedly sung both while marching to the Square and within the Square...many hundreds of people (not only students) appeared on the street. They ran after the trucks and shouted protest slogans. A few stones were thrown. The soldiers opened fire with live ammunition. The crowd threw themselves on the ground, but quickly followed the convoy again. The more shots were fired, the more the crowd got determined and outraged. Suddenly they started singing \"The Internationale\"; they armed themselves with stones and threw them towards the soldiers. There were also a few Molotov cocktails and the last truck was set on fire.\n\nWhen commemorating the 55th anniversary of the Paris Commune on 18 March 1926, the National Revolutionary Army printed a music sheet with three lyrics of \"The Internationale\" in Chinese, roughly corresponding to the first, second, and sixth French lyrics by Eugène Pottier. When singing refrain twice after each lyric, \"The Internationale\" is transliterated first as Yīngtè'ěrlāxióngnà'ěr (Chinese: 英特爾拉雄納爾) and second as Yīngtè'ěrnàxióngnà'ěr (Chinese: 英特爾納雄納爾).\n\nThe third, fourth, and fifth French stanzas are not sung in Chinese in the above two versions of Qu and the National Revolutionary Army. Chinese translator Shen Baoji (simplified Chinese: 沈宝基; traditional Chinese: 沈寶基, 1908–2002) has made a complete Chinese translation, published in 1957, of all six French stanzas. Shen's translation has transliterated \"The Internationale\" as Yīngdāi'ěrnàxī'àonà'ěr (simplified Chinese: 因呆尔那西奥纳尔; traditional Chinese: 因呆爾那西奧納爾) in the stanzas, different from the transliterations of Qu and the National Revolutionary Army. As the Copyright Law of the People's Republic of China grants individuals copyright for their lifetime plus 50 years, Shen's translation is expected to remain copyrighted there until the end of 2052.\n\nIn addition to the Mandarin version, \"The Internationale\" also has Cantonese and Taiwanese Hokkien versions, occasionally used in Hong Kong and Taiwan. The word \"Internationale\" is not translated in either version.\n\nThere were three Filipino versions of the song. The first was composed by Juan Feleo of the Partido Komunista ng Pilipinas-1930 under the title \"Pandaigdigang Awit ng Manggagawa\" (The International Worker's Anthem) which was translated from the English version. The second version was a retranslation of the first two stanzas on the basis of the French original by the Communist Party of the Philippines. The third version, which introduced the third stanza, was derived from both Chinese and French versions and translated by Jose Maria Sison, the CPP's founding chairman.\n\n\n\n"}
{"id": "287860", "url": "https://en.wikipedia.org/wiki?curid=287860", "title": "Public law", "text": "Public law\n\nPublic law is that part of law which governs relationships between individuals and the government, and those relationships between individuals which are of direct concern to society. Public law comprises constitutional law, administrative law, tax law and criminal law, as well as all procedural law. In public law, mandatory rules prevail. Laws concerning relationships between individuals belong to private law.\n\nThe relationships public law governs are asymmetric and unequal – government bodies (central or local) can make decisions about the rights of individuals. However, as a consequence of the rule of law doctrine, authorities may only act within the law (\"secundum et intra legem\"). The government must obey the law. For example, a citizen unhappy with a decision of an administrative authority can ask a court for judicial review.\n\nRights, too, can be divided into \"private rights\" and \"public rights\". A paragon of a public right is the right to welfare benefits – only a natural person can claim such payments, and they are awarded through an administrative decision out of the government budget.\n\nThe distinction between public law and private law dates back to Roman law. It has been picked up in the countries of civil law tradition at the beginning of the nineteenth century, but since then spread to common law countries, too.\n\nThe borderline between public law and private law is not always clear in particular cases, giving rise to attempts of theoretical understanding of its basis.\n\nRule of law, the idea that the administration of the state should be controlled by a set of laws, originated in Greek Antiquity and was revitalized by modern philosophers in France (Rousseau), Germany (Kant) and Austria in the 18th century. It is related to the strong position of the central government in the era of enlightened absolutism, and was inspired by the French Revolution and enlightenment. It developed hand in hand with the creation of civil codes and criminal codes.\n\nIn modern states, constitutional law lays out the foundations of the state. Above all, it postulates the supremacy of law in the functioning of the state – the rule of law.\n\nSecondly, it sets out the form of government – how its different branches work, how they are elected or appointed, and the division of powers and responsibilities between them. Traditionally, the basic elements of government are the executive, the legislature and the judiciary.\n\nAnd thirdly, in describing what are the basic human rights, which must be protected for every person, and what further civil and political rights citizens have, it sets the fundamental borders to what \"any\" government must and must not do.\n\nIn most jurisdictions, constitutional law is enshrined in a written document, the Constitution, sometimes together with amendments or other constitutional laws. In some countries, however, such a supreme entrenched written document does not exist for historical and political reasons – the Constitution of the United Kingdom is an unwritten one.\n\nAdministrative law refers to the body of law which regulates bureaucratic managerial procedures and defines the powers of administrative agencies. These laws are enforced by the executive branch of a government rather than the judicial or legislative branches (if they are different in that particular jurisdiction). This body of law regulates international trade, manufacturing, pollution, taxation, and the like. This is sometimes seen as a subcategory of civil law and sometimes seen as public law as it deals with regulation and public institutions\n\nCriminal law involves the state imposing sanctions for defined crimes committed by individuals or businesses, so that society can achieve its brand of justice and a peaceable social order.\n\nIn German-language legal literature, there is an extensive discussion on the precise nature of the distinction between public law and private law. Several theories have evolved, which are neither exhaustive, nor are they mutually exclusive or separate from each other.\n\nThe interest theory has been developed by the Roman jurist Ulpian: \"\"Publicum ius est, quod ad statum rei Romanae spectat, privatum quod ad singulorum utilitatem.\" (Public law is that, which concerns Roman state, private law is concerned with the interests of citizens.) The weak point of this theory is that many issues of private law also affect the public interest. Also, what exactly is this public interest?\n\nCharles-Louis Montesquieu (1689–1755) amplified supremely this distinction: International (law of nations), Public (politic law) and Private (civil law) Law, in his major work: (On) The Spirit of the Law (1748). “\"Considered as inhabitants of so great a planet, which necessarily contains a variety of nations, they have laws relating to their mutual intercourse, which is what we call the law of nations. As members of a society that must be properly supported, they have laws relating to the governors and the governed, and this we distinguish by the name of politic law. They have also another sort of law, as they stand in relation to each other; by which is understood the civil law.\"”\n\nThe subjection theory focuses on explaining the distinction by emphasizing the subordination of private persons to the state. Public law is supposed to govern this relationship, whereas private law is considered to govern relationships where the parties involved meet on a level playing field. This theory fails in areas commonly considered private law which also imply subordination, such as employment law. Also, the modern state knows relationships in which it appears as equal to a person.\n\nThe subject theory is concerned with the position of the subject of law in the legal relationship in question. If it finds itself in a particular situation as a public person (due to membership in some public body, such as a state or a municipality), public law applies, otherwise it is private law.\n\nA combination of the subjection theory and the subject theory arguably provides a workable distinction. Under this approach, a field of law is considered public law where one actor is a public authority endowed with the power to act unilaterally (\"imperium\") and this actor uses that \"imperium\" in the particular relationship. In other words, all depends whether the public authority is acting as a public or a private entity, say when ordering office supplies. This latest theory considers public law to be a special instance.\n\nThere are areas of law, which do not seem to fit into either public or private law, such as employment law – parts of it look like private law (the employment contract), other parts like public law (the activities of an employment inspectorate when investigating workplace safety).\n\nThe distinction between public and private law might seem to be a purely academic debate, but it also affects legal practice. It has bearing on the delineation between competences of different courts and administrative bodies. Under the Austrian constitution, for example, private law is among the exclusive competences of federal legislation, whereas public law is partly a matter of state legislation.\n\n"}
{"id": "287872", "url": "https://en.wikipedia.org/wiki?curid=287872", "title": "Flash mob", "text": "Flash mob\n\nA flash mob (or flashmob) is a group of people who assemble suddenly in a public place, perform for a brief time, then quickly disperse, often for the purposes of entertainment, satire, and artistic expression. Flash mobs may be organized via telecommunications, social media, or viral emails.\n\nThe term, coined in 2003, is generally not applied to events and performances organized for the purposes of politics (such as protests), commercial advertisement, publicity stunts that involve public relation firms, or paid professionals. In these cases of a planned purpose for the social activity in question, the term smart mobs is often applied instead.\n\nThe term \"flash rob\" or \"flash mob robberies\", a reference to the way flash mobs assemble, has been used to describe a number of robberies and assaults perpetrated suddenly by groups of teenage youth. Bill Wasik, originator of the first flash mobs, and a number of other commentators have questioned or objected to the usage of \"flash mob\" to describe criminal acts.\n\nThe first flash mobs were created in Manhattan in 2003, by Bill Wasik, senior editor of \"Harper's Magazine\". The first attempt was unsuccessful after the targeted retail store was tipped off about the plan for people to gather. Wasik avoided such problems during the first successful flash mob, which occurred on June 17, 2003 at Macy's department store, by sending participants to preliminary staging areas—in four Manhattan bars—where they received further instructions about the ultimate event and location just before the event began.\n\nMore than 130 people converged upon the ninth floor rug department of the store, gathering around an expensive rug. Anyone approached by a sales assistant was advised to say that the gatherers lived together in a warehouse on the outskirts of New York, that they were shopping for a \"love rug\", and that they made all their purchase decisions as a group. Subsequently, 200 people flooded the lobby and mezzanine of the Hyatt hotel in synchronized applause for about 15 seconds, and a shoe boutique in SoHo was invaded by participants pretending to be tourists on a bus trip.\n\nWasik claimed that he created flash mobs as a social experiment designed to poke fun at hipsters and to highlight the cultural atmosphere of conformity and of wanting to be an insider or part of \"the next big thing\". \"The Vancouver Sun\" wrote, \"It may have backfired on him ... [Wasik] may instead have ended up giving conformity a vehicle that allowed it to appear nonconforming.\" In another interview he said \"the mobs started as a kind of playful social experiment meant to encourage spontaneity and big gatherings to temporarily take over commercial and public areas simply to show that they could\".\n\nIn 19th-century Tasmania, the term \"flash mob\" was used to describe a subculture consisting of female prisoners, based on the term \"flash language\" for the jargon that these women used. The 19th-century Australian term \"flash mob\" referred to a segment of society, not an event, and showed no other similarities to the modern term \"flash mob\" or the events it describes.\n\nIn 1973, the story \"Flash Crowd\" by Larry Niven described a concept similar to flash mobs. With the invention of popular and very inexpensive teleportation, an argument at a shopping mall—which happens to be covered by a news crew—quickly swells into a riot. In the story, broadcast coverage attracts the attention of other people, who use the widely available technology of the teleportation booth to swarm first that event—thus intensifying the riot—and then other events as they happen. Commenting on the social impact of such mobs, one character (articulating the police view) says, \"We call them flash crowds, and we watch for them.\" In related short stories, they are named as a prime location for illegal activities (such as pickpocketing and looting) to take place.\n\nFlash mobs began as a form of performance art. While they started as an apolitical act, flash mobs may share superficial similarities to political demonstrations. In the 1960s, groups such as the Yippies used street theatre to expose the public to political issues. Flash mobs can be seen as a specialized form of smart mob, a term and concept proposed by author Howard Rheingold in his 2002 book \"\".\n\nThe first documented use of the term \"flash mob\" as it is understood today was in 2003 in a blog entry posted in the aftermath of Wasik's event. The term was inspired by the earlier term \"smart mob\".\n\nFlash mob was added to the 11th edition of the \"Concise Oxford English Dictionary\" on July 8, 2004 where it noted it as an \"unusual and pointless act\" separating it from other forms of smart mobs such as types of performance, protests, and other gatherings. Also recognized noun derivatives are flash mobber and flash mobbing. \"Webster's New Millennium Dictionary of English\" defines \"flash mob\" as \"a group of people who organize on the Internet and then quickly assemble in a public place, do something bizarre, and disperse.\" This definition is consistent with the original use of the term; however, both news media and promoters have subsequently used the term to refer to any form of smart mob, including political protests; a collaborative Internet denial of service attack; a collaborative supercomputing demonstration; and promotional appearances by pop musicians. The press has also used the term \"flash mob\" to refer to a practice in China where groups of shoppers arrange online to meet at a store in order to drive a collective bargain.\n\nThe city of Brunswick, Germany has stopped flash mobs by strictly enforcing the already existing law of requiring a permit to use any public space for an event. In the United Kingdom, a number of flash mobs have been stopped over concerns for public health and safety. The British Transport Police have urged flash mob organizers to \"refrain from holding such events at railway stations\".\n\nReferred to as \"flash robs\", \"flash mob robberies\", or \"flash robberies\" by the media, crimes organized by teenage youth using social media rose to international notoriety beginning in 2011. The National Retail Federation does not classify these crimes as \"flash mobs\" but rather \"multiple offender crimes\" that utilize \"flash mob tactics\". In a report, the NRF noted, \"multiple offender crimes tend to involve groups or gangs of juveniles who already know \neach other, which does not earn them the term \"flash mob\".\" Mark Leary, a professor of psychology and neuroscience at Duke University, said that most \"flash mob thuggery\" involves crimes of violence that are otherwise ordinary, but are perpetrated suddenly by large, organized groups of people: \"What social media adds is the ability to recruit such a large group of people, that individuals who would not rob a store or riot on their own feel freer to misbehave without being identified.\"\n\n\"HuffPost\" raised the question asking if \"the media was responsible for stirring things up\", and added that in some cases the local authorities did not confirm the use of social media making the \"use of the term flash mob questionable.\" Amanda Walgrove wrote that criminals involved in such activities don't refer to themselves as \"flash mobs\", but that this use of the term is nonetheless appropriate. Dr. Linda Kiltz drew similar parallels between flash robs and the Occupy Movement stating, \"As the use of social media increases, the potential for more flash mobs that are used for political protest and for criminal purposes is likely to increase.\".\n\n\n"}
{"id": "292285", "url": "https://en.wikipedia.org/wiki?curid=292285", "title": "State religion", "text": "State religion\n\nA state religion (also called an established religion or official religion) is a religious body or creed officially endorsed by the state. A state with an official religion, while not secular, is not necessarily a theocracy, a country whose rulers have both secular and spiritual authority. State religions are official or government-sanctioned establishments of a religion, but the state does not need be under the control of the religion (as in a theocracy) nor is the state-sanctioned religion necessarily under the control of the state.\n\nOfficial religions have been known throughout human history in almost all types of cultures, reaching into the Ancient Near East and prehistory. The relation of religious cult and the state was discussed by Varro, under the term of \"theologia civilis\" (\"civic theology\"). The first state-sponsored Christian church was the Armenian Apostolic Church, established in 301 CE. In Christianity, as the term \"church\" is typically applied to a Christian place of worship or organisations incorporating such ones, the term \"state church\" is associated with Christianity as sanctioned by the government, historically the state church of the Roman Empire in the last centuries of the Empire's existence, and is sometimes used to denote a specific modern national branch of Christianity. Closely related to state churches are ecclesiae, which are similar but carry a more minor connotation.\n\nIn the Middle East, many states with primarily Islamic population have Islam as their state religion, either as the nondenominational Muslim, Shiite or Sunni variety, though the degree of religious restrictions on the citizen's everyday life varies by country. Rulers of Saudi Arabia use both secular and religious power, while Iran's secular presidents are supposed to follow the decisions of religious authorities since the revolution of 1979. Turkey, which also has a primarily Muslim population, became a secular country after Atatürk's Reforms, although unlike the Russian Revolution of the same time period, it did not result in the adoption of state atheism.\n\nThe degree to which an official national religion is imposed upon citizens by the state in contemporary society varies considerably; from high as in Saudi Arabia to minimal or none at all as in Denmark, England, Iceland, and Greece.\n\nThe degree and nature of state backing for denomination or creed designated as a state religion can vary. It can range from mere endorsement (with or without financial support) with freedom for other faiths to practice, to prohibiting any competing religious body from operating and to persecuting the followers of other sects. In Europe, competition between Catholic and Protestant denominations for state sponsorship in the 16th century evolved the principle \"Cuius regio, eius religio\" (states follow the religion of the ruler) embodied in the text of the treaty that marked the Peace of Augsburg, 1555. In England, Henry VIII broke with Rome in 1534, being declared the Supreme Head of the Church of England, the official religion of England continued to be \"Catholicism without the Pope\" until after his death in 1547, while in Scotland the Church of Scotland opposed the religion of the ruler.\n\nIn some cases, an administrative region may sponsor and fund a set of religious denominations; such is the case in Alsace-Moselle in France under its local law, following the pre-1905 French concordatory legal system and patterns in Germany.\n\nIn some communist states, notably in North Korea and Cuba, the state sponsors religious organizations, and activities outside those state-sponsored religious organizations are met with various degrees of official disapproval. In these cases, state religions are widely seen as efforts by the state to prevent alternate sources of authority.\n\nThere is also a difference between a \"state church\" and the broader term of \"state religion\". A \"state church\" is a state religion created by a state for use exclusively by that state. An example of a \"state religion\" that is not also a \"state church\" is Roman Catholicism in Costa Rica, which was accepted as the state religion in the 1949 Constitution, despite the lack of a national church. In the case of a \"state church\", the state has absolute control over the church, but in the case of a \"state religion\", the church is ruled by an exterior body; in the case of Catholicism, the Vatican has control over the church. In either case, the official state religion has some influence over the ruling of the state. As of 2012, there are only five state churches left, as most countries that once featured state churches have separated the church from their government.\n\nDisestablishment is the process of repealing a church's status as an organ of the state. In a state where an established church is in place, those opposed to such a move may be described as antidisestablishmentarians. This word is, however, most usually associated with the debate on the position of the Anglican churches in the British Isles: the Church of Ireland (disestablished in 1871), the Church of England in Wales (disestablished in 1920), and the Church of England itself (which remains established).\n\nCurrently, the following religions have been established as state religions in some countries. All are versions of Christianity, Islam or Buddhism.\n\nGovernments where Buddhism, either a specific form of it, or Buddhism as a whole, has been established as an official religion:\n\nThe following states recognize some form of Christianity as their state or official religion (by denomination):\n\nJurisdictions where Catholicism has been established as a state or official religion:\n\nJurisdictions that give various degrees of recognition in their constitutions to Roman Catholicism without establishing it as the state religion:\n\nThe jurisdictions below give various degrees of recognition in their constitutions to Eastern Orthodoxy, but without establishing it as the state religion:\n\nThe following states recognize some form of Protestantism as their state or official religion:\n\nThe Anglican Church of England is the established church in England as well as all three of the Crown dependencies.\n\n\nJurisdictions where a Lutheran church has been established as a state religion include the Nordic countries.\n\n\nMany Muslim-majority countries have constitutionally established Islam, or a specific form of it, as a state religion. Proselytism (converting people to another religion) is often illegal in such states.ref>Sheen J. \"Freedom of Religion and Belief: A World Report.\" (Routledge, 1997) p.452.</ref>\n\nIsrael is defined in several of its laws as a \"Jewish and democratic state\" (\"medina yehudit ve-demokratit\"). However, the term \"Jewish\" is a polyseme that can describe the Jewish people as either an ethnic or a religious group. The debate about the meaning of the term \"Jewish\" and its legal and social applications is one of the most profound issues with which Israeli society deals. The problem of the status of religion in Israel, even though it is relevant to all religions, usually refers to the status of Judaism in Israeli society. Thus, even though from a constitutional point of view Judaism is not the state religion in Israel, its status nevertheless determines relations between religion and state and the extent to which religion influences the political centre.\n\nThe State of Israel supports religious institutions, particularly Orthodox Jewish ones, and recognizes the \"religious communities\" as carried over from those recognized under the British Mandate—in turn derived from the pre-1917 Ottoman system of \"millets\". These are Jewish and Christian (Eastern Orthodox, Latin [Catholic], Gregorian-Armenian, Armenian-Catholic, Syrian [Catholic], Chaldean [Uniate], Greek Catholic Melkite, Maronite, and Syrian Orthodox). The fact that the Muslim population was not defined as a religious community does not affect the rights of the Muslim community to practice their faith. At the end of the period covered by the 2009 U.S. International Religious Freedom Report, several of these denominations were pending official government recognition; however, the Government has allowed adherents of not officially recognized groups the freedom to practice. In 1961, legislation gave Muslim Shari'a courts exclusive jurisdiction in matters of personal status. Three additional religious communities have subsequently been recognized by Israeli law: the Druze (prior under Islamic jurisdiction), the Evangelical Episcopal Church, and the Bahá'í. These groups have their own religious courts as official state courts for personal status matters (see millet system).\n\nThe structure and goals of the Chief Rabbinate of Israel are governed by Israeli law, but the law does not say explicitly that it is a state Rabbinate. However, outspoken Israeli secularists such as Shulamit Aloni and Uri Avnery have long maintained that it is that in practice. Non-recognition of other streams of Judaism such as Reform Judaism and Conservative Judaism is the cause of some controversy; rabbis belonging to these currents are not recognized as such by state institutions and marriages performed by them are not recognized as valid. As pointed out by Avnery and Aloni, the essential problem is that Israel carries on the top-down Ottoman \"millet\" system, under which the government reserves the complete discretion of recognizing some religious groups and not recognizing others. marriage in Israel provides no provision for civil marriage, marriage between people of different religions, marriages by people who do not belong to one of nine recognised religious communities, or same-sex marriages, although there is recognition of marriages performed abroad.\n\nIn some countries, there is a political ideology sponsored by the government that may be called political religion.\n\n\nThe concept of state religions was known as long ago as the empires of Egypt and Sumer, when every city state or people had its own god or gods. Many of the early Sumerian rulers were priests of their patron city god. Some of the earliest semi-mythological kings may have passed into the pantheon, like Dumuzid, and some later kings came to be viewed as divine soon after their reigns, like Sargon the Great of Akkad. One of the first rulers to be proclaimed a god during his actual reign was Gudea of Lagash, followed by some later kings of Ur, such as Shulgi. Often, the state religion was integral to the power base of the reigning government, such as in Egypt, where Pharaohs were often thought of as embodiments of the god Horus.\n\nZoroastrianism was the state religion of the Sassanid dynasty which lasted until 651, when Persia was conquered by the Rashidun Caliphate. However, it persisted as the state religion of the independent state of Hyrcania until the 15th century.\n\nThe tiny kingdom of Adiabene in northern Mesopotamia converted to Judaism around 34 CE.\n\nMany of the Greek city-states also had a god or goddess associated with that city. This would not be its only god or goddess, but the one that received special honors. In ancient Greece, the city of Athens had Athena, Sparta had Ares, Delphi had Apollo and Artemis, Olympia had Zeus, Corinth had Poseidon and Thebes had Demeter.\n\nIn Rome, the office of \"Pontifex Maximus\" came to be reserved for the Emperor, who was often declared a god posthumously, or sometimes during his reign. Failure to worship the Emperor as a god was at times punishable by death, as the Roman government sought to link emperor worship with loyalty to the Empire. Many Christians and Jews were subject to persecution, torture and death in the Roman Empire because it was against their beliefs to worship the Emperor.\n\nIn 311, Emperor Galerius, on his deathbed, declared a religious indulgence to Christians throughout the Roman Empire, focusing on the ending of anti-Christian persecution. Constantine I and Licinius, the two \"Augusti\", by the Edict of Milan of 313, enacted a law allowing religious freedom to everyone within the Roman Empire. Furthermore, the Edict of Milan cited that Christians may openly practice their religion unmolested and unrestricted, and provided that properties taken from Christians be returned to them unconditionally. Although the Edict of Milan allowed religious freedom throughout the Empire, it did not abolish nor disestablish the Roman state cult (Roman polytheistic paganism). The Edict of Milan was written in such a way as to implore the blessings of the deity.\n\nConstantine called up the First Council of Nicaea in 325, although he was not a baptised Christian until years later. Despite enjoying considerable popular support, Christianity was still not the official state religion in Rome, although it was in some neighbouring states such as Armenia, Iberia, and Aksum.\n\nRoman Religion (Neoplatonic Hellenism) was restored for a time by the Emperor Julian from 361 to 363. Julian does not appear to have reinstated the persecutions of the earlier Roman emperors.\n\nCatholic Christianity, as opposed to Arianism and other ideologies deemed heretical, was declared to be the state religion of the Roman Empire on 27 February 380 by the decree \"De fide catolica\" of Emperor Theodosius I.\n\nIn China, the Han dynasty (206 BCE – 220 CE) advocated Confucianism as the \"de facto\" state religion, establishing tests based on Confucian texts as an entrance requirement into government service—although, in fact, the \"Confucianism\" advocated by the Han emperors may be more properly termed a sort of Confucian Legalism or \"State Confucianism\". This sort of Confucianism continued to be regarded by the emperors, with a few notable exceptions, as a form of state religion from this time until the overthrow of the imperial system of government in 1911. Note, however, there is a debate over whether Confucianism (including Neo-Confucianism) is a religion or purely a philosophical system.\n\nDuring the Mongol Yuan dynasty (1271–1368 CE), Tibetan Buddhism was established as the \"de facto\" state religion by the Mongol ruler Kublai Khan, the founder of the Yuan dynasty. The top-level department and government agency known as the Bureau of Buddhist and Tibetan Affairs (Xuanzheng Yuan) was set up in Khanbaliq (modern Beijing) to supervise Buddhist monks throughout the empire. Since Kublai Khan only esteemed the Sakya sect of Tibetan Buddhism, other religions became less important. Before the end of the Yuan dynasty, 14 leaders of the Sakya sect had held the post of Imperial Preceptor (Dishi), thereby enjoy special power.\n\nShamanism and Buddhism were once the dominant religions among the ruling class of the Mongol khanates of Golden Horde and Ilkhanate, the two western khanates of the Mongol Empire. In the early days, the rulers of both khanates increasingly adopted Tibetan Buddhism, similar to the Yuan dynasty at that time. However, the Mongol rulers Ghazan of Ilkhanate and Uzbeg of Golden Horde converted to Islam in 1295 CE because of the Muslim Mongol emir Nawruz and in 1313 CE because of Sufi Bukharan sayyid and sheikh Ibn Abdul Hamid respectively. Their official favoring of Islam as the state religion coincided with a marked attempt to bring the regime closer to the non-Mongol majority of the regions they ruled. In Ilkhanate, Christian and Jewish subjects lost their equal status with Muslims and again had to pay the poll tax; Buddhists had the starker choice of conversion or expulsion. In Golden Horde, Buddhism and Shamanism among the Mongols were proscribed, and by 1315, Uzbeg had successfully Islamicized the Horde, killing Jochid princes and Buddhist lamas who opposed his religious policy and succession of the throne.\n\n\n\n\n\nThese areas were disestablished and dissolved, yet their presences were tolerated by the English and later British colonial governments, as Foreign Protestants, whose communities were expected to observe their own ways without causing controversy or conflict for the prevalent colonists. After the Revolution, their ethno-religious backgrounds were chiefly sought as the most compatible non-British Isles immigrants.\n\nThe State of Deseret was a provisional state of the United States, proposed in 1849, by Mormon settlers in Salt Lake City. The provisional state existed for slightly over two years, but attempts to gain recognition by the United States government floundered for various reasons. The Utah Territory which was then founded was under Mormon control, and repeated attempts to gain statehood met resistance, in part due to concerns that the principle of separation of church and state conflicted with the practice of members of The Church of Jesus Christ of Latter-day Saints placing their highest value on \"following counsel\" in virtually all matters relating to their church-centered lives. The state of Utah was eventually admitted to the union on 4 January 1896, after the various issues had been resolved.\n\n"}
{"id": "297681", "url": "https://en.wikipedia.org/wiki?curid=297681", "title": "Curfew", "text": "Curfew\n\nA curfew is an order specifying a time during which certain regulations apply. Typically it refers to the time when individuals are required to return to and stay in their homes. Such an order may be issued by public authorities but also by the head of a household to those living in the household. For instance, an au pair is typically given a curfew, which regulates when they must return to the host family's home in the evening.\n\nThe word \"curfew\" comes from the Old French phrase \"couvre-feu\", which means \"cover fire\". It was later adopted into Middle English as \"curfeu\", which later became the modern \"curfew\". Its original meaning refers to a law made by William The Conqueror that all lights and fires should be covered at the ringing of an eight o'clock bell to prevent the spread of destructive fire within communities in timber buildings.\n\n\nOn 28 January 2011, and following the collapse of the police system, President Hosni Mubarak declared a country-wide military enforced curfew. However, it was ignored by demonstrators who continued their sit-in in Tahrir Square. Concerned residents formed neighborhood vigilante groups to defend their communities against looters and the newly escaped prisoners.\n\nOn the second anniversary of the revolution, January 2013, a wave of demonstrations swept the country against President Mohamed Morsi who declared a curfew in Port Said, Ismaïlia, and Suez, three cities where deadly street clashes had occurred. In defiance, the locals took to the streets during the curfew, organizing football tournaments and street festivals, prohibiting police and military forces from enforcing the curfew.\n\nUnder Iceland's Child Protection Act (no. 80/2002 Art. 92), children aged 12 and under may not be outdoors after 20:00 (8:00 p.m.) unless accompanied by an adult. Children aged 13 to 16 may not be outdoors after 22:00 (10:00 p.m.), unless on their way home from a recognized event organized by a school, sports organization or youth club. During the period 1 May to 1 September, children may be outdoors for two hours longer.\n\nChildren and teenagers that break curfew are taken to the local police station and police officers inform their parents to get them. The age limits stated here shall be based upon year of birth, not date of birth. If a parent cannot be reached, the child or teenager is taken to a shelter.\n\nIn Sri Lanka, the Sri Lanka Police are empowered to declare and enforce a \"Police Curfew\" in any police area for any particular period to maintain the peace, law and order. Under the emergency regulations of the \"Public Security Ordinance\", the President may declare a curfew over the whole or over any part of the country. Travel is restricted, during a curfew, to authorised persons such as police, armed forces personal and public officers. Civilians may gain a Curfew Pass from a police station to travel during a curfew.\n\nThe United Kingdom's 2003 Anti-Social Behaviour Act created zones that allow police from 9 PM to 6 AM to hold and escort home unaccompanied minors under the age of 16, whether badly behaved or not. Although hailed as a success, the High Court ruled in one particular case that the law did not give the police a power of arrest, and officers could not force someone to come with them. On appeal the court of appeal held that the act gave police powers to escort minors home only if they are involved in, or at risk from, actual or imminently anticipated bad behaviour.\n\nIn a few towns in the United Kingdom, the curfew bell is still rung as a continuation of the medieval tradition where the bell used to be rung from the parish church to guide travelers safely towards a town or village as darkness fell, or when bad weather made it difficult to follow trackways and for the villagers to extinguish their lights and fires as a safety measure to combat accidental fires. Until 1100 it was against the law to burn any lights after the ringing of the curfew bell. In Morpeth, the curfew is rung each night at 8pm from Morpeth Clock Tower. In Chertsey, it is rung at 8pm, from Michaelmas to Lady Day. A short story concerning the Chertsey curfew, set in 1471, and entitled \"Blanche Heriot. A legend of old Chertsey Church\" was published by Albert Richard Smith in 1843, and formed a basis for the poem \"Curfew Must Not Ring Tonight\". At Castleton in the Peak District, the curfew is rung from Michaelmas to Shrove Tuesday. At Wallingford in Oxfordshire, the curfew bell continues to be rung at 9pm rather than 8pm which is a one-hour extension granted by William The Conqueror as the Lord of the town was a Norman sympathiser. However, none of these curfew bells serve their original function.\n\nCurfew law in the United States is usually a matter of local ordinance (mainly applied by a municipality or county), rather than federal law. However, the Constitution guarantees certain rights, which have been applied to the states through the 14th Amendment. Hence, any curfew law may be overruled and struck down if, for example, it violates a juvenile's 1st, 4th, 5th or 14th Amendment rights.\nNonetheless, curfews are set by state and local governments. They vary by state and even by county or municipality. In some cities, there are curfews for persons under the age of 18. American military curfews are a tool used by commanders at various installations to shape the behavior of soldiers.\n\nThe stated purpose of such laws is generally to deter disorderly behavior and crime, while others can include to protect youth from victimization and to strengthen parental responsibility, but their effectiveness is subject to debate. Generally, curfews attempt to address vandalism, shootings, and property crimes, which are believed to happen mostly at night, but are less commonly used to address underage drinking, drunk driving and teenage pregnancy. Parents can be fined, charged or ordered to take parenting classes for willingly, or through insufficient control or supervision, permitting the child to violate the curfew. Many local curfew laws were enacted in the 1950s and 1960s to attack the \"juvenile delinquent\" problem of youth gangs. Most curfew exceptions include:\nSome cities make it illegal for a business owner, operator, or any employee to knowingly allow a minor to remain in the establishment during curfew hours. A business owner, operator, or any employee may be also subject to fines.\n\nA 2011 UC-Berkeley study looked at the 54 larger U.S. cities that enacted youth curfews between 1985 and 2002 and found that arrests of youths affected by curfew restrictions dropped almost 15% in the first year and approximately 10% in following years. However, not all studies agree with the conclusion that youth curfew laws actually reduce crime, and many studies find no benefit or sometimes even the opposite. For example, one 2016 systematic review of 12 studies on the matter found that the effect on crime is close to zero, and can perhaps even backfire somewhat.\n\nThere are also concerns about racial profiling. In response to concerns about racial profiling, Montgomery County, Maryland passed a limited curfew, which would permit police officers to arrest juveniles in situations that appear threatening.\n\nMany malls in the United States have policies that prohibit minors under a specified age from entering the mall after specified times, unless they are accompanied by a parent or another adult or are working at the mall during curfew times. Such policies are known as \"mall curfews.\" Malls that have policies prohibiting unaccompanied minors at any time are known as \"parental escort policies.\"\n\nStates and municipalities in the United States have occasionally enacted curfews on the population at large, often as a result of severely inclement weather or political unrest. Some such curfews require all citizens simply to refrain from driving. Others require all citizens to remain inside, with exceptions granted to those in important positions, such as elected officials, law enforcement personnel, first responders, health care workers, and the mass media.\n\nIn 2015, the city of Baltimore enacted a curfew on all citizens that lasted for five days and prohibited all citizens from going outdoors from 10 pm to 5 am with the exception of those traveling to or from work and those with medical emergencies. This was in response to the 2015 Baltimore protests.\n\nHowever, unlike juvenile curfews, all-ages curfews have always been very limited in terms of both location and duration. That is, they are temporary and restricted to very specific areas, and generally only implemented during states of emergency, then subsequently lifted or allowed to sunset.\n\n\nPeople\n\n"}
{"id": "364578", "url": "https://en.wikipedia.org/wiki?curid=364578", "title": "Identity document", "text": "Identity document\n\nAn identity document (also called a piece of identification or ID, or colloquially as papers) is any document which may be used to prove a person's identity. If issued in a small, standard credit card size form, it is usually called an identity card (IC, ID card, citizen card), or passport card. Some countries issue formal identity documents, as national identification cards which may be compulsory or non-compulsory, while others may require identity verification using regional identification or informal documents. When the identity document incorporates a person's photograph, it may be called photo ID.\n\nIn the absence of a formal identity document, a driver's license may be accepted in many countries for identity verification. Some countries do not accept driver's licenses for identification, often because in those countries they do not expire as documents and can be old or easily forged. Most countries accept passports as a form of identification.\nSome countries require all people to have an identity document available at any time. Many countries require all foreigners to have a passport or occasionally a national identity card from their home country available at any time if they do not have a residence permit in the country.\n\nThe identity document is used to connect a person to information about the person, often in a database. The photo and the possession of it is used to connect the person with the document. The connection between the identity document and information database is based on personal information present on the document, such as the bearer's full name, age, birth date, address, an identification number, card number, gender, citizenship and more. A unique national identification number is the most secure way, but some countries lack such numbers or don't mention them on identity documents.\n\nA version of the passport considered to be the earliest identity document inscribed into law was introduced by King Henry V of England with the Safe Conducts Act 1414.\n\nFor the next 500 years and before World War I, most people did not have or need an identity document.\n\nPhotographic identification appeared in 1876 but it did not become widely used until the early 20th century when photographs became part of passports and other ID documents such as driver's licenses, all of which came to be referred to as \"photo IDs\". Both Australia and Great Britain, for example, introduced the requirement for a photographic passport in 1915 after the so-called Lody spy scandal.\n\nThe shape and size of identity cards were standardized in 1985 by ISO/IEC 7810. Some modern identity documents are smart cards including a difficult-to-forge embedded integrated circuit that were standardized in 1988 by ISO/IEC 7816. New technologies allow identity cards to contain biometric information, such as a photograph; face, hand, or iris measurements; or fingerprints. Many countries now issue electronic identity cards.\n\nLaw enforcement officials claim that identity cards make surveillance and the search for criminals easier and therefore support the universal adoption of identity cards. In countries that don't have a national identity card, there is, however, concern about the projected large costs and potential abuse of high-tech smartcards.\n\nIn many countries – especially English-speaking countries such as Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States – there are no government-issued compulsory identity cards for all citizens. Ireland's Public Services Card is not considered a national identity card by the Department of Employment Affairs and Social Protection (DEASP), but many say it is in fact becoming that, and without public debate or even a legislative foundation.\n\nThere is debate in these countries about whether such cards and their centralised database constitute an infringement of privacy and civil liberties. Most criticism is directed towards the enhanced possibilities of extensive abuse of centralised and comprehensive databases storing sensitive data. A 2006 survey of UK Open University students concluded that the planned compulsory identity card under the Identity Cards Act 2006 coupled with a central government database generated the most negative response among several alternative configurations. None of the countries listed above mandate possession of identity documents, but they have de facto equivalents since these countries still require proof of identity in many situations. For example, all vehicle drivers must have a driving licence, and young people may need to use specially issued \"proof of age cards\" when purchasing alcohol. In addition, and uniquely among native English-speaking countries without ID cards, the United States requires all its male residents between the ages of 18 and 25, including foreigners, to register for military conscription.\n\nArguments for identity documents as such:\nArguments for national identity documents:\n\nArguments against identity documents as such:\n\nArguments against national identity documents:\nArguments against overuse or abuse of identity documents:\n\nAccording to Privacy International, , possession of identity cards was compulsory in about 100 countries, though what constitutes \"compulsory\" varies. In some countries (see below), it is compulsory to have an identity card when a person reaches a prescribed age. The penalty for non-possession is usually a fine, but in some cases it may result in detention until identity is established. For people suspected with crimes such as shoplifting or no bus ticket, non-possession might result in such detention, also in countries not formally requiring identity cards. In practice, random checks are rare, except in certain times.\n\nA number of countries do not have national identity cards. These include Andorra, Australia, the Bahamas, Canada, Denmark, India (see below), Japan (see below), Kiribati, the Marshall Islands, Nauru, New Zealand, Norway, Palau, Samoa, Turkmenistan, Tuvalu, the United Kingdom and Uzbekistan. Other identity documents such as passports or driver's licenses are then used as identity documents when needed. However, governments of Kiribati, Norway, Samoa and Uzbekistan are planning to introduce new national identity cards in the near future. Some of these, e.g. Denmark, have more simple official identity cards, which do not match the security and level of acceptance of a national identity card, used by people without driver's licenses.\n\nA number of countries have voluntary identity card schemes. These include Austria, Belize, Finland, France (see France section), Hungary (however, all citizens of Hungary must have at least one of: valid passport, photo-based driving licence, or the National ID card), Iceland, Ireland, Saint Lucia, Sweden, Switzerland and the United States. The United Kingdom's scheme was scrapped in January 2011 and the database was destroyed.\n\nIn the United States, the Federal government issues optional identity cards known as \"Passport Cards\" (which include important information such as the nationality). On the other hand, states issue optional identity cards for people who do not hold a driver's license as an alternate means of identification. These cards are issued by the same organisation responsible for driver's licenses, usually called the Department of Motor Vehicles. Passport Cards hold limited travel status or provision, usually for domestic travel requirements. Note, this is not an obligatory identification card for citizens.\n\nFor the Sahrawi people of Western Sahara, pre-1975 Spanish identity cards are the main proof that they were Saharawi citizens as opposed to recent Moroccan colonists. They would thus be allowed to vote in an eventual self-determination referendum.\n\nCompanies and government departments may issue ID cards for security purposes, proof of identity, or proof of a qualification. For example, all taxicab drivers in the UK carry ID cards. Managers, supervisors, and operatives in construction in the UK have a photographic ID card, the CSCS (Construction Skills Certification Scheme) card, indicating training and skills including safety training. Those working on UK railway lands near working lines must carry a photographic ID card to indicate training in track safety (PTS and other cards) possession of which is dependent on periodic and random alcohol and drug screening. In Queensland and Western Australia, anyone working with children has to take a background check and get issued a Blue Card or Working with Children Card, respectively.\n\nLiberia has begun the issuance process of its national biometric identification card, which citizens and foreign residents will use to open bank accounts and participate in other government services on a daily basis.\n\nMore than 4.5 million people are expected to register and obtain ID cards of citizenship or residence in Liberia. The project has already started where NIR (National Identification Registry) is issuing Citizen National ID Cards. The centralized National Biometric Identification System (NBIS) will be integrated with other government ministries. Resident ID Cards and ECOWAS ID Cards will also be issued.\n\nIt is compulsory for all Egyptian citizens age 16 or older to possess an ID card ( \"Biṭāqat taḥqīq shakhṣiyya\", literally, \"Personal Verification Card\"). In daily colloquial speech, it is generally simply called \"el-biṭāqa\" (\"the card\"). It is used for:\nEgyptian ID cards consist of 14 digits, the national identity number, and expire after 7 years from the date of issue. Some feel that Egyptian ID cards are problematic, due to the general poor quality of card holders' photographs and the compulsory requirements for ID card holders to identify their religion and for married women to include their husband's name on their cards.\n\nEvery citizen of Tunisia is expected to apply for an ID card by the age of 18; however, with the approval of a parent(s), a Tunisian citizen may apply for, and receive, an ID card prior to their eighteenth birthday upon parental request.\n\nIn 2016, The government has introduced a new bill to the parliament to issue new biometric ID documents. The bill has created controversy amid civil society organizations.\n\nAll Gambian citizens over 18 years of age are required to hold a Gambian National Identity Card. In July 2009, a new biometric identity card was introduced. The biometric card is one of the acceptable documents required to apply for a Gambian Driving Licence.\n\nMauritius requires all citizens who have reached the age of 18 to apply for a National Identity Card. The National Identity Card is one of the few accepted forms of identification, along with passports. A National Identity Card is needed to apply for a passport for all adults, and all minors must take with them the National Identity Card of a parent(s) when applying for a passport.\n\nNigeria first introduced a national identity card in 2005, but its adoption back then was limited and not widespread.\nThe country is now in the process of introducing a new biometric ID card complete with a SmartCard and other security features. The National Identity Management Commission (NIMC) is the federal government agency responsible for the issuance of these new cards, as well as the management of the new National Identity Database.\nThe Federal Government of Nigeria announced in April 2013 that after the next general election in 2015, all subsequent elections will require that voters will only be eligible to stand for office or vote provided the citizen possesses a NIMC-issued identity card.\nThe Central Bank of Nigeria is also looking into instructing banks to request for a National Identity Number (NIN) for any citizen maintaining an account with any of the banks operating in Nigeria. The proposed kick off date is yet to be determined.\n\nSouth African citizens aged 15 years and 6 months or older are eligible for an ID card. The South African identity document is not valid as a travel document or valid for use outside South Africa. Although carrying the document is not required in daily life, it is necessary to show the document or a certified copy as proof of identity when:\nThe South African identity document used to also contain driving and firearms licences; however, these documents are now issued separately in card format.\nIn mid 2013 a smart card ID was launched to replace the ID book. The cards were launched on 18 July 2013 when a number of dignitaries received the first cards at a ceremony in Pretoria. The government plans to have the ID books phased out over a six to eight-year period. The South African government is looking into possibly using this smart card not just as an identification card but also for licences, National Health Insurance, and social grants.\n\nZimbabweans are required to apply for National Registration at the age of 16. Zimbabwean citizens are issued with a plastic card which contains a photograph and their particulars onto it. Before the introduction of the plastic card, the Zimbabwean ID card used to be printed on anodised aluminium. Along with Driving Licences, the National Registration Card (including the old metal type) is universally accepted as proof of identity in Zimbabwe. Zimbabweans are required by law to carry identification on them at all times and visitors to Zimbabwe are expected to carry their passport with them at all times.\n\nAfghan citizens over the age of 18 are required to carry a national ID document called Tazkira.\n\nBahraini citizens must have both an ID card, called a \"smart card\", which is recognized as an official document and can be used within the Gulf Cooperation Council, and a passport, which is recognized worldwide.\n\nBiometric identification has existed in Bangladesh since 2008. All Bangladeshis who are 18 years of age and older are included in a central Biometric Database, which is used by the Bangladesh Election Commission to oversee the electoral procedure in Bangladesh. All Bangladeshis are issued with an \"NID Card\" which can be used to obtain a passport, Driving Licence, credit card, and to register land ownership.\n\nThe Bhutanese national identity card (called the Buthanese Citizenship card) is an electronic ID card, compulsory for all Bhutanese nationals and costs 100 Bhutanese ngultrum.\n\nThe People's Republic of China requires each of its citizens aged 16 and over to carry an identity card. The card is the only acceptable legal document to obtain employment, a residence permit, driving licence or passport, and to open bank accounts or apply for entry to tertiary education and technical colleges.\n\nThe Hong Kong Identity Card (or HKID) is an official identity document issued by the Immigration Department of Hong Kong to all people who hold the right of abode, right to land or other forms of limited stay longer than 180 days in Hong Kong. According to Basic Law of Hong Kong, all permanent residents are eligible to obtain the Hong Kong Permanent Identity Card which states that the holder has the right of abode in Hong Kong. All persons aged 16 and above must carry a valid legal government identification document in public. All persons aged 16 and above must be able to produce valid legal government identification documents when requested by legal authorities; otherwise, they may be held in detention to investigate his or her identity and legal right to be in Hong Kong.\n\nWhile there is no mandatory identity card in India, the Aadhaar card, a multi-purpose national identity card, carrying 16 personal details and a unique identification number, has been available to all citizens since 2007. The card contains a photograph, full name, date of birth, and a unique, randomly generated 12-digit National Identification Number. However, the card itself is rarely required as proof, the number or a copy of the card being sufficient. The card has a SCOSTA QR code embedded on the card, through which all the details on the card are accessible. In addition to Aadhaar, PAN cards, ration cards, voter cards and driving licences are also used. These may be issued by either the government of India or the government of any state, and are valid throughout the nation. The Indian passport may also be used.\n\nResidents over 17 are required to hold a KTP (Kartu Tanda Penduduk) identity card. The card will identify whether the holder is an Indonesian citizen or foreign national. In 2011, the Indonesian government started a two-year ID issuance campaign that utilizes smartcard technology and biometric duplication of fingerprint and iris recognition. This card, called the Electronic KTP (e-KTP), will replace the conventional ID card beginning in 2013. By 2013, it is estimated that approximately 172 million Indonesian nationals will have an e-KTP issued to them.\n\nEvery citizen of Iran has an identification document called Shenasnameh in Persian (شناسنامه). This is a booklet based on the citizen's birth certificate which features their Shenasnameh National ID number, their birth date, their birthplace, and the names, birth dates and National ID numbers of their legal ascendant(s). In other pages of the Shenasnameh, their marriage status, spouse(s) name(s), names of children, date of every vote cast and eventually their death would be recorded.\n\nEvery Iranian permanent resident above the age of 15 must hold a valid National Identity Card (Persian:کارت ملی) or at least obtain their unique National Number from any of the local Vital Records branches of the Iranian Ministry of Interior.\n\nIn order to apply for an NID card, the applicant must be at least 15 years old and have a photograph attached to their Birth Certificate, which is undertaken by the Vital Records branch.\n\nSince June 21, 2008, NID cards have been compulsory for many things in Iran and Iranian missions abroad (e.g., obtaining a passport, driver's license, any banking procedure, etc.).\n\nEvery Iraqi citizen must have a National Card (البطاقة الوطنية).\n\nIsraeli law requires every permanent resident above the age of 16, whether a citizen or not, to carry an identification card called \"te'udat zehut\" () in Hebrew or \"biţāqat huwīya\" () in Arabic.\n\nThe card is designed in a bilingual form, printed in Hebrew and Arabic; however, the personal data is presented in Hebrew by default and may be presented in Arabic as well if the owner decides so. The card must be presented to an official on duty (e.g., a policeman) upon request, but if the resident is unable to do this, one may contact the relevant authority within five days to avoid a penalty.\n\nUntil the mid-1990s, the identification card was considered the only legally reliable document for many actions such as voting or opening a bank account. Since then, the new Israeli driver's licenses which include photos and extra personal information are now considered equally reliable for most of these transactions. In other situations any government-issued photo ID, such as a passport or a military ID, may suffice.\n\nThe Kuwaiti identity card is issued to Kuwaiti citizens. It can be used as a travel document when visiting countries in the Gulf Cooperation Council.\n\nThe Palestinian Authority issues identification cards following agreements with Israel. Since 1995, in accordance to the Oslo Accords, the data is forwarded to Israeli databases and verified. In February 2014, a presidential decision issued by Palestinian president Mahmoud Abbas to abolish the religion field was announced. Israel has objected to abolishing religion on Palestinian IDs because it controls their official records, IDs and passports and the PA does not have the right to make amendments to this effect without the prior approval of Israel. The Palestinian Authority in Ramallah said that abolishing religion on the ID has been at the center of negotiations with Israel since 1995. The decision was criticized by Hamas officials in Gaza Strip, saying it is unconstitutional and will not be implemented in Gaza because it undermines the Palestinian cause.\n\nE-National ID cards were rolled out in 2015.\n\nJapanese citizens are not required to have identification documents with them within the territory of Japan. When necessary, official documents, such as one's Japanese driver's license, basic resident registration card, radio operator license, social insurance card, health insurance card or passport are generally used and accepted. On the other hand, mid- to long-term foreign residents are required to carry their Zairyū cards, while short-term visitors and tourists (those with a Temporary Visitor status sticker in their passport) are required to carry their passports.\n\nThe Macau Resident Identity Card is an official identity document issued by the Identification Department to permanent residents and non-permanent residents.\n\nIn Malaysia, the MyKad is the compulsory identity document for Malaysian citizens aged 12 and above. Introduced by the \"National Registration Department of Malaysia\" on 5 September 2001 as one of four MSC Malaysia flagship applications and a replacement for the High Quality Identity Card (\"Kad Pengenalan Bermutu Tinggi\"), Malaysia became the first country in the world to use an identification card that incorporates both photo identification and fingerprint biometric data on an in-built computer chip embedded in a piece of plastic.\n\nMyanmar citizens are required to obtain a National Registration Card (NRC), while non-citizens are given a Foreign Registration Card (FRC).\n\nNew biometric cards rolled out in 2018. Information displayed in both English and Nepali.\n\nIn Pakistan, all adult citizens must register for the Computerized National Identity Card (CNIC), with a unique number, at age 18. CNIC serves as an identification document to authenticate an individual's identity as a citizen of Pakistan.\n\nEarlier on, National Identity Cards (NICs) were issued to citizens of Pakistan. Now, the government has shifted all its existing records of National Identity Cards (NIC) to the central computerized database managed by NADRA.\nNew CNIC's are machine readable and have security features such as facial and finger print information. At the end of 2013, smart national identity cards, SNICs, were also made available.\n\nA new Philippines identity card known as the Philippine Identification System (PhilSys) ID card began to be issued in August 2018 to Filipino citizens and foreign residents age 18 and above. This national ID card is non-compulsory but should harmonize existing government-initiated identification cards that have been issued – including the Unified Multi-Purpose ID issued to members of the Social Security System, Government Service Insurance System, Philippine Health Insurance Corporation and the Home Development Mutual Fund (Pag-IBIG Fund).\n\nIn Singapore, every citizen, and permanent resident (PR) must register at the age of 15 for an Identity Card (IC). The card is necessary not only for procedures of state but also in the day-to-day transactions of registering for a mobile phone line, obtaining certain discounts at stores, and logging on to certain websites on the internet. Schools frequently use it to identify students, both on-line and in exams.\n\nEvery citizen of South Korea over the age of 17 is issued an ID card called \"Jumindeungrokjeung\" (주민등록증). It has had several changes in its history, the most recent form being a plastic card meeting the ISO 7810 standard. The card has the holder's photo and a 15 digit ID number calculated from the holder's birthday and birthplace. A hologram is applied for the purpose of hampering forgery. This card has no additional features used to identify the holder, save the photo. Other than this card, the South Korean government accepts a Korean driver's license card, an Alien Registration Card, a passport and a public officer ID card as an official ID card.\n\nThe E-National Identity Card (abbreviation: E-NIC) is the identity document in use in Sri Lanka. It is compulsory for all Sri Lankan citizens who are sixteen years of age and older to have a NIC. NICs are issued from the Department for Registration of Persons. The Registration of Persons Act No.32 of 1968 as amended by Act Nos 28 and 37 of 1971 and Act No.11 of 1981 legislates the issuance and usage of NICs.\n\nSri Lanka is in the process of developing a Smart Card based RFID NIC card which will replace the obsolete 'laminated type' cards by storing the holders information on a chip that can be read by banks, offices, etc., thereby reducing the need to have documentation of these data physically by storing in the cloud.\n\nThe NIC number is used for unique personal identification, similar to the social security number in the US.\n\nIn Sri Lanka, all citizens over the age of 16 need to apply for a National Identity Card (NIC). Each NIC has a unique 10 digit number, in the format 000000000A (where 0 is a digit and A is a letter). The first two digits of the number are your year of birth (e.g.: 93xxxxxxxx for someone born in 1993). The final letter is generally a 'V' or 'X'. An NIC number is required to apply for a passport (over 16), driving license (over 18) and to vote (over 18). In addition, all citizens are required to carry their NIC on them at all times as proof of identity, given the security situation in the country. NICs are not issued to non-citizens, who are still required to carry a form of photo identification (such as a photocopy of their passport or foreign driving license) at all times. At times the Postal ID card may also be used.\n\nThe \"National Identification Card\" () is issued to all nationals of the Republic of China (Official name of Taiwan) aged 14 and older who have household registration in the Taiwan area. The Identification Card is used for virtually all activities that require identity verification within Taiwan such as opening bank accounts, renting apartments, employment applications and voting.\n\nThe Identification Card contains the holder's photo, ID number, Chinese name, and (Minguo calendar) date of birth. The back of the card also contains the person's registered address where official correspondence is sent, place of birth, and the name of legal ascendant(s) and spouse (if any).\n\nIf residents move, they must re-register at a municipal office ().\n\nROC nationals with household registration in Taiwan are known as \"registered nationals\". ROC nationals who do not have household registration in Taiwan (known as \"unregistered nationals\") do not qualify for the Identification Card and its associated privileges (e.g., the right to vote and the right of abode in Taiwan), but qualify for the Republic of China passport, which unlike the Identification Card, is not indicative of residency rights in Taiwan. If such \"unregistered nationals\" are residents of Taiwan, they will hold a Taiwan Area Resident Certificate as an identity document, which is nearly identical to the Alien Resident Certificate issued to foreign nationals/citizens residing in Taiwan.\n\nIn Thailand, the Thai National ID Card (Thai: บัตรประจำตัวประชาชน; RTGS: bat pracham tua pracha chon) is an official identity document issued only to Thai Nationals. The card proves the holder's identity for receiving government services and other entitlements.\n\nThe Federal Authority For Identity and Citizenship is a government agency that is responsible for issuing the National Identity Cards for the citizens (UAE nationals), GCC (Gulf Corporation Council) nationals and residents in the country. All individuals are mandated to apply for the ID card at all ages. For individuals of 15 years and above, fingerprint biometrics (10 fingerprints, palm, and writer) are captured in the registration process. Each person has a unique 15-digit identification number (IDN) that a person holds throughout his/her life.\n\nThe Identity Card is a smart card that has a state-of-art technology in the smart cards field with very high security features which make it difficult to duplicate. It is a 144KB Combi Smart Card, where the electronic chip includes personal information, 2 fingerprints, 4-digit pin code, digital signature, and certificates (digital and encryption). Personal photo, IDN, name, date of birth, signature, nationality, and the ID card expiry date are fields visible on the physical card.\n\nIn the UAE it is used as an official identification document for all individuals to benefit from services in the government, some of the non-government, and private entities in the UAE. This supports the UAE's vision of smart government as the ID card is used to securely access e-services in the country. The ID card could also be used by citizens as an official travel document between GCC countries instead of using passports. The implementation of the national ID program in the UAE enhanced security of the individuals by protecting their identities and preventing identity theft.\n\nIn Vietnam, all citizens above 14 years old must possess a People's Identity Card provided by the local authority.\n\nNational identity cards issued to citizens of the EEA (European Union, Iceland, Liechtenstein, Norway) and Switzerland, which states EEA or Swiss citizenship, can not only be used as an identity document within the home country, but also as a travel document to exercise the right of free movement in the EEA and Switzerland.\n\nDuring the UK Presidency of the EU in 2005 a decision was made to: \"Agree common standards for security features and secure issuing procedures for ID cards (December 2005), with detailed standards agreed as soon as possible thereafter. In this respect, the UK Presidency put forward a proposal for the EU-wide use of biometrics in national identity cards\".\n\nThe Austrian identity card is issued to Austrian citizens. It can be used as a travel document when visiting countries in the EEA (EU plus EFTA) countries, Europe's microstates, the British Crown Possessions, Albania, Bosnia and Herzegovina, Georgia, Kosovo, Moldova, Montenegro, the Republic of Macedonia,[citation needed] North Cyprus, Serbia, Montserrat, the French overseas territories and British Crown Possessions, and on organized tours to Jordan (through Aqaba airport) and Tunisia. Only around 10% of the citizens of Austria had this card in 2012,[2] as they can use the Austrian driver's licenses or other identity cards domestically and the more widely accepted Austrian passport abroad.\n\nIn Belgium, everyone above the age of 12 is issued an identity card ( in French, in Dutch and in German), and from the age of 15 carrying this card at all times is mandatory. For foreigners residing in Belgium, similar cards (foreigner's cards, in Dutch, in French) are issued, although they may also carry a passport, a work permit, or a (temporary) residence permit.\n\nSince 2000, all newly issued Belgian identity cards have a chip (eID card), and roll-out of these cards is expected to be complete in the course of 2009. Since 2008, the aforementioned foreigner's card has also been replaced by an eID card, containing a similar chip. The eID cards can be used both in the public and private sector for identification and for the creation of legally binding electronic signatures.\n\nUntil end 2010 Belgian consulates issued old style ID cards (105 x 75 mm) to Belgian citizens who were permanently residing in their jurisdiction and who chose to be registered at the consulate (which is strongly advised).\nSince 2011 Belgian consulates issue electronic ID cards, the electronic chip on which is not activated however.\n\nIn Bulgaria, it is obligatory to possess an identity card (Bulgarian – лична карта, lichna karta) at the age of 14. Any person above 14 being checked by the police without carrying at least some form of identification is liable to a fine of 50 Bulgarian levs (about €25).\n\nAll Croatian citizens may request an Identity Card, called Osobna iskaznica (literally Personal card). All persons over the age of 18 must have an Identity Card and carry it at all times. Refusal to carry or produce an Identity Card to a police officer can lead to a fine of 100 kuna or more and detention until the individual's identity can be verified by fingerprints.\n\nThe Croatian ID card is valid in the entire European Union, and can also be used to travel throughout the non-EU countries of the Balkans.\n\nThe 2013 design of the Croatian ID card is prepared for future installation of an electronic identity card chip, which is set for implementation in 2014.\n\nThe acquisition and possession of a Civil Identity Card is compulsory for any eligible person who has reached twelve years of age. On 29 January 2015, it was announced that all future IDs to be issued will be biometric. They can be applied for at Citizen Service Centres (KEP) or at consulates with biometric data capturing facilities.\n\nAn ID card costs €30 for adults and €20 for children with 10/5 years validity respectively. It is a valid travel document for the entire European Union.\n\nAn identity card with a photo is issued to all citizens of the Czech Republic at the age of 15. It is officially recognised by all member states of the European Union for intra EU travel. Travelling outside the EU mostly requires the Czech passport.\n\nDenmark is one of few EU countries that currently do not issue EU standard national identity cards (not counting driving licences and passports issued for other purposes).\n\nDanish citizens are not required by law to carry an identity card. A traditional identity document (without photo), the \"personal identification number certificate\" (Danish:\"Personnummerbevis\") is of little use in Danish society, as it has been largely replaced by the much more versatile \"National Health Insurance Card\" (Danish:\"Sundhedskortet\") which contains the same information and more. The National Health Insurance Card is issued to all citizens age 12 and above. It is commonly referred to as an identity card despite the fact it has no photo of the holder. Both certificates retrieve their information from the Civil Registration System. However, the \"personnummerbevis\" is still issued today and has been since September 1968.\n\nDanish driver's licenses and passports are the only identity cards issued by the government containing both the personal identification number and a photo. A foreign citizen without driving skills living in Denmark cannot get such documents. Foreign driving licenses and passports are accepted with limitations. A foreigner living in Denmark will have a residence permit with their personal identification number and a photo.\n\nUntil 2004, the national debit card Dankort contained a photo of the holder and was widely accepted as an identity card. The Danish banks lobbied successfully to have pictures removed from the national debit cards and so since 2004 the Dankort has no longer contained a photo. Hence it is rarely accepted for identification. 2004-2016 counties issued a \"photo identity card\" (), which can be used as age verification, but it is limited for identification purposes because of limited security for issuing, and it is not valid for EU travel.\n\nFrom 2017 counties issue identity cards () which have higher security and are valid for identification purposes, but still not for EU travel. From early 2018 it got possible to add nationality to the card in order to fulfill Swedish border control requirements, but still they don't fully confirm to international travel document requirements and aren't approved for EU travel, as they don't have the gender and birth date (but has Danish identity number which has this encoded) and don't have the machine readable zone and chip.\n\nThe Estonian identity card () is a chipped picture ID in the Republic of Estonia. An Estonian identity card is officially recognised by all member states of the European Union for intra EU travel. For travelling outside the EU, Estonian citizens may also require a passport.\n\nThe card's chip stores a key pair, allowing users to cryptographically sign digital documents based on principles of public key cryptography using DigiDoc. Under Estonian law, since 15 December 2000 the cryptographic signature is legally equivalent to a manual signature.\n\nThe Estonian identity card is also used for authentication in Estonia's ambitious Internet-based voting programme. In February 2007, Estonia was the first country in the world to institute electronic voting for parliamentary elections. Over 30 000 voters participated in the country's first e-election. By 2014, at the European Parliament elections, the number of e-voters has increased to more than 100,000 comprising 31% of the total votes cast.\n\nIn Finland, any citizen can get an identification card (\"henkilökortti\"/\"identitetskort\"). This, along with the passport, is one of two official identity documents. It is available as an electronic ID card (\"sähköinen henkilökortti\"/\"elektroniskt identitetskort\"), which enables logging into certain government services on the Internet.\n\nDriving licenses and KELA (social security) cards with a photo are also widely used for general identification purposes even though they are not officially recognized as such. However, KELA has ended the practice of issuing social security cards with the photograph of the bearer, while it has become possible to embed the social security information onto the national ID card. For most purposes when identification is required, only valid documents are ID card, passport or driving license. However, a citizen is not required to carry any of these.\n\nFrance has had a national ID card for all citizens since the beginning of World War II in 1940. Compulsory identity documents were created before, for workers from 1803 to 1890, nomads (\"\") in 1912, and foreigners in 1917 during World War I.\nNational identity cards were first issued as the \"carte d'identité Française\" under the law of October 27, 1940, and were compulsory for everyone over the age of 16. Identity cards were valid for 10 years, had to be updated within a year in case of change of residence, and their renewal required paying a fee. Under the Vichy regime, in addition to the face photograph, the family name, first names, date and place of birth, the card included the national identity number managed by the national statistics INSEE, which is also used as the national service registration number, as the Social Security account number for health and retirement benefits, for access to court files and for tax purposes.\nUnder the decree 55-1397 of October 22, 1955 a revised non-compulsory card, the \"carte nationale d'identité\" (CNI) was introduced.\n\nThe law (Art. 78-1 to 78–6 of the French Code of criminal procedure (\"Code de procédure pénale\") mentions only that during an ID check performed by police, gendarmerie or customs officer, one can prove his identity \"by any means\", the validity of which is left to the judgment of the law enforcement official. Though not stated explicitly in the law, an ID card, a driving licence, a passport, a visa, a \"Carte de Séjour\", a voting card are sufficient according to jurisprudency. The decision to accept other documents, with or without the bearer's photograph, like a Social Security card, a travel card or a bank card, is left to the discretion of the law enforcement officer.\n\nAccording to Art. 78-2 of the French Penal Procedure Code ID checks are only possible:\nThe last case allows checks of passers-by ID by the police, especially in neighborhoods with a higher criminality rate which are often the poorest at the condition, according to the \"Cour de cassation\", that the policeman doesn't refer only to \"general and abstract conditions\" but to \"particular circumstances able to characterise a risk of breach of public order and in particular an offence against the safety of persons or property\" (Cass. crim. 05/12/1999, n°99-81153, Bull., n°95).\n\nIn case of necessity to establish your identity, not being able to prove it \"by any means\" (for example the legality of a road traffic \"procès-verbal\" depends of it), may lead to a temporary arrest (\"vérification d'identité\") of up to 4 hours for the time strictly required for ascertaining your identity according to art. 78-3 of the French Code of criminal procedure (\"Code de procédure pénale\").\n\nFor financial transactions, ID cards and passports are almost always accepted as proof of identity. Due to possible forgery, driver's licenses are sometimes refused. For transactions by cheque involving a larger sum, two different ID documents are frequently requested by merchants.\n\nThe current identification cards are now issued free of charge and optional, and are valid for ten years for minors, and fifteen for adults. The current government has proposed a compulsory biometric card system, which has been opposed by human rights groups and by the national authority and regulator on computing systems and databases, the \"Commission nationale de l'informatique et des libertés\", CNIL. Another non-compulsory project is being discussed.\n\nIt is compulsory for all German citizens aged 16 or older to possess either a \"Personalausweis\" (identity card) or a passport but not to carry one. Police officers and other officials have a right to demand to see one of those documents (obligation of identification); however the law does not state that one is obliged to submit the document at that very moment. But as driver's licences, although sometimes accepted, are not legally accepted forms of identification in Germany, people usually choose to carry their \"Personalausweis\" with them.\nBeginning in November 2010, German ID cards are issued in the ID-1 format and can also contain an integrated digital signature, if so desired. Until October 2010, German ID cards were issued in ISO/IEC 7810 ID-2 format. The cards have a photograph and a chip with biometric data, including, optionally, fingerprints.\n\nGibraltar has operated an identity card system since 1943.\n\nThe cards issued were originally folded cardboard, similar to the wartime UK Identity cards abolished in 1950. There were different colours for British and non-British residents. Gibraltar requires all residents to hold identity cards, which are issued free.\n\nIn 1993 the cardboard ID card was replaced with a laminated version. However, although valid as a travel document to the UK, they were not accepted by Spain.\n\nA new version in an EU compliant format was issued and is valid for use around the EU although as very few are seen there are sometimes problems in its use, even in the UK. ID cards are needed for some financial transactions, but apart from that and to cross the frontier with Spain, they are not in common use.\n\nA compulsory, universal ID system based on personal ID cards has been in place in Greece since World War II. ID cards are issued by the police on behalf of the Headquarters of the Police (previously issued by the Ministry of Public Order, now incorporated in the Ministry of Internal Affairs) and display the holder's signature, standardized face photograph, name and surname, legal ascendant(s) name and surname, date and place of birth, height, municipality, and the issuing police precinct. There are also two optional fields designed to facilitate emergency medical care: ABO and Rhesus factor blood typing.\n\nFields included in previous ID card formats, such as vocation or profession, religious denomination, domiciliary address, name and surname of spouse, fingerprint, eye and hair color, citizenship and ethnicity were removed permanently as being intrusive of personal data and/or superfluous for the sole purpose of personal identification.\n\nSince 2000, name fields have been filled in both Greek and Latin characters. According to the Signpost Service of the European Commission [reply to Enquiry 36581], old type Greek ID cards \"are as valid as the new type according to Greek law and thus they constitute valid travel documents that all other EU Member States are obliged to accept\". In addition to being equivalent to passports within the European Economic Area, Greek ID cards are the principal means of identification of voters during elections.\n\nSince 2005, the procedure to issue an ID card has been automated and now all citizens over 12 years of age must have an ID card, which is issued within one work day. Prior to that date, the age of compulsory issue was at 14 and the whole procedure could last several months.\n\nIn Greece, an ID card is a citizen's most important state document. For instance, it is required to perform banking transactions if the teller personnel is unfamiliar with the apparent account holder, to interact with the Citizen Service Bureaus (KEP), receive parcels or registered mail etc. Citizens are also required to produce their ID card at the request of law enforcement personnel.\n\nAll the above functions can be fulfilled also with a valid Greek passport (e.g., for people who have lost their ID card and have not yet applied for a new one, people who happen to carry their passport instead of their ID card or Greeks who reside abroad and do not have an identity card, which can be issued only in Greece in contrast to passports also issued by consular authorities abroad).\n\nCurrently, there are three types of valid ID documents (\"Személyazonosító igazolvány\", née \"Személyi igazolvány\", abbr. \"Sz.ig.\") in Hungary: the oldest valid ones are hard-covered, multi-page booklets and issued before 1989 by the People's Republic of Hungary, the second type is a soft-cover, multi-page booklet issued after the change of regime; these two have one, original photo of the owner embedded, with original signatures of the owner and the local police's representative. The third type is a plastic card with the photo and the signature of the holder digitally reproduced. These are generally called Personal Identity Card.\n\nThe plastic card shows the owners full name, maiden name if applicable, birth date and place, mother's maiden name, the cardholder's gender, the ID's validity period and the local state authority which issued the card. The card has a 6 digit number + 2 letter unique ID and a separate machine readable zone on the back for identity document scanning devices. It does not have any information about the owner's residential address, nor their personal identity number – this sensitive information is contained on a separate card, called a Residency Card (\"Lakcímkártya\"). Personal identity numbers have been issued since 1975; they have the following format in numbers: gender (1 number) – birth date (6 numbers) – unique ID (4 numbers). They are no longer used as a personal identification number, but as a statistical signature.\n\nOther valid documents are the passport (blue colored or red colored with RFID chip) and the driver's license; an individual is required to have at least one of them on hand all the time. The Personal Identity Card is mandatory to vote in state elections or open a bank account in the country.\n\nID cards are issued to permanent residents of Hungary; the card has a different color for foreign citizens.\n\nThe Icelandic state-issued identity cards are called \"Nafnskírteini\". Most people use driver's licences instead. Identity documents are not mandatory to carry by law (unless driving a car), but can be needed for bank services, age verification and other situations.\n\nIreland does not issue mandatory national identity cards as such. Except for a brief period during the second world war when the Irish Department of External Affairs issued ID Cards to those wishing to travel to the UK, Ireland has never issued national identity cards as such.\n\nIdentity documentation is optional for Irish and British citizens. Nevertheless, identification is mandatory to obtain certain services such as air travel, banking, interactions regarding welfare and public services, age verification and additional situations.\n\n\"Non-nationals\" (no connection to an EEA country or Switzerland) aged 16 years and over must produce identification on demand to any immigration officer or a member of the Garda Síochána (police).\n\nPassport booklets, passport cards, driver's licenses, GNIB Registration Certificates and other forms of identity cards can be used for identification. Ireland has issued optional passport cards since October 2015. The cards are the size of a credit card and have all the information from the biographical page of an Irish passport booklet and can be explicitly used for travel in the EEA.\n\nIreland issues a \"Public Services Card\" which are useful when identification is needed for contacts regarding welfare and public services. They have photos but not birth dates and are therefore not accepted by banks. The card is also not considered as being an ID card by the Department of Employment Affairs and Social Protection (DEASP). In an Oireachtas committee hearing held on February 22, 2018, Tim Duggan of that department stated \"A national ID card is an entirely different idea. People are generally compelled to carry (such a card).\"\n\nAnyone who is legally resident in Italy, whether a citizen or not, is entitled to request an identity card at the local municipality. Also, any Italian citizen residing abroad in any of the European countries where there is the right of free movement, is entitled to request it at the local Italian embassy/consulate.\n\nAn identity card issued to an Italian citizen is accepted in lieu of a passport in all Europe (except in Belarus, Russia and Ukraine) and to travel to Turkey, Georgia, Egypt and Tunisia.\n\nFor an Italian citizen it is not compulsory to carry the card itself, as the authorities only have the right to ask for the identity of a person, not for a specific document. However, if public-security officers are not convinced of the claimed identity, such as may be the case for a verbally provided identity claim, they may keep the claimant in custody until his/her identity is ascertained; such an arrest is limited to the time necessary for identification and has no legal consequence.\n\nInstead, all foreigners in Italy are required by law to have an ID with them at all times. Citizens of EU member countries must be always ready to display an identity document that is legally government-issued in their country. Non-EU residents must have their passport with customs entrance stamp or a residence permit issued by Italian authorities; while all resident/immigrant aliens must have a residence permit (they are otherwise illegal and face deportation), foreigners from certain non-EU countries staying in Italy for a limited amount of time (typically for tourism) may be only required to have their passport with a proper customs stamp.\n\nThe current Italian identity document is a contactless electronic card made of polycarbonate in the ID-1 format with many security features and containing the following items printed by laser engraving:\n\n\nMoreover, the embedded electronic microprocessor chip stores the holder's picture, name, surname, place and date of birth, residency and (only if aged 12 and more) two fingerprints.\n\nThe card is issued by the Ministry of the Interior in collaboration with the IPZS in Rome and sent to the applicant within 6 business days.\n\nThe validity is 10 years for adults, 5 years for minors aged 3–18, 3 years for children aged 0–3 and it is extended or shortened in order to expire always on birthday.\n\nHowever, the old is still valid and in the process of being replaced with the new eID card since 4 July 2016. The lack of a Machine Readable Zone, the odd size, the fact that is made of paper and so easy to forge, often cause delays at border controls and, furthermore, foreign countries outside the EU sometimes refuse to accept it as a valid document. These common criticism were considered in the development of the new Italian electronic identity card, which is in the more common credit-card format and now has many of the latest security features available today.\n\nThe Principality of Liechtenstein has a voluntary ID card system for citizens, the Identitätskarte\n\nDutch citizens from the age of 14 are required to be able to show a valid identity document upon request by a police officer or similar official. Furthermore, identity documents are required when opening bank accounts and upon start of work for a new employer. Official identity documents for residents in the Netherlands are:\nFor the purpose of identification in public (but not for other purposes), also a Dutch driving license often may serve as an identity document.\nIn the Caribbean Netherlands, Dutch and other EEA identity cards are not valid; and the Identity card BES is an obligatory document for all residents.\n\nIn Norway there is no law penalising non-possession of an identity document. But there are rules requiring it for services like banking, air travel and voting (where personal recognition or other identification methods have not been possible).\n\nThe following documents are generally considered valid (varying a little, since no law lists them): Nordic driving licence, passport (often only from EU and EFTA), national ID card from EU, Norwegian ID card from banks and some more. \nThere is no ID card for anyone except bank ID card (normally printed on the reverse of a credit card). To get a bank ID card either a Nordic passport or another passport together with Norwegian residence and work permit is needed.\n\nThere is an ongoing plan to introduce a national ID card accrediting Norwegian citizenship, usable for travel within the EU, and for general identification. The plan started in 2007 and has been delayed several times and is now expected in 2020. Banks are campaigning to be freed from the task of issuing ID cards, stating that it should be the responsibility of state authorities. Some banks have already ceased issuing ID cards, so people need to bring their passport for such things as credit card purchases or buying prescribed medication if not in possession of a driving licence.\n\nEvery Polish citizen 18 years of age or older residing permanently in Poland must have an Identity Card (\"Dowód osobisty\") issued by the local Office of Civic Affairs.\nPolish citizens living permanently abroad are entitled, but not required, to have one.\n\nAll Portuguese citizens are required by law to obtain an Identity Card as they turn 6 years of age. They are not required to carry with them always but are obligated to present them to the lawful authorities if requested.\n\nThe old format of the cards (yellow laminated paper document) featured a portrait of the bearer, their fingerprint, and the name of parent(s), among other information.\nThey are currently being replaced by grey plastic cards with a chip, called \"Cartão de Cidadão\" (Citizen's Card), which now incorporate NIF (Tax Number), Cartão de Utente (Health Card) and Social Security, all of which are protected by a PIN obtained when the card is issued.\n\nThe new Citizen's Card is technologically more advanced than the former Identity Card and has the following characteristics:\n\nEvery citizen of Romania must register for an ID card (Carte de identitate, abbreviated CI) at the age of 14. The CI offers proof of the identity, address, sex and other data of the possessor. It has to be renewed every 10 years. It can be used instead of a passport for travel inside the European Union and several other countries outside the EU.\n\nAnother ID card is the Provisional ID Card (Cartea de Identitate Provizorie) issued temporarily when an individual cannot get a normal ID card. Its validity extends for up to 1 year. It cannot be used in order to travel within the EU, unlike the normal ID card.\n\nOther forms of officially accepted identification include the driver's license and the birth certificate. However, these are accepted only in limited circumstances and cannot take the place of the ID card in most cases. The ID card is mandatory for dealing with government institutions, banks or currency exchange shops. A valid passport may also be accepted, but usually only for foreigners.\n\nIn addition, citizens can be expected to provide the personal identification number (CNP) in many circumstances; purposes range from simple unique identification and internal book-keeping (for example when drawing up the papers for the warranty of purchased goods) to being asked for identification by the police. The CNP is 13 characters long, with the format S-YY-MM-DD-RR-XXX-Y. Where S is the sex, YY is year of birth, MM is month of birth, DD is day of birth, RR is a regional id, XXX is a unique random number and Y is a control digit.\n\nPresenting the ID card is preferred but not mandatory when asked by police officers; however, in such cases people are expected to provide a CNP or alternate means of identification which can be checked on the spot (via radio if needed).\n\nThe information on the ID card is required to be kept updated by the owner; current address of domicile in particular. Doing otherwise can expose the citizen to certain fines or be denied service by those institutions that require a valid, up to date card. In spite of this, it is common for people to let the information lapse or go around with expired ID cards.\n\nThe Slovak ID card () is a picture ID in Slovakia. It is issued to citizens of the Slovak Republic who are 15 or older. A Slovak ID card is officially recognised by all member states of the European Economic Area and Switzerland for travel. For travel outside the EU, Slovak citizens may also require a passport, which is a legally accepted form of picture ID as well. Police officers and some other officials have a right to demand to see one of those documents, and the law states that one is obliged to submit such a document at that very moment. If one fails to comply, law enforcement officers are allowed to insist on personal identification at the police station.\n\nEvery Slovenian citizen regardless of age has the right to acquire an Identity Card () where every citizen of the Republic of Slovenia of 18 years of age or older is obliged by law to acquire one and carry it at all times (or any other Identity document with a picture i.e., Slovene Passport). The Card is a valid Identity Document within all members states of the European Union for travel within the EU. With exception of the Faroe Islands and Greenland, though it may be used to travel outside of the EU: Norway, Liechtenstein, BiH, Macedonia, Montenegro, Serbia, Switzerland. The front side displays the name and surname, sex, nationality, date of birth and expiration date of the card, as well as the number of the ID card a black and white photograph and a signature. On the back, permanent address, administrative unit, date of issue, EMŠO, and a code with key information in a machine-readable zone.\nDepending on the holder's age (and sometimes also other factors), the card had a validity of 5 years or 10 years, and 1 year for foreigners living in Slovenia.\n\nIn Slovenia the ID cards importance is equaled only by the Slovenian passport, but a due to size a lot more practical.\n\nIn Spain, citizens, resident foreigners, and companies have similar but distinct identity numbers, some with prefix letters, all with a check-code\n\n\nDespite the NIF/CIF/NIE/NIF distinctions the \"identity number\" is unique and always has eight digits (the NIE has 7 digits) followed by a letter calculated from a 23-Modular arithmetic check used to verify the correctness of the number. The letters I, Ñ, O and U are not used and the \nsequence is as follows:\n\nThis number is the same for tax, social security and all legal purposes. Without this number (or a foreign equivalent such as a passport number), a contract may not be enforceable.\n\nIn Spain the formal identity number on an ID card is the most important piece of identification. It is used in all public and private transactions. It is required to open a bank account, to sign a contract, to have state insurance and to register at a university and should be shown when being fined by a police officer. It is one of the official documents required to vote at any election, although any other form of official ID such as a driving licence or passport may be used. The card also constitutes a valid travel document within the European Union.\n\nNon-resident citizens of countries such as the United Kingdom, where passport numbers are not fixed for the holder's life but change with renewal, may experience difficulty with legal transactions after the document is renewed since the old number is no longer verifiable on a valid (foreign) passport. However a NIE is issued for life and does not change and can be used for the same purposes.\n\nSweden does not have a legal statute for compulsory identity documents. However ID-cards are regularly used to ascertain a person's identity when completing certain transactions. These include but are not limited to banking and age verification. Also interactions with public authorities often require it, in spite of the fact that there is no law explicitly requiring it, because there are laws requiring authorities to somehow verify people's identity. Without Swedish identity documents difficulties can occur accessing health care services, receiving prescription medications and getting salaries or grants. From 2008, EU passports have been accepted for these services due to EU legislation (with exceptions including banking), but non-EU passports are not accepted.\nIdentity cards have therefore become an important part of everyday life.\n\nThere are currently three public authorities that issue ID-cards: the Swedish Tax Agency, the Swedish Police Authority, and the Swedish Transport Agency.\n\nThe Tax Agency cards can only be used within Sweden to validate a persons identity, but they can be obtained both by Swedish citizens and those that currently reside in Sweden. A Swedish personal identity number is required. It is possible to get one without having any Swedish ID-card. In this case a person holding such a card must guarantee the identity, and the person must be a verifiable relative or the boss at the company the person has been working or a few other verifiable people.\n\nThe Police can only issue identity documents to Swedish citizens. They issue an internationally recognised id-card according to EU standard usable for intra-European travel, and Swedish passports which are acceptable as identity documents worldwide.\n\nThe Transport Agency issues driving licences, which are valid as identity documents in Sweden. To obtain one, one must be approved as a driver and strictly have another Swedish identity document as proof of identity.\n\nIn the past there have been certain groups that have experienced problems obtaining valid identification documents. This was due to the initial process that was required to validate one's identity, unregulated security requirements by the commercial companies which issued them. Since July 2009, the Tax Agency has begun to issue ID-cards, which has simplified the identity validation process for foreign passport holders. There are still requirements for identity validation that can cause trouble, especially for foreign citizens, but the list of people who can validate one's identity has been extended.\n\nSwiss citizens have no obligation of identification in Switzerland and thus, are not required by law to be able to show a valid identity document upon request by a police officer or similar official. Furthermore, identity documents are required when opening a bank account or when dealing with public administration.\n\nRelevant in daily life of Swiss citizens are Swiss ID card and Swiss driver's license; latter needs to presented upon request by a police officer, when driving a motor-vehicle as e.g. a car, a motorcycle, a bus or a truck. Swiss passport is needed only for e.g. travel abroad to countries not accepting Swiss ID card as travel document.\n\nThe UK had an identity card during World War II as part of a package of emergency powers; this was abolished in 1952 by repealing the National Registration Act 1939. Identity cards were first proposed in the mid-1980s for people attending football matches, following a series of high-profile hooliganism incidents involving English football fans. However, this proposed identity card scheme never went ahead as Lord Taylor of Gosforth ruled it out as \"unworkable\" in the Taylor Report of 1990.\n\nBy 2006 several groups such as No2ID had formed to campaign against ID cards in Britain. The UK Labour government progressively introduced compulsory identity cards for non-EU residents in Britain starting late 2008. After the 2010 general election a new government was formed, comprising a coalition between two parties that had pledged to scrap ID cards – the Conservatives and Liberal Democrats – and the Home Office announced that the national identity register had been destroyed on 10 February 2011.\n\nIdentity cards for British nationals were introduced in 2009 on a voluntary basis. Only workers in certain high-security professions, such as airport workers, were required to have an identity card, and this general lack of ID being compulsory tends to remain the case today.\n\nDriving licences, particularly the photocard driving licence introduced in 1998, and passports are now the most widely used ID documents in the United Kingdom, but the former cannot be used as travel documents, except within the Common Travel Area. However, driving licences from the UK and other EU countries are usually accepted within other EEA countries for identity verification. Given that passports do not fit in a typical wallet or purse, most people do not carry their passports in public without an advance that they are going to need them. For people from the UK and other countries where national ID cards are not used or not common, this leaves driving licences as the only valid form of ID to be presented, if requested by an authority for a legitimately-given reason, but unlike a travel document, they do not show the holder's nationality or immigration status. Colloquially, in day-to-day life, most authorities do not ask for identification from individuals in a sudden, spot check type manner, such as by police or security guards, although this may become a concern in instances of stop and search.\n\nThere are also various PASS-accredited cards, used mainly for proof-of-age purposes, but \n\nFrom January 12, 2009 the Government of Albania is issuing a compulsory electronic and biometric ID Card \"(Letërnjoftim)\" for its citizens.\nEvery citizen at age 16 must apply for Biometric ID card.\n\nAzerbaijan is issuing a compulsory ID Card \"(Şəxsiyyət vəsiqəsi)\" for its citizens.\n\nEvery citizen at age 16 must apply for ID card.\n\nBelarus has combined the international passport and the internal passport into one document which is compulsory from age 14. It follows the international passport convention but has extra pages for domestic use.\n\nBosnia and Herzegovina allows every person over the age of 15 to apply for an ID card, and all citizens over the age of 18 must have the national ID card with them at all times. A penalty is issued if the citizen does not have the acquired ID card on them or if the citizen refuses to show proof of identification.\n\nThe Macedonian identity card () is a compulsory identity document issued in the Republic of North Macedonia. The document is issued by the police on behalf of the Ministry of Interior. Every citizen over 18 must be issued this identity card.\n\nIn Moldova identity cards () are being issued since 1996. The first person to get identity card was former president of Moldova – Mircea Snegur. Since then all the Moldovan citizens are required to have and use it inside the country. It can't be used to travel outside the country, however it is possible to pass the so-called Transnistrian border with it.\n\nThe Moldovan identity card may be obtained by a child from his/her date of birth. State company \"Registru\" is responsible for issuing identity cards and for storing data of all Moldovan citizens.\n\nMonégasque identity cards are issued to Monégasque citizens and can be used for travel within the Schengen Area.\n\nIn Montenegro every resident citizen over the age of 14 can have their \"Lična karta\" issued, and all persons over the age of 18 must have ID cards and carry them at all times when they are in public places. It can be used for international travel to Bosnia and Herzegovina, Serbia, Macedonia, Kosovo and Albania instead of the passport.\n\nThe role of identity documentation is primarily played by the so-called Russian internal passport, a passport-size booklet which contains a person's photograph, birth information and other data such as registration at the place of residence (informally known as propiska), marital data, information about military service and underage children. Internal passports are issued by the Main Directorate for Migration Affairs to all citizens who reach their 14th birthday and do not reside outside Russia. They are re-issued at the age 20 and 45.\n\nThe internal passport is commonly considered the only acceptable ID document in governmental offices, banks, while traveling by train or plane, getting a subscription service, etc. If the person does not have an internal passport (i.e., foreign nationals or Russian citizens who live abroad), an international passport can be accepted instead, theoretically in all cases. Another exception is army conscripts, who produce the Identity Card of the Russian Armed Forces.\n\nInternal passports can also be used to travel to Belarus, Kazakhstan, Tajikistan, Kyrgyzstan, Abkhazia and South Ossetia.\n\nOther documents, such as driver's licenses or student cards, can sometimes be accepted as ID, subject to regulations.\n\nThe national identity card is compulsory for all Sanmarinese citizens. Biometric and valid for international travel since 2016.\n\nIn Serbia every resident citizen over the age of 10 can have their \"Lična karta\" issued, and all persons over the age of 16 must have ID cards and carry them at all times when they are in public places. It can be used for international travel to Bosnia and Herzegovina, Montenegro and Macedonia instead of the passport. Contact microchip on ID is optional.\n\nKosovo issues its own identity cards. These documents are accepted by Serbia when used as identification while crossing the Serbia-Kosovo border. They can also be used for international travel to Montenegro and Albania.\n\nThe Ukrainian identity card or Passport of the Citizen of Ukraine (also known as the Internal passport or Passport Card) is an identity document issued to citizens of Ukraine. Every Ukrainian citizen aged 14 or above and permanently residing in Ukraine must possess an identity card issued by local authorities of the State Migration Service of Ukraine.\n\nUkrainian identity cards are valid for 10 years (or 4 years, if issued for citizens aged 14 but less than 18) and afterwards must be exchanged for a new document.\n\nThe Turkish national ID card () is compulsory for all Turkish citizens from birth. Cards for males and females have a different color. The front shows the first and last name of the holder, first name of legal ascendant(s), birth date and place, and an 11 digit ID number. The back shows marital status, religious affiliation, the region of the county of origin, and the date of issue of the card. On February 2, 2010 the European Court of Human Rights ruled in a 6 to 1 vote that the religious affiliation section of the Turkish identity card violated articles 6, 9, and 12 of the European Convention of Human Rights, to which Turkey is a signatory. The ruling should coerce the Turkish government to completely omit religious affiliation on future identity cards. The Turkish police are allowed to ask any person to show ID, and refusing to comply may lead to arrest. It can be used for international travel to Northern Cyprus, Georgia and Ukraine instead of a passport.\n\nMinistry of Interior of Turkey has released EU-like identity cards for all Turkish citizens starting from 02.01.2017. New identity cards are fully and can be used as a bank card, bus ticket or at international trips.\n\nCalled the \"Identification Card R.R\". Optional, although compulsory for voting and other government transactions. Available also for any Commonwealth country citizen who has lived in Belize for a year without leaving and been at least 2 months in an area where the person has been registered in.\n\nIn Canada, different forms of identification documentation are used, but there is no de jure national identity card. The Canadian passport is issued by the federal (national) government, and the provinces and territories issue various documents which can be used for identification purposes. The most commonly used forms of identification within Canada are the health card and driver's licence issued by provincial and territorial governments. The widespread usage of these two documents for identification purposes has made them de facto identity cards.\n\nIn Canada, a driver's license usually lists the name, home address, height and date of birth of the bearer. A photograph of the bearer is usually present, as well as additional information, such as restrictions to the bearer's driving licence. The bearer is required by law to keep the address up to date.\n\nA few provinces, such as Québec and Ontario, issue provincial health care cards which contain identification information, such as a photo of the bearer, their home address, and their date of birth. British Columbia, Saskatchewan and Ontario are among the provinces that produce photo identification cards for individuals who do not possess a driving licence, with the cards containing the bearer's photo, home address, and date of birth.\n\nFor travel abroad, a passport is almost always required. There are a few minor exceptions to this rule; required documentation to travel among North American countries is subject to the Western Hemisphere Travel Initiative , such as the NEXUS programme and the Enhanced Drivers License programme implemented by a few provincial governments as a pilot project. These programmes have not yet gained widespread acceptance, and the Canadian passport remains the most useful and widely accepted international travel document.\n\nEvery Costa Rican citizen must carry an identity card immediately after turning 18. The card is named \"Cédula de Identidad\" and it is issued by the local registrar's office (\"Registro Civil\"), an office belonging to the local elections committee (\"Tribunal Supremo de Elecciones\"), which in Costa Rica has the same rank as the Supreme Court. Each card has a unique number composed of nine numerical digits, the first of them being the province where the citizen was born (with other significance in special cases such as granted citizenship to foreigners, adopted persons, or in rare cases, old people for whom no birth certificate was processed at birth). After this digit, two blocks of four digits follow; the combination corresponds to the unique identifier of the citizen.\n\nIt is widely requested as part of every legal and financial purpose, often requested at payment with credit or debit cards for identification guarantee and requested for buying alcoholic beverages or cigarettes or upon entrance to adults-only places like bars.\n\nThe card must be renewed every ten years and is freely issued again if lost. Among the information included there are, on the front, two identification pictures and digitized signature of the owner, identification number (known colloquially just as the \"cédula\"), first name, first and second-last names and an optional \"known as\" field. On the back, there is again the identification number, birth date, where the citizen issues its vote for national elections or referendums, birthplace, gender, date when it must be renewed and a matrix code that includes all this information and even a digitized fingerprint of the thumb and index finger.\n\nThe matrix code is not currently being used nor inspected by any kind of scanner.\n\nBesides this identification card, every vehicle driver must carry a driving licence, an additional card that uses the same identification number as the ID card (Cédula de Identidad) for the driving license number. A passport is also issued with the same identification number used in the ID card. The same situation occurs with the Social Security number; it is the same number used for the ID card.\n\nAll non-Costa Rican citizens with a \"resident status\" must carry an ID card (\"Cédula de Residencia\"), otherwise, a passport and a valid visa. Each resident's ID card has a unique number composed of 12 digits; the first three of them indicate their nationality and the rest of them a sequence used by the immigration authority (called Dirección General de Migración y Extranjería). As with the Costa Rican citizens, their Social Security number and their driver's license (if they have it) would use the same number as in their own resident's ID card.\n\nA \"Cédula de Identidad y Electoral\" (Identity and Voting Document) is a National ID that is also used for voting in both Presidential and Congressional ballots. Each \"Cédula de Identidad y Electoral\" has its unique serial number composed by the serial of the municipality of current residence, a sequential number plus a verification digit. This National ID card is issued to all legal residents of adult age. It is usually required to validate job applications, legally binding contracts, official documents, buying/selling real estate, opening a personal bank account, obtaining a Driver's License and the like. It is issued free of charge by the \"Junta Central Electoral\" (Central Voting Committee) to all Dominicans not living abroad at the time of reaching adulthood (16 years of age) or younger is they are legally emancipated. Foreigners who have taken permanent residence and have not yet applied for Dominican naturalization (i.e., have not opted for Dominican citizenship but have taken permanent residence) are required to pay an issuing tariff and must bring along their non-expired Country of Origin passport and deposit photocopies of their Residential Card and Dominican Red Cross Blood Type card. Foreigners residing on a permanent basis must renew their \"Foreign ID\" on a 2-, 4-, or 10-year renewal basis (about US$63–US$240, depending on desired renewal period).\n\nIn El Salvador, ID Card is called Documento Único de Identidad (DUI) (Unique Identity Document). Every citizen above 18 years must carry this ID for identification purposes at any time. It is not based on a smartcard but on a standard plastic card with two-dimensional bar-coded information with picture and signature.\n\nIn January 2009, the National Registry of Persons (RENAP) in Guatemala began offering a new identity document in place of the \"Cédula de Vecindad\" (neighborhood identity document) to all Guatemala citizens and foreigners. The new document is called \"Documento Personal de Identification\" (DPI) (Personal Identity Document). It is based on a smartcard with a chip and includes an electronic signature and several measures against fraud. \n\nhttps://web.archive.org/web/20160715174910/http://www.cse.gob.ni/index.php/servicios/tramite-cedula\n\nNot mandatory, but needed in almost all official documents, the CURP is the standardized version of an identity document. It actually could be a printed green wallet-sized card (without a photo) or simply an 18-character identification key printed on a birth or death certificate.\n\nWhile Mexico has a national identity card (\"cédula de identitad personal\"), it is only issued to children aged 4–17\n\nUnlike most other countries, Mexico has assigned a CURP to nearly all minors, since both the government and most private schools ask parent(s) to supply their children's CURP to keep a data base of all the children. Also, minors must produce their CURP when applying for a passport or being registered at Public Health services by their parent(s).\n\nMost adults need the CURP code too, since it is required for almost all governmental paperwork like tax filings and passport applications. Most companies ask for a prospective employee's CURP, voting card, or passport rather than birth certificates.\n\nTo have a CURP issued for a person, a birth certificate or similar proof must be presented to the issuing authorities to prove that the information supplied on the application is true. Foreigners applying for a CURP must produce a certificate of legal residence in Mexico. Foreign-born naturalized Mexican citizens must present their naturalization certificate.\nOn August 21, 2008, the Mexican cabinet passed the National Security Act, which compels all Mexican citizens to have a biometric identity card, called Citizen Identity Card (\"Cédula de identidad ciudadana\") before 2011.\n\nOn February 13, 2009 the Mexican government designated the state of Tamaulipas to start procedures for issuing a pilot program of the national Mexican ID card.\n\nAlthough the CURP is the \"de jure\" official identification document in Mexico, the Instituto Nacional Electoral's voting card is the \"de facto\" official identification and proof of legal age for citizens of ages 18 and older.\n\nOn July 28, 2009 Mexican President Felipe Calderón, facing the Mexican House of Representatives, announced the launch of the Mexican national Identity card project, which will see the first card issued before the end of 2009.\n\nCedula de Identidad. Required at 12 (cedula juvenil) and 18 years of age. Panamanian citizens must carry their Cedula at all times. New biometric national identity cards rolling out in 2019. The card must be renewed every 10 years (every 5 years for those under 18) and it can only be replaced 3 times (with each replacement costing more than the previous one) without requiring a background check, to confirm and verify that the card holder is not selling his or her identity to third parties for human trafficking or other criminal activities. All cards have QR, PDF417, and Code 128 barcodes. The QR Code holds all printed (on the front of the card) text information about the card holder, while the PDF417 barcode holds, in JPEG format encoded with Base64, an image of the fingerprint of the left index finger of the card holder. Panamanian biometric/electronic/machine readable ID cards are similar to biometric passports and current European/Czech national ID cards and have only a small PDF417 barcode, with a machine readable area, a contactless smart card RFID chip and golden contact pads similar to those found in smart card credit cards and SIM cards. The machine readable code contains all printed text information about the card holder (it replaces the QR Code) while both chips (the smart card chip is hidden under the golden contact pads) contain all personal information about the card holder along with a JPEG photo of the card holder, a JPEG photo with the card holder's signature, and another JPEG photo but with all 10 fingerprints of both hands of the card holder. Earlier cards used Code 16K and Code 49 barcodes with magnetic stripes.\n\nThere is no compulsory federal-level ID card issued to all US citizens. US citizens and nationals may obtain passports or US passport cards if they chose to, but the alternatives described below are more popular.\nFor most people, driver's licenses issued by the respective state and territorial governments have become the \"de facto\" identity cards, and are used for many identification purposes, such as when purchasing alcohol and tobacco, opening bank accounts, and boarding planes, along with confirming a voter's identity in states with voter photo identification initiatives. Individuals who do not drive are able to obtain an identification card with the same functions from the same state agency that issues driver's licenses. In addition, many schools issue student and teacher ID cards.\n\nThe United States passed a bill entitled the REAL ID Act on May 11, 2005. The bill compels states to begin redesigning their driver's licenses to comply with federal security standards by December 2009. Federal agencies would reject licenses or identity cards that do not comply, which would force Americans accessing everything from airplanes to national parks and courthouses to have the federally mandated cards. At airports, those not having compliant licenses or cards would simply be redirected to a secondary screening location. The REAL ID Act is highly controversial, and with 25 states have approved either resolutions or binding legislation not to participate in the program, and with President Obama's selection of Janet Napolitano (a prominent critic of the program) to head the Department of Homeland Security, the future of the law remains uncertain, and bills have been introduced into Congress to amend or repeal it. The most recent of these, dubbed PASS ID, would eliminate many of the more burdensome technological requirements but still require states to meet federal standards in order to have their ID cards accepted by federal agencies.\n\nThe bill takes place as governments are growing more interested in implanting technology in ID cards to make them smarter and more secure. In 2006, the U.S. State Department studied issuing passports with Radio-frequency identification, or RFID, chips embedded in them. Virginia may become the first state to glue RFID tags into all its driver's licenses. Seventeen states, however, have passed statutes opposing or refusing to implement the Real ID Act.\n\nThe United States passport verifies both personal identity and citizenship, but is not mandatory for citizens to possess within the country and is issued by the US State Department on a discretionary basis.\n\nSince February 1, 2008, U.S. citizens may apply for passport cards, in addition to the usual passport books. Although their main purpose is for land and sea travel within North America, the passport card may also be accepted by federal authorities (such as for domestic air travel or entering federal buildings), which may make it an attractive option for people residing where state driver's licenses and I.D. cards are not REAL ID-compliant, should those requirements go into effect. TSA regulations list the passport card as an acceptable identity document at airport security checkpoints.\n\nU.S. Citizenship and Immigration Services has indicated that the U.S. Passport Card may be used in the Employment Eligibility Verification Form I-9 (form) process. The passport card is considered a \"List A\" document that may be presented by newly hired employees during the employment eligibility verification process to show work authorized status. \"List A\" documents are those used by employees to prove both identity and work authorization when completing the Form I-9.\n\nThe basic document needed to establish a person's identity and citizenship in order to obtain a passport is a birth certificate. These are issued by either the US state of birth or by the US Department of State for overseas births to US citizens. A child born in the US is in nearly all cases (except for children of foreign diplomats) automatically a US citizen. A child born overseas to US citizens must have the birth registered with the corresponding US embassy/consulate to obtain citizenship at birth, or they will need to apply for recognition of their citizenship at a later date.\n\nSocial Security numbers and cards are issued by the US Social Security Administration for tracking of Social Security taxes and benefits. They have become the \"de facto\" national identification number for federal and state taxation, private financial services, and identification with various companies. SSNs do not establish citizenship because they can also be issued to permanent residents as well as citizens. They typically can only be part of the establishment of a person's identity; a photo ID that verifies date of birth is also usually requested.\n\nA mix of various documents can be presented to, for instance, verify one's legal eligibility to take a job within the US. \"Identity\" and \"citizenship\" is established by presenting a passport alone, but this must be accompanied by a Social Security card for taxation ID purposes. A driver's license/state ID establishes \"identity\" alone, but does not establish \"citizenship\", as these can be provided to non-citizens as well. In this case, an applicant without a passport may sign an affidavit of citizenship or be required to present a birth certificate. They must still also submit their Social Security number.\n\n\"Residency\" within a certain US jurisdiction, such as a voting precinct, can be proven if the driver's license or state ID has the home address printed on it corresponding to that jurisdiction. Utility bills or other pieces of official printed mail can also suffice for this purpose. In the case of voting, citizenship must also be proven with a passport, birth certificate, or signed citizenship affidavit. Receiving in-state tuition at a state's public college or university also requires proof of residency using one of the above methods. Ownership of property, proved by a deed, also immediately confers residency in most cases.\n\nA Social Security number does not prove any form of residency, and neither does a passport, as neither of these documents is tied to a specific jurisdiction apart from the US as a whole, and a person can be issued either of these without living in the US (such as being born abroad to American citizen parent(s)). Thus, \"residency in the US\" is not clearly defined, and determining this often depends on the particular administrative process at hand.\n\nThe Selective Service System has in the past, in times of a military draft, issued something close to a National ID Card, only for men that were eligible for the draft.\n\nAustralia does not have a national identity card. Instead, various identity documents are used or required to prove a person's identity, whether for government or commercial purposes.\nCurrently, driver licences and photo cards, both issued by the states and territories, are the most widely used personal identification documents in Australia. Additionally, the Australia Post Keypass identity card, issued by Australia Post, can be used by people who do not have an Australian drivers licence or an Australian state and territory issued identity photo card.\n\nPhoto cards are also called \"Proof of Age Cards\" or similar and can be issued to people as another type of identity. Identification indicating age is commonly required to purchase alcohol and tobacco and to enter nightclubs and gambling venues.\n\nOther important identity documents include a passport, an official birth certificate, an official marriage certificate, cards issued by government agencies (typically social security cards), some cards issued by commercial organisations (e.g., a debit or credit card), and utility accounts. Often, some combination of identity documents is required, such as an identity document linking a name, photograph and signature (typically photo-ID in the form of a driver licence or passport), evidence of operating in the community, and evidence of a current residential address.\n\nNew alcohol laws in the state of Queensland require some Brisbane-based pubs and bars to scan ID documents against a database of people who should be denied alcohol, for which foreign passports and driver's licences are not valid.\n\nNational Identity cards, called “FSM Voters National Identity card”, are issued on an optional basis, free of charge. The Identity Cards were introduced in 2005.\n\nLegal forms of identification are used mainly for buying alcohol and cigarettes where the purchaser looks younger than 25 and entry to nightclubs. They can also be required for the purchase of spray paint and glues, and for some government and bank transactions. Forms of legal identification are New Zealand and overseas passports, New Zealand driver licences and Kiwi Access cards (which replaced the 18+ cards in January 2019) from the Hospitality Association of New Zealand. Overseas driver licences may not be sufficient for the purchase of alcohol and tobacco. Firearms licences are a form of photo identification issued by the New Zealand Police.\n\n\"National Voter's Identity card\" are optional upon request.\n\nTonga's National ID Card was first issued in 2010, and it is optional, along with the driver's licenses and passports. Either one of these are mandatory for to vote though. Applicants need to be 14 years of age or older to apply for a National ID Card.\n\nNational Identity Cards are being issued since October 2017. Plans for rolling out biometric cards are due for the late 2018.\n\nDocumento Nacional de Identidad or DNI (which means National Identity Document) is the main identity document for Argentine citizens. It is issued at a person's birth, and must be updated at 8 and 14 years of age, and thereafter every 15 years in one format: a card (DNI tarjeta); it is valid if identification is required, and is required for voting. They are produced at a special plant in Buenos Aires by the Argentine national registry of people (ReNaPer).\n\nIn Brazil, at the age of 18, all Brazilian citizens are supposed to be issued a \"cédula de identidade\" (ID card), usually known by its number, the \"Registro Geral (RG)\", Portuguese for \"General Registry\". The cards are needed to obtain a job, to vote, and to use credit cards. Foreigners living in Brazil have a different kind of ID card. Since the RG is not unique, being issued in a state-basis, in many places the CPF (the Brazilian revenue agency's identification number) is used as a replacement. The current Brazilian driver's license contains both the RG and the CPF, and as such can be used as an identification card as well.\n\nThere are plans in course to replace the current RG system with a new \"Documento Nacional de Identificação\" (National Identification Document), which will be electronic (accessible by a mobile application) and national in scope, and to change the current ID card to a new smartcard.\n\nEvery resident of Colombia over the age of 14 is issued an identity card (\"Tarjeta de Identidad\"). Upon turning 18 every resident must obtain a \"Cédula de Ciudadanía\", which is the only document that proves the identity of a person for legal purposes. ID cards must be carried at all times and must be presented to the police upon request. If the individual fails to present the ID card upon request by the police or the military, he/she is most likely going to be detained at police station even if he/she is not a suspect of any wrongdoing. ID cards are needed to obtain employment, open bank accounts, obtain a passport, driver's license, military card, to enroll in educational institutions, vote or enter public buildings including airports and courthouses. Failure to produce ID is a misdemeanor punishable with a fine.\n\nID duplicate costs must be assumed by citizens.\n\nEvery resident of Chile over the age of 18 must have and carry at all times their ID Card called \"Cédula de Identidad\" issued by the Civil Registry and Identification Service of Chile. It contains the full name, gender, nationality, date of birth, photograph of the data subject, right thumb print, ID number, and personal signature.\n\nThis is the only official form of identification for residents in Chile and is widely used and accepted as such. It is necessary for every contract, most bank transactions, voting, driving (along with the driver's licence) and other public and private situations.\n\nBiometrics collection is mandatory.\n\nIn Peru, it is mandatory for all citizens over the age of 18, whether born inside or outside the territory of the Republic, to obtain a National Identity Document (\"Documento Nacional de Identidad\").\nThe DNI is a public, personal and untransferable document.\n\nThe DNI is the only means of identification permitted for participating in any civil, legal, commercial, administrative, and judicial acts. It is also required for voting and must be presented to authorities upon request. The DNI can be used as a passport to travel to all South American countries that are members of UNASUR.\n\nThe DNI is issued by the National Registry of Identification and Civil Status (RENIEC). For Peruvians abroad, service is provided through the Consulates of Peru, in accordance with Articles 26, 31 and 8 of Law No. 26,497.\n\nThe document is card-sized as defined by ISO format ID-1 (prior to 2005 the DNI was size ISO ID-2; renewal of the card due to the size change was not mandatory, nor did previously-emitted cards lose validity). The front of the card presents photographs of the holder's face, their name, date and place of birth (the latter in coded form), gender and marital status; the bottom quarter consists of machine-readable text. Three dates are listed as well; the date the citizen was first registered at RENIEC; the date the document was issued; and the expiration date of the document. The back of the DNI features the holder's address (including district, department and/or province) and voting group. Eight voting record blocks are successively covered with metallic labels when the citizen presents themselves at their voting group on voting days. The back also denotes whether the holder is an organ donor, presents the holder's right index finger print, a PDF417 bar code, and a 1D bar code.\n\nIdentity Card (Renewal)\n\nIn Uruguay, the identity card is issued by the Ministry of Interior and the National Civil Identification (DNIC). It is mandatory and essential for several steps at either governmental or private. The document is mandatory for all inhabitants of the Eastern Republic of Uruguay, whether they are native citizens, legal citizens, or resident aliens in the country, even for children as young as 45 days old.\n\nIt is a laminated cardboard 9 cm wide and approximately 5 cm high, dominated by the blue color, showing the flag in the center of the Treinta y Tres Orientales, with the inscription \"Liberty or Death.\" On the back appears the photo of the owner, the number assigned by the DNIC (Including a self-generated or check digit), name / s full / s with name / s and the corresponding signature (or proof of not knowing or not to sign).\nOn the reverse appears nationality, date of birth, date of issuing the document and the date it is due (usually 10 years after the date of issue, even if issued after 70 years of age, lifetime and for the children is valid for five years). There is also the right thumbprint and observations if any.\n\nIdentity cards are demanded widespread in all formal transactions, from credit card purchases to any identity validation, proof of age, and so on.\n\nNot to be confused with the civic badge, which is used exclusively for voting in elections (elections and plebiscites).\n\nCheck Digit Calculation '\nThey take the 7 card numbers and multiply each by 2987634 one by one (the first number by 2, the second by 9 and so on, when each result exceeds one digit, the unit takes only).\n\nExample:\nUT: 1234567-X -> 2987634 -> 2, 8, 4, 8, 0, 8, 8\n\nIt is the sum of the results, the example would be 2 +8 +4 +8 +0 +8 +8 = 38 for the first number is greater than 38 that ends in 0 and is subtracted: 40-38 = 2 (is the same as 10 – (38 mod 10)). X = 2 then the check digit for the card 1,234,567.\n\nAnother simple way to look at it as a scalar product of vectors in module 10. The first 7 digits of the card can be viewed as a vector of length 7. This vector is multiplied by the vector scalar obtaining a number N 8123476 The check digit is found to be N module 10.\n\nExample: CI: 1234567-X -> X = [(1x8) + (2x1) + (3x2) + (4x3) + (5x4) + (6x7) + (7x6)] mod 10 -> X = [8 +2 +6 +12 +20 +42 +42] mod 10 = 132 mod 10 = 2\n\nIdentity cards in Venezuela consist of a plastic-laminated paper which contains the national ID number (\"Cédula de Identidad\") as well as a color-photo and the last names, given names, date of birth, right thumb print, signature, and marital status (single, married, divorced, widowed) of the bearer. It also contains the documents expedition and expiration date. Two different prefixes can be found before the ID number: \"V\" for Venezuelans and \"E\" for foreigners (\"extranjeros\" in Spanish). This distinction is also shown in the document at the very bottom by a bold all-caps typeface displaying either the word VENEZOLANO or EXTRANJERO, respectively.\n\nDespite Venezuela being the second country in the Americas (after the United States) to adopt a biometric passport, the current Venezuelan ID document is remarkably low-security, even for regional standards. It can hardly be called a card. The paper inside the laminated cover contains only two security measures, first, it is a special type of government-issued paper, and second, it has microfilaments in the paper that glow in the presence of UV light. The laminated cover itself is very simplistic and quite large for the paper it covers and the photo, although is standard sized (3x3.5 cm) is rather blurred. Government officials in charge of issuing the document openly recommend each individual to cut the excess plastic off and re-laminate the document in order to protect it from bending. The requirements for getting a Venezuelan identity document are quite relaxed and Venezuela lacks high-security in its birth certificates and other documents that give claim to citizenship.\nBecause one can get a Venezuelan passport and register to vote only by virtue of possessing a Venezuelan identity card and since the Venezuelan government has been accused by the media and the opposition of naturalizing substantial numbers of foreigners for electoral purposes, many Venezuelans accused the government of a lack of a plan to ramp up the security of the \"cédula de identidad\" along with other Venezuelan vital documents such as birth certificates as part of a strategy by the Chávez regime to continue the practice of naturalizing foreigners for electoral purposes. The government has announced that a new \"cédula de identidad\" will be available to all citizens somewhere around the first quarter of 2011. This proposed ID is indeed a polycarbonate bankcard-sized document with biometric and RFID technology. It resembles the analogous card that has been in place in the Venezuelan biometric passports since 2007. However, the release of this new card to the public has been delayed on several occasions and as of October 2018 there are no news as to when it will be available.\n\n\n\n"}
{"id": "463734", "url": "https://en.wikipedia.org/wiki?curid=463734", "title": "Public health", "text": "Public health\n\nPublic health has been defined as \"the science and art of preventing disease”, prolonging life and promoting human health through organized efforts and informed choices of society, organizations, public and private, communities and individuals\". Analyzing the health of a population and the threats it faces is the basis for public health. The \"public\" can be as small as a handful of people or as large as a village or an entire city; in the case of a pandemic it may encompass several continents. The concept of \"health\" takes into account physical, psychological, and social well-being. As such, according to the World Health Organization, it is not merely the absence of disease or infirmity.\n\nPublic health is an interdisciplinary field. For example, epidemiology, biostatistics and management of health services are all relevant. Other important subfields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, occupational safety, gender issues in health, and sexual and reproductive health.\n\nPublic health aims to improve the quality of life through prevention and treatment of disease, including mental health. This is done through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promotion of handwashing and breastfeeding, delivery of vaccinations, suicide prevention, and distribution of condoms to control the spread of sexually transmitted diseases.\n\nModern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, medical assistants, public health nurses, midwives, medical microbiologists, economists, sociologists, geneticists, data managers, and physicians. Depending on the need, environmental health officers or public health inspectors, bioethicists, and even veterinarians, gender experts, or sexual and reproductive health specialists might be called on.\n\nAccess to health care and public health initiatives are difficult challenges in developing countries. Public health infrastructures are still forming in those countries.\n\nThe focus of a public health intervention is to prevent and mitigate diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behaviors, communities and environments. Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases. In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures. Measures such as these have contributed greatly to the health of populations and increases in life expectancy.\n\nPublic health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations. The World Health Organization (WHO) is the international agency that coordinates and acts on global public health issues. Most countries have their own governmental public health agency, often called the ministry of health, with responsibility for domestic health issues.\n\nIn the United States, state and local health departments are on the front line of public health initiatives. In addition to their national duties, the United States Public Health Service (PHS), led by the Surgeon General of the United States, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are also involved with international health activities.\n\nIn Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention. The Public health system in India is managed by the Ministry of Health & Family Welfare of the government of India with state-owned health care facilities.\n\nMost governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine. Public health programs providing vaccinations have made strides in promoting health, including the eradication of smallpox, a disease that plagued humanity for thousands of years.\nThe World Health Organization (WHO) identifies core functions of public health programs including:\n\nIn particular, public health surveillance programs can:\n\nPublic health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011. Although the prioritization of pressing public health issues is important, Laurie Garrett argues that there are following consequences. When foreign aid is funnelled into disease-specific programs, the importance of public health in general is disregarded. This public health problem of stovepiping is thought to create a lack of funds to combat other existing diseases in a given country.\n\nFor example, the WHO reports that at least 220 million people worldwide suffer from diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by the year 2030. In a June 2010 editorial in the medical journal \"The Lancet\", the authors opined that \"The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.\" The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO's latest estimates as of June 2016 highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014. The United States is the leading country with 30.6% of its population being obese. Mexico follows behind with 24.2% and the United Kingdom with 23%. Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings. Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise.\n\nSome programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programmes. Another is the control of tobacco smoking. Changing smoking behavior requires long-term strategies, unlike the fight against communicable diseases, which usually takes a shorter period for effects to be observed. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Proponents argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents say that this undermines individual freedom and personal responsibility, and worry that the state may be emboldened to remove more and more choice in the name of better population health overall.\n\nSimultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.\n\nMany health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool. In fact, film festivals and competitions have been established to specifically promote films about health. Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.\n\nAs well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:\n\nTo improve public health, one important strategy is to promote modern medicine and scientific neutrality to drive the public health policy and campaign, which is recommended by Armanda Solorzana, through a case study of the Rockefeller Foundation's hookworm campaign in Mexico in the 1920s. Soloranza argues that public health policy can't concern only politics or economics. Political concerns can lead government officials to hide the real numbers of people affected by disease in their regions, such as upcoming elections. Therefore, scientific neutrality in making public health policy is critical; it can ensure treatment needs are met regardless of political and economic conditions.\n\nThe history of public health care clearly shows the global effort to improve health care for all. However, in modern-day medicine, real, measurable change has not been clearly seen, and critics argue that this lack of improvement is due to ineffective methods that are being implemented. As argued by Paul E. Farmer, structural interventions could possibly have a large impact, and yet there are numerous problems as to why this strategy has yet to be incorporated into the health system. One of the main reasons that he suggests could be the fact that physicians are not properly trained to carry out structural interventions, meaning that the ground level health care professionals cannot implement these improvements. While structural interventions can not be the only area for improvement, the lack of coordination between socioeconomic factors and health care for the poor could be counterproductive, and end up causing greater inequity between the health care services received by the rich and by the poor. Unless health care is no longer treated as a commodity, global public health will ultimately not be achieved. This being the case, without changing the way in which health care is delivered to those who have less access to it, the universal goal of public health care cannot be achieved.\n\nAnother reason why measurable changes may not be noticed in public health is because agencies themselves may not be measuring their programs' efficacy. Perrault et al. analyzed over 4,000 published objectives from Community Health Improvement Plans (CHIPs) of 280 local accredited and non-accredited public health agencies in the U.S., and found that the majority of objectives – around two-thirds – were focused on achieving agency outputs (e.g., developing communication plans, installing sidewalks, disseminating data to the community). Only about one-third focused on seeking measurable changes in the populations they serve (i.e., changing people's knowledge, attitudes, behaviors). What this research showcases is that if agencies are only focused on accomplishing tasks (i.e., outputs) and do not have a focus on measuring actual changes in their populations with the activities they perform, it should not be surprising when measurable changes are not reported. Perrault et al. advocate for public health agencies to work with those in the discipline of Health Communication to craft objectives that are measurable outcomes, and to assist agencies in developing tools and methods to be able to track more proximal changes in their target populations (e.g., knowledge and attitude shifts) that may be influenced by the activities the agencies are performing.\n\n\"Public Health 2.0\" is a movement within public health that aims to make the field more accessible to the general public and more user-driven. The term is used in three senses. In the first sense, \"Public Health 2.0\" is similar to \"Health 2.0\" and describes the ways in which traditional public health practitioners and institutions are reaching out (or could reach out) to the public through social media and health blogs.\n\nIn the second sense, \"Public Health 2.0\" describes public health research that uses data gathered from social networking sites, search engine queries, cell phones, or other technologies. A recent example is the proposal of statistical framework that utilizes online user-generated content (from social media or search engine queries) to estimate the impact of an influenza vaccination campaign in the UK.\n\nIn the third sense, \"Public Health 2.0\" is used to describe public health activities that are completely user-driven. An example is the collection and sharing of information about environmental radiation levels after the March 2011 tsunami in Japan. In all cases, Public Health 2.0 draws on ideas from Web 2.0, such as crowdsourcing, information sharing, and user-centred design. While many individual healthcare providers have started making their own personal contributions to \"Public Health 2.0\" through personal blogs, social profiles, and websites, other larger organizations, such as the American Heart Association (AHA) and United Medical Education (UME), have a larger team of employees centered around online driven health education, research, and training. These private organizations recognize the need for free and easy to access health materials often building libraries of educational articles.\n\nThere is a great disparity in access to health care and public health initiatives between developed countries and developing countries, as well as within developing countries. In developing countries, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention. As a result, a large majority of disease and mortality in developing countries results from and contributes to extreme poverty. For example, many African governments spend less than US$10 per person per year on health care, while, in the United States, the federal government spent approximately US$4,500 per capita in 2000. However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered \"health care\" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.\n\nLarge parts of the world remained plagued by largely preventable or treatable infectious diseases. In addition to this however, many developing countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases with, the poorer communities being heavily affected by both chronic and infectious diseases. Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year. Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.\n\nEach day brings new front-page headlines about public health: emerging infectious diseases such as SARS, rapidly making its way from China (see Public health in China) to Canada, the United States and other geographically distant countries; reducing inequities in health care access through publicly funded health insurance programs; the HIV/AIDS pandemic and its spread from certain high-risk groups to the general population in many countries, such as in South Africa; the increase of childhood obesity and the concomitant increase in type II diabetes among children; the social, economic and health effects of adolescent pregnancy; and the public health challenges related to natural disasters such as the 2004 Indian Ocean tsunami, 2005's Hurricane Katrina in the United States and the 2010 Haiti earthquake.\n\nSince the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that our health is affected by many factors including where we live, genetics, our income, our educational status and our social relationships; these are known as \"social determinants of health\". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy. A social gradient in health runs through society. The poorest generally suffer the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social stratum. The new public health advocates for population-based policies that improve health in an equitable manner.\n\nHealth aid to developing countries is an important source of public health funding for many developing countries. Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced. From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health. Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion between 2000 and 2010 which was more than twice the increase seen in any other sector during those years. Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF. In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid. In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare. Recent data, however, shows that international health aid has plateaued and may begin to decrease.\n\nDebates exist questioning the efficacy of international health aid. Proponents of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients. For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into fruition.\n\nEconomic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates. However, a 2014–2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement. That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.\n\nTo address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals building off the Millennium Development Goals of 2000 to be completed by 2030. These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1–6 directly address health disparities, primarily in developing countries. These six goals address key issues in global public health: Poverty, Hunger and food security, Health, Education, Gender equality and women's empowerment, and water and sanitation. Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals hope to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future.\n\nThe links between the various sustainable development goals and public health are numerous and well established:\n\nThe U.S. Global Health Initiative was created in 2009 by President Obama in an attempt to have a more holistic, comprehensive approach to improving global health as opposed to previous, disease-specific interventions. The Global Health Initiative is a six-year plan, \"to develop a comprehensive U.S. government strategy for global health, building on the President's Emergency Plan for AIDS Relief (PEPFAR) to combat HIV as well as U.S. efforts to address tuberculosis (TB) and malaria, and augmenting the focus on other global health priorities, including neglected tropical diseases (NTDs), maternal, newborn and child health (MNCH), family planning and reproductive health (FP/RH), nutrition, and health systems strengthening (HSS)\". The GHI programs are being implemented in more than 80 countries around the world and works closely with the United States Agency for International Development, the Centers for Disease Control and Prevention, the United States Deputy Secretary of State.\n\nThere are seven core principles:\n\n\nThe aid effectiveness agenda is a useful tool for measuring the impact of these large scale programs such as The Global Fund to Fight AIDS, Tuberculosis and Malaria and the Global Alliance for Vaccines and Immunization (GAVI) which have been successful in achieving rapid and visible results. The Global Fund claims that its efforts have provided antiretroviral treatment for over three million people worldwide. GAVI claims that its vaccination programs have prevented over 5 million deaths since it began in 2000.\n\nEducation and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition, and environmental and occupational health.\n\nIn the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.\n\nIn the United States, the Welch-Rose Report of 1915 has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation. The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education. Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.\n\nEven though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions. A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914. The Yale School of Public Health was founded by Charles-Edward Avory Winslow in 1915. The Johns Hopkins School of Hygiene and Public Health became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States, when it was founded in 1916. By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.\n\nOver the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.\n\nSchools of public health offer a variety of degrees which generally fall into two categories: professional or academic. The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspeciality of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.\n\nProfessional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc/DHS) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more eclectic range of methodologies. A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research. The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.\n\nAcademic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.\n\nIn the United States, the Association of Schools of Public Health represents Council on Education for Public Health (CEPH) accredited schools of public health. Delta Omega is the honor society for graduate studies in public health. The society was founded in 1924 at the Johns Hopkins School of Hygiene and Public Health. Currently, there are approximately 68 chapters throughout the United States and Puerto Rico.\n\nFrom the beginnings of human civilization, communities promoted health and fought disease at the population level. Definitions of health as well as methods to pursue it differed according to the medical, religious and natural-philosophical ideas groups held, the resources they had, and the changing circumstances in which they lived. Yet few early societies displayed the hygienic stagnation or even apathy often attributed to them. The latter reputation is mainly based on the absence of present-day bioindicators, especially immunological and statistical tools developed in light of the germ theory of disease transmission.\n\nPublic health, at any rate, was born neither in Europe nor as a response to the Industrial Revolution. Preventative health interventions are attested almost anywhere historical communities have left their mark. In Southeast Asia, for instance, Ayurvedic medicine and subsequently Buddhism fostered occupational, dietary and sexual regimens that promised balanced bodies, lives and communities, a notion strongly present in Traditional Chinese Medicine as well. Among the Mayans, Aztecs and other early civilizations in the Americas, population centers pursued hygienic programs, including by holding medicinal herbal markets. And among Aboriginal Australians, techniques for preserving and protecting water and food sources, micro-zoning to reduce pollution and fire risks, and screens to protect people against flies were common, even in temporary camps.\n\nWestern European, Byzantine and Islamicate civilizations, which generally adopted a Hippocratic, Galenic or humoral medical system, fostered preventative programs as well. These were developed on the basis of evaluating the quality of local climates, including topography, wind conditions and exposure to the sun, and the properties and availability of water and food, for both humans and nonhuman animals. Diverse authors of medical, architectural, engineering and military manuals explained how to apply such theories to groups of different origins and under different circumstances. This was crucial, since under Galenism bodily constitutions were thought to be heavily shaped by their material environments, so their balance required specific regimens as they traveled during different seasons and between climate zones.\n\nIn complex, pre-industrialized societies, interventions designed to reduce health risks could be the initiative of different stakeholders. For instance, in Greek and Roman antiquity, army generals learned to provide for soldiers’ wellbeing, including off the battlefield, where most combatants died prior to the twentieth century. In Christian monasteries across the Eastern Mediterranean and western Europe since at least the fifth century CE, monks and nuns pursued strict but balanced regimens, including nutritious diets, developed explicitly to extend their lives. And royal, princely and papal courts, which were often mobile as well, likewise adapted their behavior to suit environmental conditions in the sites they occupied. They could also choose sites they considered salubrious for their members and sometimes had them modified.\n\nIn cities, residents and rulers developed measures to benefit the general population, which faced a broad array of recognized health risks. These provide some of the most sustained evidence for preventative measures in earlier civilizations. In numerous sites the upkeep of infrastructures, including roads, canals and marketplaces, as well as zoning policies, were introduced explicitly to preserve residents’ health. Officials such as the muhtasib in the Middle East and the Road master in Italy, fought the combined threats of pollution through sin, ocular intromission and miasma. Craft guilds were important agents of waste disposal and promoted harm reduction through honesty and labor safety among their members. Medical practitioners, including public physicians, collaborated with urban governments in predicting and preparing for calamities and identifying and isolating people perceived as lepers, a disease with strong moral connotations. Neighborhoods were also active in safeguarding local people’s health, by monitoring at-risk sites near them and taking appropriate social and legal action against artisanal polluters and neglectful owners of animals. Religious institutions, individuals and charitable organizations in both Islam and Christianity likewise promoted moral and physical wellbeing by endowing urban amenities such as wells, fountains, schools and bridges, also in the service of pilgrims. In western Europe and Byzantium, religious processions commonly took place, which purported to act as both preventative and curative measures for the entire community. \n\nUrban residents and other groups also developed preventative measures in response to calamities such as war, famine, floods and widespread disease. During and after the Black Death (1346-53), for instance, inhabitants of the Eastern Mediterranean and Western Europe reacted to massive population decline in part on the basis of existing medical theories and protocols, for instance concerning meat consumption and burial, and in part by developing new ones. The latter included the establishment of quarantine facilities and health boards, some of which eventually became regular urban (and later national) offices. Subsequent measures for protecting cities and their regions included issuing health passports for travelers, deploying guards to create sanitary cordons for protecting local inhabitants, and gathering morbidity and mortality statistics. Such measures relied in turn on better transportation and communication networks, through which news on human and animal disease was efficiently spread. \n\nThe 18th century saw rapid growth in voluntary hospitals in England. The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.\nThe practice of vaccination became prevalent in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy. Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published \"Observations on the Diseases of the Army in Camp and Garrison\", in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.\n\nWith the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanisation exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.\n\nThe first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay. Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.\n\nThe Poor Law Commission reported in 1838 that \"the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered\". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease. The Health of Towns Association was formed at Exeter Hall London on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom. Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.\n\nThese national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on \"The Sanitary Condition of the Labouring Population\" was published in 1842 and was followed up with a supplementary report a year later.\n\nVaccination for various diseases was made compulsory in the United Kingdom in 1851, and by 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.\n\nFurther interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included latrinization, the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.\n\nThe Infectious Disease (Notification) Act 1889 mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.\n\nIn the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.\n\nThe science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Dr. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. He first publicized his theory in an essay, \"On the Mode of Communication of Cholera\", in 1849, followed by a more detailed treatise in 1855 incorporating the results of his investigation of the role of the water supply in the Soho epidemic of 1854.\n\nBy talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street). Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to disable the well pump by removing its handle.\n\nSnow later used a dot map to illustrate the cluster of cholera cases around the pump. He also used statistics to illustrate the connection between the quality of the water source and cholera cases. He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography. It is regarded as the founding event of the science of epidemiology.\n\nWith the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease. Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats, and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever. Brazilian scientist Carlos Chagas identified a tropical disease and its vector.\n\nWith onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventive methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.\n\nFrance 1871–1914 followed well behind Bismarckian Germany, as well as Great Britain, in developing the welfare state including public health. Tuberculosis was the most dreaded disease of the day, especially striking young people in their 20s. Germany set up vigorous measures of public hygiene and public sanatoria, but France let private physicians handle the problem, which left it with a much higher death rate. The French medical profession jealously guarded its prerogatives, and public health activists were not as well organized or as influential as in Germany, Britain or the United States. For example, there was a long battle over a public health law which began in the 1880s as a campaign to reorganize the nation's health services, to require the registration of infectious diseases, to mandate quarantines, and to improve the deficient health and housing legislation of 1850. However the reformers met opposition from bureaucrats, politicians, and physicians. Because it was so threatening to so many interests, the proposal was debated and postponed for 20 years before becoming law in 1902. Success finally came when the government realized that contagious diseases had a national security impact in weakening military recruits, and keeping the population growth rate well below Germany's.\n\nModern public health began developing in the 19th century, as a response to advances in science that led to the understanding of, the source and spread of disease. As the knowledge of contagious diseases increased, means to control them and prevent infection were soon developed. Once it became understood that these strategies would require community-wide participation, disease control began being viewed as a public responsibility. Various organizations and agencies were then created to implement these disease preventing strategies.\n\nMost of the Public health activity in the United States took place at the municipal level before the mid-20th century. There was some activity at the national and state level as well.\n\nIn the administration of the second president of the United States John Adams, the Congress authorized the creation of hospitals for mariners. As the U.S. expanded, the scope of the governmental health agency expanded.\n\nIn the United States, public health worker Sara Josephine Baker, M.D. established many programs to help the poor in New York City keep their infants healthy, leading teams of nurses into the crowded neighborhoods of Hell's Kitchen and teaching mothers how to dress, feed, and bathe their babies.\n\nAnother key pioneer of public health in the U.S. was Lillian Wald, who founded the Henry Street Settlement house in New York. The Visiting Nurse Service of New York was a significant organization for bringing health care to the urban poor.\n\nDramatic increases in average life span in the late 19th century and 20th century, is widely credited to public health achievements, such as vaccination programs and control of many infectious diseases including polio, diphtheria, yellow fever and smallpox; effective health and safety policies such as road traffic safety and occupational safety; improved family planning; tobacco control measures; and programs designed to decrease non-communicable diseases by acting on known risk factors such as a person's background, lifestyle and environment.\n\nAnother major public health improvement was the decline in the \"urban penalty\" brought about by improvements in sanitation. These improvements included chlorination of drinking water, filtration and sewage treatment which led to the decline in deaths caused by infectious waterborne diseases such as cholera and intestinal diseases.\nThe federal Office of Indian Affairs (OIA) operated a large-scale field nursing program. Field nurses targeted native women for health education, emphasizing personal hygiene and infant care and nutrition.\n\nPublic health issues were important for the Spanish empire during the colonial era. Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.\n\nIn the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view. As in the U.S., food safety became a public health issue, particularly focusing on meat slaughterhouses and meatpacking. Even during the Mexican Revolution (1910–20), public health was an important concern, with a text on hygiene published in 1916. During the Mexican Revolution, feminist and trained nurse Elena Arizmendi Mejia founded the Neutral White Cross, treating wounded soldiers no matter for what faction they fought.\n\nIn the post-revolutionary period after 1920, improved public health was a revolutionary goal of the Mexican government.\nThe Mexican state promoted the health of the Mexican population, with most resources going to cities. Concern about disease conditions and social impediments to the improvement of Mexicans' health were important in the formation of the Mexican Society for Eugenics. The movement flourished from the 1920s to the 1940s. Mexico was not alone in Latin America or the world in promoting eugenics. Government campaigns against disease and alcoholism were also seen as promoting public health.\n\nThe Mexican Social Security Institute was established in 1943, during the administration of President Manuel Avila Camacho to deal with public health, pensions, and social security.\n\nSince the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted. Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.\n\nPublic health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state. In Colombia, public health was a means for creating and implementing ideas of citizenship. In Bolivia, a similar push came after their 1952 revolution.\n\nThough curable and preventive, malaria remains a huge public health problem and is the third leading cause of death in Ghana. In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria. These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets. Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, household and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.\n\n"}
{"id": "4599275", "url": "https://en.wikipedia.org/wiki?curid=4599275", "title": "Human overpopulation", "text": "Human overpopulation\n\nHuman overpopulation (or population overshoot) is when there are too many people for the environment to sustain (with food, drinkable water, breathable air, etc.). In more scientific terms, there is overshoot when the ecological footprint of a human population in a geographical area exceeds that place's carrying capacity, damaging the environment faster than nature can repair it, potentially leading to an ecological and societal collapse. Overpopulation could apply to the population of a specific region, or to world population as a whole.\n\nOverpopulation can result from an increase in births, a decline in mortality rates, an increase in immigration, or an unsustainable biome and depletion of resources. It is possible for very sparsely populated areas to be overpopulated if the area has a meager or non-existent capability to sustain life (e.g. a desert).\n\nAdvocates of population moderation cite issues like exceeding the Earth's carrying capacity, global warming, potential or imminent ecological collapse, impact on quality of life, and risk of mass starvation or even extinction as a basis to argue for population decline.\n\nA more controversial definition of overpopulation, as advocated by Paul Ehrlich, is a situation where a population is in the process of depleting non-renewable resources. Under this definition, changes in lifestyle could cause an overpopulated area to no longer be overpopulated without any reduction in population, or vice versa.\n\nScientists suggest that the overall human impact on the environment, due to overpopulation, overconsumption, pollution, and proliferation of technology, has pushed the planet into a new geological epoch known as the Anthropocene.\n\nAs of 07, 2020 the world's human population is estimated to be  billion. Or, 7,622,106,064 on May 14, 2018 and the United States Census Bureau calculates 7,472,985,269 for that same date and over 7 billion by the United Nations. Most contemporary estimates for the carrying capacity of the Earth under existing conditions are between 4 billion and 16 billion . Depending on which estimate is used, human overpopulation may have already occurred.\n\nNevertheless, the rapid recent increase in human population has created concern. The population is expected to reach between 8 and 10.5 billion between the years 2040 and 2050. In 2017, the United Nations increased the medium variant projections to 9.8 billion for 2050 and 11.2 billion for 2100.\nAs pointed out by Hans Rosling, the critical factor is that the population is not \"just growing\", but that the growth ratio reached its peak and the total population is now growing much slower. The UN population forecast of 2017 was predicting \"near end of high fertility\" globally and anticipating that by 2030 over ⅔ of world population will be living in countries with fertility below the replacement level and for total world population to stabilize between 10-12 billion people by year 2100.\nThe rapid increase in world population over the past three centuries has raised concerns that the planet may not be able to sustain the future or even present number of its inhabitants. The InterAcademy Panel Statement on Population Growth, circa 1994, stated that many environmental problems, such as rising levels of atmospheric carbon dioxide, global warming, and pollution, are aggravated by the population expansion.\nOther problems associated with overpopulation include the increased demand for resources such as fresh water and food, starvation and malnutrition, consumption of natural resources (such as fossil fuels) faster than the rate of regeneration, and a deterioration in living conditions. Wealthy but highly populated territories like Britain rely on food imports from overseas. This was severely felt during the World Wars when, despite food efficiency initiatives like \"dig for victory\" and food rationing, Britain needed to fight to secure import routes. However, many believe that waste and over-consumption, especially by wealthy nations, is putting more strain on the environment than overpopulation itself.\n\nWorld population has been rising continuously since the end of the Black Death, around the year 1350, although the most significant increase has been since the 1950s, mainly due to medical advancements and increases in agricultural productivity.\n\nDue to its dramatic impact on the human ability to grow food, the Haber process served as the \"detonator of the population explosion\", enabling the global population to increase from 1.6 billion in 1900 to 7.7 billion by November 2018.\n\nThe rate of population growth has been declining since the 1980s, while the absolute total numbers are still increasing. Recent rate increases in several countries previously enjoying steady declines have apparently been contributing to continued growth in total numbers. The United Nations has expressed concerns on continued population growth in sub-Saharan Africa. Recent research has demonstrated that those concerns are well grounded.\n\nConcern about overpopulation is an ancient topic. Tertullian was a resident of the city of Carthage in the second century CE, when the population of the world was about 190 million (only 3–4% of what it is today). He notably said: \"What most frequently meets our view (and occasions complaint) is our teeming population. Our numbers are burdensome to the world, which can hardly support us... In very deed, pestilence, and famine, and wars, and earthquakes have to be regarded as a remedy for nations, as the means of pruning the luxuriance of the human race.\" Before that, Plato, Aristotle and others broached the topic as well.\n\nThroughout recorded history, population growth has usually been slow despite high birth rates, due to war, plagues and other diseases, and high infant mortality. During the 750 years before the Industrial Revolution, the world's population increased very slowly, remaining under 250 million.\n\nBy the beginning of the 19th century, the world population had grown to a billion individuals, and intellectuals such as Thomas Malthus predicted that humankind would outgrow its available resources, because a finite amount of land would be incapable of supporting a population with a limitless potential for increase. Mercantillists argued that a large population was a form of wealth, which made it possible to create bigger markets and armies.\n\nDuring the 19th century, Malthus's work was often interpreted in a way that blamed the poor alone for their condition and helping them was said to worsen conditions in the long run. This resulted, for example, in the English poor laws of 1834 and in a hesitating response to the Irish Great Famine of 1845–52.\n\nThe UN publication 'World population prospects' (2017) projects that the world population will reach 9.8 billion in 2050 and 11.2 billion in 2100. Human population is predicted to stabilise soon thereafter.\n\nA 2014 study published in \"Science\" asserts that population growth will continue into the next century. Adrian Raftery, a University of Washington professor of statistics and sociology and one of the contributors to the study, says: \"The consensus over the past 20 years or so was that world population, which is currently around 7 billion, would go up to 9 billion and level off or probably decline. We found there's a 70 percent probability the world population will not stabilize this century. Population, which had sort of fallen off the world's agenda, remains a very important issue.\" UN projections from 2011 suggest the population could grow to as many as 15 billion by 2100.\n\nIn 2017, more than a third of 50 Nobel prize-winning scientists surveyed by the \"Times Higher Education\" at the Lindau Nobel Laureate Meetings said that human overpopulation and environmental degradation are the two greatest threats facing humankind. In November that same year, a statement by 15,364 scientists from 184 countries indicated that rapid human population growth is the \"primary driver behind many ecological and even societal threats.\"\n\nIn spite of concerns about overpopulation, widespread in developed countries, the number of people living in extreme poverty globally shows a stable decline (this has been disputed by some experts), even though the population has grown seven-fold over the last 200 years. Child mortality has declined, which in turn has led to reduced birth rates, thus slowing overall population growth. The global number of famine-related deaths have declined, and food supply per person has increased with population growth.\n\nIn 2019, a warning on climate change signed by 11,000 scientists from 153 nations said that human population growth adds 80 million humans annually, and \"the world population must be stabilized—and, ideally, gradually reduced—within a framework that ensures social integrity\" to reduce the impact of \"population growth on GHG emissions and biodiversity loss.\"\n\nWorld population has gone through a number of periods of growth since the dawn of civilization in the Holocene period, around 10,000 BCE. The beginning of civilization roughly coincides with the receding of glacial ice following the end of the last glacial period.\nIt is estimated that between 1–5 million people, subsisting on hunting and foraging, inhabited the Earth in the period before the Neolithic Revolution, when human activity shifted away from hunter-gathering and towards very primitive farming.\n\nAround 8000 BCE, at the dawn of agriculture, the human population of the world was approximately 5 million. The next several millennia saw a steady increase in the population, with very rapid growth beginning in 1000 BCE, and a peak of between 200 and 300 million people in 1 BCE.\n\nThe Plague of Justinian caused Europe's population to drop by around 50% between 541 and the 8th century. Steady growth resumed in 800 CE. However, growth was again disrupted by frequent plagues; most notably, the Black Death during the 14th century. The effects of the Black Death are thought to have reduced the world's population, then at an estimated 450 million, to between 350 and 375 million by 1400. The population of Europe stood at over 70 million in 1340; these levels did not return until 200 years later. England's population reached an estimated 5.6 million in 1650, up from an estimated 2.6 million in 1500. New crops from the Americas via the Spanish colonizers in the 16th century contributed to the population growth.\n\nIn other parts of the globe, China's population at the founding of the Ming dynasty in 1368 stood close to 60 million, approaching 150 million by the end of the dynasty in 1644. The population of the Americas in 1500 may have been between 50 and 100 million.\n\nEncounters between European explorers and populations in the rest of the world often introduced local epidemics of extraordinary virulence. Archaeological evidence indicates that the death of around 90% of the Native American population of the New World was caused by Old World diseases such as smallpox, measles, and influenza. Europeans introduced diseases alien to the indigenous people, therefore they did not have immunity to these foreign diseases.\n\nAfter the start of the Industrial Revolution, during the 18th century, the rate of population growth began to increase. By the end of the century, the world's population was estimated at just under 1 billion. At the turn of the 20th century, the world's population was roughly 1.6 billion. By 1940, this figure had increased to 2.3 billion. Each subsequent addition of a billion humans took less and less time: 33 years to reach three billion in 1960, 14 years for four billion in 1974, 13 years for five billion in 1987, and 12 years for six billion in 1999.\n\nDramatic growth beginning in 1950 (above 1.8% per year) coincided with greatly increased food production as a result of the industrialization of agriculture brought about by the Green Revolution. The rate of human population growth peaked in 1964, at about 2.1% per year. For example, Indonesia's population grew from 97 million in 1961 to 237.6 million in 2010, a 145% increase in 49 years. In India, the population grew from 361.1 million people in 1951 to just over 1.2 billion by 2011, a 235% increase in 60 years.\n\nThere is concern over the sharp population increase in many countries, especially in Sub-Saharan Africa, that has occurred over the last several decades, and that it is creating problems with land management, natural resources and access to water supplies.\n\nThe population of Chad has, for example, grown from 6,279,921 in 1993 to 10,329,208 in 2009. Niger, Uganda, Nigeria, Tanzania, Ethiopia and the DRC are witnessing a similar growth in population. The situation is most acute in western, central and eastern Africa. Refugees from places like Sudan have further strained the resources of neighboring states like Chad and Egypt. Chad is also host to roughly 255,000 refugees from Sudan's Darfur region, and about 77,000 refugees from the Central African Republic, while approximately 188,000 Chadians have been displaced by their own civil war and famines, have either fled to either the Sudan, the Niger or, more recently, Libya.\n\nAccording to UN data, there are on average 250 babies born each minute, or more than 130 million a year. UN data from 2019 shows that the human population grows by 100 million every 14 months.\n\nFrom a historical perspective, technological revolutions have coincided with population expansion. There have been three major technological revolutions – the tool-making revolution, the agricultural revolution, and the industrial revolution – all of which allowed humans more access to food, resulting in subsequent population explosions. For example, the use of tools, such as bow and arrow, allowed primitive hunters greater access to more high energy foods (e.g. animal meat). Similarly, the transition to farming about 10,000 years ago greatly increased the overall food supply, which was used to support more people. Food production further increased with the industrial revolution as machinery, fertilizers, herbicides, and pesticides were used to increase land under cultivation as well as crop yields. Today, starvation is caused by economic and political forces rather than a lack of the means to produce food.\n\nSignificant increases in human population occur whenever the birth rate exceeds the death rate for extended periods of time. Traditionally, the fertility rate is strongly influenced by cultural and social norms that are rather stable and therefore slow to adapt to changes in the social, technological, or environmental conditions. For example, when death rates fell during the 19th and 20th century – as a result of improved sanitation, child immunizations, and other advances in medicine – allowing more newborns to survive, the fertility rate did not adjust downward, resulting in significant population growth. Until the 1700s, seven out of ten children died before reaching reproductive age. Today, more than nine out of ten children born in industrialized nations reach adulthood.\n\nThere is a strong correlation between overpopulation and poverty. In contrast, the invention of the birth control pill and other modern methods of contraception resulted in a dramatic decline in the number of children per household in all but the very poorest countries.\n\nAgriculture has sustained human population growth and has been the main driving factor behind it. With more food supply, the population grows with it. This occurs most in regions which are fertile and capable of higher food production in contrast to barren regions unable to support crop production on larger or any scales at all. This dates back to prehistoric times, when agricultural methods were first developed, and continues to the present day, with fertilizers, agrochemicals, large-scale mechanization, genetic manipulation, and other technologies.\n\nHumans have historically exploited the environment using the easiest, most accessible resources first. The richest farmland was plowed and the richest mineral ore mined first. Anne Ehrlich, Gerardo Ceballos, and Paul Ehrlich note that overpopulation is demanding the use of ever more creative, expensive and/or environmentally destructive means in order to exploit ever more difficult to access and/or poorer quality natural resources to satisfy consumers.\n\nThinkers from a wide range of academic fields and political backgrounds—including agricultural scientist David Pimentel, behavioral scientist Russell Hopfenberg, right-wing anthropologist Virginia Abernethy, ecologist Garrett Hardin, ecologist and anthropologist Peter Farb, journalist Richard Manning, environmental biologist Alan D. Thornhill, cultural critic and writer Daniel Quinn, and anarcho-primitivist John Zerzan,—propose that, like all other animal populations, human populations predictably grow and shrink according to their available food supply, growing during an abundance of food and shrinking in times of scarcity.\n\nProponents of this theory argue that every time food production is increased, the population grows. Most human populations throughout history validate this theory, as does the overall current global population. Populations of hunter-gatherers fluctuate in accordance with the amount of available food. The world human population began increasing after the Neolithic Revolution and its increased food supply. This was, subsequent to the Green Revolution, followed by even more severely accelerated population growth, which continues today. Often, wealthier countries send their surplus food resources to the aid of starving communities; however, proponents of this theory argue that this seemingly beneficial notion only results in further harm to those communities in the long run. Peter Farb, for example, has commented on the paradox that \"intensification of production to feed an increased population leads to a still greater increase in population.\" Daniel Quinn has also focused on this phenomenon, which he calls the \"Food Race\" (comparable, in terms of both escalation and potential catastrophe, to the nuclear arms race).\n\nCritics of this theory point out that, in the modern era, birth rates are lowest in the developed nations, which also have the highest access to food. In fact, some developed countries have both a diminishing population and an abundant food supply. The United Nations projects that the population of 51 countries or areas, including Germany, Italy, Japan, and most of the states of the former Soviet Union, is expected to be lower in 2050 than in 2005. This shows that, limited to the scope of the population living within a single given political boundary, particular human populations do not always grow to match the available food supply. However, the global population as a whole still grows in accordance with the total food supply and many of these wealthier countries are major \"exporters\" of food to poorer populations, so that, \"it is through exports from food-rich to food-poor areas (Allaby, 1984; Pimentel et al., 1999) that the population growth in these food-poor areas is further fueled.\"\n\nRegardless of criticisms against the theory that population is a function of food availability, the human population is, on the global scale, undeniably increasing, as is the net quantity of human food produced—a pattern that has been true for roughly 10,000 years, since the human development of agriculture. The fact that some affluent countries demonstrate negative population growth fails to discredit the theory as a whole, since the world has become a globalized system with food moving across national borders from areas of abundance to areas of scarcity. Hopfenberg and Pimentel's findings support both this and Quinn's direct accusation that \"First World farmers are fueling the Third World population explosion.\"\n\nMany of the problems associated with overpopulation are explored in the dystopic science fiction film \"Soylent Green\", where an overpopulated Earth suffers from food shortages, depleted resources and poverty and in the documentary \"Aftermath: Population Overload\".\n\nDavid Attenborough described the level of human population on the planet as a multiplier of all other environmental problems. In 2013, he described humanity as \"a plague on the Earth\" that needs to be controlled by limiting population growth.\n\nMost biologists and sociologists see overpopulation as a serious threat to the quality of human life. Some deep ecologists, such as the radical thinker and polemicist Pentti Linkola, see human overpopulation as a threat to the entire biosphere.\n\nThe effects of overpopulation are compounded by overconsumption. According to Paul R. Ehrlich:\n\nRich western countries are now siphoning up the planet's resources and destroying its ecosystems at an unprecedented rate. We want to build highways across the Serengeti to get more rare earth minerals for our cellphones. We grab all the fish from the sea, wreck the coral reefs and put carbon dioxide into the atmosphere. We have triggered a major extinction event ... A world population of around a billion would have an overall pro-life effect. This could be supported for many millennia and sustain many more human lives in the long term compared with our current uncontrolled growth and prospect of sudden collapse ... If everyone consumed resources at the US level – which is what the world aspires to – you will need another four or five Earths. We are wrecking our planet's life support systems.\n\nSome economists, such as Thomas Sowell and Walter E. Williams argue that third world poverty and famine are caused in part by bad government and bad economic policies.\n\nSome problems associated with or exacerbated by human overpopulation and over-consumption are presented in the sections below...\n\nThe United Nations indicates that about 850 million people are malnourished or starving, and 1.1 billion people do not have access to safe drinking water. Since 1980, the global economy has grown by 380 percent, but the number of people living on less than 5 US dollars a day increased by more than 1.1 billion.\nThe UN Human Development Report of 1997 states: \"During the last 15–20 years, more than 100 developing countries, and several Eastern European countries, have suffered from disastrous growth failures. The reductions in standard of living have been deeper and more long-lasting than what was seen in the industrialised countries during the depression in the 1930s. As a result, the income for more than one billion people has fallen below the level that was reached 10, 20 or 30 years ago\". Similarly, although the proportion of \"starving\" people in sub-Saharan Africa has decreased, the absolute number of starving people has increased due to population growth. The percentage dropped from 38% in 1970 to 33% in 1996 and was expected to be 30% by 2010. But the region's population roughly doubled between 1970 and 1996. To keep the numbers of starving constant, the percentage would have dropped by more than half.\n\nAs of 2004, there were 108 countries in the world with more than five million people. All of these in which women have, on the average, more than 4 children in their lifetime, have a per capita GDP of less than $5,000. Only in two countries with per capita GDP above ~$15,000 do women have, on the average, more than 2 children in their lifetime: these are Israel and Saudi Arabia, with average lifetime births per woman between 2 and 4.\n\nHigh rates of infant mortality are associated with poverty. Rich countries with high population densities have low rates of infant mortality. However, both global poverty and infant mortality has declined over the last 200 years of population growth.\n\nOverpopulation has substantially adversely impacted the environment of Earth starting at least as early as the 20th century. According to the Global Footprint Network, \"today humanity uses the equivalent of 1.5 planets to provide the resources we use and absorb our waste\". There are also economic consequences of this environmental degradation in the form of ecosystem services attrition. Beyond the scientifically verifiable harm to the environment, some assert the moral right of other species to simply exist rather than become extinct. Environmental author Jeremy Rifkin has said that \"our burgeoning population and urban way of life have been purchased at the expense of vast ecosystems and habitats. ... It's no accident that as we celebrate the urbanization of the world, we are quickly approaching another historic watershed: the disappearance of the wild.\"\n\nSays Peter Raven, former President of the American Association for the Advancement of Science (AAAS) in their seminal work AAAS Atlas of Population & Environment, \"Where do we stand in our efforts to achieve a sustainable world? Clearly, the past half century has been a traumatic one, as the collective impact of human numbers, affluence (consumption per individual) and our choices of technology continue to exploit rapidly an increasing proportion of the world's resources at an unsustainable rate. ... During a remarkably short period of time, we have lost a quarter of the world's topsoil and a fifth of its agricultural land, altered the composition of the atmosphere profoundly, and destroyed a major proportion of our forests and other natural habitats without replacing them. Worst of all, we have driven the rate of biological extinction, the permanent loss of species, up several hundred times beyond its historical levels, and are threatened with the loss of a majority of all species by the end of the 21st century.\"\n\nFurther, even in countries which have both large population growth and major ecological problems, it is not necessarily true that curbing the population growth will make a major contribution towards resolving all environmental problems. However, as developing countries with high populations become more industrialized, pollution and consumption will invariably increase.\n\nThe Worldwatch Institute said in 2006 that the booming economies of China and India are \"planetary powers that are shaping the global biosphere\". The report states:\n\nAccording to Worldwatch Institute, if China and India were to consume as much resources per capita as the United States, in 2030 they would each require a full planet Earth to meet their needs. In the long term these effects can lead to increased conflict over dwindling resources and in the worst case a Malthusian catastrophe.\n\nMany studies link population growth with emissions and the effect of climate change. The global consumption of meat is projected to rise by as much as 76% by 2050 as the global population surges to more than 9 billion, resulting in further biodiversity loss and increased GHG emissions.\n\nHuman overpopulation, continued population growth, and overconsumption are the primary drivers of biodiversity loss and the 6th (and ongoing) mass species extinction. Present extinction rates may be as high as 140,000 species lost per year due to human activity, such as slash-and-burn techniques that sometimes are practiced by shifting cultivators, especially in countries with rapidly expanding rural populations, which have reduced habitat in tropical forests. As of February 2011, the IUCN Red List lists a total of 801 animal species having gone extinct during recorded human history, although the vast majority of extinctions are thought to be undocumented. Biodiversity would continue to grow at an exponential rate if not for human influence. Sir David King, former chief scientific adviser to the UK government, told a parliamentary inquiry: \"It is self-evident that the massive growth in the human population through the 20th century has had more impact on biodiversity than any other single factor.\" Paul and Anne Ehrlich said population growth is one of the main drivers of the Earth's extinction crisis. The \"Global Assessment Report on Biodiversity and Ecosystem Services\", released by IPBES in 2019, says that human population growth is a significant factor in biodiversity loss. The report asserts that expanding human land use for agriculture and overfishing are the main causes of this decline.\n\nBoth animal and plant species are affected, by habitat destruction and habitat fragmentation.\n\nPopulation growth increases levels of air pollution, water pollution, soil contamination and noise pollution.\n\nAir pollution is causing changes in atmospheric composition and consequent global warming and ocean acidification.\n\nEcological collapse refers to a situation where an ecosystem suffers a drastic, possibly permanent, reduction in carrying capacity for all organisms, often resulting in mass extinction. Usually, an ecological collapse is precipitated by a disastrous event occurring on a short time scale. Ecological collapse can be considered as a consequence of ecosystem collapse on the biotic elements that depended on the original ecosystem.\n\nThe ocean is in great danger of collapse. In a study of 154 different marine fish species, David Byler came to the conclusion that many factors such as overfishing, climate change, and fast growth of fish populations will cause ecosystem collapse. When humans fish, they usually will fish the populations of the higher trophic levels such as salmon and tuna. The depletion of these trophic levels allow the lower trophic level to overpopulate, or populate very rapidly. For example, when the population of catfish is depleting due to overfishing, plankton will then overpopulate because their natural predator is being killed off. This causes an issue called eutrophication. Since the population all consumes oxygen the dissolved oxygen (DO) levels will plummet. The DO levels dropping will cause all the species in that area to have to leave, or they will suffocate. This along with climate change, and ocean acidification can cause the collapse of an ecosystem.\n\nFurther environmental impacts that overpopulation may cause include depletion of natural resources, especially fossil fuels. Overpopulation does not depend only on the size or density of the population, but on the ratio of population to available sustainable resources. It also depends on how resources are managed and distributed throughout the population.\n\nThe resources to be considered when evaluating whether an ecological niche is overpopulated include clean water, clean air, food, shelter, warmth, and other resources necessary to sustain life. If the quality of human life is addressed, there may be additional resources considered, such as medical care, education, proper sewage treatment, waste disposal and energy supplies. Overpopulation places competitive stress on the basic life sustaining resources, leading to a diminished quality of life.\n\nDirectly related to maintaining the health of the human population is water supply, and it is one of the resources that experience the biggest strain. With the global population at about 7.5 billion, and each human theoretically needing 2 liters of drinking water, there is a demand for 15 billion liters of water each day to meet the minimum requirement for healthy living (United). Weather patterns, elevation, and climate all contribute to uneven distribution of fresh drinking water. Without clean water, good health is not a viable option. Besides drinking, water is used to create sanitary living conditions and is the basis of creating a healthy environment fit to hold human life. In addition to drinking water, water is also used for bathing, washing clothes and dishes, flushing toilets, a variety of cleaning methods, recreation, watering lawns, and farm irrigation.\nIrrigation poses one of the largest problems, because without sufficient water to irrigate crops, the crops die and then there is the problem of food rations and starvation. In addition to water needed for crops and food, there is limited land area dedicated to food production, and not much more that is suitable to be added. Arable land, needed to sustain the growing population, is also a factor because land being under or over cultivated easily upsets the delicate balance of nutrition supply. There are also problems with location of arable land with regard to proximity to countries and relative population (Bashford 240). Access to nutrition is an important limiting factor in population sustainability and growth. No increase in arable land added to the still increasing human population will eventually pose a serious conflict. Only 38% of the land area of the globe is dedicated to agriculture, and there is not room for much more. Although plants produce 54 billion metric tons of carbohydrates per year, when the population is expected to grow to 9 billion by 2050, the plants may not be able to keep up (Biello). Food supply is a primary example of how a resource reacts when its carrying capacity is exceeded. By trying to grow more and more crops off of the same amount of land, the soil becomes exhausted. Because the soil is exhausted, it is then unable to produce the same amount of food as before, and is overall less productive. Therefore, by using resources beyond a sustainable level, the resource become nullified and ineffective, which further increases the disparity between the demand for a resource and the availability of a resource. There must be a shift to provide adequate recovery time to each one of the supplies in demand to support contemporary human lifestyles.\nDavid Pimentel has stated that \"With the imbalance growing between population numbers and vital life sustaining resources, humans must actively conserve cropland, freshwater, energy, and biological resources. There is a need to develop renewable energy resources. Humans everywhere must understand that rapid population growth damages the Earth's resources and diminishes human well-being.\"\n\nThese reflect the comments also of the United States Geological Survey in their paper \"The Future of Planet Earth: Scientific Challenges in the Coming Century\": \"As the global population continues to grow...people will place greater and greater demands on the resources of our planet, including mineral and energy resources, open space, water, and plant and animal resources.\" \"Earth's natural wealth: an audit\" by \"New Scientist\" magazine states that many of the minerals that we use for a variety of products are in danger of running out in the near future. A handful of geologists around the world have calculated the costs of new technologies in terms of the materials they use and the implications of their spreading to the developing world. All agree that the planet's booming population and rising standards of living are set to put unprecedented demands on the materials that only Earth itself can provide. Limitations on how much of these materials is available could even mean that some technologies are not worth pursuing long term... \"Virgin stocks of several metals appear inadequate to sustain the modern 'developed world' quality of life for all of Earth's people under contemporary technology\".\n\nOn the other hand, some cornucopian researchers, such as Julian L. Simon and Bjørn Lomborg believe that resources exist for further population growth. In a 2010 study, they concluded that \"there are not (and will never be) too many people for the planet to feed\" according to The Independent. Some critics warn, this will be at a high cost to the Earth: \"the technological optimists are probably correct in claiming that overall world food production can be increased substantially over the next few decades...[however] the environmental cost of what Paul R. and Anne H. Ehrlich describe as 'turning the Earth into a giant human feedlot' could be severe. A large expansion of agriculture to provide growing populations with improved diets is likely to lead to further deforestation, loss of species, soil erosion, and pollution from pesticides and fertilizer runoff as farming intensifies and new land is brought into production.\" Since we are intimately dependent upon the living systems of the Earth, some scientists have questioned the wisdom of further expansion.\n\nAccording to the Millennium Ecosystem Assessment, a four-year research effort by 1,360 of the world's prominent scientists commissioned to measure the actual value of natural resources to humans and the world, \"The structure of the world's ecosystems changed more rapidly in the second half of the twentieth century than at any time in recorded human history, and virtually all of Earth's ecosystems have now been significantly transformed through human actions.\" \"Ecosystem services, particularly food production, timber and fisheries, are important for employment and economic activity. Intensive use of ecosystems often produces the greatest short-term advantage, but excessive and unsustainable use can lead to losses in the long term. A country could cut its forests and deplete its fisheries, and this would show only as a positive gain to GDP, despite the loss of capital assets. If the full economic value of ecosystems were taken into account in decision-making, their degradation could be significantly slowed down or even reversed.\"\n\nAnother study was done by the United Nations Environment Programme (UNEP) called the Global Environment Outlook.\n\nAlthough all resources, whether mineral or other, are limited on the planet, there is a degree of self-correction whenever a scarcity or high-demand for a particular kind is experienced. For example, in 1990 known reserves of many natural resources were higher, and their prices lower, than in 1970, despite higher demand and higher consumption. Whenever a price spike would occur, the market tended to correct itself whether by substituting an equivalent resource or switching to a new technology.\n\nOverpopulation may lead to inadequate fresh water for drinking as well as sewage treatment and effluent discharge. Some countries, like Saudi Arabia, use energy-expensive desalination to solve the problem of water shortages.\nFresh water supplies, on which agriculture depends, are running low worldwide. This water crisis is only expected to worsen as the population increases.\n\nPotential problems with dependence on desalination are reviewed below, however, the majority of the world's freshwater supply is contained in the polar icecaps, and underground river systems accessible through springs and wells.\n\nFresh water can be obtained from salt water by desalination. For example, Malta derives two thirds of its freshwater by desalination. A number of nuclear powered desalination plants exist; however, the high costs of desalination, especially for poor countries, makes the transport of large amounts of desalinated seawater to interiors of large countries impractical. The cost of desalination varies; Israel is now desalinating water for a cost of 53 cents per cubic meter, Singapore at 49 cents per cubic meter. In the United States, the cost is 81 cents per cubic meter ($3.06 for 1,000 gallons).\n\nAccording to a 2004 study by Zhou and Tol, \"one needs to lift the water by 2000 m, or transport it over more than 1600 km to get transport costs equal to the desalination costs. Desalinated water is expensive in places that are both somewhat far from the sea and somewhat high, such as Riyadh and Harare. In other places, the dominant cost is desalination, not transport. This leads to somewhat lower costs in places like Beijing, Bangkok, Zaragoza, Phoenix, and, of course, coastal cities like Tripoli.\" Thus while the study is generally positive about the technology for affluent areas that are proximate to oceans, it concludes that \"Desalinated water may be a solution for some water-stress regions, but not for places that are poor, deep in the interior of a continent, or at high elevation. Unfortunately, that includes some of the places with biggest water problems.\" \"Another potential problem with desalination is the byproduction of saline brine, which can be a major cause of marine pollution when dumped back into the oceans at high temperatures.\"\n\nThe world's largest desalination plant is the Jebel Ali Desalination Plant (Phase 2) in the United Arab Emirates, which can produce 300 million cubic metres of water per year, or about 2500 gallons per second. The largest desalination plant in the US is the one at Tampa Bay, Florida, which began desalinating 25 million gallons (95000 m) of water per day in December 2007. A 17 January 2008, article in the \"Wall Street Journal\" states, \"Worldwide, 13,080 desalination plants produce more than 12 billion gallons of water a day, according to the International Desalination Association.\" After being desalinated at Jubail, Saudi Arabia, water is pumped inland though a pipeline to the capital city of Riyadh.\n\nHowever, new data originating from the GRACE experiments and isotopic testing done by the IAEA show that the Nubian aquifer—which is under the largest, driest part of the earth's surface, has enough water in it to provide for \"at least several centuries\". In addition to this, new and highly detailed maps of the earth's underground reservoirs will be soon created from these technologies that will further allow proper budgeting of cheap water.\n\nWater deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China or India, if technology is not used. The water tables are falling in scores of countries (including Northern China, the US, and India) owing to widespread overdrafting beyond sustainable yields. Other countries affected include Pakistan, Iran, and Mexico. This overdrafting is already leading to water scarcity and cutbacks in grain harvest. Even with the overpumping of its aquifers, China has developed a grain deficit. This effect has contributed in driving grain prices upward. Most of the 3 billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages. Desalination is also considered a viable and effective solution to the problem of water shortages.\n\nOverpopulation together with water deficits could trigger regional tensions, including warfare.\n\nThe World Resources Institute states that \"Agricultural conversion to croplands and managed pastures has affected some 3.3 billion [hectares] – roughly 26 percent of the land area. All totaled, agriculture has displaced one-third of temperate and tropical forests and one-quarter of natural grasslands.\" Forty percent of the land area is under conversion and fragmented; less than one quarter, primarily in the Arctic and the deserts, remains intact. Usable land may become less useful through salinization, deforestation, desertification, erosion, and urban sprawl. Global warming may cause flooding of many of the most productive agricultural areas. The development of energy sources may also require large areas, for example, the building of hydroelectric dams. Thus, available useful land may become a limiting factor. By most estimates, at least half of cultivable land is already being farmed, and there are concerns that the remaining reserves are greatly overestimated.\n\nHigh crop yield vegetables like potatoes and lettuce use less space on inedible plant parts, like stalks, husks, vines, and inedible leaves. New varieties of selectively bred and hybrid plants have larger edible parts (fruit, vegetable, grain) and smaller inedible parts; however, many of these gains of agricultural technology are now historic, and new advances are more difficult to achieve. With new technologies, it is possible to grow crops on some marginal land under certain conditions. Aquaculture could theoretically increase available area. Hydroponics and food from bacteria and fungi, like quorn, may allow the growing of food without having to consider land quality, climate, or even available sunlight, although such a process may be very energy-intensive. Some argue that not all arable land will remain productive if used for agriculture because some marginal land can only be made to produce food by unsustainable practices like slash-and-burn agriculture. Even with the modern techniques of agriculture, the sustainability of production is in question.\n\nSome countries, such as the United Arab Emirates and particularly the Emirate of Dubai have constructed large artificial islands, or have created large dam and dike systems, like the Netherlands, which reclaim land from the sea to increase their total land area. Some scientists have said that in the future, densely populated cities will use vertical farming to grow food inside skyscrapers. The notion that space is limited has been decried by skeptics, who point out that the Earth's population of roughly 6.8 billion people could comfortably be housed an area comparable in size to the state of Texas, in the United States (about ). However, the impact of humanity extends over a far greater area than that required simply for housing.\n\nPopulation optimists have been criticized for failing to take into account the depletion of fossil fuels required for the production of fertilizers, tillage, transportation, etc. In his 1992 book \"Earth in the Balance\", Al Gore wrote, \"... it ought to be possible to establish a coordinated global program to accomplish the strategic goal of completely eliminating the internal combustion engine over, say, a twenty-five-year period...\" Approximately half of the oil produced in the United States is refined into gasoline for use in internal combustion engines.\n\nThe report \"Peaking of World Oil Production: Impacts, Mitigation, and Risk Management\", commonly referred to as the Hirsch report, was created by request for the US Department of Energy and published in February 2005.\nSome information was updated in 2007.\nIt examined the time frame for the occurrence of peak oil, the necessary mitigating actions, and the likely impacts based on the timeliness of those actions. It concludes that world oil peaking is going to happen, and will likely be abrupt. Initiating a mitigation crash program 20 years before peaking appears to offer the possibility of avoiding a world liquid fuels shortfall for the forecast period.\n\nOptimists counter that fossil fuels will be sufficient until the development and implementation of suitable replacement technologies—such as nuclear power or various sources of renewable energy—occurs. Methods of manufacturing fertilizers from garbage, sewage, and agricultural waste by using thermal depolymerization have been discovered.\n\nWith increasing awareness about global warming, the question of peak oil has become less relevant. According to many studies, about 80% of the remaining fossil fuels must be left untouched because the bottleneck has shifted from resource availability to the resource of absorbing the generated greenhouse gases when burning fossil fuels.\n\nSome scientists argue that there is enough food to support the world population, and some dispute this, particularly if sustainability is taken into account.\n\nMany countries rely heavily on imports. Egypt and Iran rely on imports for 40% of their grain supply. Yemen and Israel import more than 90%. And just 6 countries – Argentina, Australia, Canada, France, Thailand and the USA – supply 90% of grain exports. In recent decades the US alone supplied almost half of world grain exports.\n\nA 2001 United Nations report says population growth is \"the main force driving increases in agricultural demand\" but \"most recent expert assessments are cautiously optimistic about the ability of global food production to keep up with demand for the foreseeable future (that is to say, until approximately 2030 or 2050)\", assuming declining population growth rates.\n\nHowever, the observed figures for 2016 show an actual increase in absolute numbers of undernourished people in the world, 815 million in 2016 versus 777 million in 2015. The FAO estimates that these numbers are still far lower than the nearly 900 million registered in 2000.\n\nThe amounts of natural resources in this context are not necessarily fixed, and their distribution is not necessarily a zero-sum game. For example, due to the Green Revolution and the fact that more and more land is appropriated each year from wild lands for agricultural purposes, the worldwide production of food had steadily increased up until 1995. World food production per person was considerably higher in 2005 than 1961.\n\nAs world population doubled from 3 billion to 6 billion, daily calorie consumption in poor countries increased from 1,932 to 2,650, and the percentage of people in those countries who were malnourished fell from 45% to 18%. This suggests that Third World poverty and famine are caused by underdevelopment, not overpopulation. However, others question these statistics. From 1950 to 1984, as the Green Revolution transformed agriculture around the world, grain production increased by over 250%. The world population has grown by about four billion since the beginning of the Green Revolution and most believe that, without the Revolution, there would be greater famine and malnutrition than the UN presently documents.\n\nThe number of people who are overweight has surpassed the number who are undernourished. In a 2006 news story, MSNBC reported, \"There are an estimated 800 million undernourished people and more than a billion considered overweight worldwide.\" The U.S. has one of the highest rates of obesity in the world.\nHowever, studies show that wealthy and educated people are far likelier to eat healthy food, indicating obesity is a disease related to poverty and lack of education and excessive advertising of unhealthy eatables at cheaper cost, high in calories, with little nutritive value are consumed.\nThe Food and Agriculture Organization of the United Nations states in its report \"The State of Food Insecurity in the World 2018\" that the new data indicates an increase of hunger in the world, reversing the recent trend. It is estimated that in 2017 the number of undernourished people increased to 821 million, around 11 per cent of the world population. The FAO states: \"Evidence shows that, for many countries, recent increases in hunger are associated with extreme climate events, especially where there is both high exposure to climate extremes and high vulnerability related to agriculture and livelihood systems.\"\n\nAs of 2008, the price of grain has increased due to more farming used in biofuels, world oil prices at over $100 a barrel, global population growth, climate change, loss of agricultural land to residential and industrial development, and growing consumer demand in China and India Food riots have recently taken place in many countries across the world. An epidemic of stem rust on wheat caused by race Ug99 is currently spreading across Africa and into Asia and is causing major concern. A virulent wheat disease could destroy most of the world's main wheat crops, leaving millions to starve. The fungus has spread from Africa to Iran, and may already be in Afghanistan and Pakistan.\n\nFood security will become more difficult to achieve as resources run out. Resources in danger of becoming depleted include oil, phosphorus, grain, fish, and water. The British scientist John Beddington predicted in 2009 that supplies of energy, food, and water will need to be increased by 50% to reach demand levels of 2030. According to the Food and Agriculture Organization (FAO), food supplies will need to be increased by 70% by 2050 to meet projected demands.\n\nThe \"Population Reference Bureau\" in the US reported that the population of Sub-Saharan Africa – the poorest region in the continent – is rising faster than most of the rest of the world, and that \"Rapid population growth makes it difficult for economies to create enough jobs to lift large numbers of people out of poverty.\" Seven of the 10 countries in Sub-Saharan Africa with the highest fertility rates also appear among the bottom 10 listed on the United Nations' Human Development Index.\n\nHunger and malnutrition kill nearly 6 million children a year, and more people are malnourished in sub-Saharan Africa this decade than in the 1990s, according to a report released by the Food and Agriculture Organization. In sub-Saharan Africa, the number of malnourished people grew to 203.5 million people in 2000–02 from 170.4 million 10 years earlier says \"The State of Food Insecurity in the World\" report. In 2001, 46.4% of people in sub-Saharan Africa were living in extreme poverty.\n\nAccording to a 2004 article from the BBC, China, the world's most populous country, suffers from an \"obesity surge\". The article stated that, \"Altogether, around 200 million people are thought to be overweight, 22.8% of the population, and 60 million (7.1%) obese\". More recent data indicate China's grain production peaked in the mid-1990s, due to increased extraction of groundwater in the North China Plain.\n\nJapan may face a food crisis that could reduce daily diets to the austere meals of the 1950s, believes a senior government adviser.\n\nOverpopulation causes crowding, and conflict over scarce resources, which in turn lead to increased levels of warfare.\nIt has been suggested that overpopulation leads to increased levels of tensions both between and within countries. Modern usage of the term \"lebensraum\" supports the idea that overpopulation may promote warfare through fear of resource scarcity and increasing numbers of youth lacking the opportunity to engage in peaceful employment (the youth bulge theory).\n\nThe hypothesis that population pressure causes increased warfare has been recently criticized on statistical grounds. Two studies focusing on specific historical societies and analyses of cross-cultural data have failed to find positive correlation between population density and incidence of warfare. Andrey Korotayev, in collaboration with Peter Turchin, has shown that such negative results do not falsify the population-warfare hypothesis.\n\nPopulation and warfare are dynamical variables, and if their interaction causes sustained oscillations, then we do not in general expect to find strong correlation between the two variables measured at the same time (that is, unlagged). Korotayev and Turchin have explored mathematically what the dynamical patterns of interaction between population and warfare (focusing on internal warfare) might be in both stateless and state societies. Next, they have tested the model predictions in several empirical case studies: early modern England, Han and Tang China, and the Roman Empire. Their empirical results have supported the population-warfare theory: that there is a tendency for population numbers and internal warfare intensity to oscillate with the same period but shifted in phase (with warfare peaks following population peaks).\n\nFurthermore, they have demonstrated that in the agrarian societies the rates of change of the two variables behave precisely as predicted by the theory: population rate of change is negatively affected by warfare intensity, while warfare rate of change is positively affected by population density.\n\n\nThe theory of demographic transition held that, after the standard of living and life expectancy increase, family sizes and birth rates decline. However, as new data has become available, it has been observed that after a certain level of development (HDI equal to 0.86 or higher) the fertility increases again and is often represented as a \"J\" shape. This means that both the worry that the theory generated about aging populations and the complacency it bred regarding the future environmental impact of population growth could need reevaluation.\n\nFactors cited in the old theory included such social factors as later ages of marriage, the growing desire of many women in such settings to seek careers outside child rearing and domestic work, and the decreased need for children in industrialized settings. The latter factor stems from the fact that children perform a great deal of work in small-scale agricultural societies, and work less in industrial ones; it has been cited to explain the decline in birth rates in industrializing regions.\n\nMany countries have high population growth rates but lower total fertility rates because high population growth in the past skewed the age demographic toward a young age, so the population still rises as the more numerous younger generation approaches maturity.\n\"Demographic entrapment\" is a concept developed by Maurice King, Honorary Research Fellow at the University of Leeds, who posits that this phenomenon occurs when a country has a population larger than its carrying capacity, no possibility of migration, and exports too little to be able to import food. This will cause starvation. He claims that for example many sub-Saharan nations are or will become stuck in demographic entrapment, instead of having a demographic transition.\n\nFor the world as a whole, the number of children born per woman decreased from 5.02 to 2.65 between 1950 and 2005. A breakdown by region is as follows:\n\n\nExcluding the theoretical reversal in fertility decrease for high development, the projected world number of children born per woman for 2050 would be around 2.05. Only the Middle East & North Africa (2.09) and Sub-Saharan Africa (2.61) would then have numbers greater than 2.05.\n\nSome groups (for example, the World Wide Fund for Nature and Global Footprint Network) have stated that the carrying capacity for the human population has been exceeded as measured using the Ecological Footprint. In 2006, WWF's \"Living Planet Report\" stated that in order for all humans to live with the current consumption patterns of Europeans, we would be spending three times more than what the planet can renew. Humanity as a whole was using, by 2006, 40 percent more than what Earth can regenerate. However, Roger Martin of Population Matters states the view: \"the poor want to get rich, and I want them to get rich,\" with a later addition, \"of course we have to change consumption habits... but we've also got to stabilise our numbers\". Another study by the World Wildlife Fund in 2014 found that it would take the equivalent of 1.5 Earths of biocapacity to meet humanity's current levels of consumption.\n\nBut critics question the simplifications and statistical methods used in calculating Ecological Footprints. Therefore, Global Footprint Network and its partner organizations have engaged with national governments and international agencies to test the results – reviews have been produced by France, Germany, the European Commission, Switzerland, Luxembourg, Japan and the United Arab Emirates. Some point out that a more refined method of assessing Ecological Footprint is to designate sustainable versus non-sustainable categories of consumption. However, if yield estimates were adjusted for sustainable levels of production, the yield figures would be lower, and hence the overshoot estimated by the Ecological Footprint method even higher.\n\nOther studies give particular attention to resource depletion and increased world affluence.\n\nIn a 1994 study titled \"Food, Land, Population and the U.S. Economy\", David Pimentel and Mario Giampietro estimated the maximum U.S. population for a sustainable economy at 200 million. And in order to achieve a sustainable economy and avert disaster, the United States would have to reduce its population by at least one-third, and world population would have to be reduced by two-thirds.\n\nMany quantitative studies have estimated the world's carrying capacity for humans, that is, a limit to the world population. A meta-analysis of 69 such studies suggests a point estimate of the limit to be 7.7 billion people, while lower and upper meta-bounds for current technology are estimated as 0.65 and 98 billion people, respectively. They conclude: \"recent predictions of stabilized world population levels for 2050 exceed several of our meta-estimates of a world population limit\".\n\nAccording to projections, the world population will continue to grow until at least 2050, with the population reaching 9 billion in 2040, and some predictions putting the population as high as 11 billion in 2050. The median estimate for future growth sees the world population reaching 8.6 billion in 2030, 9.8 billion in 2050 and 11.2 billion by 2100 assuming a continuing decrease in average fertility rate from 2.5 births per woman in 2010–2015 to 2.2 in 2045–2050 and to 2.0 in 2095–2100, according to the medium-variant projection. Walter Greiling projected in the 1950s that world population would reach a peak of about nine billion, in the 21st century, and then stop growing, after a readjustment of the Third World and a sanitation of the tropics.\n\nIn 2000, the United Nations estimated that the world's population was growing at the rate of 1.14% (or about 75 million people) per year and according to data from the CIA's World Factbook, the world human population currently increases by 145 every minute.\n\nAccording to the United Nations' World Population Prospects report:\n\nIn 1800 only 3% of the world's population lived in cities. By the 20th century's close, 47% did so. In 1950 there were 83 cities with populations exceeding one million; but by 2007 this had risen to 468 \"agglomerations\". If the trend continues, the world's urban population will double every 38 years. In 2007 UN forecasted that urban population would rise to three out of five or 60% by 2030 and an increase in urban population from 3.2 billion to nearly 5 billion by 2030. As of 2018 55% live in cities and UN predicts that it will be 68% by 2050.\n\nThe increase will be most dramatic in the poorest and least-urbanised continents, Asia and Africa. Projections indicate that most urban growth over the next 25 years will be in developing countries. One billion people, one-seventh of the world's population, or one-third of urban population, now live in shanty towns, which are seen as \"breeding grounds\" for social problems such as unemployment, poverty, crime, drug addiction, alcoholism, and other social ills. In many poor countries, slums exhibit high rates of disease due to unsanitary conditions, malnutrition, and lack of basic health care.\n\nIn 2000, there were 18 megacitiesconurbations such as Tokyo, Beijing, Guangzhou, Seoul, Karachi, Mexico City, Mumbai, São Paulo, London and New York Citythat have populations in excess of 10 million inhabitants. Greater Tokyo already has 38 million, more than the entire population of Canada (at 36.7 million).\n\nAccording to the \"Far Eastern Economic Review\", Asia alone will have at least 10 'hypercities' by 2025, that is, cities inhabited by more than 19 million people, including Jakarta (24.9 million people), Dhaka (25 million), Karachi (26.5 million), Shanghai (27 million) and Mumbai (33 million). Lagos has grown from 300,000 in 1950 to an estimated 15 million today, and the Nigerian government estimates that city will have expanded to 25 million residents by 2015. Chinese experts forecast that Chinese cities will contain 800 million people by 2020.\n\nSeveral solutions and mitigation measures have the potential to reduce overpopulation. Some solutions are to be applied on a global planetary level (e.g., via UN resolutions), while some on a country or state government organization level, and some on a family or an individual level. Some of the proposed mitigations aim to help implement new social, cultural, behavioral and political norms to replace or significantly modify current norms.\n\nFor example, in societies like China, the government has put policies in place that regulate the number of children allowed to a couple. Other societies have implemented social marketing strategies in order to educate the public on overpopulation effects. \"The intervention can be widespread and done at a low cost. A variety of print materials (flyers, brochures, fact sheets, stickers) needs to be produced and distributed throughout the communities such as at local places of worship, sporting events, local food markets, schools and at car parks (taxis / bus stands).\"\n\nSuch prompts work to introduce the problem so that new or modified social norms are easier to implement. Certain government policies are making it easier and more socially acceptable to use contraception and abortion methods. An example of a country whose laws and norms are hindering the global effort to slow population growth is Afghanistan. \"The approval by Afghan President Hamid Karzai of the Shia Personal Status Law in March 2009 effectively destroyed Shia women's rights and freedoms in Afghanistan. Under this law, women have no right to deny their husbands sex unless they are ill, and can be denied food if they do.\"\n\nScientists and technologists including e.g. Huesemann and Ehrlich caution that science and technology, as currently practiced, cannot solve the serious problems global human society faces, and that a cultural-social-political shift is needed to reorient science and technology in a more socially responsible and environmentally sustainable direction.\n\nOne option is to focus on education about overpopulation, family planning, and birth control methods, and to make birth-control devices like male and female condoms, contraceptive pills and intrauterine devices easily available. Worldwide, nearly 40% of pregnancies are unintended (some 80 million unintended pregnancies each year). An estimated 350 million women in the poorest countries of the world either did not want their last child, do not want another child or want to space their pregnancies, but they lack access to information, affordable means and services to determine the size and spacing of their families. In the United States, in 2001, almost half of pregnancies were unintended. In the developing world, some 514,000 women die annually of complications from pregnancy and abortion, with 86% of these deaths occurring in the sub-Saharan Africa region and South Asia. Additionally, 8 million infants die, many because of malnutrition or preventable diseases, especially from lack of access to clean drinking water.\n\nWomen's rights and their reproductive rights in particular are issues regarded to have vital importance in the debate.\n\nEgypt announced a program to reduce its overpopulation by family planning education and putting women in the workforce. It was announced in June 2008 by the Minister of Health and Population, and the government has set aside 480 million Egyptian pounds (about $90 million US) for the program.\n\nSeveral scientists (including e.g. Paul and Anne Ehrlich and Gretchen Daily) proposed that humanity should work at stabilizing its absolute numbers, as a starting point towards beginning the process of reducing the total numbers. They suggested the following solutions and policies: following a small-family-size socio-cultural-behavioral norm worldwide (especially one-child-per-family ethos), and providing contraception to all along with proper education on its use and benefits (while providing access to safe, legal abortion as a backup to contraception), combined with a significantly more equitable distribution of resources globally. In the book \"Evolution Science and Ethics in the Third Millennium\", Robert Cliquet and Dragana Avramov also point out that the one (and a half)-child-per-family ethos is certainly a good one and that we should reduce the world population so that it is no larger than 1 to 3 billion.\n\nBusiness magnate Ted Turner proposed a \"voluntary, non-imposed\" one-child-per-family cultural norm. A \"pledge two or fewer\" campaign is run by Population Matters (a UK population concern organisation), in which people are encouraged to limit themselves to small family size.\n\nPopulation planning that is intended to reduce population size or growth rate may promote or enforce one or more of the following practices, although there are other methods as well:\n\n\nThe methods chosen can be strongly influenced by the cultural and religious beliefs of community members.\n\nOverpopulation can be mitigated by birth control; some nations, like the People's Republic of China, use strict measures to reduce birth rates. Religious and ideological opposition to birth control has been cited as a factor contributing to overpopulation and poverty.\n\nSanjay Gandhi, son of late Prime Minister of India Indira Gandhi, implemented a forced sterilization programme between 1975 and 1977. Officially, men with two children or more had to submit to sterilization, but there was a greater focus on sterilizing women than sterilizing men. Some unmarried young men and political opponents may also have been sterilized . This program is still remembered and criticized in India, and is blamed for creating a public aversion to family planning, which hampered government programs for decades.\n\nAnother choice-based approach is financial compensation or other benefits (free goods and/or services) by the state (or state-owned companies) offered to people who voluntarily undergo sterilization. Such compensation has been offered in the past by the government of India.\n\nIn 2014 the United Nations estimated there is an 80% likelihood that the world's population will be between 9.6 billion and 12.3 billion by 2100. Most of the world's expected population increase will be in Africa and southern Asia. Africa's population is expected to rise from the current one billion to four billion by 2100, and Asia could add another billion in the same period.\n\nVarious scientists and science fiction authors have contemplated that overpopulation on Earth may be remedied in the future by the use of extraterrestrial settlements. In the 1970s, Gerard K. O'Neill suggested building space habitats that could support 30,000 times the carrying capacity of Earth using just the asteroid belt, and that the Solar System as a whole could sustain current population growth rates for a thousand years. Marshall Savage (1992, 1994) has projected a human population of five quintillion (5 × 10)\nthroughout the Solar System by 3000, with the majority in the asteroid belt. Freeman Dyson (1999) favours the Kuiper belt as the future home of humanity, suggesting this could happen within a few centuries. In \"\", John S. Lewis suggests that the resources of the solar system could support 10 quadrillion (10) people. In an interview, Stephen Hawking claimed that overpopulation is a threat to human existence and \"our only chance of long-term survival is not to remain inward looking on planet Earth but to spread out into space.\"\n\nK. Eric Drexler, famous inventor of the futuristic concept of molecular nanotechnology, has suggested in \"Engines of Creation\" that colonizing space will mean breaking the Malthusian limits to growth for the human species.\n\nIt may be possible for other parts of the Solar System to be inhabited by humanity at some point in the future. Geoffrey Landis of NASA's Glenn Research Center in particular has pointed out that \"[at] cloud-top level, Venus is the paradise planet\", as one could construct aerostat habitats and floating cities there easily, based on the concept that breathable air is a lifting gas in the dense Venusian atmosphere. Venus would, like also Saturn, Uranus, and Neptune, in the upper layers of their atmospheres, even afford a gravitation almost exactly as strong as that on Earth (\"see\" colonization of Venus).\n\nMany science fiction authors, including Carl Sagan, Arthur C. Clarke, and Isaac Asimov, have argued that shipping any excess population into space is not a viable solution to human overpopulation. According to Clarke, \"the population battle must be fought or won here on Earth\". The problem for these authors is not the lack of resources in space (as shown in books such as \"Mining the Sky\"), but the physical impracticality of shipping vast numbers of people into space to \"solve\" overpopulation on Earth. However, Gerard K. O'Neill's calculations show that Earth could offload all new population growth with a launch services industry about the same size as the current airline industry.\n\nThe StarTram concept, by James R. Powell (the co-inventor of maglev transport) and others, envisions a capability to send up to 4 million people a decade to space per facility. A hypothetical extraterrestrial colony could potentially grow by reproduction only (i.e., without any immigration), with all of the inhabitants being the direct descendants of the original colonists.\n\nDespite the increase in population density within cities (and the emergence of megacities), UN Habitat states in its reports that urbanization may be the best compromise in the face of global population growth. Cities concentrate human activity within limited areas, limiting the breadth of environmental damage. But this mitigating influence can only be achieved if urban planning is significantly improved and city services are properly maintained.\n\nThe idea that densification is somehow a solution however is easily criticizable in that it ignores that the densely populated cities require massive resource extraction and intensive agriculture outside the cities in order to sustain them, which must also be transported into the cities. As such proponents of dense city life as a solution can be seen to be ignoring the intrinsic web of connections that allow a densely populated city to exist. The pollution levels in the world's most dense cities alone can be seen to be self-evident of the amount of environmental destruction that densely populated cities cause.\n\nPaul Ehrlich pointed out in his book \"The Population Bomb\" (1968) and in \"The Annhilation of Nature: Human Extinction of Birds and Mammals\" (2015) that dense cities create severely toxic environments, especially through the pollution of water sources. Cities and their expansion are also harmful to migrating birds and other species, which face the threat of starvation due to the clearing of their natural habitat for human habitation.\n\nIt may be that the rhetoric supporting city densification is a means of avoiding dealing with the actual problem of over population to begin with and rather than treating city densification as a symptom of the root problem, it has been promoted by the same interests that have profited from population increase e.g. property developers, the banking system, which invests in property development, industry, municipal councils seeking to increase their tax base and governments seeking permanent economic growth and expansion at any cost disregarding the impact it might have on the environment.\n\n\n\n\n"}
{"id": "6787314", "url": "https://en.wikipedia.org/wiki?curid=6787314", "title": "Demonstration (political)", "text": "Demonstration (political)\n\nA demonstration is action by a mass group or collection of groups of people in favor of a political or other cause or people partaking in a protest against a cause of concern; it often consists of walking in a mass march formation and either beginning with or meeting at a designated endpoint, or rally, to hear speakers. It is different from mass meeting.\n\nActions such as blockades and sit-ins may also be referred to as demonstrations. Demonstrations can be nonviolent or violent (usually referred to by participants as \"militant\"), or can begin as nonviolent and turn violent depending on the circumstances. Sometimes riot police or other forms of law enforcement become involved. In some cases this may be in order to try to prevent the protest from taking place at all. In other cases, it may be to prevent clashes between rival groups, or to prevent a demonstration from spreading and turning into a riot.\n\nThe term has been in use since the mid-19th century, as was the term \"monster meeting\", which was coined initially with reference to the huge assemblies of protesters inspired by Daniel O'Connell (1775–1847) in Ireland. Demonstrations are a form of activism, usually taking the form of a public gathering of people in a rally or walking in a march. Thus, the opinion is \"demonstrated\" to be significant by gathering in a crowd associated with that opinion.\n\nDemonstrations can promote a viewpoint (either positive or negative) regarding a public issue, especially relating to a perceived grievance or social injustice. A demonstration is usually considered more successful if more people participate. Research shows that anti-government demonstrations occur more frequently in affluent countries than in poor ones.\n\nHistorian Eric Hobsbawm wrote of demonstrations: \n\nThere are many types of demonstrations, including a variety of elements. These may include:\n\nDemonstrations are sometimes spontaneous gatherings, but are also utilized as a tactical choice by movements. They often form part of a larger campaign of nonviolent resistance, often also called civil resistance. Demonstrations are generally staged in public, but private demonstrations are certainly possible, especially if the demonstrators wish to influence the opinions of a small or very specific group of people. Demonstrations are usually physical gatherings, but virtual or online demonstrations are certainly possible.\n\nTopics of demonstrations often deal with political, economic, and social issues. Particularly with controversial issues, sometimes groups of people opposed to the aims of a demonstration may themselves launch a counter-demonstration with the aim of opposing the demonstrators and presenting their view. Clashes between demonstrators and counter-demonstrators may turn violent.\n\nGovernment-organized demonstrations are demonstrations which are organized by a government. The Islamic Republic of Iran, the People's Republic of China, Republic of Cuba, the Soviet Union and Argentina, among other nations, have had government-organized demonstrations.\n\nSometimes the date or location chosen for the demonstration is of historical or cultural significance, such as the anniversary of some event that is relevant to the topic of the demonstration.\n\nLocations are also frequently chosen because of some relevance to the issue at hand. For example, if a demonstration is targeted at issues relating to foreign nation, the demonstration may take place at a location associated with that nation, such as an embassy of the nation in question.\n\nProtest marches and demonstrations are a common nonviolent tactic. They are thus one tactic available to proponents of strategic nonviolence. However, the reasons for avoiding the use of violence may also derive, not from a general doctrine of nonviolence or pacifism, but from considerations relating to the particular situation that is faced, including its legal, cultural and power-political dimensions: this has been the case in many campaigns of civil resistance.\n\nSome demonstrations and protests can turn, at least partially, into riots or mob violence against objects such as automobiles and businesses, bystanders and the police. Police and military authorities often use non-lethal force or less-lethal weapons, such as tasers, rubber bullets, pepper spray, and tear gas against demonstrators in these situations. Sometimes violent situations are caused by the preemptive or offensive use of these weapons which can provoke, destabilize, or escalate a conflict.\n\nAs a known tool to prevent the infiltration by agents provocateurs, the organizers of large or controversial assemblies may deploy and coordinate demonstration marshals, also called stewards.\n\nFreedom of assembly in Brazil is granted by art. 5th, item XVI, of the Constitution of Brazil (1988): Constitution of Brazil – Text in English.\n\nFreedom of assembly in the Russian Federation is granted by Art. 31 of the Constitution adopted in 1993: Citizens of the Russian Federation shall have the right to gather peacefully, without weapons, and to hold meetings, rallies, demonstrations, marches and pickets.\n\nDemonstrations and protests are further regulated by the Federal Law of the Russian Federation No.54-FZ \"On Meetings, Rallies, Demonstrations, Marches and Pickets\". If the assembly in public is expected to involve more than one participant, its organisers are obliged to notify executive or local self-government authorities of the upcoming event few days in advance in writing. However, legislation does not foresee an authorisation procedure, hence the authorities have no right to prohibit an assembly or change its place unless it threatens the security of participants or is planned to take place near hazardous facilities, important railways, viaducts, pipelines, high voltage electric power lines, prisons, courts, presidential residences or in the border control zone. The right to gather can also be restricted in close proximity of cultural and historical monuments.\n\nPublic demonstrations are rare in Singapore, where it is illegal to hold cause-related events without a valid licence from the authorities. Such laws include the Public Entertainment and Meetings Act and the Public Order Act.\n\nUnder the Serious Organised Crime and Police Act 2005 and the Terrorism Act 2006, there are areas designated as 'protected sites' where people are not allowed to go. Previously, these were military bases and nuclear power stations, but the law is changing to include other, generally political areas, such as Downing Street, the Palace of Westminster, and the headquarters of MI5 and MI6. Previously, trespassers to these areas could not be arrested if they had not committed another crime and agreed to be escorted out, but this will change following amendments to the law.\n\nHuman rights groups fear the powers could hinder peaceful protest. Nick Clegg, the then Liberal Democrat home affairs spokesman, said: \"I am not aware of vast troops of trespassers wanting to invade MI5 or MI6, still less running the gauntlet of security checks in Whitehall and Westminster to make a point. It's a sledgehammer to crack a nut.\" Liberty, the civil liberties pressure group, said the measure was \"excessive\".\n\nOne of the biggest demonstration in the UK was the people vote march, on 19 October 2019, with around 1 million demonstrators related to the Brexit.\n\nThe First Amendment of the United States Constitution specifically allows the freedom of assembly as part of a measure to facilitate the redress of such grievances. \"Amendment I: Congress shall make no law ... abridging ... the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.\"\n\nA growing trend in the United States has been the implementation of \"free speech zones\", or fenced-in areas which are often far-removed from the event which is being protested; critics of free-speech zones argue that they go against the First Amendment of the United States Constitution by their very nature, and that they lessen the impact the demonstration might otherwise have had. In many areas it is required to get permission from the government to hold a demonstration.\n\n\n"}
{"id": "13311819", "url": "https://en.wikipedia.org/wiki?curid=13311819", "title": "Therapy", "text": "Therapy\n\nA therapy or medical treatment (often abbreviated tx, Tx, or T) is the attempted remediation of a health problem, usually following a diagnosis. \n\nAs a rule, each therapy has indications and contraindications. There are many different types of therapy. Not all therapies are effective. Many therapies can produce unwanted adverse effects.\n\n\"Treatment\" and \"therapy\" are generally considered synonyms. However, in the context of mental health, the term \"therapy\" may refer specifically to psychotherapy. \n\nThe words care, therapy, treatment, and intervention overlap in a semantic field, and thus they can be synonymous depending on context. Moving rightward through that order, the connotative level of holism decreases and the level of specificity (to concrete instances) increases. Thus, in health care contexts (where its senses are always noncount), the word \"care\" tends to imply a broad idea of everything done to protect or improve someone's health (for example, as in the terms \"preventive care\" and \"primary care\", which connote ongoing action), although it sometimes implies a narrower idea (for example, in the simplest cases of wound care or postanesthesia care, a few particular steps are sufficient, and the patient's interaction with that provider is soon finished). In contrast, the word \"intervention\" tends to be specific and concrete, and thus the word is often countable; for example, one instance of cardiac catheterization is one intervention performed, and coronary care (noncount) can require a series of interventions (count). At the extreme, the piling on of such countable interventions amounts to interventionism, a flawed model of care lacking holistic circumspection—merely treating discrete problems (in billable increments) rather than maintaining health. \"Therapy\" and \"treatment\", in the middle of the semantic field, can connote either the holism of \"care\" or the discreteness of \"intervention\", with context conveying the intent in each use. Accordingly, they can be used in both noncount and count senses (for example, \"therapy for chronic kidney disease can involve several dialysis treatments per week\").\n\nThe words \"aceology\" and \"iamatology\" are obscure and obsolete synonyms referring to the study of therapies.\n\nThe English word \"therapy\" comes via Latin \"therapīa\" from and literally means \"curing\" or \"healing\".\n\nLevels of care classify health care into categories of chronology, priority, or intensity, as follows:\n\nTreatment decisions often follow formal or informal algorithmic guidelines. Treatment options can often be ranked or prioritized into lines of therapy: first-line therapy, second-line therapy, third-line therapy, and so on. First-line therapy (sometimes called induction therapy, primary therapy, or front-line therapy) is the first therapy that will be tried. Its priority over other options is usually either: (1) formally recommended on the basis of clinical trial evidence for its best-available combination of efficacy, safety, and tolerability or (2) chosen based on the clinical experience of the physician. If a first-line therapy either fails to resolve the issue or produces intolerable side effects, additional (second-line) therapies may be substituted or added to the treatment regimen, followed by third-line therapies, and so on.\n\nAn example of a context in which the formalization of treatment algorithms and the ranking of lines of therapy is very extensive is chemotherapy regimens. Because of the great difficulty in successfully treating some forms of cancer, one line after another may be tried. In oncology the count of therapy lines may reach 10 or even 20.\n\nOften multiple therapies may be tried simultaneously (combination therapy or polytherapy). Thus combination chemotherapy is also called polychemotherapy, whereas chemotherapy with one agent at a time is called single-agent therapy or monotherapy.\n\nAdjuvant therapy is therapy given in addition to the primary, main, or initial treatment, but simultaneously (as opposed to second-line therapy). Neoadjuvant therapy is therapy that is begun before the main therapy. Thus one can consider surgical excision of a tumor as the first-line therapy for a certain type and stage of cancer even though radiotherapy is used before it; the radiotherapy is neoadjuvant (chronologically first but not primary in the sense of the main event). Premedication is conceptually not far from this, but the words are not interchangeable; cytotoxic drugs to put a tumor \"on the ropes\" before surgery delivers the \"knockout punch\" are called neoadjuvant chemotherapy, not premedication, whereas things like anesthetics or prophylactic antibiotics before dental surgery are called premedication.\n\nStep therapy or stepladder therapy is a specific type of prioritization by lines of therapy. It is controversial in American health care because unlike conventional decision-making about what constitutes first-line, second-line, and third-line therapy, which in the U.S. reflects safety and efficacy first and cost only according to the patient's wishes, step therapy attempts to mix cost containment by someone other than the patient (third-party payers) into the algorithm. Therapy freedom and the negotiation between individual and group rights are involved.\n\nTreatments can be classified according to the method of treatment:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "18247391", "url": "https://en.wikipedia.org/wiki?curid=18247391", "title": "Socialist International", "text": "Socialist International\n\nThe Socialist International (SI) is a worldwide organisation of political parties which seek to establish democratic socialism. It consists mostly of democratic socialist, social-democratic and labour political parties and other organisations.\n\nAlthough formed in 1951 as a successor to the Labour and Socialist International, it has antecedents to the late 19th century. The organisation currently includes 147 member parties and organisations from over 100 countries. Its members have governed in many countries including most of Europe.\n\nThe current secretary general of the SI is (Chile) and the current president of the SI is the former Prime Minister of Greece, George Papandreou, both of whom were re-elected at the last SI Congress held in Cartagena, Colombia in March 2017.\n\nThe International Workingmen's Association, also known as the First International, was the first international body to bring together organisations representing the working class. It was formed in London on 28 September 1864 by socialist, communist and anarchist political groups and trade unions. Tensions between moderates and revolutionaries led to its dissolution in 1876 in Philadelphia.\n\nThe Second International was formed in Paris on 14 July 1889 as an association of the socialist parties. Differences over World War I led to the Second International being dissolved in 1916.\n\nThe International Socialist Commission (ISC), also known as the Berne International, was formed in February 1919 at a meeting in Berne by parties that wanted to resurrect the Second International. In March 1919 communist parties formed Comintern (the Third International) at a meeting in Moscow.\n\nParties which did not want to be a part of the resurrected Second International (ISC) or Comintern formed the International Working Union of Socialist Parties (IWUSP, also known as Vienna International/Vienna Union/Two-and-a-Half International) on 27 February 1921 at a conference in Vienna. The ISC and the IWUSP joined to form the Labour and Socialist International (LSI) in May 1923 at a meeting in Hamburg. The rise of Nazism and the start of World War II led to the dissolution of the LSI in 1940.\n\nThe Socialist International was formed in Frankfurt in July 1951 as a successor to the LSI.\n\nDuring the post-World War II period, the SI aided social democratic parties in re-establishing themselves when dictatorship gave way to democracy in Portugal (1974) and Spain (1975). Until its 1976 Geneva Congress, the SI had few members outside Europe and no formal involvement with Latin America. In the 1980s, most SI parties gave their backing to the Nicaraguan Sandinistas (FSLN), whose democratically-elected left-wing government was subject to a campaign to overthrow it backed by the United States, which culminated in the Iran–Contra affair after the Reagan administration covertly continued US support for the Contras after such support was banned by Congress.\n\nIn the late 1970s and in the 1980s the SI had extensive contacts and discussion with the two leading powers of the Cold War period, the United States and the Soviet Union, on issues concerning East–West relations and arms control. The SI supported détente and disarmament agreements, such as SALTII, START and INF. They had several meetings and discussion in Washington, D.C. with President Jimmy Carter and Vice-President George Bush and in Moscow with Secretaries General Leonid Brezhnev and Mikhail Gorbachev. The SI's delegations to these discussions were led by the Finnish Prime Minister Kalevi Sorsa.\n\nSince then, the SI has admitted as members an increasing number of parties and organisations from Africa, Asia, Europe and Latin America (see below for current list).\n\nFollowing the Tunisian revolution, the Constitutional Democratic Rally was expelled from the SI in January 2011; later that month the Egyptian National Democratic Party was also expelled; and as a result of the 2010–2011 Ivorian crisis, the Ivorian Popular Front was expelled in March 2011, in accordance with section 7.1 of the statutes of the Socialist International. These decisions were approved at the subsequent SI Congress in Cape Town in 2012 in line with section 5.1.3 of the statutes.\n\nDespite the carrying out of open, transparent, fully democratic elections overseen by an electoral committee headed by the Social Democratic Party of Germany (SPD) at the 2012 SI Congress, on 22 May 2013 the SPD along with some other current and former member parties of the SI founded a rival international network of social-democratic parties known as the Progressive Alliance, citing their perceived undemocratic and outmoded nature of the SI, as well as the Socialist International's admittance and continuing inclusion of undemocratic political movements into the organization.\n\nFor a long time, the Socialist International remained distant from Latin America, considering the region as a zone of influence of the United States. For example, it does not denounce the coup d'état against Socialist President Jacobo Arbenz in Guatemala in 1954 or the invasion of the Dominican Republic by the United States in 1964. It was not until the 1973 Chilean coup d'état that we discovered \"a world we did not know\", explains Antoine Blanca, a diplomat for the French PS. According to him, solidarity with the Chilean left was \"the first challenge worthy of the name, against Washington, of an International which, until then, had done everything to appear subject to American strategy and NATO\". Subsequently, notably under the leadership of François Mitterrand, the SI supported the sandinistas in Nicaragua and other movements in El Salvador, Guatemala and Honduras in their struggle against US-supported dictatorships.\n\nIn the 1990s, it was joined by non-socialist parties that took note of the economic power of the European countries governed or to be governed by their partners across the Atlantic and calculated the benefits they could derive from it. During this period, \"the socialist international works in a clientist way; some parties come here to rub shoulders with Europeans as if they were in the upper class,\" says Porfirio Muñoz Ledo, one of the representatives of the Party of the Democratic Revolution (Mexico) at the SI. It is home to \"the very centrist Argentinean Radical Civic Union (UCR); the Mexican Institutional Revolutionary Party (PRI), which was not very democratically in power for seventy years; the Colombian Liberal Party - under whose governments the left-wing formation Patriotic Union (1986-1990) was exterminated - introduced the neoliberal model (1990-1994) and to which, until 2002, Alvaro Uribe will belong\". In the following decade, many left-wing parties that came to power (in Brazil, Venezuela, Bolivia, Ecuador and El Salvador) preferred to keep their distance from the SI.\n\n\nCurrent and honorary presidents include:\n\n\n\nThe following parties are full members:\nThe following parties are consultative parties:\nThe following parties are observer parties:\n\n\n"}
{"id": "20949522", "url": "https://en.wikipedia.org/wiki?curid=20949522", "title": "Bureaucracy", "text": "Bureaucracy\n\nBureaucracy () refers to both a body of non-elected government officials and an administrative policy-making group. Historically, a bureaucracy was a government administration managed by departments staffed with non-elected officials. Today, bureaucracy is the administrative system governing any large institution, whether publicly owned or privately owned. The public administration in many countries is an example of a bureaucracy, but so is the centralized hierarchical structure of a business firm.\n\nVarious commentators have noted the necessity of bureaucracies in modern society. The German sociologist Max Weber argued that bureaucracy constitutes the most efficient and rational way in which human activity can be organized and that systematic processes and organized hierarchies are necessary to maintain order, maximize efficiency, and eliminate favoritism. On the other hand, Weber also saw unfettered bureaucracy as a threat to individual freedom, with the potential of trapping individuals in an impersonal \"iron cage\" of rule-based, rational control.\n\nThe term \"bureaucracy\" originated in the French language: it combines the French word \"bureau\" – desk or office – with the Greek word κράτος (\"kratos\") – rule or political power. The French economist Jacques Claude Marie Vincent de Gournay (1712-1759) coined the word in the mid-18th century. Gournay never wrote the term down but a letter from a contemporary later quoted him:\nThe first known English-language use dates to 1818 with Irish novelist Lady Morgan referring to the apparatus used by the British to subjugate their Irish colony as \"the Bureaucratie, or office tyranny, by which Ireland has so long been governed.\" By the mid-19th century the word appeared in a more neutral sense, referring to a system of public administration in which offices were held by unelected career officials. In this context \"bureaucracy\" was seen as a distinct form of management, often subservient to a monarchy. In the 1920s the German sociologist Max Weber expanded the definition to include any system of administration conducted by trained professionals according to fixed rules. Weber saw bureaucracy as a relatively positive development; however, by 1944 the Austrian economist Ludwig von Mises opined in the context of his experience in the Nazi regime that the term bureaucracy was \"always applied with an opprobrious connotation,\" and by 1957 the American sociologist Robert Merton suggested that the term \"bureaucrat\" had become an \"epithet, a \"Schimpfwort\"\" in some circumstances. The word \"bureaucracy\" is also used in politics and government with a disapproving tone to disparage official rules that make it difficult to do things. In workplaces, the word is used very often to blame complicated rules, processes, and written work that make it hard to get something done. Socio-bureaucracy would then refer to certain social influences that may affect the function of a society.\n\nAlthough the term \"bureaucracy\" first originated in the mid-18th century, organized and consistent administrative systems existed much earlier. The development of writing ( 3500 BC) and the use of documents was critical to the administration of this system, and the first definitive emergence of bureaucracy occurred in ancient Sumer, where an emergent class of scribes used clay tablets to administer the harvest and to allocate its spoils. Ancient Egypt also had a hereditary class of scribes that administered the civil-service bureaucracy.\n\nA hierarchy of regional proconsuls and their deputies administered the Roman Empire. The reforms of Diocletian (Emperor from 284 to 305) doubled the number of administrative districts and led to a large-scale expansion of Roman bureaucracy. The early Christian author Lactantius ( 250 – 325) claimed that Diocletian's reforms led to widespread economic stagnation, since \"the provinces were divided into minute portions, and many presidents and a multitude of inferior officers lay heavy on each territory.\" After the Empire split, the Byzantine Empire developed a notoriously complicated administrative hierarchy, and in the 20th century the term \"Byzantine\" came to refer to any complex bureaucratic structure.\n\nIn China, when the Qin dynasty (221–206 BC) unified China under the Legalist system, the emperor assigned administration to dedicated officials rather than nobility, ending feudalism in China, replacing it with a centralized, bureaucratic government. The form of government created by the first emperor and his advisors was used by later dynasties to structure their own government. Under this system, the government thrived, as talented individuals could be more easily identified in the transformed society. The Han dynasty (202 BC - 220 AD) established a complicated bureaucracy based on the teachings of Confucius, who emphasized the importance of ritual in a family, in relationships, and in politics. With each subsequent dynasty, the bureaucracy evolved. In 165 BC, Emperor Wen introduced the first method of recruitment to civil service through examinations, while\nEmperor Wu (r. 141–87 BC), cemented the ideology of Confucius into mainstream governance installed a system of recommendation and nomination in government service known as \"xiaolian\", and a national academy whereby officials would select candidates to take part in an examination of the Confucian classics, from which Emperor Wu would select officials. In the Sui dynasty (581–618) and the subsequent Tang dynasty (618–907) the \"shi\" class would begin to present itself by means of the fully standardized civil service examination system, of partial recruitment of those who passed standard exams and earned an official degree. Yet recruitment by recommendations to office was still prominent in both dynasties. It was not until the Song dynasty (960–1279) that the recruitment of those who passed the exams and earned degrees was given greater emphasis and significantly expanded. During the Song dynasty (960–1279) the bureaucracy became meritocratic. Following the Song reforms, competitive examinations took place to determine which candidates qualified to hold given positions.\nThe imperial examination system lasted until 1905, six years before the Qing dynasty collapsed, marking the end of China's traditional bureaucratic system.\n\nInstead of the inefficient and often corrupt system of tax farming that prevailed in absolutist states such as France, the Exchequer was able to exert control over the entire system of tax revenue and government expenditure. By the late 18th century, the ratio of fiscal bureaucracy to population in Britain was approximately 1 in 1300, almost four times larger than the second most heavily bureaucratized nation, France. Thomas Taylor Meadows, Britain's consul in Guangzhou, argued in his \"Desultory Notes on the Government and People of China\" (1847) that \"the long duration of the Chinese empire is solely and altogether owing to the good government which consists in the advancement of men of talent and merit only,\" and that the British must reform their civil service by making the institution meritocratic. Influenced by the ancient Chinese imperial examination, the Northcote–Trevelyan Report of 1854 recommended that recruitment should be on the basis of merit determined through competitive examination, candidates should have a solid general education to enable inter-departmental transfers, and promotion should be through achievement rather than \"preferment, patronage, or purchase\". This led to implementation of Her Majesty's Civil Service as a systematic, meritocratic civil service bureaucracy.\n\nIn the British civil service, just as it was in China, entrance to the civil service was usually based on a general education in ancient classics, which similarly gave bureaucrats greater prestige. The Cambridge-Oxford ideal of the civil service was identical to the Confucian ideal of a general education in world affairs through humanism. (Well into the 20th century, Classics, Literature, History and Language remained heavily favoured in British civil service examinations. In the period of 1925–1935, 67 percent of British civil service entrants consisted of such graduates.) Like the Chinese model's consideration of personal values, the British model also took personal physique and character into account.\n\nLike the British, the development of French bureaucracy was influenced by the Chinese system. Under Louis XIV of France, the old nobility had neither power nor political influence, their only privilege being exemption from taxes. The dissatisfied noblemen complained about this \"unnatural\" state of affairs, and discovered similarities between absolute monarchy and bureaucratic despotism. With the translation of Confucian texts during the Enlightenment, the concept of a meritocracy reached intellectuals in the West, who saw it as an alternative to the traditional \"ancien regime\" of Europe. Western perception of China even in the 18th century admired the Chinese bureaucratic system as favourable over European governments for its seeming meritocracy; Voltaire claimed that the Chinese had \"perfected moral science\" and François Quesnay advocated an economic and political system modeled after that of the Chinese.\nThe governments of China, Egypt, Peru and Empress Catherine II were regarded as models of Enlightened Despotism, admired by such figures as Diderot, D'Alembert and Voltaire.\n\nNapoleonic France adopted this meritocracy system and soon saw a rapid and dramatic expansion of government, accompanied by the rise of the French civil service and its complex systems of bureaucracy. This phenomenon became known as \"bureaumania\". In the early 19th century, Napoleon attempted to reform the bureaucracies of France and other territories under his control by the imposition of the standardized Napoleonic Code. But paradoxically, that led to even further growth of the bureaucracy.\n\nFrench civil service examinations adopted in the late 19th century were also heavily based on general cultural studies. These features have been likened to the earlier Chinese model. \n\nBy the mid-19th century, bureaucratic forms of administration were firmly in place across the industrialized world. Thinkers like John Stuart Mill and Karl Marx began to theorize about the economic functions and power-structures of bureaucracy in contemporary life. Max Weber was the first to endorse bureaucracy as a necessary feature of modernity, and by the late 19th century bureaucratic forms had begun their spread from government to other large-scale institutions.\n\nThe trend toward increased bureaucratization continued in the 20th century, with the public sector employing over 5% of the workforce in many Western countries. Within capitalist systems, informal bureaucratic structures began to appear in the form of corporate power hierarchies, as detailed in mid-century works like \"The Organization Man\" and \"The Man in the Gray Flannel Suit\". Meanwhile, in the Soviet Union and Eastern Bloc nations, a powerful class of bureaucratic administrators termed \"nomenklatura\" governed nearly all aspects of public life.\n\nThe 1980s brought a backlash against perceptions of \"big government\" and the associated bureaucracy. Politicians like Margaret Thatcher and Ronald Reagan gained power by promising to eliminate government regulatory bureaucracies, which they saw as overbearing, and return economic production to a more purely capitalistic mode, which they saw as more efficient. In the business world, managers like Jack Welch gained fortune and renown by eliminating bureaucratic structures inside corporations. Still, in the modern world, most organized institutions rely on bureaucratic systems to manage information, process records, and administer complex systems, although the decline of paperwork and the widespread use of electronic databases is transforming the way bureaucracies function.\n\nKarl Marx theorized about the role and function of bureaucracy in his \"Critique of Hegel's Philosophy of Right\", published in 1843. In \"Philosophy of Right\", Hegel had supported the role of specialized officials in public administration, although he never used the term \"bureaucracy\" himself. By contrast, Marx was opposed to bureaucracy. Marx posited that while corporate and government bureaucracy seem to operate in opposition, in actuality they mutually rely on one another to exist. He wrote that \"The Corporation is civil society's attempt to become state; but the bureaucracy is the state which has really made itself into civil society.\"\n\nWriting in the early 1860s, political scientist John Stuart Mill theorized that successful monarchies were essentially bureaucracies, and found evidence of their existence in Imperial China, the Russian Empire, and the regimes of Europe. Mill referred to bureaucracy as a distinct form of government, separate from representative democracy. He believed bureaucracies had certain advantages, most importantly the accumulation of experience in those who actually conduct the affairs. Nevertheless, he believed this form of governance compared poorly to representative government, as it relied on appointment rather than direct election. Mill wrote that ultimately the bureaucracy stifles the mind, and that \"a bureaucracy always tends to become a pedantocracy.\"\n\nThe German sociologist Max Weber was the first to formally study bureaucracy and his works led to the popularization of this term. In his essay \"Bureaucracy\", published in his magnum opus \"Economy and Society\", Weber described many ideal-typical forms of public administration, government, and business. His ideal-typical bureaucracy, whether public or private, is characterized by:\n\nWeber listed several preconditions for the emergence of bureaucracy, including an increase in the amount of space and population being administered, an increase in the complexity of the administrative tasks being carried out, and the existence of a monetary economy requiring a more efficient administrative system. Development of communication and transportation technologies make more efficient administration possible, and democratization and rationalization of culture results in demands for equal treatment.\n\nAlthough he was not necessarily an admirer of bureaucracy, Weber saw bureaucratization as the most efficient and rational way of organizing human activity and therefore as the key to rational-legal authority, indispensable to the modern world. Furthermore, he saw it as the key process in the ongoing rationalization of Western society. Weber also saw bureaucracy, however, as a threat to individual freedoms, and the ongoing bureaucratization as leading to a \"polar night of icy darkness\", in which increasing rationalization of human life traps individuals in a soulless \"iron cage\" of bureaucratic, rule-based, rational control. Weber's critical study of the bureaucratization of society became one of the most enduring parts of his work. Many aspects of modern public administration are based on his work, and a classic, hierarchically organized civil service of the Continental type is called \"Weberian civil service\".\n\nWriting as an academic while a professor at Bryn Mawr College, Woodrow Wilson's essay \"The Study of Administration\" argued for bureaucracy as a professional cadre, devoid of allegiance to fleeting politics. Wilson advocated a bureaucracy that \"is a part of political life only as the methods of the counting house are a part of the life of society; only as machinery is part of the manufactured product. But it is, at the same time, raised very far above the dull level of mere technical detail by the fact that through its greater principles it is directly connected with the lasting maxims of political wisdom, the permanent truths of political progress.\"\n\nWilson did not advocate a replacement of rule by the governed, he simply advised that, \"Administrative questions are not political questions. Although politics sets the tasks for administration, it should not be suffered to manipulate its offices\". This essay became a foundation for the study of public administration in America.\n\nIn his 1944 work \"Bureaucracy\", the Austrian economist Ludwig von Mises compared bureaucratic management to profit management. Profit management, he argued, is the most effective method of organization when the services rendered may be checked by economic calculation of profit and loss. When, however, the service in question can not be subjected to economic calculation, bureaucratic management is necessary. He did not oppose universally bureaucratic management; on the contrary, he argued that bureaucracy is an indispensable method for social organization, for it is the only method by which the law can be made supreme, and is the protector of the individual against despotic arbitrariness. Using the example of the Catholic Church, he pointed out that bureaucracy is only appropriate for an organization whose code of conduct is not subject to change. He then went on to argue that complaints about bureaucratization usually refer not to the criticism of the bureaucratic methods themselves, but to \"the intrusion of bureaucracy into all spheres of human life.\" Mises saw bureaucratic processes at work in both the private and public spheres; however, he believed that bureaucratization in the private sphere could only occur as a consequence of government interference. According to him, \"What must be realized is only that the strait jacket of bureaucratic organization paralyzes the individual's initiative, while within the capitalist market society an innovator still has a chance to succeed. The former makes for stagnation and preservation of inveterate methods, the latter makes for progress and improvement.\"\n\nAmerican sociologist Robert K. Merton expanded on Weber's theories of bureaucracy in his work \"Social Theory and Social Structure\", published in 1957. While Merton agreed with certain aspects of Weber's analysis, he also noted the dysfunctional aspects of bureaucracy, which he attributed to a \"trained incapacity\" resulting from \"over conformity\". He believed that bureaucrats are more likely to defend their own entrenched interests than to act to benefit the organization as a whole but that pride in their craft makes them resistant to changes in established routines. Merton stated that bureaucrats emphasize formality over interpersonal relationships, and have been trained to ignore the special circumstances of particular cases, causing them to come across as \"arrogant\" and \"haughty\".\n\nIn his book “A General Theory of Bureaucracy”, first published in 1976, Dr. Elliott Jaques describes the discovery of a universal and uniform underlying structure of managerial or work levels in the bureaucratic hierarchy for any type of employment systems.\n\nElliott Jaques argues and presents evidence that for the bureaucracy to provide a valuable contribution to the open society some of the following conditions must be met:\nThe definition of effective bureaucratic hierarchy by Elliott Jaques is of importance not only to sociology but to social psychology, social anthropology, economics, politics, and social philosophy. They also have a practical application in business and administrative studies.\n\n\n"}
{"id": "30874303", "url": "https://en.wikipedia.org/wiki?curid=30874303", "title": "Volunteering", "text": "Volunteering\n\nVolunteering is generally considered an altruistic activity where an individual or group provides services for no financial or social gain \"to benefit another person, group or organization\". Volunteering is also renowned for skill development and is often intended to promote goodness or to improve human quality of life. Volunteering may have positive benefits for the volunteer as well as for the person or community served. It is also intended to make contacts for possible employment. Many volunteers are specifically trained in the areas they work, such as medicine, education, or emergency rescue. Others serve on an as-needed basis, such as in response to a natural disaster.\n\nIn a military context a volunteer is someone who joins an armed force of their own volition rather than being conscripted, and is usually paid.\n\nThe verb was first recorded in 1755. It was derived from the noun \"volunteer\", in C.1600, \"one who offers himself for military service,\" from the Middle French \"voluntaire\". In the non-military sense, the word was first recorded during the 1630s. The word \"volunteering\" has more recent usage—still predominantly military—coinciding with the phrase \"community service\".\nIn a military context, a volunteer army is a military body whose soldiers chose to enter service, as opposed to having been conscripted. Such volunteers do not work \"for free\" and are given regular pay.\n\nDuring this time, America experienced the Great Awakening. People became aware of the disadvantaged and realized the cause for movement against slavery. Younger people started helping the needy in their communities . In 1851, the first YMCA in the United States was started, followed seven years later by the first YWCA. During the American Civil War, women volunteered their time to sew supplies for the soldiers and the \"Angel of the Battlefield\" Clara Barton and a team of volunteers began providing aid to servicemen. Barton founded the American Red Cross in 1881 and began mobilizing volunteers for disaster relief operations, including relief for victims of the Johnstown Flood in 1889.\n\nThe Salvation Army is one of the oldest and largest organizations working for disadvantaged people. Though it is a charity organization, it has organized a number of volunteering programs since its inception.\nPrior to the 19th century, few formal charitable organizations existed to assist people in need.\n\nIn the first few decades of the 20th century, several volunteer organizations were founded, including the Rotary International, Kiwanis International, Association of Junior Leagues International, and Lions Clubs International.\n\nThe Great Depression saw one of the first large-scale, nationwide efforts to coordinate volunteering for a specific need. During World War II, thousands of volunteer offices supervised the volunteers who helped with the many needs of the military and the home front, including collecting supplies, entertaining soldiers on leave, and caring for the injured.\n\nAfter World War II, people shifted the focus of their altruistic passions to other areas, including helping the poor and volunteering overseas. A major development was the Peace Corps in the United States in 1960. When President Lyndon B. Johnson declared a \"War on Poverty\" in 1964, volunteer opportunities started to expand and continued into the next few decades. The process for finding volunteer work became more formalized, with more volunteer centers forming and new ways to find work appearing on the World Wide Web.\n\nAccording to the Corporation for National and Community Service (in 2012), about 64.5 million Americans, or 26.5 percent of the adult population, gave 7.9 billion hours of volunteer service worth $175 billion. This calculates at about 125–150 hours per year or 3 hours per week at a rate of $22 per hour. Volunteer hours in the UK are similar; the data for other countries is unavailable.\n\nIn 1960, after the so-called revolutionary war in Cuba ended, Ernesto Che Guevara created the concept of volunteering work. It was created with the intention that workers across the country volunteer a few hours of work on their work centers.\n\nMany schools on all education levels offer service-learning programs, which allow students to serve the community through volunteering while earning educational credit. According to Alexander Astin in the foreword to \"Where's the Learning in Service-Learning?\" by Janet Eyler and Dwight E. Giles, Jr.,\"...we promote more wide-spread adoption of service-learning in higher education because we see it as a powerful means of preparing students to become more caring and responsible parents and citizens and of helping colleges and universities to make good on their pledge to 'serve society.'\" When describing service learning, the Medical Education at Harvard says, \"Service learning unites academic study and volunteer community service in mutually reinforcing ways. ...service learning is characterized by a relationship of partnership: the student learns from the service agency and from the community and, in return, gives energy, intelligence, commitment, time and skills to address human and community needs.\" Volunteering in service learning seems to have the result of engaging both mind and heart, thus providing a more powerful learning experience; according to Janet Eyler and Dwight E. Giles, it succeeds by the fact that it \"...fosters student development by capturing student interest...\" While not recognized by everyone as a legitimate approach, research on the efficacy of service learning has grown. Janet Eyler and Dwight E. Giles conducted a national study of American college students to ascertain the significance of service learning programs, According to Eyler and Giles,\"These surveys, conducted before and after a semester of community service, examine the impact of service-learning on students.\" They describe their experience with students involved in service-learning in this way: \"Students like service-learning. When we sit down with a group of students to discuss service-learning experiences, their enthusiasm is unmistakable. ...it is clear that [the students]believe that what they gain from service-learning differs qualitatively from what they often derive from more traditional instruction.\"\n\n\"Skills-based volunteering\" is leveraging the specialized skills and the talents of individuals to strengthen the infrastructure of nonprofits, helping them build and sustain their capacity to successfully achieve their missions. This is in contrast to traditional volunteering, where specific training is not required. The average hour of traditional volunteering is valued by the Independent Sector at between $18–20 an hour. Skills-based volunteering is valued at $40–500 an hour, depending on the market value of the time.\n\nAn increasingly popular form of volunteering among young people, particularly gap year students and graduates, is to travel to communities in the developing world to work on projects with local organisations. Activities include teaching English, working in orphanages, conservation, assisting non-governmental organizations and medical work. International volunteering often aims to give participants valuable skills and knowledge in addition to benefits to the host community and organization.\n\nAlso called \"e-volunteering\" or \"online volunteering\", virtual volunteering is a volunteer who completes tasks, in whole or in part, offsite from the organization being assisted. They use the Internet and a home, school, telecenter or work computer, or other Internet-connected device, such as a PDA or smartphone. Virtual volunteering is also known as cyber service, telementoring, and teletutoring, as well as various other names. Virtual volunteering is similar to telecommuting, except that instead of online employees who are paid, these are online volunteers who are not paid.\n\nMicro-volunteering is a task performed via an internet-connected device. An individual typically does this task in small, un-paid increments of time. Micro-volunteering is distinct from \"virtual volunteering\" in that it typically does not require the individual volunteer to go through an application process, screening process, or training period.\n\nEnvironmental volunteering refers to the volunteers who contribute towards environmental management or conservation. Volunteers conduct a range of activities including environmental monitoring, ecological restoration such as re-vegetation and weed removal, protecting endangered animals, and educating others about the natural environment.\n\nVolunteering often plays a pivotal role in the recovery effort following natural disasters, such as tsunamis, floods, droughts, hurricanes, and earthquakes. For example, the 1995 Great Hanshin-Awaji earthquake in Japan was a watershed moment, bringing in many first-time volunteers for earthquake response. The 2004 Indian Ocean earthquake and tsunami attracted a large number of volunteers worldwide, deployed by non-governmental organizations, government agencies, and the United Nations.\n\nDuring the 2012 hurricane Sandy emergency, Occupy Sandy volunteers, formed a \"laterally organized rapid-response team\" that provided much needed help during and after the storm, from food to shelter to reconstruction. It is an example of mutualism at work, pooling resources and assistance and leveraging social media.\n\nResource poor schools around the world rely on government support or on efforts from volunteers and private donations, in order to run effectively. In some countries, whenever the economy is down, the need for volunteers and resources increases greatly. There are many opportunities available in school systems for volunteers. Yet, there are not many requirements in order to volunteer in a school system. Whether one is a high school or TEFL (Teaching English as a Foreign Language) graduate or college student, most schools require just voluntary and selfless effort.\n\nMuch like the benefits of any type of volunteering there are great rewards for the volunteer, student, and school. In addition to intangible rewards, volunteers can add relevant experience to their resumes. Volunteers who travel to assist may learn foreign culture and language.\n\nVolunteering in schools can be an additional teaching guide for the students and help to fill the gap of local teachers. Cultural and language exchange during teaching and other school activities can be the most essential learning experience for both students and volunteers.\n\nBenefacto, a volunteering brokerage, describe corporate volunteering as \"Companies giving their employees an allowance of paid time off annually, which they use to volunteer at a charity of their choice.\"\n\nA majority of the companies at the Fortune 500 allow their employees to volunteer during work hours. These formalized Employee Volunteering Programs (EVPs), also called Employer Supported Volunteering (ESV), are regarded as a part of the companies' sustainability efforts and their social responsibility activities. About 40% of Fortune 500 companies provide monetary donations, also known as volunteer grants, to nonprofits as a way to recognize employees who dedicate significant amounts of time to volunteering in the community.\n\nAccording to the information from VolunteerMatch, a service that provides Employee Volunteering Program solutions, the key drivers for companies that produce and manage EVPs are building brand awareness and affinity, strengthening trust and loyalty among consumers, enhancing corporate image and reputation, improving employee retention, increasing employee productivity and loyalty, and providing an effective vehicle to reach strategic goals.\n\nIn April 2015, David Cameron pledged to give all UK workers employed by companies with more 250 staff mandatory three days’ paid volunteering leave, which if implemented will generate an extra 360 million volunteering hours a year.\n\nCommunity volunteering, in the US called \"community service\", refers globally to those who work to improve their local community. This activity commonly occurs through not for profit organizations, local governments and churches; but also encompasses ad-hoc or informal groups such as recreational sports teams.\n\nThere are many proven personal benefits of community volunteerism. Working together with a group of people who have different ethnicity, backgrounds, and views reduces stereotypes. Community volunteerism has also been proven to improve student's academic success.\n\nAccording to \"Where's the Learning in Service Learning?\" by Janet Eyler and Dwight E. Giles, immersing oneself into service learning and serving others has many positive effects both academic and personal. Not only does surrounding oneself with new people and learning how to work together as a group help one improve teamwork and relational skills, it reduces stereotypes, increases appreciation of other cultures, and works to allow young people to find others that they relate to.\n\nEyler and Giles noted that at the beginning and end of a college semester that included three hours of community service a week, students reported a much higher regard for cultural differences. At the end of the semester those who had participated in service-learning were noted as saying that the most important things that they had learned were not to judge others, and to appreciate every type of person because everyone shares some similar key characteristics.\n\nCommunity volunteer work has proven to be a powerful predictor in students' academic lives and college experience as a whole. Studies have shown that students who participate in community service as a part of their college course of study have a much higher correlation of completing their degree (Astin, 1992; Pascarella and Terenzini, 1991). In addition, college students who participate in community volunteer projects as a part of their college experience report finding a much greater relevance in their academic studies after completing community volunteer projects. According to University Health Services, studies have found that volunteering can positively impact a student's overall mental and emotional health.\n\nIn some European countries government organisations and non-government organisations provide auxiliary positions for a certain period in institutions like hospitals, schools, memorial sites and welfare institutions. The difference to other types of volunteering is that there are strict legal regulations, what organisation is allowed to engage volunteers and about the period a volunteer is allowed to work in a voluntary position. Due to that fact, the volunteer is getting a limited amount as a pocket money from the government. An organization having one of the biggest manpower in Europe is the German Federal volunteers service (Bundesfreiwilligendienst), that was founded in 2011, by having more than 35.000 federal volunteers in 2012. A much older institution is the Voluntary social year (Freiwilliges Soziales Jahr) in Austria and Germany.\n\nSochi Olympics 25,000 volunteers worked at the 2014 Sochi Winter Olympics. They supported the organisers in more than 20 functional areas: meeting guests, assisting navigation, organising the opening and closing ceremonies, organising food outlets, etc. Volunteer applications were open to any nationals of Russia and other countries. The Sochi 2014 Organising Committee received about 200,000 applications, 8 applicants per place. Volunteers received training over the course of more than a year at 26 volunteer centres in 17 cities across Russia. The majority of participants were between 17 and 22 years old. At the same time, 3000 applications were submitted from people over 55 years old. Some of them worked as volunteers during the 1980 Olympics in Moscow. It was the first experience with such a large-scale volunteer program in the contemporary Russia.\n\nFor the first time in its history, Russia will hosted the FIFA World Cup from 14 June till 15 July 2018. Moreover, it was the first time the World Cup games was be played both in Europe and Asia. The games were be hosted by 12 stadiums in 11 Russian cities.\n\nThe volunteer program of the 2018 FIFA World Cup has engaged thousands of people from Russia and other countries around the world.\n\nThe program included several stages: recruitment, selection and training of volunteers, organisation of their work during the championship. The recruitment of volunteers for the FIFA Confederations Cup and the FIFA World Cup via FIFA.com started on 1 June 2016 and closed on 30 December 2016. Some of the volunteers worked at the 2017 FIFA Confederations Cup: 1733 people assisted the organisers in Saint Petersburg, 1590 worked in Moscow, 1261 in Sochi, 1260 in Kazan, a total of 5844 participants.\n\nThe FIFA World Cup will be supported by 17,040 volunteers of the Russia 2018 Local Organising Committee.\n\nCandidates living in Russia were selected by 15 volunteer centres in the host cities based in some of Russia's leading higher educational institutions: Synergy University, Moscow State Institute of International Relations, Plekhanov Russian University of Economics, Russian State Social University, Moscow Automobile and Road Construction University, Saint Petersburg State University of Economics, Samara State University, Volga Region State Academy of Physical Culture, Sport and Tourism, Don State Technical University, Ogarev Mordovia State University, Volgograd State University, State University of Nizhny Novgorod, Samara State Aerospace University, Immanuel Kant Baltic Federal University, and Ural Federal University.\n\nCandidates from other countries were selected remotely.\n\nCandidates had to be at least 18 years old, have a good knowledge of English, have a higher or vocational secondary education, and possess teamwork skills.\n\nVolunteers were trained remotely, in volunteer centres and at World Cup venues.\n\nVolunteers will be providing assistance in a variety of areas:\n\n\nTheir work started ahead of the events: on 10 May 2017 for the 2017 FIFA Confederations Cup, and on 10 May 2018 for the 2018 FIFA World Cup.\n\nOn 20 October 2017, the Russian National Competition of Important Social Projects \"Legacy of 2018 FIFA World Cup Volunteer Program\" was launched. The competition has engaged about 1500 people: applicants to the 2018 FIFA World Cup volunteer program and future city volunteers.\n\nThe idea of the competition was that anyone could submit a project that would draw the attention of Russian cities residents to the FIFA World Cup in Russia and leave a legacy after the championship was over.\n\nThe project was expected to produce tangible (work of art, place of attraction for guests and residents in the city, open playground, graffiti, developed areas in city parks, films, etc.) or intangible (events, conferences, festivals, exhibitions) legacy.\n\n26 projects qualified to the final and were supported by the Russia 2018 Local Organising Committee and the host cities of the 2018 FIFA World Cup. The jury included the General Director of the Russia 2018 Local Organising Committee Alexey Sorokin, Ambassador of the 2018 FIFA World Cup in Russia Alexey Smertin and Advisor to the Head of the Federal Tourism Agency Svetlana Sergeeva.\n\nSome of the projects were combined or further developed by the Local Organising Committee.\n\nAmong the projects were: Football Championship for Moms, Ramp Production out of Recycled Plastic, Your Championship Sticker Packs, etc.\n\nDesignated days, weeks and years observed by a country or as designated by the United Nations to encourage volunteering / community service \n\nModern societies share a common value of people helping each other; not only do volunteer acts assist others, but they also benefit the volunteering individual on a personal level. Despite having similar objectives, tension can arise between volunteers and state-provided services. In order to curtail this tension, most countries develop policies and enact legislation to clarify the roles and relationships among governmental stakeholders and their voluntary counterparts; this regulation identifies and allocates the necessary legal, social, administrative, and financial support of each party. This is particularly necessary when some voluntary activities are seen as a challenge to the authority of the state(e.g., on 29 January 2001, President Bush cautioned that volunteer groups should supplement—not replace—government agencies’ work).\n\nVolunteering that benefits the state but challenges paid counterparts angers labor unions that represent those who are paid for their volunteer work; this is particularly seen in combination departments, such as volunteer fire departments.\n\nDifficulties in the cross-national aid model of volunteering can arise when it is applied across national borders. The presence of volunteers who are sent from one state to another can be viewed as a breach of sovereignty and showing a lack of respect towards the national government of the proposed recipients. Thus, motivations are important when states negotiate offers to send aid and when these proposals are accepted, particularly if donors may postpone assistance or stop it altogether. Three types of conditionality have evolved:\n\nSome international volunteer organizations define their primary mission as being altruistic: to fight poverty and improve the living standards of people in the developing world, (e.g. Voluntary Services Overseas has almost 2,000 skilled professionals working as volunteers to pass on their expertise to local people so that the volunteers' skills remain long after they return home). When these organizations work in partnership with governments, the results can be impressive. However, when other organizations or individual First World governments support the work of volunteer groups, there can be questions as to whether the organizations' or governments' real motives are poverty alleviation. Instead, a focus on creating wealth for some of the poor or developing policies intended to benefit the donor states is sometimes reported. Many low-income countries’ economies suffer from industrialization without prosperity and investment without growth. One reason for this is that development assistance guides many Third World governments to pursue development policies that have been wasteful, ill-conceived, or unproductive; some of these policies have been so destructive that the economies could not have been sustained without outside support.\n\nIndeed, some offers of aid have distorted the general spirit of volunteering, treating local voluntary action as contributions in kind, i.e., existing conditions requiring the modification of local people's behavior in order for them to earn the right to donors’ charity. This can be seen as patronizing and offensive to the recipients because the aid expressly serves the policy aims of the donors rather than the needs of the recipients.\n\nBased on a case study in China, Xu and Ngai (2011) revealed that the developing grassroots volunteerism can be an enclave among various organizations and may be able to work toward the development of civil society in the developing countries. The researchers developed a \"Moral Resources and Political Capital\" approach to examine the contributions of volunteerism in promoting the civil society. Moral resource means the available morals could be chosen by NGOs. Political capital means the capital that will improve or enhance the NGOs’ status, possession or access in the existing political system.\n\nMoreover, Xu and Ngai (2011) distinguished two types of Moral Resources: Moral Resource-I and Moral Resource-II (ibid).\n\nThanks to the intellectual heritage of Blau and Duncan (1967), two types of political capital were identified:\nObviously, \"Moral resource-I itself contains the self-determination that gives participants confidence in the ethical beliefs they have chosen\", almost any organizations may have Moral Resource-I, while not all of them have the societal recognized Moral Resource-II. However, the voluntary service organizations predominantly occupy Moral Resource-II because a sense of moral superiority makes it possible that for parties with different values, goals and cultures to work together in promoting the promotion of volunteering. Thus the voluntary service organizations are likely to win the trust and support of the masses as well as the government more easily than will the organizations whose morals are not accepted by mainstream society. In other words, Moral Resource II helps the grassroots organizations with little Political Capital I to win Political Capital-II, which is a crucial factor for their survival and growth in developing countries such as China. Therefore, the voluntary service realm could be an enclave of the development of civil society in the developing nations.\n\nVolunteering has the ability to improve the quality of life and health including longevity of those who donate their time and research has found that older adults will benefit the most from volunteering. Physical and mental ailments plaguing older adults can be healed through the simple act of helping others; however, one must be performing the good deed from a selfless nature. There are barriers that can prevent older adults from participating in volunteer work, such as socio-economic status, opinions held by others, and even current health issues. However, these barriers can be overcome so that if one would like to be involved in volunteer work they can do so. Volunteering improves not only the communities in which one serves, but also the life of the individual who is providing help to the community.\n\nVolunteering is known not only to be related to happiness but also to increase happiness. Also, giving help was a more important benefit of better reported mental health than receiving help. Studies have also shown that volunteering can cause a decrease in loneliness for those volunteering as well as those for whom people volunteer.\n\nIn the United States, statistics on volunteering have historically been limited, according to volunteerism expert Susan J. Ellis. In 2013, the U.S. Current Population Survey (US) included a volunteering supplement which produced statistics on volunteering.\n\nIn the 1960s, Ivan Illich offered an analysis of the role of American volunteers in Mexico in his speech entitled \"To Hell With Good Intentions\". His concerns, along with those of critics such as Paulo Freire and Edward Said, revolve around the notion of altruism as an extension of Christian missionary ideology. In addition, he mentions the sense of responsibility/obligation as a factor, which drives the concept of noblesse oblige—first developed by the French aristocracy as a moral duty derived from their wealth. Simply stated, these apprehensions propose the extension of power and authority over indigenous cultures around the world. Recent critiques of volunteering come from Westmier and Kahn (1996) and bell hooks (née Gloria Watkins) (2004). Also, Georgeou (2012) has critiqued the impact of neoliberalism on international aid volunteering.\n\nThe field of the medical tourism (referring to volunteers who travel overseas to deliver medical care) has recently attracted negative criticism when compared to the alternative notion of sustainable capacities, i.e., work done in the context of long-term, locally-run, and foreign-supported infrastructures. A preponderance of this criticism appears largely in scientific and peer-reviewed literature. Recently, media outlets with more general readerships have published such criticisms as well.\n\nAnother problem noted with volunteering is that it can be used to replace low paid entry positions. This can act to decrease social mobility, with only those capable of affording to work without payment able to gain the experience. Trade unions in the United Kingdom have warned that long term volunteering is a form of expoitation, used by charities to avoid minimum wage legislation. Some sectors now expect candidates for paid roles to have undergone significant periods of volunteer experience whether relevant to the role or not, setting up 'Volunteer Credentialism'\n\n\n\n\n"}
{"id": "42976903", "url": "https://en.wikipedia.org/wiki?curid=42976903", "title": "Dictatorship of the proletariat", "text": "Dictatorship of the proletariat\n\nIn Marxist philosophy, the dictatorship of the proletariat is a state of affairs in which the working class hold political power. Proletarian dictatorship is the intermediate stage between a capitalist economy and a communist economy, whereby the government nationalises ownership of the means of production from private to collective ownership. The socialist revolutionary Joseph Weydemeyer coined the term \"dictatorship of the proletariat\", which Karl Marx and Friedrich Engels adopted to their philosophy and economics. The Paris Commune (1871), which controlled the capital city for two months, before being suppressed, was an example of the dictatorship of the proletariat. In Marxist philosophy, the term \"Dictatorship of the bourgeoisie\" is the antonym to \"dictatorship of the proletariat\".\n\nThe term \"dictatorship\" indicates the retention of the state apparatus, but differs from individual dictatorship, the rule of one man. The term \"dictatorship of the proletariat\" implies the complete \"socialization of the major means of production\", the planning of material production in service to the social and economic needs of the population, such as the right to work, education, health and welfare services, public housing. \n\nThere are multiple popular trends for this political thought, all of which believe the state will be retained post-revolution for its enforcement capabilities:\n\nIn \"The Road to Serfdom\" (1944), the neoliberal economist Friedrich Hayek wrote that the dictatorship of the proletariat likely would destroy personal freedom as completely as does an autocracy. The European Commission of Human Rights found pursuing the dictatorship of the proletariat incompatible with the European Convention on Human Rights in \"Communist Party of Germany v. the Federal Republic of Germany\" (1957).\n\nKarl Marx did not write much about the nature of the dictatorship of the proletariat, but in \"The Communist Manifesto\" (1848) he and Engels said that \"their ends can be attained only by the forcible overthrow of all existing social conditions\". In light of the Hungarian Revolution of 1848, Marx said that \"there is only one way in which the murderous death agonies of the old society and the bloody birth throes of the new society can be shortened, simplified and concentrated, and that way is revolutionary terror\".\n\nOn 1 January 1852, the communist journalist Joseph Weydemeyer published an article entitled \"Dictatorship of the Proletariat\" in the German language newspaper \"Turn-Zeitung\", where he wrote that \"it is quite plain that there cannot be here any question of gradual, peaceful transitions\" and recalled the examples of Oliver Cromwell (England) and Committee of Public Safety (France) as examples of \"dictatorship\" and \"terrorism\" (respectively) required to overthrow the bourgeoisie. In that year, Marx wrote to him, saying: \n\nMarx expanded upon his ideas about the dictatorship of the proletariat in his short 1875 work, \"Critique of the Gotha Program\", a scathing criticism and attack on the principles laid out in the programme of the German Workers' Party (predecessor to the Social Democratic Party of Germany). The programme presented a moderate, evolutionary way to socialism as opposed to revolutionary, violent approach of the \"orthodox\" Marxists. As a result the latter accused the Gotha program as being \"revisionist\" and ineffective. Nevertheless, he allowed for the possibility of a peaceful transition in some countries with strong democratic institutional structures (such as the case of the Great Britain, the US, and the Netherlands), suggesting however that in other countries in which workers can not \"attain their goal by peaceful means\" the \"lever of our revolution must be force\", on the principle that the working people had the right to revolt if they were denied political expression.\n\nMarx stated that in a proletarian-run society the state should control the \"proceeds of labour\" (i.e. all the food and products produced) and take from them that which was \"an economic necessity\", namely enough to replace \"the means of production used up\", an \"additional portion for expansion of production\" and \"insurance funds\" to be used in emergencies such as natural disasters. Furthermore, he believed that the state should then take enough to cover administrative costs, funds for the running of public services and funds for those who were physically incapable of working. Once enough to cover all of these things had been taken out of the \"proceeds of labour\", Marx believed that what was left should then be shared out amongst the workers, with each individual getting goods to the equivalent value of how much labour they had invested. In this meritocratic manner, those workers who put in more labour and worked harder would get more of the proceeds of the collective labour than someone who had not worked as hard.\n\nIn the \"Critique\", he noted that \"defects are inevitable\" and there would be many difficulties in initially running such a workers' state \"as it emerges from capitalistic society\" because it would be \"economically, morally and intellectually... still stamped with the birth marks of the old society from whose womb it emerges\", thereby still containing capitalist elements.\n\nIn other works, Marx stated that he considered the Paris Commune (a revolutionary socialism supporting government that ran the city of Paris from March to May 1871) as an example of the proletarian dictatorship. Describing the short-lived regime, he remarked: The Commune was formed of the municipal councilors, chosen by universal suffrage in the various wards of the town, responsible, and revocable at short terms. The majority of its members were naturally workers, or acknowledged representatives of the working class. The Commune was to be a working, not a parliamentary body, executive, and legislative at the same time.\n\nThis form of popular government, featuring revocable election of councilors and maximal public participation in governance, resembles contemporary direct democracy.\n\nForce and violence played an important role in Friedrich Engels's vision of the revolution and rule of proletariat. In 1877, arguing with Eugen Dühring, Engels ridiculed his reservations against use of force: \n\nIn the 1891 postscript to \"The Civil War in France\" (1872) pamphlet, Engels said: \"Well and good, gentlemen, do you want to know what this dictatorship looks like? Look at the Paris Commune. That was the Dictatorship of the Proletariat\"; to avoid bourgeois political corruption: [...] the Commune made use of two infallible expedients. In this first place, it filled all posts—administrative, judicial, and educational—by election on the basis of universal suffrage of all concerned, with the right of the same electors to recall their delegate at any time. And, in the second place, all officials, high or low, were paid only the wages received by other workers. The highest salary paid by the Commune to anyone was 6,000 francs. In this way an effective barrier to place-hunting and careerism was set up, even apart from the binding mandates to delegates [and] to representative bodies, which were also added in profusion.\n\nIn the same year, he criticised \"anti-authoritarian socialists\", again referring to the methods of the Paris Commune: \n\nMarx's attention to the Paris Commune placed the commune in the centre of later Marxist forms.\n\nThis statement was written in \"Address of the Central Committee to the Communist League\", which is credited to Marx and Engels: \n\nIn the 20th century, Vladimir Lenin developed Leninism—the adaptation of Marxism to the socio-economic and political conditions of Imperial Russia (1721–1917). This body of theory later became the official ideology of some Communist states.\n\n\"The State and Revolution\" (1917) explicitly discusses the practical implementation of \"dictatorship of the proletariat\" through means of violent revolution. Lenin denies any reformist interpretations of Marxism, such as the one of Karl Kautsky's. Lenin especially focused on Engels' phrase of the state \"withering away\", denying that it could apply to \"bourgeois state\" and highlighting that Engels work is mostly \"panegyric on violent revolution\". Based on these arguments, he denounces reformists as \"opportunistic\", reactionary and points out the red terror as the only method of introducing dictatorship of the proletariat compliant with Marx and Engels work.\n\nIn Imperial Russia, the Paris Commune model form of government was realised in the soviets (councils of workers and soldiers) established in the Russian Revolution of 1905, whose revolutionary task was deposing the capitalist (monarchical) state to establish socialism—the dictatorship of the proletariat—the stage preceding communism.\n\nIn Russia, the Bolshevik Party (described by Lenin as the \"vanguard of the proletariat\") elevated the soviets to power in the October Revolution of 1917. Throughout 1917, Lenin argued that the Russian Provisional Government was unrepresentative of the proletariat's interests because in his estimation they represented the \"dictatorship of the bourgeoisie\". He argued that because they continually put off democratic elections, they denied the prominence of the democratically constituted soviets and all the promises made by liberal bourgeois parties prior to the February Revolution remained unfulfilled, the soviets would need to take power for themselves.\n\nLenin argued that in an underdeveloped country such as Russia the capitalist class would remain a threat even after a successful socialist revolution. As a result, he advocated the repression of those elements of the capitalist class that took up arms against the new soviet government, writing that as long as classes existed a state would need to exist to exercise the democratic rule of one class (in his view, the working class) over the other (the capitalist class). He said: \n\nThe use of violence, terror and rule of single communist party was criticised by Karl Kautsky, Rosa Luxemburg and Mikhail Bakunin. In response, Lenin accused Kautsky of being a \"renegade\" and \"liberal\" and these socialist movements that did not support the Bolshevik party line were condemned by the Communist International and called social fascism.\n\nSoviet democracy granted voting rights to the majority of the populace who elected the local soviets, who elected the regional soviets and so on until electing the Supreme Soviet of the Soviet Union. Capitalists were disenfranchised in the Russian soviet model. However, according to Lenin in a developed country it would be possible to dispense with the disenfranchisement of capitalists within the democratic proletarian dictatorship as the proletariat would be guaranteed of an overwhelming majority.\n\nThe Bolsheviks in 1917–1924 did not claim to have achieved a communist society. In contrast the preamble to the 1977 Constitution (Fundamental Law) of the Union of Soviet Socialist Republics (the \"Brezhnev Constitution\"), stated that the 1917 Revolution established the dictatorship of the proletariat as \"a society of true democracy\" and that \"the supreme goal of the Soviet state is the building of a classless, communist society in which there will be public, communist self-government\".\n\nDuring the Russian Civil War (1918–1922), all the major opposition parties either took up arms against the new Soviet government, took part in sabotage, collaboration with the deposed Tsarists, or made assassination attempts against Lenin and other Bolshevik leaders. When opposition parties such as the Cadets and Mensheviks were democratically elected to the Soviets in some areas, they proceeded to use their mandate to welcome in Tsarist and foreign capitalist military forces. In one incident in Baku, the British military, once invited in, proceeded to execute members of the Bolshevik Party (who had peacefully stood down from the Soviet when they failed to win the elections). As a result, the Bolsheviks banned each opposition party when it turned against the Soviet government. In some cases, bans were lifted. This banning of parties did not have the same repressive character as later bans under Stalin would.\n\nInternally, Lenin's critics argued that such political suppression always was his plan. Supporters argued that the reactionary civil war of the foreign-sponsored White movement required it—given Fanya Kaplan's unsuccessful assassination of Lenin on 30 August 1918 and the successful assassination of Moisei Uritsky the same day.\n\nAfter 1919, the Soviets had ceased to function as organs of democratic rule as the famine induced by forced grain requisitions led to the Soviets emptying out of ordinary people. Half the population of Moscow and a third of Petrograd had by this stage fled to the countryside to find food and political life ground to a halt.\n\nThe Bolsheviks became concerned that under these conditions—the absence of mass participation in political life and the banning of opposition parties—counter-revolutionary forces would express themselves within the Bolshevik Party itself (some evidence existed for this in the mass of ex opposition party members who signed up for Bolshevik membership immediately after the end of the Civil War).\n\nDespite the principle of democratic centralism in the Bolshevik Party, internal factions were banned. This was considered an extreme measure and did not fall within Marxist doctrine. The ban remained until the Soviet Union's dissolution in 1991. In 1921, vigorous internal debate and freedom of opinion were still present within Russia and the beginnings of censorship and mass political repression had not yet emerged. For example, the Workers Opposition faction continued to operate despite being nominally dissolved. The debates of the Communist Party of the Soviet Union continued to be published until 1923.\n\nElements of the later censorship and attacks on political expression would appear during Lenin's illness and after his death, when members of the future Stalinist clique clamped down on party democracy among the Georgian Bolsheviks and began to censor material. \"Pravda\" ceased publishing the opinions of political oppositions after 1924 and at the same time, the ruling clique (Grigory Zinoviev, Lev Kamenev and Joseph Stalin) admitted large numbers of new members into the party in order to shout down the voices of oppositionists at party meetings, severely curtailing internal debate. Their policies were partly directed by the interests of the new bureaucracy that had accumulated a great deal of social weight in the absence of an active participation in politics by the majority of people. By 1927, many supporters of the Left Opposition began to face political repression and Leon Trotsky was exiled.\n\nSome modern critics of the concept of the \"dictatorship of the proletariat\"—including various anti-communists, libertarian Marxists, anarcho-communists and anti-Stalinist communists and socialists—argue that the Stalinist Soviet Union and other Stalinist countries used the \"dictatorship of the proletariat\" to justify the monopolisation of political power by a new ruling layer of bureaucrats, derived partly from the old Tsarist bureaucracy and partly created by the impoverished condition of Russia.\n\nHowever, the rising Stalinist clique rested on other grounds for political legitimacy rather than a confusion between the modern and Marxist use of the term \"dictatorship\". Rather, they took the line that since they were the vanguard of the proletariat, their right to rule could not be legitimately questioned. Hence, opposition parties could not be permitted to exist. From 1936 onward, Stalinist-inspired state constitutions enshrined this concept by giving the various communist parties a \"leading role\" in society—a provision that was interpreted to either ban other parties altogether or force them to accept the Stalinists guaranteed right to rule as a condition of being allowed to exist.\n\nThis justification was adopted by subsequent communist parties that built upon the Stalinist model, such as the ones in China, North Korea, Vietnam and Cuba (initially the 26th of July Movement).\n\nAt the 22nd Congress of the Communist Party of the Soviet Union, Nikita Khrushchev declared an end to the \"dictatorship of the proletariat\" and the establishment of the \"all people's government\".\n\n\n"}
{"id": "39353050", "url": "https://en.wikipedia.org/wiki?curid=39353050", "title": "International Workers' Day", "text": "International Workers' Day\n\nInternational Workers' Day, also known as Workers' Day, Labour Day in some countries and often referred to as May Day, is a celebration of labourers and the working classes that is promoted by the international labour movement which occurs every year on May Day (1 May), an ancient European spring festival.\n\nThe date was chosen by a pan-national organization of socialist and communist political parties to commemorate the Haymarket affair, which occurred in Chicago on 4 May 1886. The 1904 Sixth Conference of the Second International, called on \"all Social Democratic Party organisations and trade unions of all countries to demonstrate energetically on the First of May for the legal establishment of the 8-hour day, for the class demands of the proletariat, and for universal peace.\"\n\nThe first of May is a national, public holiday in many countries across the world, in most cases as \"Labour Day\", \"International Workers' Day\" or some similar name – although some countries celebrate a Labour Day on other dates significant to them, such as the United States and Canada, which celebrate Labor Day on the first Monday of September.\nBeginning in the late 19th century, as the trade union and labour movements grew, a variety of days were chosen by trade unionists as a day to celebrate labour. 1 May was chosen to be International Workers' Day to commemorate the 1886 Haymarket affair in Chicago. In that year beginning on 1 May, there was a general strike for the eight-hour workday. On 4 May, the police acted to disperse a public assembly in support of the strike when an unidentified person threw a bomb. The police responded by firing on the workers. The event led to the deaths of seven police officers and at least four civilians; sixty police officers were injured, as were an unknown number of civilians. Hundreds of labour leaders and sympathizers were later rounded-up and four were executed by hanging, after a trial that was seen as a miscarriage of justice. The following day on 5 May in Milwaukee Wisconsin, the state militia fired on a crowd of strikers killing seven, including a schoolboy and a man feeding chickens in his yard.\n\nIn 1889, a meeting in Paris was held by the first congress of the Second International, following a proposal by that called for international demonstrations on the 1890 anniversary of the Chicago protests. May Day was formally recognized as an annual event at the International's second congress in 1891. Subsequently, the May Day riots of 1894 occurred. The International Socialist Congress, Amsterdam 1904 called on \"all Social Democratic Party organisations and trade unions of all countries to demonstrate energetically on the First of May for the legal establishment of the 8-hour day, for the class demands of the proletariat, and for universal peace.\" The congress made it \"mandatory upon the proletarian organisations of all countries to stop work on 1 May, wherever it is possible without injury to the workers.\"\n\nIn the United States and Canada, a September holiday, called Labor or Labour Day, was first proposed in the 1880s. In 1882, Matthew Maguire, a machinist, first proposed a Labor Day holiday on the first Monday of September while serving as secretary of the Central Labor Union (CLU) of New York. Others argue that it was first proposed by Peter J. McGuire of the American Federation of Labor in May 1882, after witnessing the annual labour festival held in Toronto, Canada. In 1887, Oregon was the first state of the United States to make it an official public holiday. By the time it became an official federal holiday in 1894, thirty US states officially celebrated Labor Day. Thus by 1887 in North America, Labour Day was an established, official holiday but in September, not on 1 May.\n\nMay Day has been a focal point for demonstrations by various socialist, communist and anarchist groups since the Second International. May Day is one of the most important holidays in communist countries such as the North Korea, Cuba and the former Soviet Union countries. May Day celebrations in these countries typically feature elaborate workforce parades, including displays of military hardware and soldiers.\n\nIn 1955, the Catholic Church dedicated 1 May to \"Saint Joseph the Worker\". Saint Joseph is the patron saint of workers and craftsmen, among others.\n\nDuring the Cold War, May Day became the occasion for large military parades in Red Square by the Soviet Union and attended by the top leaders of the Kremlin, especially the Politburo, atop Lenin's Mausoleum. It became an enduring symbol of that period.\n\nToday, the majority of countries around the world celebrate a workers' day on 1 May.\n\nIn Algeria, 1 May is a public holiday celebrated as Labour Day.\n\n1 May is recognized as public holiday in Angola and called Workers' Day.\n\n1 May is known as Labour Day and is considered a paid holiday. The President of Egypt traditionally presides over the official May Day celebrations.\n\nIn Ethiopia, 1 May is a public holiday and celebrated as Worker's Day.\n\n1 May is a holiday in Ghana. It is a day to celebrate all workers across the country. It is celebrated with a parade by trade unions and labour associations. The parades are normally addressed by the Secretary General of the trade union congress and by regional secretaries in the regions. Workers from different workplaces through banners and T-shirts identify their companies.\n\nIn Kenya, 1 May is a public holiday and celebrated as Labour Day. It is a big day addressed by the leaders of the workers' umbrella union body – the Central Organisation of Trade Unions (COTU). The Minister for Labour (and occasionally the President) address the workers. Each year, the government approves (and increases) the minimum wage on Labour Day.\n\nInternational Workers' Day was declared a national public holiday by the National Transitional Council in 2012 the first year of the post-Qaddafi era.\n\nOn 1 May 1978, then Libyan leader Colonel Mu'ammar Al-Qaddafi addressed the nation in the capital city of Tripoli calling for administrative and also economic reforms across Libya.\n\nIt is recognized as a public holiday on 1 May.\n\nCelebrates workers day on 1 May.\n\n1 May is recognized as public holiday in Namibia and celebrated as Workers' Day.\n\nSince 1981, 1 May is a public holiday in Nigeria. On the day, people gather while, traditionally, the president of the Nigeria Labour Congress and other politicians address workers.\n\nIn Somalia, 1 May is a public holiday and celebrated as the Labour Day.\n\nIn South Africa, Workers' Day has been celebrated as a national public holiday on 1 May each year since 1995. May Day started to get more attention by African workers in 1928, which saw thousands of workers in a mass march. In 1950, the South African Communist Party called for a strike on 1 May in response to the Suppression of Communism Act declaring it illegal. Police violence caused the death of 18 people across Soweto. It has its origins within the historical struggles of workers and their trade unions internationally for solidarity between working people in their struggles to win fair employment standards and more importantly, to establish a culture of human and worker rights and to ensure that these are enshrined in international law and the national law.\n\nIn 1986, the hundredth anniversary of the Haymarket affair, the Congress of South African Trade Unions (COSATU) called for the government to establish an official holiday on 1 May. It also called for workers to stay home from work that day. COSATU was joined by a number of prominent anti-apartheid organizations, including the National Education Crisis Committee and the United Democratic Front (South Africa). The call was also supported by a number of organizations regarded as conservative, such as the African Teachers Association, the National African Federated Chamber of Commerce, and the Steel and Engineering Industries Federation of South Africa, an organization that represented employers in the metal industries. More than 1,500,000 workers observed the call and stayed home, as did thousands of students, taxi drivers, vendors, shopkeepers, domestic workers, and self-employed people. In the following years, 1 May became a popular, if not official, holiday. As a result of the killings on May Day 1950 and the success of COSATU's call in 1986, 1 May became associated with resistance to the apartheid government. After its first universal election in 1994, 1 May was adopted as a public holiday, celebrated for the first time in 1995. On its website, the city of Durban states that the holiday \"celebrate[s] the role played by trade unions and other labour movements in the fight against South Africa's apartheid regime\".\n\nIn Tanzania, it is a public holiday on 1 May and celebrated as the Worker's Day.\n\n1 May is recognized as Labour Day within Tunisia.\n\nIn Uganda, Labour Day is a public holiday on 1 May.\n\n1 May is recognized as public holiday in Zimbabwe and called Workers' Day.\n\nIn Argentina, Workers' Day is an official holiday, and is frequently associated with the labour unions. Celebrations related to labour are held, including demonstrations in major cities.\n\nThe first Workers' Day celebration was in 1890, when Argentinian unions, controlled in those days by socialists and anarchists, organized several joint celebrations in Buenos Aires and other cities, at the same time that the international labour movement celebrated it for the first time. In 1930, it was established as an official holiday by the Radical Civic Union president Hipólito Yrigoyen. The day became particularly relevant during the worker-oriented government of Juan Domingo Perón (1946–55). He permitted and endorsed national recognition of the holiday during his tenure in office.\n\n1 May is known as Labour Day and is a holiday.\n\nIn Brazil, Workers' Day is an official holiday that is celebrated on 1 May, and unions commemorate it with day-long public events. It is also when salaries for most professional categories and the minimum wage are traditionally readjusted.\n\nIn Canada, Labour Day is celebrated in September. In 1894, the government of Prime Minister John Sparrow David Thompson declared the first Monday in September as Canada's official Labour Day. Labor Day in the United States is on the same day.\n\nMay Day is however marked by unions and leftists. It is celebrated on 1 May. May Day is an important day of trade union and community group protest in the province of Quebec (though not a provincial state holiday). Celebration of the International Labour Day (or \"International Workers' Day\"; ) in Montreal goes back to 1906, organized by the Mutual Aid circle. The tradition had a renaissance at the time of a mass strike in 1972. On the 1973 May Day, the first contemporary demonstration was organized by the major trade union confederations; over 30,000 trade unionists took part in this demonstration. Further, it is the customary date on which the minimum wage rises.\n\nPresident Carlos Ibáñez del Campo decreed 1 May a national holiday in 1931, in honour of the dignity of workers. All stores and public services must close for the entire day, and the major trade unions of Chile, represented in the national organization Workers' United Center of Chile (Central Unitaria de Trabajadores), organize rallies during the morning hours, with festivities and cookouts in the later part of the day, in all the major cities of Chile. During these rallies, representatives of the major left-wing political parties speak to the assemblies on the issues of the day concerning workers’ rights.\n\n1 May has long been recognized as Labour Day and almost all workers respect it as a national holiday. As in many other countries, it is common to see rallies by the trade unions in all over the main regional capitals of the country.\n\nFirst celebrated in 1913, labor day is a public holiday, and at the same time an important day for government activities. On this day, the President of Costa Rica gives a speech to the citizens and the legislature of Costa Rica about the duties that were undertaken through the previous year. The president of the legislature is also chosen by its members.\n\nThis day is known as Labour Day in Cuba. People march in the streets, showing their support to the Cuban Communist government and the Cuban Revolution during the whole morning.\n\n1 May is a national holiday known as Labour Day and celebrated by workers' parades and demonstration.\n\nIn Ecuador, 1 May is an official public holiday known as Labour Day. People don't go to work and spend time with their relatives or gather for demonstrations.\n\n1 May is an official public holiday known as Labour Day.\n\n1 May is an official public holiday known as Labour Day.\n\n1 May is an official public holiday known as Agriculture and Labour Day.\n\n1 May is an official holiday, known as \"Labour Day\" within Honduras.\n\n1 May is a federal holiday. It also commemorates the Cananea Strike of 1906 in the Mexican state of Sonora.\n\n1 May is an official public holiday, known as \"Labour Day\" within Panama.\n\n1 May is an official public holiday, known as \"Labour Day\" within Paraguay.\n\n1 May is an official public holiday, known as \"Labour Day\" within Peru.\n\nIn the United States, a \"Labor Day\", celebrated on the first Monday of each September was given increasing state recognition from 1887, and became an official federal holiday in 1894.\n\nEfforts to switch Labor Day from September to 1 May have not been successful.\n\nIn 1947, 1 May was established as Loyalty Day by the U.S. Veterans of Foreign Wars as a way to counter communist influence and recruitment at May Day rallies. Loyalty Day was celebrated across the country with patriotic parades and ceremonies, however the growing conflict over U.S. involvement in Vietnam detracted from the popularity of these celebrations. In 1958, the American Bar Association campaigned to have 1 May designated as Law Day, which was acknowledged in 1961 by a joint resolution of Congress. Law Day exercises, such as mock trials and courthouse tours, are often sponsored by the American Bar Association.\nUnions and radical organizations including anarchist groups and socialist and communist parties have kept the international May Day tradition alive with rallies and demonstrations. In 1919 the left mounted especially large demonstrations, and violence greeted the normally peaceful parades in Boston, New York, and Cleveland and a number of people were killed. In Milwaukee, an annual commemoration takes place at the site of the killing of seven workers during an 8 hour march. Some of the largest examples of this occurred during the Great Depression of the 1930s, when hundreds of thousands of workers marched in May Day parades in New York's Union Square, while cities like Chicago and Duluth saw large demonstrations organized by the communist party.\nIn 2006, 1 May was chosen by mostly Latino immigrant groups in the United States as the day for the Great American Boycott, a general strike of undocumented immigrant workers and supporters to protest H.R. 4437, immigration reform legislation that they felt was draconian.\nFrom 10 April to 1 May of that year, immigrant families in the U.S. called for immigrant rights, workers' rights and amnesty for undocumented workers. They were joined by socialist and other leftist organizations on 1 May. On 1 May 2007, a mostly peaceful demonstration in Los Angeles in support of undocumented immigrant workers ended with a widely televised dispersal by police officers. In March 2008, the International Longshore and Warehouse Union announced that dockworkers will move no cargo at any West Coast ports on 1 May 2008, as a protest against the continuation of the Iraq War and the diversion of resources from domestic needs.\n\nOn 1 May 2012, members of Occupy Wall Street and labor unions held protests together in a number of cities in the United States and Canada to commemorate May Day and to protest the state of the economy and economic inequality.\n\nOn 1 May 2017, immigrants' rights advocates, labor unions and leftists held protests against the immigration and economic policies of President Donald Trump in cities throughout the US, Chicago and Los Angeles having some of the largest marches.\n\nIn Uruguay, 1 May – Workers' Day – is an official holiday. Even when it is associated with labour unions, almost all workers tend to respect it. Since the late 1990s, the main event takes place at the First of May Square in Montevideo.\n\n1 May is an official holiday in Venezuela. is celebrated on 1 May in Venezuela since 1936, but from 1938 to 1945 it was held on 24 July, by an order of Eleazar López Contreras. However, Isaías Medina Angarita changed it back to 1 May in 1945.\n\nIn Bahrain, 1 May is known as Labour Day and is a public holiday.\n\nIn Bangladesh, 1 May Day is a public holiday and called May Day. A parade and other events are held on the day to commemorate the occasion.\n\nIn Cambodia, it is known as International Labour Day and is a public holiday. No marches for labour day were permitted in Cambodia for several years after the 2013 Cambodian general election and surrounding mass protetsts. A tightly controlled march on a limited scale was first permitted again in 2019.\n\n1 May is a statutory holiday in the People's Republic of China. Prior to 2008, it was a three-day holiday, but is now just the one day. However, it is usually supplemented by two other days to give the appearance of a three-day holiday, but not being statutory holidays the extra days have to be \"made up\" by working either the preceding or following weekend. For example, in 2013, 1 May fell on Wednesday. Most workplaces, including all government offices, took Monday 29 April, Tuesday 30 April, and Wednesday 1 May off. As the first two days were not statutory holidays they had to be \"made up\" by working the preceding weekend (27 and 28 April).\n\nInternational Worker's Day is also celebrated in China's special administrative regions. In Hong Kong, 1 May is known as Labour Day and has been considered a public holiday since 1999. In Macau, it is a public holiday and is officially known as (Portuguese for \"Workers' Day\").\n\nIn India, Labour Day is a public holiday held on every 1 May. The holiday is tied to labour movements for communist and socialist political parties. Labour Day is known as \"Uzhaipalar dhinam\" in Tamil and was first celebrated in Madras, \"Kamgar Din\" in Hindi, \"Karmikara Dinacharane\" in Kannada,\"Karmika Dinotsavam in Telugu, \"Kamgar Divas\" in Marathi, \"Thozhilaali Dinam\" in Malayalam and \"Shromik Dibosh\" in Bengali. Since Labour day is not a national holiday, Labour day is observed as public holiday at State Government's discretion. Many parts especially in North Indian States it's not a public holiday.\n\nThe first May Day celebration in India was organized in Madras (now Chennai) by the Labour Kisan Party of Hindustan on 1 May 1923. This was also the first time the red flag was used in India. The party leader Singaravelu Chettiar made arrangements to celebrate May Day in two places in 1923. One meeting was held at the beach opposite to the Madras High Court; the other meeting was held at the Triplicane beach. \"The Hindu\" newspaper, published from Madras reported,\n\nThe Labour Kisan party has introduced May Day celebrations in Madras. Comrade Singaravelar presided over the meeting. A resolution was passed stating that the government should declare May Day as a holiday. The president of the party explained the non-violent principles of the party. There was a request for financial aid. It was emphasised that workers of the world must unite to achieve independence.\n\n1 May is also celebrated as \"Maharashtra Day\" and \"Gujarat Day\" to mark the date in 1960, when the two western states attained statehood after the erstwhile Bombay State was divided on linguistic lines. Maharashtra Day is held at Shivaji Park in central Mumbai. Schools and offices in Maharashtra remain closed on 1 May. A similar parade is held to celebrate Gujarat Day in Gandhinagar.\n\nVaiko (Vai Gopalsamy), General Secretary of Marumalarchi Dravida Munnetra Kazhagam, appealed to the then Prime Minister V. P. Singh to declare 1 May as a national holiday, to which the PM heeded and from then on it became a national holiday to celebrate International Labour Day.\n\nMay Day (often referred locally as Labour Day) in Indonesia was first observed as a public holiday from 2014. Every year on the day, labourers take over the streets in major cities across the country, voicing their demands for better income & a supportive policy by the ministries.\n\nIn Iran, 1 May is known as the International Workers' Day. It is not a public holiday but according to article 63 of Iranian labour law on top of the official public holidays observed in the Islamic Republic of Iran, Labour Day shall be considered an official holiday for workers.\n\nIn Iraq, it is known as the International Workers' Day and is a public holiday.\n\nAfter varying popularity of labour day in the State of Israel, 1 May is not an official holiday. In the 1980s there were several large marches in Tel Aviv numbering as much as 350.000 in 1983 and a perhaps even more in 1988 but a steady decline in numbers led to only 5000 marchers in 2010. During the 1990s businesses began to treat it like a regular business day as the number of May Day related activities decreased and opened as usual.\n\nMay Day is not officially designated by the Japanese government as a national holiday, but as it lies between other national holidays, it is a day off work for the vast majority of Japanese workers. Many employers give it as a day off, and otherwise workers take it as \"paid leave\". 1 May occurs during \"Golden Week\", together with 29 April (\"Shōwa Day\"), 3 May (\"Constitution Memorial Day\"), 4 May (\"Greenery Day\") and 5 May (\"Children's Day\"). Workers generally take the day off work not so much to join street rallies or labour union gatherings, but more to go on holiday for several consecutive days (in Japanese corporate culture, taking weekdays off for personal pleasure is widely frowned upon).\n\nSome major labour unions organize rallies and demonstrations in Tokyo, Osaka, and Nagoya. Japan has a long history of labour activism and has had a communist and socialist party in the Diet since 1945. In 2008, the National Confederation of Trade Unions (\"Zenrōren\") held a rally in Yoyogi Park attended by 44,000 participants, while the National Trade Unions Council (\"Zenrōkyō\") held its May Day rally at Hibiya Park. \"Rengō\", the largest Japanese trade union, held its May Day rally on the following Saturday (3 May), allegedly to distance itself from the more radical labour unions.\n\n1 May is known as Labour Day and is a public holiday.\n\n1 May known as the Workers' Day and is a public holiday. Left-wing parties and workers' unions organize marches on the 1st of May.\n\nMalaysia began observing the holiday in 1972 following an announcement by the late Malaysian Deputy Prime Minister, Ismail Abdul Rahman.\n\nMaldives first observed the holiday in 2011, after a declaration by President Mohamed Nasheed. He noted that this move highlighted the government’s commitment as well as efforts of private parties to protect and promote workers’ rights in the Maldives.\n\nIn Myanmar, 1 May is known as Labour Day () and is a public holiday.\n\n1 May Day has been celebrated in Nepal since 1963. The day became a public holiday in 2007.\n\nIn the Democratic People's Republic of Korea, 1 May is known as International Workers' Day, and is a public holiday. Celebrations, local meetings and rallies are held every year throughout the country to honor the holiday. The Rungnado May Day Stadium in the capital of Pyongyang is named in honor of the holiday.\n\nInternational Labour Day is observed in Pakistan on 1 May to commemorate the social and economic achievements of workers. It is a public and national holiday. Many organized street demonstrations take place on Labor Day, where workers and labor unions protest against labor repression and demand for more rights, better wages and benefits.\n\n1 May is known as Labour Day and is a public holiday.\n\n1 May is known as Labor Day () and is a public holiday in the Philippines. On this day, labour organizations and unions hold protests in major cities. On 1 May 1903, during the American colonial period the (Filipino Democratic Labor Union) held a 100,000-person rally in front of the Malacañang Palace demanding workers' economic rights and Philippine independence. Ten years later, the first official celebration was held on 1 May 1913 when 36 labour unions convened for a congress in Manila. On 1 May 2001, a mass demonstration occurred near Malacañang Palace known as EDSA 3 or 1 May Riots.\n\nDuring the Presidency of Gloria Macapagal-Arroyo, a policy was adopted called holiday economics policy that moved holidays to either a Monday or a Friday to create a long weekend of three days. In 2002, Labor Day was moved to the Monday nearest to 1 May. Labour groups protested, as they accused the Arroyo administration of belittling the holiday. By 2008, Labor Day was excluded in the holiday economics policy, returning the commemorations to 1 May, no matter what day of the week it falls on.\n\nIn Singapore, it is known as Labour Day and is a public holiday.\n\nIn the Republic of Korea, 1 May is known simply as \"Workers' Day\". It is not a public holiday, but a paid holiday for workers by the .\n\nIn Sri Lanka, it is observed on 1 May and is a government and public holiday. The government has held official May Day celebrations in major towns and cities, with the largest being in the capital, Colombo. During celebrations, it is common to witness party leaders greeting the crowds. Workers frequently carry banners with political slogans and many parties decorate their vehicles.\n\n1 May is known as Labour Day and is a public holiday.\n\nMay 1 is known as Labor Day in Taiwan, an official holiday, though not everybody gets a day off. Students and teachers do not have this day off.\n\nIn Thailand, the day is known in English as National Labour Day, and is one of 17 official public holidays in Thailand.\n\nIn Vietnam, it is known as International Labour Day () and is a public holiday. It was first celebrated in 1913.\n\nEastern Bloc countries such as the Soviet Union and most countries of central and eastern Europe that were under the rule of communist governments held official May Day celebrations in every town and city, during which party leaders greeted the crowds. Workers carried banners with political slogans and many companies decorated their company cars. The biggest celebration of 1 May usually occurred in the capital of a particular communist country and usually included a military display and the presence of the president and the secretary general of the party. In Poland, since 1982, party leaders led the official parades. In Hungary, May Day was officially celebrated under the communist regime, and remains a public holiday. Traditionally, the day was marked by dancing around designated \"May trees\". Some factories in communist countries were named in honour of International Workers' Day, such as 1 Maja Coal Mine in , Poland.\n\nLabour Day () is an official holiday celebrated on 1 May and thus schools and most businesses are closed.\n\nLabour Day (, ) is an official holiday celebrated on 1 May.\n\nLabour Day (), officially called (state's holiday), is a public holiday in Austria. Left parties, especially social democrats organize celebrations with marches and speeches in all major cities. In smaller towns and villages those marches are held the night before.\n\nIn Belgium, Labour Day (, , , ), is observed on 1 May and is an official holiday since 1948. Various socialist and communist organizations hold parades and other events in different cities.\n\nIn Bosnia and Herzegovina, 1 and 2 May (Bosnian and / , ) are an official holiday and day-off for public bodies and schools at the national level. Most people celebrate this holiday by visiting natural parks and resorts. Additionally, in some places public events are organized. In its capital city, Sarajevo, 12 and 13 June are also celebrated as Labour day due to its many natural parks and springs.\n\nLabour Day is one of the public holidays in Bulgaria, where it is known as \"Labour Day and International Workers' Solidarity Day\" (). The first attempt to celebrate it was in 1890 by the Bulgarian Typographical Association. In 1939, Labour Day was declared an official holiday. Since 1945 the communist authorities in the People's Republic of Bulgaria began to celebrate the holiday every year. After the end of socialism in Bulgaria in 1989 Labour Day continues to be an official and public holiday, but state authorities are not committed to the organization of mass events. It is celebrated annually on 1 May.\n\nIn Croatia, 1 May is a national holiday. Many public events are organized and held all over the country where bean soup is given out to all people as a symbol of a real workers' dish. Red carnations are also handed out to symbolise the origin of the day. In Zagreb, the capital, a major gathering is in Maksimir Park, which is located in the east part of Zagreb. In Split, the largest city on the coast, people go to Marjan, a park-forest at the western end of Split peninsula. During World War II in Yugoslavia in the Independent State of Croatia under the Ustashe fascist regime it was marked in honor of labour, social justice and solidarity of workers.\n\nIn Cyprus, 1 May () is considered as an official Public Holiday (Labour Day). In general, all stores remain closed in public and private sector. The Labor Union and Syndicates celebrate with various festivals and events across the country.\n\nIn the Czech Republic, 1 May is an official and national holiday known as Labour Day ().\n\nIn Denmark, 1 May is not an official holiday, but a variety of individuals, mostly in the public sector, construction industry, and production industry, get a half or a whole day off. It was first celebrated in Copenhagen in 1890. The location of the first celebration, the Fælledparken, still plays an important part today with speeches by politicians and trade unionists to mark the occasion. Many other events are also held around the country to commemorate the day.\n\nIn Estonia, 1 May is a public holiday and celebrated as Disc golf day. A national holiday, celebrated with marching bands, parades and everybody plays disc golf and drink large amounts of local beer\n\nIn Finland, 1 May is an official and national holiday. Apart from Workers' Day (officially: , \"day of Finnish labour\"), it is also celebrated as a feast of students, and spring, called or Walpurgis Night.\n\nIn France, 1 May is a public holiday. It is, in fact, the only day of the year when employees are legally obliged to be given leave, save professions that cannot be interrupted due to their nature (such as workers in hospitals and public transport). Demonstrations and marches are a Labour Day tradition in France, where trade unions organize parades in major cities to defend workers' rights.\nIt is also customary to offer a lily of the valley to friends or family. This custom dates back to 1561, when king Charles IX, aged 10, waiting for his accession to the throne, gave a lily of the valley to all ladies present. Today, the fiscal administration exempts individuals and workers' organizations from any tax or administrative duties related to the sales of lilies of the valley, provided they are gathered from the wild, and not bought to be resold.\n\nIn April 1933, the recently installed Nazi government declared 1 May the \"Day of National Work\", an official state holiday, and announced that all celebrations were to be organized by the government. Any separate celebrations by communists, social democrats or labour unions were banned. After World War II, 1 May remained a state holiday in both East and West Germany. In communist East Germany, workers were \"de facto\" required to participate in large state-organized parades on May Day. Today in Germany it is simply called \"Labour Day\" (), and there are numerous demonstrations and celebrations by independent workers' organizations. Today, Berlin witnesses yearly demonstrations on May Day, the largest organized by labour unions, political parties and others by the far left and .\nSince 1987, May Day has also become known for riots in some districts of Berlin. After police actions against radical leftists in that year's annual demonstrations, the Autonome scattered and sought cover at the ongoing annual street fair in Kreuzberg. Three years prior to the reunification of Germany, violent protests would only take place in the former West Berlin. The protesters began tipping over police cars, violently resisting arrest, and began building barricades after the police withdrew due to the unforeseen resistance. Cars were set on fire, shops plundered and burned to the ground. The police eventually ended the riots the following night. These violent forms of protests by the radical left, later increasingly involved participants without political motivation. (Read more: May Day in Kreuzberg)\n\nAnnual street fairs have proven an effective way to prevent riots, and May Day in 2005 and 2006 have been among the most peaceful known to Berlin in nearly 25 years. In recent years, neo-Nazis and other groups on the far right, such as the National Democratic Party of Germany, have used the day to schedule public demonstrations, often leading to clashes with left-wing protesters, which turned especially violent in Leipzig in 1998 and 2005.\n\nMay Day violence flared again in 2010. After an approved far right demonstration was blocked by leftists, a parade by an estimated 10,000 leftists and anarchists turned violent and resulted in an active response by Berlin police.\n\nIn Greece 1 May is an optional public holiday. The Ministry of Labour retains the right to classify it as an official public holiday on an annual basis, and it customarily does so. The day is called (, \"Workers' 1 May\") and celebrations are marked by demonstrations in which left-wing political parties, anti-authority groups, and workers' unions participate.\nOn May Day in 2010, there were major protests all over Greece, most notably Athens and Thessaloniki, by many left, anarchist and communist supporters and some violent clashes with riot police who were sent out to contain the protesters. They opposed economic reforms, an end to job losses and wage cuts in the face of the government's proposals of massive public spending cuts. These reforms are to fall in line with the IMF-EU-ECB loan proposals, which demand that Greece liberalize its economy and cut its public spending and private sector wages, which many believe will decrease living standards.\n\nHungary celebrates 1 May as a national holiday, with open-air festivities and fairs all over the country. Many towns raise May poles and festivals with various themes are organized around the holiday. Left-wing parties and trade unions hold public rallies commemorating Labour Day.\n\nIn Iceland the Labour Day () is a public holiday. The first demonstration for workers rights in Island occurred in 1923. A parade composed of trade unions and other groups marches through towns and cities across the country and speeches are delivered. However, some private businesses are open, mainly in the capital.\n\nThe Irish Congress of Trade Unions (ITUC) marks May Day with rallies take place in Belfast and Dublin and other events such as lectures, concerts and film screenings also take place around a wider May Day festival. The first Monday in May has been a public holiday in the Republic of Ireland since 1994 and in Northern Ireland since 1978. In the Republic the public holiday was demanded by the ITUC and proposed by the Labour Party in negotiating its 1992–94 coalition government with Fianna Fáil, and marked the centenary of the ITUC's predecessor, the Irish Trades Union Congress. The public holiday has no official designation, as \"Workers' Day\" or otherwise. In 2005, Labour's Ruairi Quinn condemned, as a slight to workers, an alleged Fianna Fáil proposal to replace the May holiday with one on 24 April commemorating the 1916 Rising; in fact the proposal was for an extra holiday.\n\nThe first May Day celebration in Italy took place in 1890. It started initially as an attempt to celebrate workers' achievements in their struggle for their rights and for better social and economic conditions. It was abolished under the Fascist regime and immediately restored after the Second World War. (During the fascist period, a \"Holiday of the Italian Labour\" () was celebrated on 21 April, the date of , when ancient Rome was allegedly founded.) May Day is now an important celebration in Italy and is a national holiday regardless of what day of the week it falls. The (\"1st of May Concert\"), organized by Italian labour unions in Rome in Piazza di Porta San Giovanni has become an important event in recent years. Every year the concert is attended by a large audience of mostly young people and involves the participation of many famous bands and songwriters, lasting from 3 pm until midnight. The concert is usually broadcast live by Rai 3.\n\nFirst May Day is an official public holiday celebrated as International Work Day (). Celebrations for workers' day were mandatory during the Soviet occupation, and carry a negative connotation as a result today. As Lithuania declared its independence in 1990, Work Day lost its public holiday status, but regained it in 2001.\n\nIn Estonia, 1 May is a public holiday celebrated as Spring Day or May Day. It is a popular celebration for students.\n\nFirst May Day is an official public holiday celebrated as Convocation of the Constituent Assembly of the Republic of Latvia, Labour Day.\n\nIn Luxembourg, 1 May, called the (\"Day of Labour\"), is a legal holiday traditionally associated with large demonstrations by trade unions in Luxembourg City and other cities.\n\nIn Malta, 1 May is an official public holiday celebrated as \"Workers' Day\", together with the religious feast of \"Saint Joseph the Worker\". (Saint Joseph's Day, 19 March, the saint's main feast, is also a public holiday in Malta.) A free music event takes place on 1 May. The Maltese also celebrate Maltese accession to the European Union on 1 May 2004.\n\n1 May is an official public holiday and a day off work and a day out of school. It is the only official holiday from socialist times that is still officially celebrated.\n\nIn the Netherlands, 1 May or Labour Day (Dutch: \"Dag van de Arbeid\") is not an official holiday.\n\nIn North Macedonia, 1 May (, ) is an official public holiday. Before 2007, 2 may was also a public holiday. People celebrate with friends and family at traditional picnics across the country, accompanied by the usual outdoor games, various grilled meats and beverages. Left organizations and some trade unions organize protests on 1 May.\n\nIn Norway, Labour Day () is celebrated 1 May and is an official public holiday. It is traditionally a day of marches, speeches and parties organized by labour unions, leftist political parties and feminist and anti-racist organizations to voice political concerns and celebrate solidarity.\n\nIn Poland, since the fall of communism, 1 May is officially celebrated as May Day, but is commonly called Labour Day. it is currently celebrated without a specific connotation, and as such it is May Day. However, due to historical connotations, most of the large organized celebrations are focused around Labour Day festivities. It is customary for labour activists to organize parades in cities and towns across Poland. The holiday is also commonly referred to as \"Labour Day\" ().\n\nIn Poland, May Day is closely followed by May 3rd Constitution Day. These two dates combined often result in a long weekend called , which may last for up to 9 days from 28 April to 6 May, at the cost of taking only 3 days off. People often travel, and is unofficially considered the start of barbecuing season in Poland.\n\nBetween these two, on 2 May, there is a patriotic holiday, the Day of the Polish Flag (), introduced by a Parliamentary Act of 20 February 2004. The day, however, does not force paid time off.\n\nIn Soviet times, streets, places, squares, parks and also factories were frequently named in honor of International Workers' Day, such as 1 Maja Coal Mine in .\n\nIn Portugal, the 1 May celebration () was surpressed during the dictatorship of António de Oliveira Salazar and Marcelo Caetanos Estado Novo regime. The first workers' day demonstration was held a week after the Carnation Revolution of 25 April 1974. It is still the largest demonstration in the history of Portugal. It is used as an opportunity for workers and workers' groups to voice their discontent over working conditions in demonstrations across Portugal, the largest being held in Lisbon. It is an official public holiday.\n\nIn Romania, 1 May, known as the International Labour Day (), the International Workers' Day (), or simply 1/First of May (), is an official public holiday. During the communist regime, like in all former Eastern Bloc countries, the day was marked by large state-organized parades in most towns and cities. After the Romanian Revolution of 1989, 1 May continues to be an official public holiday, but without any state organized events or parades. Most people celebrate together with friends and family, organising picnics and barbecues. It is also the first day of the year when people, especially those from the southeastern part of the country including the capital Bucharest, go to spend the day in one of the Romanian Black Sea resorts.\n\nMay Day was celebrated illegally in Russia until the February Revolution enabled the first legal celebration in 1917. The following year, after the Bolshevik seizure of power, the May Day celebrations were boycotted by Mensheviks, Left Socialist Revolutionaries and anarchists. It became an important official holiday of the Soviet Union, celebrated with elaborate popular parade in the centre of the major cities. The biggest celebration was traditionally organized in Red Square, where the General Secretary of the CPSU and other party and government leaders stood atop Lenin's Mausoleum and waved to the crowds. Until 1969, the holiday was marked by military parades throughout the RSFSR and the union republics. In 1991, which preceded the last year that demonstrations were held in Red Square, May Day grew into high-spirited political action. Around 50,000 people participated in a rally in Red Square in 1991 after which the tradition was interrupted for 13 years. In the early post-Soviet period the holiday turned into massive political gatherings of supporters of radically minded politicians. For instance, an action dubbed as \"a rally of communist-oriented organisations\" was held in Red Square in 1992. The rally began with performance of the Soviet Union anthem and raising the Red Flag and ended with appeals from the leader of opposition movement Working Moscow, Viktor Anpilov, \"for early dismissal of President Boris Yeltsin, ousting Moscow Mayor Gavriil Popov from power and putting the latter on trial\". Since 1992, May Day is officially called \"The Day of Spring and Labour\", and remains a major holiday in present-day Russia.\n\nSince 2014 a national civil parade is held on that day on Red Square with similar events held in major cities and regional capitals.\n\nIn Serbia, 1 May (and also 2 May) is a day off work and a day out of school. It is one of the major popular holidays, and the only official holiday from socialist times that is still officially celebrated. People celebrate it all over the country. By tradition 1 May is celebrated by countryside picnics and outdoor barbecues. May is marked by warm weather in Serbia. In Belgrade, the capital, most people go to or , which are parks located in and . People go around the country to enjoy nature. A major religious holiday of is on 6 May so quite often days off work are given to connect these two holidays and weekend, creating a small spring break. 1 May is celebrated by most of the population regardless of political views.\n\nIn Slovakia, 1 May is an official holiday. Celebrations are held surrounding workers' day but are also connected with the commemoration of the entry of the Slovak Republic into the European Union (1 May 2004).\n\nIn Slovenia, 1 May and 2 May are public holidays. There are many official events all over the country to celebrate workers' day. In Ljubljana, the capital, the main celebration is held on Rožnik Hill in the city. On the night of 30 April, bonfires are traditionally burned.\n\nThe first Día del Trabajador was celebrated in 1889 but only became a public holiday with the beginning of the Spanish Second Republic in 1931. It was banned afterwards by the Franco regime in 1937. The year after it was decreed that the “Fiesta de la Exaltacion del Trabajo,” or Labor Festival, be held on July 18, his birthday, instead. After the death of Francisco Franco in 1975 and the move towards democracy, the first large rallies on May 1 began again in 1977. It was re-introduced as a public holiday in 1978. Commonly, peaceful demonstrations and parades occur in major and minor cities.\n\n1 May has been an important part of Swedish history since the late 19th century. The day was made a public holiday in 1938 but had been celebrated by the Swedish Social Democratic Party and the left since 1890. The first May Day celebration gathered more than 50,000 people in central Stockholm. The crowd went to hear speeches by the leading figures in the Swedish labour movement such as Hjalmar Branting (later prime minister), August Palm and Hinke Bergegren. During World War I the demonstrations mainly had a peace message and the Liberal Party also joined the demonstrations. The 8-hour working day and women's suffrage were the principal themes during the troubled times after World War I.\n\nRecognizing the central contributions of workers and international worker solidarity in Swedish social, economic, political and cultural development, May Day demonstrations are an important part of Swedish politics and culture for social democrats, left parties, and unions. In Stockholm the Social Democratic Party always marches towards Norra Bantorget, the historical, physical centre of the Swedish labour movement, where they hold speeches in front of the headquarters of the Swedish Trade Union Confederation, while the smaller Left Party marches in larger numbers towards Kungsträdgården.\n\nSince 1967, the Communist Party and its youth wing, Revolutionary Communist Youth, have held their own May Day march, known as ('Red Front'). In 2016, Röd Front marches were held at 33 locations across the country. The largest Röd Front marches are usually held in the industrial and financial port town of Gothenburg, Sweden's second-largest city and one of the party's strongholds.\nIn Switzerland, the status of 1 May differs depending on the canton and sometimes on the municipality. Labour Day is known as in German-speaking cantons, as in the French-speaking cantons, and as in the Italian-speaking canton of .\n\n\nThe largest Labour Day celebrations in Switzerland are held in the city of . Each year, 's 1 May committee, together with the Swiss Federation of Trade Unions, organizes a festival and 1 May rally. It is the largest rally held on a regular basis in Switzerland.\n\n1 May is an official holiday celebrated in Turkey. It was a holiday until 1981 when it was cancelled after the 1980 coup d'état. In 2010, the Turkish government restored the holiday after some casualties and demonstrations. Taksim Square is the centre of the celebrations due to the Taksim Square massacre.\n\nWorkers' Day was first celebrated in 1912 in and in 1899 in . After the establishment of the Turkish Republic, it became an official holiday. In 1924, it was forbidden by a decree and in both 1924 and 1925, demonstrations were intervened by arm floats. In 1935, The National Assembly declared Workers' Day to be a holiday again.\n\nDuring the events leading to the 1980 Turkish coup d'etat, a massacre occurred on 1 May 1977 (Taksim Square massacre), in which unknown people (agents provocateurs) opened fire on the crowd. The crowd was the biggest in Turkish workers' history with the number of people approximating 500,000. In the next two years, provocations and confusion continued and peaked before the 1980 coup d'etat. The Workers' Day holiday was cancelled once again. Still, demonstrations continued with small crowds, and in 1996, three people were killed by police bullets, and a plain-clothes man who spied in the crowd was revealed and lynched by workers. On the same evening, a video broadcast on TV showed that two participants in the demonstration were lynched by far right-wing nationalist groups and this lynching occurred in front of police forces who were watching the scene with happy faces. Thus, 1 May 1996 has been remembered by workers' movements.\n\nIn 2007, the 30th anniversary of the Taksim square massacre, leftist workers' unions wanted to commemorate the massacre in Taksim square. Since the government would not let them into the square, 580–700 people were stopped and 1 person died under police control. After these events, the government declared 1 May as \"Work and Solidarity Day\" but not as a holiday. In the next year, the day was declared as a holiday, but people were still not allowed to gather in Taksim Square. The year 2008 was remembered with police violence in Istanbul. Police fired tear gas grenades among the crowds, and into hospitals and a primary school. Workers pushed forward so that in 2010, 140,000 people gathered in Taksim, and in 2011 there were more than half a million demonstrators.\n\nAfter 3 years of peaceful meetings in 2013, meetings in Taksim Square were forbidden by the government. Clashes occurred between police and workers; water cannon and tear gas have been widely used.\n\nIt is a public holiday in Ukraine, inherited from the Soviet era. Until 2018, 2 May was also a public holiday (as in the Soviet era), instead in 2017 Western Christianity's Christmas celebrated 25 December became a new Ukrainian public holiday. The 1 May International Workers' Day remained a Ukrainian public holiday, although it was renamed (also in 2017) from \"Day of International Solidarity of Workers\" to \"Labour Day\".\n\nIn 2015, May Day rallies were banned in Kyiv and Kharkiv.\n\nLate May 2015 laws that ban communist symbols came into effect in Ukraine, thus banning communist symbols, singing the Soviet national hymn or the Internationale.\n\nAccording to Interior Minister Arsen Avakov during the 2016 May Day rallies in some major cities the number of police officers far outnumbered the number of rally participants. With in Dnipro 193 policemen protecting 25 rally participants.\n\nThe spring bank holiday in the United Kingdom known as May Day was created in 1978 and is held on the first Monday in May each year.\n\nThere are many traditional May Day rites and celebrations, some of which have been held for hundreds of years. However, it wasn't until the late 20th century that May Day in Great Britain became linked to International Workers' Day, and the holiday is not officially a \"Labour Day\".\n\nIn recent years, the anti-capitalist movement has organized a number of large protests in London, Glasgow, Edinburgh, and Doncaster. In London, these have resulted in clashes with the police. In 2000, the clashes ended with a branch of McDonald's being smashed and a statue of Winston Churchill being given a grass Mohawk hairstyle. The Cenotaph was also defaced with graffiti. In the last few years, demonstrations have been more peaceful, with marches and gatherings, particularly in central London. The Conservative-led coalition government in March 2011 announced plans to move the May Day bank holiday to October to lengthen the tourist season, although as of 2017 nothing had come of this. A London rally on May Day is organized by the London May Day Organising Committee.\n\nWhile unofficial activities and commemorations associated with International Workers Day occur on May Day in Australia, Labour Day in the various states and territories generally falls on other days. Only in the Northern Territory (where it is called May Day) and Queensland is Labour Day celebrated on the first Monday in May, which is a public holiday under the name of \"May Day\". Queensland holds the biggest rallies in Australia, with the rally in Brisbane averaging 30,000 people. (Citation needed) In Australia, one of the first May Day marches occurred in Queensland on 1 May 1891. There are also rallies held in Cairns, Rockhampton, Townsville, Barcaldine, Ipswich, Toowoomba, Bundaberg, Maryborough, Sunshine Coast, Gold Coast and other regional centres. (Citation needed)\n\nNew Zealand workers were among the first in the world to claim the right for an eight-hour working day when, in 1840, the carpenter Samuel Parnell won an eight-hour day in Wellington. Labour Day was first celebrated in New Zealand on 28 October 1890. Labour day falls every year on the fourth Monday of October.\n\n\n"}
{"id": "42437083", "url": "https://en.wikipedia.org/wiki?curid=42437083", "title": "Class conflict", "text": "Class conflict\n\nClass conflict (also class warfare and class struggle) is the political tension and economic antagonism that exists in society consequent to socio-economic competition among the social classes. \n\nThe forms of class conflict include direct violence, such as wars for resources and cheap labor and assassinations; indirect violence, such as deaths from poverty and starvation, illness and unsafe working conditions. Economic coercion, such as the threat of unemployment or the withdrawal of investment capital; or ideologically, by way of political literature. Additionally, political forms of class warfare are: legal and illegal lobbying, and bribery of legislators. \n\nThe social-class conflict can be direct, as in a dispute between labour and management, such as an employer's industrial lockout of their employees in effort to weaken the bargaining power of the corresponding trade union; or indirect, such as a workers' slowdown of production in protest of unfair labor practices, such as low wages and poor workplace conditions.\n\nIn the political and economic philosophies of Karl Marx and Mikhail Bakunin, class struggle is a central tenet and a practical means for effecting radical social and political changes for the social majority.\n\nIn political science, socialists and Marxists use the term \"class conflict\" to define a social class by its relationship to the means of production, such as factories, agricultural land, and industrial machinery. The social control of labor and of the production of goods and services, is a political contest between the social classes. \n\nThe anarchist Mikhail Bakunin said that the class struggles of the working class, the peasantry, and the working poor were central to realizing a social revolution to depose and replace the ruling class, and the creation of libertarian socialism. In contrast, Marx's theory of history proposes that class conflict is decisive in the history of economic systems organized by hierarchies of social class, such as capitalism and feudalism. Marxists refer to its overt manifestations as class war, a struggle whose resolution in favor of the working class is viewed by them as inevitable under plutocratic capitalism.\n\nWhere societies are socially divided based on status, wealth, or control of social production and distribution, class structures arise and are thus coeval with civilization itself. It is well documented since at least European Classical Antiquity (Conflict of the Orders, Spartacus, etc.) and the various popular uprisings in late medieval Europe and elsewhere. One of the earliest analysis of these conflicts is Friedrich Engels' The Peasant War in Germany. One of the earliest analyses of the development of class as the development of conflicts between emergent classes is available in Peter Kropotkin's \"\". In this work, Kropotkin analyzes the disposal of goods after death in pre-class or hunter-gatherer societies, and how inheritance produces early class divisions and conflict.\n\nIn the speech \"The Great American Class War\" (2013) the journalist Bill Moyers asserted the existence of social-class conflict between democracy and plutocracy in the U.S. Chris Hedges wrote a column for \"Truthdig\" called \"Let's Get This Class War Started,\" which was a play on Pink's song \"Let's Get This Party Started.\"\n\nHistorian Steve Fraser, author of \"The Age of Acquiescence: The Life and Death of American Resistance to Organized Wealth and Power\", asserts that class conflict is an inevitability if current political and economic conditions continue, noting that “people are increasingly fed up… their voices are not being heard. And I think that can only go on for so long without there being more and more outbreaks of what used to be called class struggle, class warfare.”\n\nThe typical example of class conflict described is class conflict within capitalism. This class conflict is seen to occur primarily between the bourgeoisie and the proletariat, and takes the form of conflict over hours of work, value of wages, division of profits, cost of consumer goods, the culture at work, control over parliament or bureaucracy, and economic inequality. The particular implementation of government programs which may seem purely humanitarian, such as disaster relief, can actually be a form of class conflict. In the USA class conflict is often noted in labor/management disputes. As far back as 1933 representative Edward Hamilton of ALPA, the Airline Pilot's Association, used the term \"class warfare\" to describe airline management's opposition at the National Labor Board hearings in October of that year. Apart from these day-to-day forms of class conflict, during periods of crisis or revolution class conflict takes on a violent nature and involves repression, assault, restriction of civil liberties, and murderous violence such as assassinations or death squads. (Zinn, \"People's History\")\n\nAlthough Thomas Jefferson (1743–1826) led the United States as president from 1801–1809 and is considered one of the founding fathers, he died with immense amounts of debt. Regarding the interaction between social classes, he wrote,\n\nThe investor, billionaire, and philanthropist Warren Buffett, one of the wealthiest people in the world, voiced in 2005 and once more in 2006 his view that his class – the \"rich class\" – is waging class warfare on the rest of society. In 2005 Buffet said to CNN: \"It's class warfare, my class is winning, but they shouldn't be.\" In a November 2006 interview in \"The New York Times\", Buffett stated that \"[t]here’s class warfare all right, but it’s my class, the rich class, that’s making war, and we’re winning.\" Later Warren gave away more than half of his fortune to charitable causes through a program developed by himself and computer software tycoon Bill Gates. In 2011 Buffett called on government legislators to, \"...stop coddling the super rich.\"\n\nMax Weber (1864–1920) agrees with the fundamental ideas of Karl Marx about the economy causing class conflict, but claims that class conflict can also stem from prestige and power. Weber argues that classes come from the different property locations. Different locations can largely affect one's class by their education and the people they associate with. He also states that prestige results in different status groupings. This prestige is based upon the social status of one's parents. Prestige is an attributed value and many times cannot be changed. Weber states that power differences led to the formation of political parties. Weber disagrees with Marx about the formation of classes. While Marx believes that groups are similar due to their economic status, Weber argues that classes are largely formed by social status. Weber does not believe that communities are formed by economic standing, but by similar social prestige. Weber does recognize that there is a relationship between social status, social prestige and classes.\n\nNumerous factors have culminated in what's known as the Arab Spring. Agenda behind the civil unrest, and the ultimate overthrow of authoritarian governments throughout the Middle-East included issues such as dictatorship or absolute monarchy, human rights violations, government corruption (demonstrated by Wikileaks diplomatic cables), economic decline, unemployment, extreme poverty, and a number of demographic structural factors, such as a large percentage of educated but dissatisfied youth within the population. Also, some, like Slovenian philosopher Slavoj Žižek attribute the 2009 Iranian protests as one of the reasons behind the Arab Spring. The catalysts for the revolts in all Northern African and Persian Gulf countries have been the concentration of wealth in the hands of autocrats in power for decades, insufficient transparency of its redistribution, corruption, and especially the refusal of the youth to accept the status quo. as they involve threats to food security worldwide and prices that approach levels of the 2007–2008 world food price crisis. Amnesty International singled out WikiLeaks' release of US diplomatic cables as a catalyst for the revolts.\n\nKarl Marx (1818–1883) was a German born philosopher who lived the majority of his adult life in London, England. In \"The Communist Manifesto\", Karl Marx argued that a class is formed when its members achieve class consciousness and solidarity. This largely happens when the members of a class become aware of their exploitation and the conflict with another class. A class will then realize their shared interests and a common identity. According to Marx, a class will then take action against those that are exploiting the lower classes.\n\nWhat Marx points out is that members of each of the two main classes have interests in common. These class or collective interests are in conflict with those of the other class as a whole. This in turn leads to conflict between individual members of different classes.\n\nMarxist analysis of society identifies two main social groups: \n\n\nNot all class struggle is violent or necessarily radical, as with strikes and lockouts. Class antagonism may instead be expressed as low worker morale, minor sabotage and pilferage, and individual workers' abuse of petty authority and hoarding of information. It may also be expressed on a larger scale by support for socialist or populist parties. On the employers' side, the use of union busting legal firms and the lobbying for anti-union laws are forms of class struggle.\n\nNot all class struggle is a threat to capitalism, or even to the authority of an individual capitalist. A narrow struggle for higher wages by a small sector of the working-class, what is often called \"economism\", hardly threatens the status quo. In fact, by applying the craft-union tactics of excluding other workers from skilled trades, an economistic struggle may even weaken the working class as a whole by dividing it. Class struggle becomes more important in the historical process as it becomes more general, as industries are organized rather than crafts, as workers' class consciousness rises, and as they self-organize away from political parties. Marx referred to this as the progress of the proletariat from being a class \"in itself\", a position in the social structure, to being one \"for itself\", an active and conscious force that could change the world.\n\nMarx largely focuses on the capital industrialist society as the source of social stratification, which ultimately results in class conflict. He states that capitalism creates a division between classes which can largely be seen in manufacturing factories. The proletariat, is separated from the bourgeoisie because production becomes a social enterprise. Contributing to their separation is the technology that is in factories. Technology de-skills and alienates workers as they are no longer viewed as having a specialized skill. Another effect of technology is a homogenous workforce that can be easily replaceable. Marx believed that this class conflict would result in the overthrow of the bourgeoisie and that the private property would be communally owned. The mode of production would remain, but communal ownership would eliminate class conflict.\n\nEven after a revolution, the two classes would struggle, but eventually the struggle would recede and the classes dissolve. As class boundaries broke down, the state apparatus would wither away. According to Marx, the main task of any state apparatus is to uphold the power of the ruling class; but without any classes there would be no need for a state. That would lead to the classless, stateless communist society.\n\nA variety of predominantly Trotskyist and anarchist thinkers argue that class conflict existed in Soviet-style societies. Their arguments describe as a class the bureaucratic stratum formed by the ruling political party (known as the nomenklatura in the Soviet Union), sometimes termed a \"new class\", that controls and guides the means of production. This ruling class is viewed to be in opposition to the remainder of society, generally considered the proletariat. This type of system is referred by them as state socialism, state capitalism, bureaucratic collectivism or new class societies. (Cliff; Ðilas 1957) Marxism was such a predominate ideological power in what became the Soviet Union since a Marxist group known as the Russian Social Democratic Labour Party was formed in the country, prior to 1917. This party soon divided into two main factions; the Bolsheviks, who were led by Vladimir Lenin, and the Mensheviks, who were led by Julius Martov.\n\nHowever, many Marxist argue that unlike in capitalism the Soviet elites did not own the means of production, or generated surplus value for their personal wealth like in capitalism as the generated profit from the economy was equally distributed into Soviet society. Even some Trotskyist like Ernest Mandel criticized the concept of a new ruling class as an oxymoron, saying: \"The hypothesis of the bureaucracy’s being a new ruling class leads to the conclusion that, for the first time in history, we are confronted with a “ruling class” which does not exist as a class before it actually rules.\"\n\nSocial commentators, historians and socialist theorists had commented on class struggle for some time before Marx, as well as the connection between class struggle, property, and law: Augustin Thierry, François Guizot, François-Auguste Mignet and Adolphe Thiers. The Physiocrats, David Ricardo, and after Marx, Henry George noted the inelastic supply of land and argued that this created certain privileges (economic rent) for landowners. According to the historian Arnold Toynbee, stratification along lines of class appears only within civilizations, and furthermore only appears during the process of a civilization's decline while not characterizing the growth phase of a civilization.\n\nProudhon, in \"What is Property?\" (1840) states that \"certain classes do not relish investigation into the pretended titles to property, and its fabulous and perhaps scandalous history.\" While Proudhon saw the solution as the lower classes forming an alternative, solidarity economy centered on cooperatives and self-managed workplaces, which would slowly undermine and replace capitalist class society, the anarchist Mikhail Bakunin, while influenced by Proudhon, insisted that a massive class struggle by the working class, peasantry and poor was essential to the creation of libertarian socialism. This would require a (final) showdown in the form of a social revolution.\n\nFascists have often opposed class struggle and instead have attempted to appeal to the working class while promising to preserve the existing social classes and have proposed an alternative concept known as class collaboration.\n\nNoam Chomsky, American linguist, philosopher, and political activist has criticized class war in the United States:\n\nSome historical tendencies of Orthodox Marxism reject racism, sexism, etc. as struggles that essentially distract from class struggle, the real conflict. These divisions within the class prevent the purported antagonists from acting in their common class interest. However, many Marxist internationalists and anti-colonial revolutionaries believe sex, race and class to be bound up together. There is an ongoing debate within Marxist scholarship about these topics.\n\nAccording to Michel Foucault, in the 19th century the essentialist notion of the \"race\" was incorporated by racists, biologists, and eugenicists, who gave it the modern sense of \"biological race\" which was then integrated to \"state racism\". On the other hand, Foucault claims that when Marxists developed their concept of \"class struggle\", they were partly inspired by the older, non-biological notions of the \"race\" and the \"race struggle\". Quoting a non-existent 1882 letter from Marx to Friedrich Engels during a lecture, Foucault erroneously claimed Marx wrote: \"You know very well where we found our idea of class struggle; we found it in the work of the French historians who talked about the race struggle.\" For Foucault, the theme of social war provides the overriding principle that connects class and race struggle.\n\nMoses Hess, an important theoretician and labor Zionist of the early socialist movement, in his \"Epilogue\" to \"Rome and Jerusalem\" argued that \"the race struggle is primary, the class struggle secondary... With the cessation of race antagonism, the class struggle will also come to a standstill. The equalization of all classes of society will necessarily follow the emancipation of all the races, for it will ultimately become a scientific question of social economics.\"\n\nW. E. B. Du Bois theorized that the intersectional paradigms of race, class, and nation might explain certain aspects of black political economy. Patricia Hill Collins writes: \"Du Bois saw race, class, and nation not primarily as personal identity categories but as social hierarchies that shaped African-American access to status, poverty, and power.\"\n\nIn modern times, emerging schools of thought in the U.S. and other countries hold the opposite to be true. They argue that the race struggle is less important, because the primary struggle is that of class since labor of all races face the same problems and injustices.\n\nRiots with a nationalist background are not included.\n\n\n\n\n\n\n"}
{"id": "39903086", "url": "https://en.wikipedia.org/wiki?curid=39903086", "title": "Working class", "text": "Working class\n\nThe working class (or labouring class) comprises those engaged in waged or salaried labour, especially in manual-labour occupations and industrial work. Working-class occupations (see also \"Designation of workers by collar color\") include blue-collar jobs, some white-collar jobs, and most pink-collar jobs. Members of the working class rely for their income exclusively upon their earnings from wage labour; thus, according to the more inclusive definitions, the category can include almost all of the working population of industrialized economies, as well as those employed in the urban areas (cities, towns, villages) of non-industrialized economies or in the rural workforce.\n\nAs with many terms describing social class, \"working class\" is defined and used in many different ways. The most general definition, used by Marxists and many socialists, is that the working class includes all those who have nothing to sell but their labour power and skills. In that sense it includes both white and blue-collar workers, manual and mental workers of all types, excluding only individuals who derive their income from business ownership and the labour of others.\n\nWhen used non-academically in the United States, however, it often refers to a section of society dependent on physical labour, especially when compensated with an hourly wage. For certain types of science, as well as less scientific or journalistic political analysis. For example, the working class is loosely defined as those without college degrees. Working-class occupations are then categorized into four groups: unskilled labourers, artisans, outworkers, and factory workers.\n\nA common alternative, is to define class by income levels. When this approach is used, the working class can be contrasted with a so-called \"middle class \"on the basis of differential terms of access to economic resources, education, cultural interests, and other goods and services. The cut-off between working class and middle class here might mean the line where a population has discretionary income, rather than simply sustenance (for example, on fashion versus merely nutrition and shelter).\n\nSome researchers have suggested that working-class status should be defined subjectively as self-identification with the working-class group. This subjective approach allows people, rather than researchers, to define their own social class.\n\nIn feudal Europe, the working class as such did not exist in large numbers. Instead, most people were part of the labouring class, a group made up of different professions, trades and occupations. A lawyer, craftsman and peasant were all considered to be part of the same social unit, a third estate of people who were neither aristocrats nor church officials. Similar hierarchies existed outside Europe in other pre-industrial societies. The social position of these labouring classes was viewed as ordained by natural law and common religious belief. This social position was contested, particularly by peasants, for example during the German Peasants' War.\n\nIn the late 18th century, under the influence of the Enlightenment, European society was in a state of change, and this change could not be reconciled with the idea of a changeless god-created social order. Wealthy members of these societies created ideologies which blamed many of the problems of working-class people on their morals and ethics (i.e. excessive consumption of alcohol, perceived laziness and inability to save money). In \"The Making of the English Working Class\", E. P. Thompson argues that the English working class was present at its own creation, and seeks to describe the transformation of pre-modern labouring classes into a modern, politically self-conscious, working class.\n\nStarting around 1917, a number of countries became ruled ostensibly in the interests of the working class (see Soviet working class). Some historians have noted that a key change in these Soviet-style societies has been a massive a new type of proletarianization, often effected by the administratively achieved forced displacement of peasants and rural workers. Since then, four major industrial states have turned towards semi-market-based governance (China, Laos, Vietnam, Cuba), and one state has turned inwards into an increasing cycle of poverty and brutalization (North Korea). Other states of this sort have collapsed (such as the Soviet Union).\n\nSince 1960, large-scale proletarianization and enclosure of commons has occurred in the third world, generating new working classes. Additionally, countries such as India have been slowly undergoing social change, expanding the size of the urban working class.\n\nKarl Marx defined the working class or proletariat as individuals who sell their labour power for wages and who do not own the means of production. He argued that they were responsible for creating the wealth of a society. He asserted that the working class physically build bridges, craft furniture, grow food, and nurse children, but do not own land, or factories.\n\nA sub-section of the proletariat, the lumpenproletariat (\"rag-proletariat\"), are the extremely poor and unemployed, such as day labourers and homeless people. Marx considered them to be devoid of class consciousness.\n\nIn \"The Communist Manifesto\", Karl Marx and Friedrich Engels argued that it was the destiny of the working class to displace the capitalist system, with the dictatorship of the proletariat (the rule of the many, as opposed to the \"dictatorship of the bourgeoisie\"), abolishing the social relationships underpinning the class system and then developing into a future communist society in which \"the free development of each is the condition for the free development of all.\" \"The Communist Manifesto\" was a response to Adam Smith's pro capitalist text \"The Wealth of Nations\" \"(\"published in 1776)\".\" Some issues in Marxist arguments about working-class membership have included:\n\nPossible responses to some of these issues are:\n\nIn general, in Marxist terms wage labourers and those dependent on the welfare state are working class, and those who live on accumulated capital are not. This broad dichotomy defines the class struggle. Different groups and individuals may at any given time be on one side or the other. Such contradictions of interests and identity within individuals' lives and within communities can effectively undermine the ability of the working class to act in solidarity to reduce exploitation, inequality, and the role of ownership in determining people's life chances, work conditions, and political power.\n\n"}
{"id": "146704", "url": "https://en.wikipedia.org/wiki?curid=146704", "title": "Riot", "text": "Riot\n\nA riot () is a form of civil disorder commonly characterized by a group lashing out in a violent public disturbance against authority, property or people. \n\nRiots typically involve theft, vandalism, and destruction of property, public or private. The property targeted varies depending on the riot and the inclinations of those involved. Targets can include shops, cars, restaurants, state-owned institutions, and religious buildings.\n\nRiots often occur in reaction to a grievance or out of dissent. Historically, riots have occurred due to poor people with no jobs or living conditions, governmental oppression, taxation or conscription, conflicts between ethnic groups, (race riot) or religions (sectarian violence, pogrom), the outcome of a sporting event (sports riot, football hooliganism) or frustration with legal channels through which to air grievances.\n\nWhile individuals may attempt to lead or control a riot, riots typically consist of disorganized groups that are frequently \"chaotic and exhibit herd behavior.\" There is a growing body of evidence to suggest that riots are not irrational, herd-like behavior, but actually follow inverted social norms.\n\nT. S. Ashton, in his study of food riots among colliers, noted that \"the turbulence of the colliers is, of course, to be accounted for by something more elementary than politics: it was the instinctive reaction of virility to hunger.\" Charles Wilson noted, \"Spasmodic rises in food prices provoked keelmen on the Tyne to riot in 1709, tin miners to plunder granaries at Falmouth in 1727.\"\n\nToday, some rioters have an improved understanding of the tactics used by police in riot situations. Manuals for successful rioting are available on the internet, with tips such as encouraging rioters to get the press involved, as there is more safety and attention with the cameras rolling. Civilians with video cameras may also have an effect on both rioters and police.\n\nDealing with riots is often a difficult task for police forces. They may use tear gas or CS gas to control rioters. Riot police may use less-than-lethal methods of control, such as shotguns that fire flexible baton rounds to injure or otherwise incapacitate rioters for easier arrest.\n\nA police riot is a term for the disproportionate and unlawful use of force by a group of police against a group of civilians. This term is commonly used to describe a police attack on civilians, or provoking civilians into violence.\n\nA prison riot is a large-scale, temporary act of concerted defiance or disorder by a group of prisoners against prison administrators, prison officers, or other groups of prisoners. It is often done to express a grievance, force change or attempt escape.\n\nIn a race riot, race or ethnicity is the key factor. The term had entered the English language in the United States by the 1890s. Early use of the term referred to riots that were often a mob action by members of a majority racial group against people of other perceived races.\n\nIn a religious riot, the key factor is religion. The rioting mob targets people and properties of a specific religion, or those believed to belong to that religion.\n\nStudent riots are riots precipitated by students, often in higher education, such as a college or university. Student riots in the US and Western Europe in the 1960s and the 1970s were often political in nature. Student riots may also occur as a result of oppression of peaceful demonstration or after sporting events. Students may constitute an active political force in a given country. Such riots may occur in the context of wider political or social grievances.\n\nUrban riots are riots in the context of urban decay, provoked by conditions such as discrimination, poverty, high unemployment, poor schools, poor healthcare, housing inadequacy and police brutality and bias. Urban riots are closely associated with race riots and police riots.\n\nSports riots such as the Nika riots can be sparked by the losing or winning of a specific team or athlete. Fans of the two teams may also fight. Sports riots may happen as a result of teams contending for a championship, a long series of matches, or scores that are close. Sports are the most common cause of riots in the United States, accompanying more than half of all championship games or series. Almost all sports riots occur in the winning team's city.\n\nFood and bread riots are caused by harvest failures, incompetent food storage, hoarding, poisoning of food, or attacks by pests like locusts. When the public becomes desperate from such conditions, groups may attack shops, farms, homes, or government buildings to obtain bread or other staple foods like grain or salt, as in the 1977 Egyptian Bread Riots.\n\nThe economic and political effects of riots can be as complex as their origins. Property destruction and harm to individuals are often immediately measurable. During the 1992 Los Angeles riots, 2,383 people were injured, 8,000 were arrested, 63 were killed and over 700 businesses burned. Property damage was estimated at over $1 billion. At least ten of those killed were shot by police or National Guard forces.\nSimilarly, the 2005 civil unrest in France lasted over three weeks and spread to nearly 300 towns. By the end of the incident, over 10,000 vehicles were destroyed and over 300 buildings burned. Over 2,800 suspected rioters were arrested and 126 police and firefighters were injured. Estimated damages were over €200 Million.\n\nMany governments and political systems have fallen after riots, including:\n\nRiots are typically dealt with by the police, although methods differ from country to country. Tactics and weapons used can include attack dogs, water cannons, plastic bullets, rubber bullets, pepper spray, flexible baton rounds, and snatch squads. Many police forces have dedicated divisions to deal with public order situations. Some examples are the Territorial Support Group, Special Patrol Group, Compagnies Républicaines de Sécurité, Mobiele Eenheid, and Arrest units.\n\nThe policing of riots has been marred by incidents in which police have been accused of provoking rioting or crowd violence. While the weapons described above are officially designated as non-lethal, a number of people have died or been injured as a result of their use. For example, seventeen deaths were caused by rubber bullets in Northern Ireland over the thirty five years between 1970 and 2005.\n\nA high risk of being arrested is even more effective against rioting than severe punishments. As more and more people join the riot, the risk of being arrested goes down, which persuades still more people to join. This leads to a vicious cycle, which is typically ended only by sufficient police or military presence to increase the risk of being arrested.\n\nIn India, rioting is an offense under the Indian Penal Code (IPC).\n\nRiot is a statutory offence in England and Wales. It is created by section 1(1) of the Public Order Act 1986. Sections 1(1) to (5) of that Act read:\n\nA single person can be liable for an offence of riot when they \"use\" violence, provided that it is shown there were at least twelve present \"using or threatening\" unlawful violence.\n\nThe word \"violence\" is defined by section 8. The violence can be against the person or against property. The \"mens rea\" is defined by section 6(1).\n\nIndictment\n\nSee R v Tyler and others, 96 Cr App R 332, [1993] Crim LR 60, CA.\n\nMode of trial and sentence\n\nRiot is an indictable-only offence. A person convicted of riot is liable to imprisonment for any term not exceeding ten years, or to a fine, or to both.\n\nSee the following cases:\n\nAssociation football matches\n\nIn the case of riot connected to football hooliganism, the offender may be banned from football grounds for a set or indeterminate period of time and may be required to surrender their passport to the police for a period of time in the event of a club or international match, or international tournament, connected with the offence. This prevents travelling to the match or tournament in question. (The measures were brought in by the Football (Disorder) Act 2000 after rioting of England fans at Euro 2000.)\n\nCompensation for riot damage\n\nSee the Riot (Damages) Act 1886 and section 235 of the Merchant Shipping Act 1995..\n\nConstruction of \"riot\" and cognate expressions in other instruments\n\nSection 10 of the Public Order Act 1986 now provides:\n\nAs to this provision, see pages 84 and 85 of the Law Commission's report.\n\nCommon law offence\n\nThe common law offence of riot was abolished for England and Wales on 1 April 1987.\n\nHistory\n\nIn the past, the Riot Act had to be read by an official - with the wording exactly correct - before violent policing action could take place. If the group did not disperse after the Act was read, lethal force could legally be used against the crowd. See also the Black Act.\n\nSection 515 of the Merchant Shipping Act 1894 formerly made provision for compensation for riot damage.\n\nRiot is a serious offence for the purposes of Chapter 3 of the Criminal Justice (Northern Ireland) Order 2008.\n\nSee paragraph 13 of Schedule 5 to the Electoral Law Act (Northern Ireland) 1962.\n\nThere is an offence under the law of Scotland which is known both as \"mobbing\" and \"mobbing and rioting\".\n\nIn July 1981, both Dundee and Edinburgh saw significant disorder as part of the events of that July, while in 1994 and in 2013, two years after the English riots of August 2011, Edinburgh saw rioting, albeit localised to one specific area and not part of any bigger 'riot wave'. Events in 1981 were very similar to those in England, although sources are severely limited. Both Niddrie and Craigmillar saw riots in the 1980s.\n\nUnder United States federal law, a riot is defined as:\n\nEach state may have its own definition of a riot. In New York, the term \"riot\" is not defined explicitly, but under § 240.08 of the N.Y. Penal Law,\"A person is guilty of inciting to riot when one urges ten or more persons to engage in tumultuous and violent conduct of a kind likely to create public alarm.\"\n\nSources:\n\n"}
{"id": "247817", "url": "https://en.wikipedia.org/wiki?curid=247817", "title": "Welfare", "text": "Welfare\n\nWelfare is a type of government support for the citizens of that society. Welfare may be provided to people of any income level, as with social security (and is then often called a social safety net), but it is usually intended to ensure that people can meet their basic human needs such as food and shelter. Welfare attempts to provide a minimal level of well-being, usually either a free- or a subsidized-supply of certain goods and social services, such as healthcare, education, and vocational training.\n\nA welfare state is a political system wherein the State assumes responsibility for the health, education, and welfare of society. The system of social security in a welfare state provides social services, such as universal medical care, unemployment insurance for workers, financial aid, free post-secondary education for students, subsidized public housing, and pensions (sickness, incapacity, old-age), etc. In 1952, with the \"Social Security (Minimum Standards) Convention (nr. 102)\", the International Labour Organization (ILO) formally defined the social contingencies covered by social security.\n\nThe first welfare state was Imperial Germany (1871–1918), where the Bismarck government introduced social security in 1889. In the early 20th century, the United Kingdom introduced social security around 1913, and adopted the welfare state with the National Insurance Act 1946, during the Attlee government (1945–51). In the countries of western Europe, Scandinavia, and Australasia, social welfare is mainly provided by the government out of the national tax revenues, and to a lesser extent by non-government organizations (NGOs), and charities (social and religious).\n\nIn the U.S., \"welfare program\" is the general term for government support of the well-being of poor people, and the term \"social security\" has come to be referred to as US social insurance program for retired and disabled people even though \"social security\" is itself a retirement insurance plan paid for by taxes taken from the individual worker's payroll check and matched by his employer, no part of it is paid by the Federal Government. In other countries, the term \"social security\" has a broader definition, which refers to the economic security that a society offers when people are sick, disabled, and unemployed. In the U.K., government use of the term \"welfare\" includes help for poor people and \"benefits\", including specific social services such as help in finding employment.\n\nIn the Roman Empire, the first emperor Augustus provided the \"Cura Annonae\" or grain dole for citizens who could not afford to buy food every month. Social welfare was enlarged by the Emperor Trajan. Trajan's program brought acclaim from many, including Pliny the Younger. The Song dynasty government (c.1000AD in China) supported multiple programs which could be classified as social welfare, including the establishment of retirement homes, public clinics, and paupers' graveyards. According to economist Robert Henry Nelson, \"The medieval Roman Catholic Church operated a far-reaching and comprehensive welfare system for the poor...\"\n\nEarly welfare programs in Europe included the English Poor Law of 1601, which gave parishes the responsibility for providing welfare payments to the poor. This system was substantially modified by the 19th-century Poor Law Amendment Act, which introduced the system of workhouses.\n\nPublic assistance programs were not called welfare until the early 20th century when the term was quickly adopted to avoid the negative connotations that had become associated with older terms such as charity.\n\nIt was predominantly in the late 19th and early 20th centuries that an organized system of state welfare provision was introduced in many countries. Otto von Bismarck, Chancellor of Germany, introduced one of the first welfare systems for the working classes. In Great Britain the Liberal government of Henry Campbell-Bannerman and David Lloyd George introduced the National Insurance system in 1911, a system later expanded by Clement Attlee. The United States inherited England's poor house laws and has had a form of welfare since before it won its independence. During the Great Depression, when emergency relief measures were introduced under President Franklin D. Roosevelt, Roosevelt's New Deal focused predominantly on a program of providing work and stimulating the economy through public spending on projects, rather than on cash payment.\n\nModern welfare states include Germany, France, the Netherlands, as well as the Nordic countries, such as Iceland, Sweden, Norway, Denmark, and Finland which employ a system known as the Nordic model. Esping-Andersen classified the most developed welfare state systems into three categories; Social Democratic, Conservative, and Liberal.\n\nIn the Islamic world, \"Zakat\" (charity), one of the Five Pillars of Islam, has been collected by the government since the time of the Rashidun caliph Umar in the 7th century. The taxes were used to provide income for the needy, including the poor, elderly, orphans, widows, and the disabled. According to the Islamic jurist Al-Ghazali (Algazel, 1058–111), the government was also expected to store up food supplies in every region in case a disaster or famine occurred. (See Bayt al-mal for further information.)\n\nThe World Bank's 2019 World Development Report on \"The Changing Nature of Work\" considers whether traditional social assistance models continue to be appropriate given that, in 2018, 8 in 10 people in developing countries still receive no social assistance while 6 in 10 work informally beyond the government's reach.\n\nWelfare can take a variety of forms, such as monetary payments, subsidies and vouchers, or housing assistance. Welfare systems differ from country to country, but welfare is commonly provided to individuals who are unemployed, those with illness or disability, the elderly, those with dependent children, and veterans. A person's eligibility for welfare may also be constrained by means testing or other conditions.\n\nWelfare is provided by governments or their agencies, by private organizations, or a combination of both. Funding for welfare usually comes from general government revenue, but when dealing with charities or NGOs, donations may be used. Some countries run conditional cash transfer welfare programs where payment is conditional on behavior of the recipients.\n\nPrior to 1900 in Australia, charitable assistance from benevolent societies, sometimes with financial contributions from the authorities, was the primary means of relief for people not able to support themselves. The 1890s economic depression and the rise of the trade unions and the Labor parties during this period led to a movement for welfare reform.\n\nIn 1900, the states of New South Wales and Victoria enacted legislation introducing non-contributory pensions for those aged 65 and over. Queensland legislated a similar system in 1907 before the Australian labor Commonwealth government led by Andrew Fisher introduced a national aged pension under the Invalid and Old-Aged Pensions Act 1908. A national invalid disability pension was started in 1910, and a national maternity allowance was introduced in 1912.\n\nDuring the Second World War, Australia under a labor government created a welfare state by enacting national schemes for: child endowment in 1941 (superseding the 1927 New South Wales scheme); a widows’ pension in 1942 (superseding the New South Wales 1926 scheme); a wife's allowance in 1943; additional allowances for the children of pensioners in 1943; and unemployment, sickness, and special benefits in 1945 (superseding the Queensland 1923 scheme).\n\nCanada has a welfare state in the European tradition; however, it is not referred to as \"welfare\", but rather as \"social programs\". In Canada, \"welfare\" usually refers specifically to direct payments to poor individuals (as in the American usage) and not to healthcare and education spending (as in the European usage).\n\nThe Canadian social safety net covers a broad spectrum of programs, and because Canada is a federation, many are run by the provinces. Canada has a wide range of government transfer payments to individuals, which totaled $145 billion in 2006. Only social programs that direct funds to individuals are included in that cost; programs such as medicare and public education are additional costs.\n\nGenerally speaking, before the Great Depression, most social services were provided by religious charities and other private groups. Changing government policy between the 1930s and 1960s saw the emergence of a welfare state, similar to many Western European countries. Most programs from that era are still in use, although many were scaled back during the 1990s as government priorities shifted towards reducing debt and deficits.\n\nDanish welfare is handled by the state through a series of policies (and the like) that seeks to provide welfare services to citizens, hence the term welfare state. This refers not only to social benefits, but also tax-funded education, public child care, medical care, etc. A number of these services are not provided by the state directly, but administered by municipalities, regions or private providers through outsourcing. This sometimes gives a source of tension between the state and municipalities, as there is not always consistency between the promises of welfare provided by the state (i.e. parliament) and local perception of what it would cost to fulfill these promises.\n\nSolidarity is a strong value of the French Social Protection system. The first article of the French Code of Social Security describes the principle of solidarity. Solidarity is commonly comprehended in relations of similar work, shared responsibility and common risks. Existing solidarities in France caused the expansion of health and social security.\n\nThe welfare state has a long tradition in Germany dating back to the industrial revolution. Due to the pressure of the workers' movement in the late 19th century, Reichskanzler Otto von Bismarck introduced the first rudimentary state social insurance scheme. Under Adolf Hitler, the National Socialist Program stated \"We demand an expansion on a large scale of old age welfare\". Today, the social protection of all its citizens is considered a central pillar of German national policy. 27.6 percent of Germany's GDP is channeled into an all-embracing system of health, pension, accident, longterm care and unemployment insurance, compared to 16.2 percent in the US. In addition, there are tax-financed services such as child benefits (\"Kindergeld\", beginning at €192 per month for the first and second child, €198 for the third and €223 for each child thereafter, until they attain 25 years or receive their first professional qualification), and basic provisions for those unable to work or anyone with an income below the poverty line.\n\nSince 2005, reception of full unemployment pay (60–67% of the previous net salary) has been restricted to 12 months in general and 18 months for those over 55. This is now followed by (usually much lower) \"Arbeitslosengeld II (ALG II)\" or \"Sozialhilfe\", which is independent of previous employment (Hartz IV concept).\n\nUnder ALG II, a single person receives €391 per month plus the cost of 'adequate' housing and health insurance. ALG II can also be paid partially to supplement a low work income.\n\nThe Italian welfare state's foundations were laid along the lines of the corporatist-conservative model, or of its Mediterranean variant. Later, in the 1960s and 1970s, increases in public spending and a major focus on universality brought it on the same path as social-democratic systems. In 1978, a universalistic welfare model was introduced in Italy, offering a number of universal and free services such as a National Health Fund.\n\nSocial welfare, assistance for the ill or otherwise disabled and for the old, has long been provided in Japan by both the government and private companies. Beginning in the 1920s, the government enacted a series of welfare programs, based mainly on European models, to provide medical care and financial support. During the postwar period, a comprehensive system of social security was gradually established.\n\nThe 1980s marked a change in the structure of Latin American social protection programs. Social protection embraces three major areas: social insurance, financed by workers and employers; social assistance to the population's poorest, financed by the state; and labor market regulations to protect worker rights. Although diverse, recent Latin American social policy has tended to concentrate on social assistance.\n\nThe 1980s had a significant effect on social protection policies. Prior to the 1980s, most Latin American countries focused on social insurance policies involving formal sector workers, assuming that the informal sector would disappear with economic development. The economic crisis of the 1980s and the liberalization of the labor market led to a growing informal sector and a rapid increase in poverty and inequality. Latin American countries did not have the institutions and funds to properly handle such a crisis, both due to the structure of the social security system, and to the previously implemented structural adjustment policies (SAPs) that had decreased the size of the state.\n\nNew Welfare programs have integrated the multidimensional, social risk management, and capabilities approaches into poverty alleviation. They focus on income transfers and service provisions while aiming to alleviate both long- and short-term poverty through, among other things, education, health, security, and housing. Unlike previous programs that targeted the working class, new programs have successfully focused on locating and targeting the very poorest.\n\nThe impacts of social assistance programs vary between countries, and many programs have yet to be fully evaluated. According to Barrientos and Santibanez, the programs have been more successful in increasing investment in human capital than in bringing households above the poverty line. Challenges still exist, including the extreme inequality levels and the mass scale of poverty; locating a financial basis for programs; and deciding on exit strategies or on the long-term establishment of programs.\n\nThe economic crisis of the 1980s led to a shift in social policies, as understandings of poverty and social programs evolved (24). New, mostly short-term programs emerged. These include:\n\n\nNew Zealand is often regarded as having one of the first comprehensive welfare systems in the world. During the 1890s a Liberal government adopted many social programmes to help the poor who had suffered from a long economic depression in the 1880s. One of the most far reaching was the passing of tax legislation that made it difficult for wealthy sheep farmers to hold onto their large land holdings. This and the invention of refrigeration led to a farming revolution where many sheep farms were broken up and sold to become smaller dairy farms. This enabled thousands of new farmers to buy land and develop a new and vigorous industry that has become the backbone of New Zealand's economy to this day. This liberal tradition flourished with increased enfranchisement for indigenous Maori in the 1880s and women. Pensions for the elderly, the poor and war casualties followed, with State-run schools, hospitals and subsidized medical and dental care. By 1960 New Zealand was able to afford one of the best-developed and most comprehensive welfare systems in the world, supported by a well-developed and stable economy.\n\nSocial welfare in Sweden is made up of several organizations and systems dealing with welfare. It is mostly funded by taxes, and executed by the public sector on all levels of government as well as private organizations. It can be separated into three parts falling under three different ministries; social welfare, falling under the responsibility of Ministry of Health and Social Affairs; education, under the responsibility of the Ministry of Education and Research and labor market, under the responsibility of Ministry of Employment.\n\nGovernment pension payments are financed through an 18.5% pension tax on all taxed incomes in the country, which comes partly from a tax category called a public pension fee (7% on gross income), and 30% of a tax category called employer fees on salaries (which is 33% on a netted income). Since January 2001 the 18.5% is divided in two parts: 16% goes to current payments, and 2.5% goes into individual retirement accounts, which were introduced in 2001. Money saved and invested in government funds, and IRAs for future pension costs, are roughly 5 times annual government pension expenses (725/150).\n\n\n\nThe United Kingdom has a long history of welfare, notably including the English Poor laws which date back to 1536. After various reforms to the program, which involved workhouses, it was eventually abolished and replaced with a modern system by laws such as National Assistance Act 1948.\n\nIn more recent times, comparing the Cameron–Clegg coalition's austerity measures with the Opposition's, the respected \"Financial Times\" commentator Martin Wolf commented that the \"big shift from Labour ... is the cuts in welfare benefits.\" The government's austerity programme, which involves reduction in government policy, has been linked to a rise in food banks. A study published in the \"British Medical Journal\" in 2015 found that each 1 percentage point increase in the rate of Jobseeker's Allowance claimants sanctioned was associated with a 0.09 percentage point rise in food bank use. The austerity programme has faced opposition from disability rights groups for disproportionately affecting disabled people. The \"bedroom tax\" is an austerity measure that has attracted particular criticism, with activists arguing that two thirds of council houses affected by the policy are occupied with a person with a disability.\n\nIn the United States, depending on the context, the term “welfare” can be used to refer to means-tested cash benefits, especially the Aid to Families with Dependent Children (AFDC) program and its successor, the Temporary Assistance for Needy Families Block Grant, or it can be used to refer to all means-tested programs that help individuals or families meet basic needs, including, for example, health care through Medicaid, Supplemental Security Income (SSI) benefits and food and nutrition programs (SNAP). It can also include Social Insurance programs such as Unemployment Insurance, Social Security, and Medicare.\n\nAFDC (originally called Aid to Dependent Children) was created during the Great Depression to alleviate the burden of poverty for families with children and allow widowed mothers to maintain their households. The New Deal employment program such as the Works Progress Administration primarily served men. Prior to the New Deal, anti-poverty programs were primarily operated by private charities or state or local governments; however, these programs were overwhelmed by the depth of need during the Depression. The United States has no national program of cash assistance for non-disabled poor individuals who are not raising children.\n\nUntil early in the year of 1965, the news media was conveying only whites as living in poverty however that perception had changed to blacks. Some of the influences in this shift could have been the civil rights movement and urban riots from the mid 60s. Welfare had then shifted from being a White issue to a Black issue and during this time frame the war on poverty had already begun. Subsequently, news media portrayed stereotypes of Blacks as lazy, undeserving and welfare queens. These shifts in media don't necessarily establish the population living in poverty decreasing.\nIn 1996, the Personal Responsibility and Work Opportunity Reconciliation Act changed the structure of Welfare payments and added new criteria to states that received Welfare funding. After reforms, which President Clinton said would \"end Welfare as we know it\", amounts from the federal government were given out in a flat rate per state based on population. Each state must meet certain criteria to ensure recipients are being encouraged to work themselves out of Welfare. The new program is called Temporary Assistance for Needy Families (TANF). It encourages states to require some sort of employment search in exchange for providing funds to individuals, and imposes a five-year lifetime limit on cash assistance. In FY 2010, 31.8% of TANF families were white, 31.9% were African-American, and 30.0% were Hispanic.\n\nAccording to the U.S. Census Bureau data released September 13, 2011, the nation's poverty rate rose to 15.1% (46.2 million) in 2010, up from 14.3% (approximately 43.6 million) in 2009 and to its highest level since 1993. In 2008, 13.2% (39.8 million) Americans lived in relative poverty.\n\nIn a 2011 op-ed in \"Forbes\", Peter Ferrara stated that, \"The best estimate of the cost of the 185 federal means tested Welfare programs for 2010 for the federal government alone is nearly $700 billion, up a third since 2008, according to the Heritage Foundation. Counting state spending, total Welfare spending for 2010 reached nearly $900 billion, up nearly one-fourth since 2008 (24.3%)\". California, with 12% of the U.S. population, has one-third of the nation's welfare recipients.\n\nIn FY 2011, federal spending on means-tested welfare, plus state contributions to federal programs, reached $927 billion per year. Roughly half of this welfare assistance, or $462 billion went to families with children, most of which are headed by single parents.\n\nThe United States has also typically relied on charitable giving through non-profit agencies and fundraising instead of direct monetary assistance from the government itself. According to Giving USA, Americans gave $358.38 billion to charity in 2014. This is rewarded by the United States government through tax incentives for individuals and companies that are not typically seen in other countries.\n\nIncome transfers can be either conditional or unconditional. Conditionalities are sometimes criticized as being paternalistic and unnecessary.\n\nCurrent programs have been built as short-term rather than as permanent institutions, and many of them have rather short time spans (around five years). Some programs have time frames that reflect available funding. One example of this is Bolivia's Bonosol, which is financed by proceeds from the privatization of utilities—an unsustainable funding source. Some see Latin America's social assistance programs as a way to patch up high levels of poverty and inequalities, partly brought on by the current economic system.\n\nSome opponents of welfare argue that it affects work incentives. They also argue that the taxes levied can also affect work incentives. A good example of this would be the reform of the Aid to Families with Dependent Children (AFDC) program. Per AFDC, some amount per recipient is guaranteed. However, for every dollar the recipient earns the monthly stipend is decreased by an equivalent amount. For most persons, this reduces their incentive to work. This program was replaced by Temporary Aid to Needy Families (TANF). Under TANF, people were required to actively seek employment while receiving aid and they could only receive aid for a limited amount of time. However, states can choose the amount of resources they will devote to the program.\n\n\n"}
{"id": "32187890", "url": "https://en.wikipedia.org/wiki?curid=32187890", "title": "Cabinet (government)", "text": "Cabinet (government)\n\nA cabinet is a body of high-ranking state officials, typically consisting of the top leaders of the executive branch. Members of a cabinet are usually called cabinet ministers or secretaries. The function of a cabinet varies: in some countries it is a collegiate decision-making body with collective responsibility, while in others it may function either as a purely advisory body or an assisting institution to a decision making head of state or head of government. Cabinets are typically the body responsible for the day-to-day management of the government and response to sudden events, whereas the legislative and judicial branches work in a measured pace, in sessions according to lengthy procedures.\n\nIn some countries, particularly those that use a parliamentary system (e.g., the UK), the Cabinet collectively decides the government's direction, especially in regard to legislation passed by the parliament. In countries with a presidential system, such as the United States, the Cabinet does not function as a collective legislative influence; rather, their primary role is as an official advisory council to the head of government. In this way, the President obtains opinions and advice relating to forthcoming decisions. Legally, under both types of system, the Westminster variant of a parliamentary system and the presidential system, the Cabinet \"advises\" the Head of State: the difference is that, in a parliamentary system, the monarch, viceroy or ceremonial president will almost always follow this advice, whereas in a presidential system, a president who is also head of government and political leader may depart from the Cabinet's advice if they do not agree with it. In practice, in nearly all parliamentary democracies that do not follow the Westminster system, and in three countries that do (Japan, Ireland, and Israel), very often the Cabinet does not \"advise\" the Head of State as they play only a ceremonial role. Instead, it is usually the head of government (usually called Prime Minister) who holds all means of power in their hands (e.g. in Germany, Sweden, etc.) and to whom the Cabinet reports.\n\nThe second role of cabinet officials is to administer executive branches, government agencies, or departments. In the United States federal government, these are the federal executive departments. Cabinets are also important originators for legislation. Cabinets and ministers are usually in charge of the preparation of proposed legislation in the ministries before it is passed to the parliament. Thus, often the majority of new legislation actually originates from the cabinet and its ministries.\n\nIn most governments, members of the Cabinet are given the title of Minister, and each holds a different portfolio of government duties (\"Minister for the Environment\", etc.). In a few governments, as in the case of Mexico, the Philippines, the United Kingdom, and United States, the title of Secretary is also used for some Cabinet members (\"Secretary of Education\", or \"Secretary of State for X\" in the UK). In many countries (e.g. Germany, Luxembourg, France, etc.), a Secretary (of State) is a cabinet member with an inferior rank to a Minister. In Finland, a Secretary of State is a career official that serves the Minister. In some countries, the Cabinet is known by names such as \"Council of Ministers\", \"Government Council\" or \"Council of State\", or by lesser known names such as \"Federal Council\" (in Switzerland), \"Inner Council\" or \"High Council\". These countries may differ in the way that the cabinet is used or established.\n\nThe supranational European Union uses a different convention: the European Commission refers to its executive cabinet as a \"college\", with its top public officials referred to as \"commissioners\", whereas a \"European Commission cabinet\" is the personal office of a European Commissioner.\n\nIn presidential systems such as the United States, members of the Cabinet are chosen by the president, and may also have to be confirmed by one or both of the houses of the legislature. In most presidential systems, cabinet members cannot be sitting legislators, and legislators who are offered appointments must resign if they wish to accept.\n\nIn parliamentary systems, several different policies exist with regard to whether legislators can also be Cabinet ministers: cabinet members must, must not, or may be members of parliament, depending on the country. In the United Kingdom, cabinet ministers are mandatorily appointed from among sitting members of the parliament. In countries with a strict separation between the executive and legislative branches of government, e.g. Luxembourg, Switzerland and Belgium, cabinet members have to give up their seat in parliament. The intermediate case is when ministers typically are members of parliament, but are not required to be, as in Finland. The candidate prime minister and/or the president selects the individual ministers to be proposed to the parliament, which may accept or reject the proposed cabinet composition. Unlike in a presidential system, the cabinet in a parliamentary system must not only be confirmed, but enjoy the continuing confidence of the parliament: a parliament can pass a motion of no confidence to remove a government or individual ministers. Often, but not necessarily, these votes are taken across party lines.\n\nIn some countries (e.g. the US) attorneys general also sit in the cabinet, while in many others this is strictly prohibited as the attorneys general are considered to be part of the judicial branch of government. Instead, there is a minister of justice, separate from the attorney general. Furthermore, in Sweden, Finland and Estonia, the cabinet includes a Chancellor of Justice, a civil servant that acts as the legal counsel to the cabinet.\n\nIn multi-party systems, the formation of a government may require the support of multiple parties. Thus, a coalition government is formed. Continued cooperation between the participating political parties is necessary for the cabinet to retain the confidence of the parliament. For this, a government platform is negotiated, in order for the participating parties to toe the line and support their cabinet. However, this is not always successful: individual parties or members of parliament can still vote against the government, and the cabinet can break up from internal disagreement or be dismissed by a motion of no confidence.\n\nThe size of cabinets varies, although most contain around ten to twenty ministers. Researchers have found an inverse correlation between a country's level of development and cabinet size: on average, the more developed a country is, the smaller is its cabinet.\n\nIn the United Kingdom and its colonies, cabinets began as smaller sub-groups of the English Privy Council. The term comes from the name for a relatively small and private room used as a study or retreat. Phrases such as \"cabinet counsel,\" meaning advice given in private to the monarch, occur from the late 16th century, and, given the non-standardized spelling of the day, it is often hard to distinguish whether \"council\" or \"counsel\" is meant.\n\nThe \"Oxford English Dictionary\" credits Francis Bacon in his \"Essays\" (1605) with the first use of \"Cabinet council\", where it is described as a foreign habit, of which he disapproves: \"For which inconveniences, the doctrine of Italy, and practice of France, in some kings' times, hath introduced cabinet counsels; a remedy worse than the disease\".\n\nCharles I began a formal \"Cabinet Council\" from his accession in 1625, as his Privy Council, or \"private council\", was evidently not private enough, and the first recorded use of \"cabinet\" by itself for such a body comes from 1644, and is again hostile and associates the term with dubious foreign practices. The process has repeated itself in recent times, as leaders have felt the need to have a Kitchen Cabinet or \"sofa government\".\n\nUnder the Westminster system, members of the cabinet are Ministers of the Crown who are collectively responsible for all government policy. All ministers, whether senior and in the cabinet or junior ministers, must publicly support the policy of the government, regardless of any private reservations. Although, in theory, all cabinet decisions are taken collectively by the cabinet, in practice many decisions are delegated to the various sub-committees of the cabinet, which report to the full cabinet on their findings and recommendations. As these recommendations have already been agreed upon by those in the cabinet who hold affected ministerial portfolios, the recommendations are usually agreed to by the full cabinet with little further discussion. The cabinet may also provide ideas on/if new laws were established, and what they include. Cabinet deliberations are secret and documents dealt with in cabinet are confidential. Most of the documentation associated with cabinet deliberations will only be publicly released a considerable period after the particular cabinet disbands, depending on provisions of a nation's freedom of information legislation.\n\nIn theory the prime minister or premier is first among equals. However, the prime minister is the person from whom the head of state will ultimately take advice on the exercise of executive power, which may include the powers to declare war, use nuclear weapons, expel ministers from the cabinet, and to determine their portfolios in a cabinet reshuffle. This position in relation to the executive power means that, in practice, the prime minister has a high degree of control over the cabinet: any spreading of responsibility for the overall direction of the government has usually been done as a matter of preference by the prime minister – either because they are unpopular with their backbenchers, or because they believe that the cabinet should collectively decide things.\n\nThe \"shadow cabinet\" consists of the leading members, or frontbenchers, of an opposition party, who generally hold critic portfolios \"shadowing\" cabinet ministers, questioning their decisions and proposing policy alternatives. In some countries, the shadow ministers are referred to as spokespersons.\n\nThe Westminster cabinet system is the foundation of cabinets as they are known at the federal and provincial (or state) jurisdictions of Australia, Bangladesh, Canada, Pakistan, India, South Africa, New Zealand, and other Commonwealth of Nations countries whose parliamentary model is closely based on that of the United Kingdom.\n\nUnder the doctrine of separation of powers, a cabinet under a presidential system of government is part of the executive branch. In addition to administering their respective segments of the executive branch, cabinet members are responsible for advising the head of government on areas within their purview.\n\nThey are appointed by and serve at the pleasure of the head of government and are therefore strongly subordinate to the president as they can be replaced at any time. Normally, since they are appointed by the president, they are members of the same political party, but the executive is free to select anyone, including opposition party members, subject to the advice and consent of the Senate.\n\nNormally, the legislature or a segment thereof must confirm the appointment of a cabinet member; this is but one of the many checks and balances built into a presidential system. The legislature may also remove a cabinet member through a usually difficult impeachment process.\n\nIn the Cabinet, members do not serve to influence legislative policy to the degree found in a Westminster system; however, each member wields significant influence in matters relating to their executive department. Since the administration of Franklin D. Roosevelt, the President of the United States has acted most often through his own executive office or the National Security Council rather than through the Cabinet as was the case in earlier administrations.\n\nAlthough the term 'Secretary' is usually used to name the most senior official of a government department, some departments have different titles to name such officials. For instance, the Department of Justice uses the term \"Attorney General\" instead of \"Justice Secretary\" but the Attorney General is nonetheless a cabinet-level position.\n\nCommunist states can be ruled \"de facto\" by the Politburo, such as the Politburo of the Communist Party of the Soviet Union. This is an organ of the Communist Party and not a state organ, but due to one-party rule, the state and its cabinet (e.g. Government of the Soviet Union) are in practice subordinate to the Politburo. Technically a Politburo is overseen and its members selected by the Central Committee, but in practice it was often the other way around: powerful members of the Politburo would ensure their support in the Central Committee through patronage. In China, political power has been further centralized into a standing committee of the Politburo.\n\n"}
{"id": "25914154", "url": "https://en.wikipedia.org/wiki?curid=25914154", "title": "Coup d'état", "text": "Coup d'état\n\nA coup d'état ( ; , literally \"blow of state\"; plural: \"coups d'état\", pronounced like the singular form), also known simply as a coup (), or an overthrow, is the overthrow of an existing government by non-democratic means; typically, it is an illegal, unconstitutional seizure of power by a dictator, the military, or a political faction. A coup d'état is considered successful when the usurpers seize and hold power for at least seven days.\n\nThe phrase coup d'état comes from French, literally meaning a \"stroke of state\" or \"blow against the state\". In French, the word \"État\" (), denoting a sovereign political entity, is capitalized.\n\nAlthough the concept of a coup d'état has featured in politics since antiquity, the phrase is of relatively recent coinage; the \"Oxford English Dictionary\" identifies it as a French expression meaning a \"stroke of state\". The phrase did not appear within an English text before the 19th century except when used in translation of a French source, there being no simple phrase in English to convey the contextualized idea of a \"knockout blow to the existing administration within a state\".\n\nOne early use within text translated from French was in 1785 in a printed translation of a letter from a French merchant, commenting on an arbitrary decree or \"arrêt\" issued by the French king restricting the import of British wool. What may be its first published use within a text composed in English is an editor's note in the London \"Morning Chronicle\", 7 January 1802, reporting the arrest by Napoleon in France, of Moreau, Berthier, Masséna, and Bernadotte:\n\nThere was a report in circulation yesterday of a sort of coup d'état having taken place in France, in consequence of some formidable conspiracy against the existing government.\nIn post-Revolutionary France, the phrase came to be used to describe the various murders by Napoleon's hated secret police, the Gens d'Armes d'Elite, who murdered the Duke of Enghien:\n...the actors in torture, the distributors of the poisoning draughts, and the secret executioners of those unfortunate individuals or families, whom Bonaparte's measures of safety require to remove. In what revolutionary tyrants call \"grand\"[s]\" coups d'état\", as butchering, or poisoning, or drowning, \"en masse\", they are exclusively employed.\nClayton Thyne and Jonathan Powell's dataset of coups defines attempted coups as \"illegal and overt attempts by the military or other elites within the state apparatus to unseat the sitting executive.\" They arrive at this definition by combining common definitions in the existing literature, and removing specificities and ambiguities that exist in many definitions.\n\nIn looser usage, as in \"intelligence coup\" or \"boardroom coup\", the term simply refers to gaining a sudden advantage on a rival.\n\nSince an unsuccessful coup d'état in 1920 (the Kapp Putsch), the Swiss-German word Putsch (pronounced , coined for the Züriputsch of 6 September 1839, in Zurich), also denotes the politico-military actions of an unsuccessful minority reactionary coup.\n\nOther recent and notable unsuccessful minority reactionary coups that are often referred to as \"Putsches\" are the 1923 Beer Hall Putsch and Küstrin Putsch, the 1961 Algiers Putsch and the 1991 August Putsch. Putsch was used as disinformation by Hitler and other Nazi party members to falsely claim that he had to suppress a reactionary coup during the Night of the Long Knives. Germans still use the term to describe the murders, the term given to it by the Nazi regime, despite its unproven implication that the murders were necessary to prevent a coup. Thus, German authors often use quotation marks or write about the (\"so-called Röhm Putsch\") for emphasis.\n\nOne author makes a distinction between a coup and a . In a coup, it is the military, paramilitary, or opposing political faction that deposes the current government and assumes power; whereas, in the , the military deposes the existing government and installs an (ostensibly) civilian government.\n\nAccording to Clayton Thyne and Jonathan Powell's coup dataset, there were 457 coup attempts from 1950 to 2010, of which 227 (49.7%) were successful and 230 (50.3%) were unsuccessful. They find that coups have \"been most common in Africa and the Americas (36.5% and 31.9%, respectively). Asia and the Middle East have experienced 13.1% and 15.8% of total global coups, respectively. Europe has experienced by far the fewest coup attempts: 2.6%.\" Most coup attempts occurred in the mid-1960s, but there were also large numbers of coup attempts in the mid-1970s and the early 1990s. Numbers of successful coups have decreased over time. Coups occurring in the post-Cold War period have been more likely to result in democratic systems. Coups that occur during civil wars shorten the war's duration. Research suggests that protests spur coups, as they help elites within the state apparatus to coordinate coups.\n\nA 2016 study categorizes coups into four possible outcomes:\n\nThe study also found that about half of all coups—both during and after the Cold War—install new autocratic regimes. New dictatorships launched by coups engage in higher levels of repression in the year that follows the coup than existed in the year leading to the coup. One-third of coups during the Cold War and 10% of post-Cold War coups reshuffled the regime leadership. Democracies were installed in the wake of 12% of Cold War coups and 40% of post-Cold War coups.\n\nA 2003 review of the academic literature found that the following factors were associated with coups:\n\n\nThe literature review in a 2016 study includes mentions of ethnic factionalism, supportive foreign governments, leader inexperience, slow growth, commodity price shocks, and poverty.\n\nThe cumulative number of coups is a strong predictor of future coups. Hybrid regimes are more vulnerable to coups than are very authoritarian states or democratic states. A 2015 study finds that terrorism is strongly associated with re-shuffling coups. A 2016 study finds that there is an ethnic component to coups: \"When leaders attempt to build ethnic armies, or dismantle those created by their predecessors, they provoke violent resistance from military officers.\" Another 2016 study shows that protests increase the risk of coups, presumably because they ease coordination obstacles among coup plotters and make international actors less likely to punish coup leaders. A third 2016 study finds that coups become more likely in the wake of elections in autocracies when the results reveal electoral weakness for the incumbent autocrat. A fourth 2016 study finds that inequality between social classes increases the likelihood of coups. A fifth 2016 study rejects the notion that participation in war makes coups more likely; on the contrary, coup risk declines in the presence of enduring interstate conflict. A sixth 2016 study finds no evidence that coups are contagious; one coup in a region does not make other coups in the region likely to follow. One study found that coups are more likely to occur in states with small populations, as there are smaller coordination problems for coup-plotters.\n\nA 2017 study in the journal \"Security Studies\" found that autocratic leaders whose states were involved in international rivalries over disputed territory were more likely to be overthrown in a coup. The authors of the study provide the following logic for why this is: \"Autocratic incumbents invested in spatial rivalries need to strengthen the military in order to compete with a foreign adversary. The imperative of developing a strong army puts dictators in a paradoxical situation: to compete with a rival state, they must empower the very agency—the military—that is most likely to threaten their own survival in office.\" However, a 2016 study in the journal \"Conflict Management and Peace Science\" found that leaders who were involved in militarized confrontations and conflicts were less likely to face a coup in the year following the dispute.\n\nA 2018 study in the \"Journal of Peace Research\" found that coup attempts were less likely in states where the militaries derived significant incomes from peacekeeping missions. The study argued that militaries were dissuaded from staging coups because they feared that the UN would no longer enlist the military in peacekeeping missions.\n\nA 2018 study in the \"Economic Journal\" found that \"oil price shocks are seen to promote coups in onshore-intensive oil countries, while preventing them in offshore-intensive oil countries.\" The study argues that states which have onshore oil wealth tend to build up their military to protect the oil, whereas states do not do that for offshore oil wealth.\n\nA 2018 study in the \"Journal of Conflict Resolution\" found that the presence of military academies were linked to coups. The authors argue that military academies make it easier for military officers to plan coups, as the schools build networks among military officers.\n\nA 2019 study in the \"Journal of Politics\" found that states that had recently signed civil war peace agreements were much more likely to experience coups, in particular when those agreements contained provisions that jeopardized the interests of the military.\n\nA 2019 study found that regional rebellions made coups by the military more likely.\n\nA 2020 study found that elections had a two-sided impact on coup attempts, depending on the state of the economy. During periods of economic expansion, elections reduced the likelihood of coup attempts, whereas elections during economic crises increased the likelihood of coup attempts.\n\nIn what is referred to as \"coup-proofing\", regimes create structures that make it hard for any small group to seize power. These coup-proofing strategies may include the strategic placing of family, ethnic, and religious groups in the military; creation of an armed force parallel to the regular military; and development of multiple internal security agencies with overlapping jurisdiction that constantly monitor one another. Research shows that some coup-proofing strategies reduce the risk of coups occurring. However, coup-proofing reduces military effectiveness, and limits the rents that an incumbent can extract.\n\nA 2016 study shows that the implementation of succession rules reduce the occurrence of coup attempts. Succession rules are believed to hamper coordination efforts among coup plotters by assuaging elites who have more to gain by patience than by plotting.\n\nAccording to political scientists Curtis Bell and Jonathan Powell, coup attempts in neighbouring countries lead to greater coup-proofing and coup-related repression in a region. A 2017 study finds that countries' coup-proofing strategies are heavily influenced by other countries with similar histories.\n\nA 2018 study in the \"Journal of Peace Research\" found that leaders who survive coup attempts and respond by purging known and potential rivals are likely to have longer tenures as leaders. A 2019 study in \"Conflict Management and Peace Science\" found that personalist dictatorships are more likely to take coup-proofing measures than other authoritarian regimes; the authors argue that this is because \"personalists are characterized by weak institutions and narrow support bases, a lack of unifying ideologies and informal links to the ruler.\"\n\nResearch suggests that coups promoting democratization in staunchly authoritarian regimes have become less likely to end democracy over time, and that the positive influence has strengthened since the end of the Cold War.\n\nA 2014 study found that \"coups promote democratization, particularly among states that are least likely to democratize otherwise\". The authors argue that coup attempts can have this consequence because leaders of successful coups have incentives to democratize quickly in order to establish political legitimacy and economic growth, while leaders who stay in power after failed coup attempts see it as a sign that they must enact meaningful reforms to remain in power. A 2014 study found that 40% of post-Cold War coups were successful. The authors argue that this may be due to the incentives created by international pressure. A 2016 study found that democracies were installed in 12% of Cold War coups and 40% of the post-Cold War coups.\n\nAccording to political scientist Ilya Somin to forcibly overthrow democratic government, sometimes might be justified.\nHe wrote:\n\nAccording to Naunihal Singh, author of \"Seizing Power: The Strategic Logic of Military Coups\" (2014), it is \"fairly rare\" for the prevailing existing government to violently purge the army after a coup has been foiled. If it starts the mass killing of elements of the army, including officers who were not involved in the coup, this may trigger a \"counter-coup\" by soldiers who are afraid they will be next. To prevent such a desperate counter-coup that may be more successful than the initial attempt, governments usually resort to firing prominent officers and replacing them with loyalists instead.\n\nSome research suggests that increased repression and violence typically follow coup attempts (whether they be successes or failures). However, some tentative analysis by political scientist Jay Ulfelder finds no clear pattern of deterioration in human rights practices in wake of failed coups in post-Cold War era.\n\nNotable counter-coups include the Ottoman countercoup of 1909, the 1960 Laotian counter-coup, the Indonesian mass killings of 1965–66, the 1966 Nigerian counter-coup, the 1967 Greek counter-coup, 1971 Sudanese counter-coup, and the Coup d'état of December Twelfth in South Korea.\n\nA 2017 study finds that the use of state broadcasting by the \"putschist\" regime after Mali's 2012 coup did not elevate explicit approval for the regime.\n\nAccording to a 2019 study, coup attempts lead to a reduction in physical integrity rights.\n\nThe international community tends to react adversely to coups by reducing aid and imposing sanctions. A 2015 study finds that \"coups against democracies, coups after the Cold War, and coups in states heavily integrated into the international community are all more likely to elicit global reaction.\" Another 2015 study shows that coups are the strongest predictor for the imposition of democratic sanctions. A third 2015 study finds that Western states react strongest against coups of possible democratic and human rights abuses. A 2016 study shows that the international donor community in the post-Cold War period penalizes coups by reducing foreign aid. The US has been inconsistent in applying aid sanctions against coups both during the Cold War and post-Cold War periods, a likely consequence of its geopolitical interests.\n\nOrganizations such as the African Union (AU) and the Organization of American States (OAS) have adopted anti-coup frameworks. Through the threat of sanctions, the organizations actively try to curb coups. A 2016 study finds that the AU has played a meaningful role in reducing African coups.\n\nA forthcoming study in the \"Journal of Conflict Resolution\" finds that negative international responses to regimes created in coups have a significant influence on the sustainability of those regimes. The study finds that \"state reactions have the strongest effect during the Cold War, while international organizations matter the most afterward.\" Negative international responses from strong actors matter the most.\n\n"}
{"id": "147116", "url": "https://en.wikipedia.org/wiki?curid=147116", "title": "Cult of personality", "text": "Cult of personality\n\nA cult of personality, or cult of the leader, arises when a country's regime – or, more rarely, an individual – uses the techniques of mass media, propaganda, the big lie, spectacle, the arts, patriotism, and government-organized demonstrations and rallies to create an idealized, heroic, and worshipful image of a leader, often through unquestioning flattery and praise. A cult of personality is similar to apotheosis, except that it is established by modern social engineering techniques, usually by the state or the party in one-party states and dominant-party states. It is often seen in totalitarian or authoritarian countries.\n\nThe term came to prominence in 1956, in Nikita Khrushchev's secret speech \"On the Cult of Personality and Its Consequences\", given on the final day of the 20th Congress of the Communist Party of the Soviet Union. In the speech, Khrushchev, who was the First Secretary of the Communist Party – in effect, the leader of the country – criticized the lionization and idealization of Joseph Stalin, and by implication, his Communist contemporary Mao Zedong, as being contrary to Marxist doctrine. The speech was later made public and was part of the \"de-Stalinization\" process the Soviet Union went through.\n\nThe Imperial cult of ancient Rome identified emperors and some members of their families with the divinely sanctioned authority (auctoritas) of the Roman State. Throughout history, monarchs and other heads of state were often held in enormous reverence and imputed super-human qualities. Through the principle of the divine right of kings, in medieval Europe for example, rulers were said to hold office by the will of God. Ancient Egypt, Imperial Japan, the Inca, the Aztecs, Tibet, Siam (now Thailand), and the Roman Empire are especially noted for redefining monarchs as \"god-kings\".\n\nThe spread of democratic and secular ideas in Europe and North America in the 18th and 19th centuries made it increasingly difficult for monarchs to preserve this aura. However, the subsequent development of mass media, such as radio, enabled political leaders to project a positive image of themselves onto the masses as never before. It was from these circumstances in the 20th century that the most notorious personality cults arose. Often these cults are a form of political religion.\n\nThe term \"cult of personality\" probably appeared in English around 1800–1850, along with the French and German use. At first it had no political connotations but was instead closely related to the Romantic \"cult of genius\". The political use of the phrase came first in a letter from Karl Marx to German political worker, Wilhelm Blos, 10 November 1877:\n\nThere are various views about what constitutes a cult of personality in a leader. Historian Jan Plamper has written that modern-day personality cults display five characteristics that set them apart from \"their predecessors\": The cults are secular and \"anchored in popular sovereignty\"; their objects are all males; they target the entire population, not only the well-to-do or just the ruling class; they use mass media; and they exist where the mass media can be controlled enough to inhibit the introduction of \"rival cults\".\n\nIn his 2013 paper, \"What is character and why it really does matter\", Thomas A. Wright states, \"The cult of personality phenomenon refers to the idealized, even god-like, public image of an individual consciously shaped and molded through constant propaganda and media exposure. As a result, one is able to manipulate others based entirely on the influence of public personality...the cult of personality perspective focuses on the often shallow, external images that many public figures cultivate to create an idealized and heroic image.\"\n\nAdrian Teodor Popan defines cult of personality as a \"quantitatively exaggerated and qualitatively extravagant public demonstration of praise of the leader\". He also identifies three causal \"necessary, but not sufficient, structural conditions, and a path dependent chain of events which, together, lead to the cult formation: a particular combination of patrimonialism and clientelism, lack of dissidence, and systematic falsification pervading the society’s culture.\"\n\nThe mass media have played an instrumental role in forging national leaders' cults of personality.\n\nThomas A. Wright in 2013 reported that \"It is becoming evident that the charismatic leader, especially in politics, has increasingly become the product of media and self-exposure.\" And, focusing on the media in the United States, Robert N. Bellah adds that, \"It is hard to determine the extent to which the media reflect the cult of personality in American politics and to what extent they have created it. Surely they did not create it all alone, but just as surely they have contributed to it. In any case, American politics is dominated by the personalities of political leaders to an extent rare in the modern world...in the personalised politics of recent years the \"charisma\" of the leader may be almost entirely a product of media exposure.\"\n\nOften, a single leader became associated with this revolutionary transformation and came to be treated as a benevolent \"guide\" for the nation without whom the transformation to a better future could not occur. This has been generally the justification for personality cults that arose in totalitarian societies, such as those of Adolf Hitler, Joseph Stalin, and Mao Zedong. The admiration for Mao Zedong has remained widespread in China. In December 2013, a \"Global Times\" poll revealed that over 85% of Chinese viewed Mao in a positive light. Jan Plamper argues while Napoleon III made some innovations it was Benito Mussolini in Italy in the 1920s who originated the model of dictator-as-cult-figure that was emulated by Hitler, Stalin and the others, using the propaganda powers of a totalitarian state.\n\nPierre du Bois argues that the Stalin cult was elaborately constructed to legitimize his rule. Many deliberate distortions and falsehoods were used. The Kremlin refused access to archival records that might reveal the truth, and key documents were destroyed. Photographs were altered and documents were invented. People who knew Stalin were forced to provide \"official\" accounts to meet the ideological demands of the cult, especially as Stalin himself presented it in 1938 in \"Short Course on the History of the All-Union Communist Party (Bolsheviks)\", which became the official history.\n\nHistorian David L. Hoffmann states \"The Stalin cult was a central element of Stalinism, and as such it was one of the most salient features of Soviet rule...Many scholars of Stalinism cite the cult as integral to Stalin's power or as evidence of Stalin's megalomania.\"\n\nIn Latin America, Cas Mudde and Cristóbal Rovira Kaltwasser link the \"cult of the leader\" to the concept of the \"caudillo\", a strong leader \"who exercises a power that is independent of any office and free of any constraint.\" These populist strongmen are portrayed as \"masculine and potentially violent\" and enhance their authority through the use of the cult of personality. Mudde and Kaltwasser trace the linkage back to Juan Peron of Argentina.\n\n\nNotes\nBibliography\n\nFurther reading\n\n"}
{"id": "10263", "url": "https://en.wikipedia.org/wiki?curid=10263", "title": "Executive (government)", "text": "Executive (government)\n\nThe executive is the branch of government exercising authority in and holding responsibility for the governance of a state. The executive executes and enforces law.\n\nIn political systems based on the principle of separation of powers, authority is distributed among several branches (executive, legislative, judicial)—an attempt to prevent the concentration of power in the hands of a single group of people. In such a system, the executive does not pass laws (the role of the legislature) or interpret them (the role of the judiciary). Instead, the executive enforces the law as written by the legislature and interpreted by the judiciary. The executive can be the source of certain types of law, such as a decree or executive order. Executive bureaucracies are commonly the source of regulations.\n\nIn the Westminster political system, the principle of separation of powers is not as entrenched as in some others. Members of the executive, called ministers, are also members of the legislature, and hence play an important part in both the writing and enforcing of law.\n\nIn this context, the executive consists of a leader(s) of an office or multiple offices. Specifically, the top leadership roles of the executive branch may include:\n\n\nIn a presidential system, the leader of the executive is both the \"head of state and head of government\". In a parliamentary system, a cabinet minister responsible to the legislature is the head of government, while the head of state is usually a largely ceremonial monarch or president.\n\n"}
{"id": "12229", "url": "https://en.wikipedia.org/wiki?curid=12229", "title": "Government", "text": "Government\n\nA government is the system or group of people governing an organized community, often a state.\n\nIn the case of its broad associative definition, government normally consists of legislature, executive, and judiciary. Government is a means by which organizational policies are enforced, as well as a mechanism for determining policy. Each government has a kind of constitution, a statement of its governing principles and philosophy. Typically the philosophy chosen is some balance between the principle of individual freedom and the idea of absolute state authority (tyranny).\n\nWhile all types of organizations have governance, the word \"government\" is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as subsidiary organizations.\n\nHistorically prevalent forms of government include monarchy, aristocracy, timocracy, oligarchy, democracy, theocracy and tyranny. The main aspect of any philosophy of government is how political power is obtained, with the two main forms being electoral contest and hereditary succession.\n\nLibertarianism and anarchism are political ideologies that seek to limit or abolish government, finding government disruptive to self organization and freedom.\n\nA government is the system to govern a state or community.\n\nThe word \"government\" derives, ultimately, from the Greek verb κυβερνάω [\"kubernáo\"] (meaning \"to steer\" with gubernaculum (rudder), the metaphorical sense being attested in Plato's Ship of State).\n\nThe Columbia Encyclopedia defines government as \"a system of social control under which the right to make laws, and the right to enforce them, is vested in a particular group in society\".\n\nWhile all types of organizations have governance, the word \"government\" is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as their subsidiary organizations.\n\nFinally, \"government\" is also sometimes used in English as a synonym for governance.\n\nThe moment and place that the phenomenon of human government developed is lost in time; however, history does record the formations of early governments. About 5,000 years ago, the first small city-states appeared. By the third to second millenniums BC, some of these had developed into larger governed areas: Sumer, Ancient Egypt, the Indus Valley Civilization, and the Yellow River Civilization.\n\nThe development of agriculture and water control projects were a catalyst for the development of governments. On occasion a chief of a tribe was elected by various rituals or tests of strength to govern his tribe, sometimes with a group of elder tribesmen as a council. The human ability to precisely communicate abstract, learned information allowed humans to become ever more effective at agriculture, and that allowed for ever increasing population densities. David Christian explains how this resulted in states with laws and governments:\n\nStarting at the end of the 17th century, the prevalence of republican forms of government grew. The Glorious Revolution in England, the American Revolution, and the French Revolution contributed to the growth of representative forms of government. The Soviet Union was the first large country to have a Communist government. Since the fall of the Berlin Wall, liberal democracy has become an even more prevalent form of government.\n\nIn the nineteenth and twentieth century, there was a significant increase in the size and scale of government at the national level. This included the regulation of corporations and the development of the welfare state.\n\nIn political science, it has long been a goal to create a typology or taxonomy of polities, as typologies of political systems are not obvious. It is especially important in the political science fields of comparative politics and international relations. Like all categories discerned within forms of government, the boundaries of government classifications are either fluid or ill-defined.\n\nSuperficially, all governments have an official or ideal form. The United States is a constitutional republic, while the former Soviet Union was a socialist republic. However self-identification is not objective, and as Kopstein and Lichbach argue, defining regimes can be tricky. For example, Voltaire argued that \"the Holy Roman Empire is neither Holy, nor Roman, nor an Empire\".\n\nIdentifying a form of government is also difficult because many political systems originate as socio-economic movements and are then carried into governments by parties naming themselves after those movements; all with competing political-ideologies. Experience with those movements in power, and the strong ties they may have to particular forms of government, can cause them to be considered as forms of government in themselves.\n\nOther complications include general non-consensus or deliberate \"distortion or bias\" of reasonable technical definitions to political ideologies and associated forms of governing, due to the nature of politics in the modern era. For example: The meaning of \"conservatism\" in the United States has little in common with the way the word's definition is used elsewhere. As Ribuffo notes, \"what Americans now call conservatism much of the world calls liberalism or neoliberalism\"; a \"conservative\" in Finland would be labeled a \"socialist\" in the United States. Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that controlled Congress from 1937 to 1963.\n\nOpinions vary by individuals concerning the types and properties of governments that exist. \"Shades of gray\" are commonplace in any government and its corresponding classification. Even the most liberal democracies limit rival political activity to one extent or another while the most tyrannical dictatorships must organize a broad base of support thereby creating difficulties for \"pigeonholing\" governments into narrow categories. Examples include the claims of the United States as being a plutocracy rather than a democracy since some American voters believe elections are being manipulated by wealthy Super PACs.\n\nThe Classical Greek philosopher Plato discusses five types of regimes: aristocracy, timocracy, oligarchy, democracy and tyranny. Plato also assigns a man to each of these regimes to illustrate what they stand for. The tyrannical man would represent tyranny for example. These five regimes progressively degenerate starting with aristocracy at the top and tyranny at the bottom.\n\nOne method of classifying governments is through which people have the authority to rule. This can either be one person (an autocracy, such as monarchy), a select group of people (an aristocracy), or the people as a whole (a democracy, such as a republic).\n\nThomas Hobbes stated on their classification:\n\nAn autocracy is a system of government in which supreme power is concentrated in the hands of one person, whose decisions are subject to neither external legal restraints nor regularized mechanisms of popular control (except perhaps for the implicit threat of a coup d'état or mass insurrection).\n\nAristocracy (Greek ἀριστοκρατία \"aristokratía\", from ἄριστος \"\" \"excellent\", and κράτος \"\" \"power\") is a form of government that places power in the hands of a small, privileged ruling class.\n\nMany monarchies were aristocracies, although in modern constitutional monarchies the monarch himself or herself has little real power. The term \"Aristocracy\" could also refer to the non-peasant, non-servant, and non-city classes in the Feudal system.\n\nDemocracy is a system of government where the citizens exercise power by voting. In a direct democracy, the citizens as a whole form a governing body and vote directly on each issue. In a representative democracy the citizens elect representatives from among themselves. These representatives meet to form a governing body, such as a legislature. In a constitutional democracy the powers of the majority are exercised within the framework of a representative democracy, but the constitution limits the majority and protects the minority, usually through the enjoyment by all of certain individual rights, e.g. freedom of speech, or freedom of association.\n\nA republic is a form of government in which the country is considered a \"public matter\" (Latin: \"res publica\"), not the private concern or property of the rulers, and where offices of states are subsequently directly or indirectly elected or appointed rather than inherited. The people, or some significant portion of them, have supreme control over the government and where offices of state are elected or chosen by elected people. A common simplified definition of a republic is a government where the head of state is not a monarch. Montesquieu included both democracies, where all the people have a share in rule, and aristocracies or oligarchies, where only some of the people rule, as republican forms of government.\n\nOther terms used to describe different republics include Democratic republic, Parliamentary republic, Federal republic, and Islamic Republic.\n\nFederalism is a political concept in which a \"group\" of members are bound together by covenant with a governing representative head. The term \"federalism\" is also used to describe a system of government in which sovereignty is constitutionally divided between a central governing authority and constituent political units, variously called states, provinces or otherwise. Federalism is a system based upon democratic principles and institutions in which the power to govern is shared between national and provincial/state governments, creating what is often called a federation. Proponents are often called federalists.\n\nHistorically, most political systems originated as socioeconomic ideologies. Experience with those movements in power and the strong ties they may have to particular forms of government can cause them to be considered as forms of government in themselves.\n\nCertain major characteristics are defining of certain types; others are historically associated with certain types of government.\n\nThis list focuses on differing approaches that political systems take to the distribution of sovereignty, and the autonomy of regions within the state.\n\n\n\n"}
{"id": "24406", "url": "https://en.wikipedia.org/wiki?curid=24406", "title": "Parliament", "text": "Parliament\n\nIn modern politics and history, a parliament is a legislative body of government. Generally, a modern parliament has three functions: representing the electorate, making laws, and overseeing the government via hearings and inquiries. The term is similar to the idea of a senate, synod or congress, and is commonly used in countries that are current or former monarchies, a form of government with a monarch as the head. Some contexts restrict the use of the word \"parliament\" to parliamentary systems, although it is also used to describe the legislature in some presidential systems (e.g. the Parliament of Burundi), even where it is not in the official name.\n\nHistorically, parliaments included various kinds of deliberative, consultative, and judicial assemblies, e.g. medieval parliaments.\n\nThe English term is derived from Anglo-Norman and dates to the 14th century, coming from the 11th century Old French , from \"parler\", meaning \"to talk\". The meaning evolved over time, originally referring to any discussion, conversation, or negotiation through various kinds of deliberative or judicial groups, often summoned by a monarch. By the 15th century, in Britain, it had come to specifically mean the legislature.\n\nSince ancient times, when societies were tribal, there were councils or a headman whose decisions were assessed by village elders. This is called tribalism. Some scholars suggest that in ancient Mesopotamia there was a primitive democratic government where the kings were assessed by council. The same has been said about ancient India, where some form of deliberative assemblies existed, and therefore there was some form of democracy. However, these claims are not accepted by most scholars, who see these forms of government as oligarchies.\n\nAncient Athens was the cradle of democracy. The Athenian assembly (, \"ekklesia\") was the most important institution, and every free male citizen could take part in the discussions. Slaves and women could not. However, Athenian democracy was not representative, but rather direct, and therefore the \"ekklesia\" was different from the parliamentary system.\n\nThe Roman Republic had legislative assemblies, who had the final say regarding the election of magistrates, the enactment of new statutes, the carrying out of capital punishment, the declaration of war and peace, and the creation (or dissolution) of alliances. The Roman Senate controlled money, administration, and the details of foreign policy.\n\nSome Muslim scholars argue that the Islamic shura (a method of taking decisions in Islamic societies) is analogous to the parliament. However, others highlight what they consider fundamental differences between the shura system and the parliamentary system.\n\nThe first recorded signs of a council to decide on different issues in ancient Iran dates back to 247 BC while the Parthian empire was in power. The Parthians established the first Iranian empire since the conquest of Persia by Alexander. In the early years of their rule, an assembly of the nobles called “Mehestan” was formed that made the final decision on serious issues of state.\n\nThe word \"Mehestan\" consists of two parts. \"Meh\", a word of the old Persian origin, which literally means \"The Great\" and \"-Stan\", a suffix in the Persian language, which describes an especial place. Altogether Mehestan means a place where the greats come together.\n\nThe Mehestan Assembly, which consisted of Zoroastrian religious leaders and clan elders exerted great influence over the administration of the kingdom.\n\nOne of the most important decisions of the council took place in 208 AD, when a civil war broke out and the Mehestan decided that the empire would be ruled by two brothers simultaneously, Ardavan V and Blash V.\nIn 224 AD, following the dissolution of the Parthian empire, after over 470 years, the Mahestan council came to an end.\n\nAlthough there are documented councils held in 873, 1020, 1050 and 1063, there was no representation of commoners. What is considered to be the first parliament (with the presence of commoners), the Cortes of León, was held in the Kingdom of León in 1188. According to the UNESCO, the Decreta of Leon of 1188 is the oldest documentary manifestation of the European parliamentary system. In addition, UNESCO granted the 1188 Cortes of Alfonso IX the title of \"Memory of the World\" and the city of Leon has been recognized as the \"Cradle of Parliamentarism\".\n\nAfter coming to power, King Alfonso IX, facing an attack by his two neighbours, Castile and Portugal, decided to summon the \"Royal Curia\". This was a medieval organisation composed of aristocrats and bishops but because of the seriousness of the situation and the need to maximise political support, Alfonso IX took the decision to also call the representatives of the urban middle class from the most important cities of the kingdom to the assembly. León's Cortes dealt with matters like the right to private property, the inviolability of domicile, the right to appeal to justice opposite the King and the obligation of the King to consult the Cortes before entering a war. Prelates, nobles and commoners met separately in the three estates of the Cortes. In this meeting, new laws were approved to protect commoners against the arbitrarities of nobles, prelates and the king. This important set of laws is known as the \"Carta Magna Leonesa\".\n\nFollowing this event, new Cortes would appear in the other different territories that would make up Spain: Principality of Catalonia in 1192, the Kingdom of Castile in 1250, Kingdom of Aragon in 1274, Kingdom of Valencia in 1283 and Kingdom of Navarre in 1300.\n\nAfter the union of the Kingdoms of Leon and Castile under the Crown of Castile, their Cortes were united as well in 1258. The Castilian Cortes had representatives from Burgos, Toledo, León, Seville, Córdoba, Murcia, Jaén, Zamora, Segovia, Ávila, Salamanca, Cuenca, Toro, Valladolid, Soria, Madrid, Guadalajara and Granada (after 1492). The Cortes' assent was required to pass new taxes, and could also advise the king on other matters. The comunero rebels intended a stronger role for the Cortes, but were defeated by the forces of Habsburg Emperor Charles V in 1521. The Cortes maintained some power, however, though it became more of a consultative entity. However, by the time of King Philip II, Charles's son, the Castilian Cortes had come under functionally complete royal control, with its delegates dependent on the Crown for their income.\n\nThe Cortes of the Crown of Aragon kingdoms retained their power to control the king's spending with regard to the finances of those kingdoms. But after the War of the Spanish Succession and the victory of another royal house – the Bourbons – and King Philip V, their Cortes were suppressed (those of Aragon and Valencia in 1707, and those of Catalonia and the Balearic islands in 1714).\n\nThe very first Cortes representing the whole of Spain (and the Spanish empire of the day) assembled in 1812, in Cadiz, where it operated as a government in exile as at that time most of the rest of Spain was in the hands of Napoleon's army.\n\nEngland has long had a tradition of a body of men who would assist and advise the king on important matters. Under the Anglo-Saxon kings, there was an advisory council, the Witenagemot. The name derives from the Old English ƿitena ȝemōt, or witena gemōt, for \"meeting of wise men\". The first recorded act of a witenagemot was the law code issued by King Æthelberht of Kent ca. 600, the earliest document which survives in sustained Old English prose; however, the witan was certainly in existence long before this time. The Witan, along with the folkmoots (local assemblies), is an important ancestor of the modern English parliament.\n\nAs part of the Norman Conquest of England, the new king, William I, did away with the Witenagemot, replacing it with a Curia Regis (\"King's Council\"). Membership of the Curia was largely restricted to the tenants in chief, the few nobles who \"rented\" great estates directly from the king, along with ecclesiastics. William brought to England the feudal system of his native Normandy, and sought the advice of the curia regis before making laws. This is the original body from which the Parliament, the higher courts of law, and the Privy Council and Cabinet descend. Of these, the legislature is formally the High Court of Parliament; judges sit in the Supreme Court of Judicature. Only the executive government is no longer conducted in a royal court.\n\nMost historians date the emergence of a parliament with some degree of power to which the throne had to defer no later than the rule of Edward I. Like previous kings, Edward called leading nobles and church leaders to discuss government matters, especially finance. A meeting in 1295 became known as the Model Parliament because it set the pattern for later Parliaments. The significant difference between the Model Parliament and the earlier Curia Regis was the addition of the Commons; that is, the inclusion of elected representatives of rural landowners and of townsmen. In 1307, Edward I agreed not to collect certain taxes without the consent of the realm. He also enlarged the court system.\n\nThe tenants-in-chief often struggled with their spiritual counterparts and with the king for power. In 1215, they secured from King John of England \"Magna Carta\", which established that the king may not levy or collect any taxes (except the feudal taxes to which they were hitherto accustomed), save with the consent of a council. It was also established that the most important tenants-in-chief and ecclesiastics be summoned to the council by personal writs from the sovereign, and that all others be summoned to the council by general writs from the sheriffs of their counties. Modern government has its origins in the Curia Regis; parliament descends from the Great Council later known as the \"parliamentum\" established by \"Magna Carta\".\n\nDuring the reign of King Henry III, 13th-Century English Parliaments incorporated elected representatives from shires and towns. These parliaments are, as such, considered forerunners of the modern parliament.\n\nIn 1265, Simon de Montfort, then in rebellion against Henry III, summoned a parliament of his supporters without royal authorization. The archbishops, bishops, abbots, earls, and barons were summoned, as were two knights from each shire and two burgesses from each borough. Knights had been summoned to previous councils, but it was unprecedented for the boroughs to receive any representation. Come 1295, Edward I later adopted de Montfort's ideas for representation and election in the so-called \"Model Parliament\". At first, each estate debated independently; by the reign of Edward III, however, Parliament recognisably assumed its modern form, with authorities dividing the legislative body into two separate chambers.\n\nThe purpose and structure of Parliament in Tudor England underwent a significant transformation under the reign of Henry VIII. Originally its methods were primarily medieval, and the monarch still possessed a form of inarguable dominion over its decisions. According to Elton, it was Thomas Cromwell, 1st Earl of Essex, then chief minister to Henry VIII, who initiated still other changes within parliament.\n\nThe Reformation Acts supplied Parliament with unlimited power over the country. This included authority over virtually every matter, whether social, economic, political, or religious ; it legalised the Reformation, officially and indisputably. The king had to rule through the council, not over it, and all sides needed to reach a mutual agreement when creating or passing laws, adjusting or implementing taxes, or changing religious doctrines. This was significant: the monarch no longer had sole control over the country. For instance, during the later years of Mary, Parliament exercised its authority in originally rejecting Mary's bid to revive Catholicism in the realm. Later on, the legislative body even denied Elizabeth her request to marry . If Parliament had possessed this power before Cromwell, such as when Wolsey served as secretary, the Reformation may never have happened, as the king would have had to gain the consent of all parliament members before so drastically changing the country's religious laws and fundamental identity .\n\nThe power of Parliament increased considerably after Cromwell's adjustments. It also provided the country with unprecedented stability. More stability, in turn, helped assure more effective management, organisation, and efficiency. Parliament printed statutes and devised a more coherent parliamentary procedure.\n\nThe rise of Parliament proved especially important in the sense that it limited the repercussions of dynastic complications that had so often plunged England into civil war. Parliament still ran the country even in the absence of suitable heirs to the throne, and its legitimacy as a decision-making body reduced the royal prerogatives of kings like Henry VIII and the importance of their whims. For example, Henry VIII could not simply establish supremacy by proclamation; he required Parliament to enforce statutes and add felonies and treasons. An important liberty for Parliament was its freedom of speech; Henry allowed anything to be spoken openly within Parliament and speakers could not face arrest – a fact which they exploited incessantly. Nevertheless, Parliament in Henry VIII's time offered up very little objection to the monarch's desires. Under his and Edward's reign, the legislative body complied willingly with the majority of the kings' decisions.\n\nMuch of this compliance stemmed from how the English viewed and traditionally understood authority. As Williams described it, \"King and parliament were not separate entities, but a single body, of which the monarch was the senior partner and the Lords and the Commons the lesser, but still essential, members.\".\n\nAlthough its role in government expanded significantly during the reigns of Henry VIII and Edward VI, the Parliament of England saw some of its most important gains in the 17th century. A series of conflicts between the Crown and Parliament culminated in the execution of King Charles I in 1649. Afterward, England became a commonwealth, with Oliver Cromwell, its lord protector, the de facto ruler. Frustrated with its decisions, Cromwell purged and suspended Parliament on several occasions.\n\nA controversial figure accused of despotism, war crimes, and even genocide, Cromwell is nonetheless regarded as essential to the growth of democracy in England. The years of the Commonwealth, coupled with the restoration of the monarchy in 1660 and the subsequent Glorious Revolution of 1688, helped reinforce and strengthen Parliament as an institution separate from the Crown.\n\nThe Parliament of England met until it merged with the Parliament of Scotland under the Acts of Union. This union created the new Parliament of Great Britain in 1707.\n\nFrom the 10th century the Kingdom of Alba was ruled by chiefs (\"toisechs\") and subkings (\"mormaers\") under the suzerainty, real or nominal, of a High King. Popular assemblies, as in Ireland, were involved in law-making, and sometimes in king-making, although the introduction of tanistry—naming a successor in the lifetime of a king—made the second less than common. These early assemblies cannot be considered \"parliaments\" in the later sense of the word, and were entirely separate from the later, Norman-influenced, institution.\n\nThe Parliament of Scotland evolved during the Middle Ages from the King's Council of Bishops and Earls. The unicameral parliament is first found on record, referred to as a \"colloquium\", in 1235 at Kirkliston (a village now in Edinburgh).\n\nBy the early fourteenth century the attendance of knights and freeholders had become important, and from 1326 burgh commissioners attended. Consisting of the Three Estates; of clerics, lay tenants-in-chief and burgh commissioners sitting in a single chamber, the Scottish parliament acquired significant powers over particular issues. Most obviously it was needed for consent for taxation (although taxation was only raised irregularly in Scotland in the medieval period), but it also had a strong influence over justice, foreign policy, war, and all manner of other legislation, whether political, ecclesiastical, social or economic. Parliamentary business was also carried out by \"sister\" institutions, before c. 1500 by General Council and thereafter by the Convention of Estates. These could carry out much business also dealt with by Parliament – taxation, legislation and policy-making – but lacked the ultimate authority of a full parliament.\n\nThe parliament, which is also referred to as the Estates of Scotland, the Three Estates, the Scots Parliament or the auld Scots Parliament (Eng: \"old\"), met until the Acts of Union merged the Parliament of Scotland and the Parliament of England, creating the new Parliament of Great Britain in 1707.\n\nFollowing the 1997 Scottish devolution referendum, and the passing of the Scotland Act 1998 by the Parliament of the United Kingdom, the Scottish Parliament was reconvened on 1 July 1999, although with much more limited powers than its 18th-century predecessor. The parliament has sat since 2004 at its newly constructed Scottish Parliament Building in Edinburgh, situated at the foot of the Royal Mile, next to the royal palace of Holyroodhouse.\n\nA \"thing\" or \"ting\" (Old Norse and ; other modern Scandinavian: \"ting\", \"ding\" in Dutch) was the governing assembly in Germanic societies, made up of the free men of the community and presided by lawspeakers.\n\nThe thing was the assembly of the free men of a country, province or a hundred \"(hundare/härad/herred)\". There were consequently, hierarchies of things, so that the local things were represented at the thing for a larger area, for a province or land. At the thing, disputes were solved and political decisions were made. The place for the thing was often also the place for public religious rites and for commerce.\n\nThe thing met at regular intervals, legislated, elected chieftains and kings, and judged according to the law, which was memorised and recited by the \"law speaker\" (the judge).\n\nThe Icelandic, Faroese and Manx parliaments trace their origins back to the Viking expansion originating from the Petty kingdoms of Norway as well as Denmark, replicating Viking government systems in the conquered territories, such as those represented by the Gulating near Bergen in western Norway.\n\nLater national diets with chambers for different estates developed, e.g. in Sweden and in Finland (which was part of Sweden until 1809), each with a House of Knights for the nobility. In both these countries, the national parliaments are now called riksdag (in Finland also \"eduskunta\"), a word used since the Middle Ages and equivalent of the German word Reichstag.\n\nToday the term lives on in the official names of national legislatures, political and judicial institutions in the North-Germanic countries. In the Yorkshire and former Danelaw areas of England, which were subject to much Norse invasion and settlement, the wapentake was another name for the same institution.\n\nThe Sicilian Parliament, dating to 1097, evolved as the legislature of the Kingdom of Sicily.\n\nThe Federal Diet of Switzerland was one of the longest-lived representative bodies in history, continuing from the 13th century to 1848.\n\nOriginally, there was only the Parliament of Paris, born out of the Curia Regis in 1307, and located inside the medieval royal palace, now the Paris Hall of Justice. The jurisdiction of the \"Parliament\" of Paris covered the entire kingdom. In the thirteenth century, judicial functions were added. In 1443, following the turmoil of the Hundred Years' War, King Charles VII of France granted Languedoc its own \"parliament\" by establishing the \"Parliament\" of Toulouse, the first \"parliament\" outside of Paris, whose jurisdiction extended over the most part of southern France. From 1443 until the French Revolution several other \"parliaments\" were created in some provinces of France (Grenoble, Bordeaux).\n\nAll the \"parliaments\" could issue regulatory decrees for the application of royal edicts or of customary practices; they could also refuse to register laws that they judged contrary to fundamental law or simply as being untimely. Parliamentary power in France was suppressed more so than in England as a result of absolutism, and parliaments were eventually overshadowed by the larger Estates General, up until the French Revolution, when the National Assembly became the lower house of France's bicameral legislature.\n\nAccording to the \"Chronicles\" of Gallus Anonymus, the first legendary Polish ruler, Siemowit, who began the Piast Dynasty, was chosen by a \"wiec\". The \"veche\" (, ) was a popular assembly in medieval Slavic countries, and in late medieval period, a parliament. The idea of the \"wiec\" led in 1182 to the development of the Polish parliament, the \"Sejm\".\n\nThe term \"sejm\" comes from an old Polish expression denoting a meeting of the populace. The power of early sejms grew between 1146–1295, when the power of individual rulers waned and various councils and wiece grew stronger. The history of the national Sejm dates back to 1182. Since the 14th century irregular sejms (described in various Latin sources as \"contentio generalis, conventio magna, conventio solemna, parlamentum, parlamentum generale, dieta\" or Polish \"sejm walny\") have been called by Polish kings. From 1374, the king had to receive sejm permission to raise taxes. The General Sejm (Polish \"Sejm Generalny\" or \"Sejm Walny\"), first convoked by the king John I Olbracht in 1493 near Piotrków, evolved from earlier regional and provincial meetings (\"sejmiks\"). It followed most closely the \"sejmik generally\", which arose from the 1454 Nieszawa Statutes, granted to the szlachta (nobles) by King Casimir IV the Jagiellonian. From 1493 forward, indirect elections were repeated every two years. With the development of the unique Polish Golden Liberty the Sejm's powers increased.\n\nThe Commonwealth's general parliament consisted of three estates: the King of Poland (who also acted as the Grand Duke of Lithuania, Russia/Ruthenia, Prussia, Mazovia, etc.), the Senat (consisting of Ministers, Palatines, Castellans and Bishops) and the Chamber of Envoys—circa 170 nobles (szlachta) acting on behalf of their Lands and sent by Land Parliaments. Also representatives of selected cities but without any voting powers. Since 1573 at a royal election all peers of the Commonwealth could participate in the Parliament and become the King's electors.\n\nCossack Rada was the legislative body of a military republic of the Ukrainian Cossacks that grew rapidly in the 15th century from serfs fleeing the more controlled parts of the Polish Lithuanian Commonwealth. The republic did not regard social origin/nobility and accepted all people who declared to be Orthodox Christians.\n\nOriginally established at the Zaporizhian Sich, the rada (council) was an institution of Cossack administration in Ukraine from the 16th to the 18th century. With the establishment of the Hetman state in 1648, it was officially known as the General Military Council until 1750.\n\nThe zemsky sobor (Russian: зе́мский собо́р) was the first Russian parliament of the feudal Estates type, in the 16th and 17th centuries. The term roughly means assembly of the land.\n\nIt could be summoned either by tsar, or patriarch, or the Boyar Duma. Three categories of population, comparable to the Estates-General of France but with the numbering of the first two Estates reversed, participated in the assembly:\n\nNobility and high bureaucracy, including the Boyar Duma\n\nThe Holy Sobor of high Orthodox clergy\n\nRepresentatives of merchants and townspeople (third estate)\n\nThe name of the parliament of nowadays Russian Federation is the Federal Assembly of Russia. The term for its lower house, State Duma (which is better known than the Federal Assembly itself, and is often mistaken for the entirety of the parliament) comes from the Russian word \"думать\" (\"dumat\"), \"to think\". The Boyar Duma was an advisory council to the grand princes and tsars of Muscovy. The Duma was discontinued by Peter the Great, who transferred its functions to the Governing Senate in 1711.\n\nThe \"veche\" was the highest legislature and judicial authority in the republic of Novgorod until 1478. In its sister state, Pskov, a separate veche operated until 1510.\n\nSince the Novgorod revolution of 1137 ousted the ruling grand prince, the veche became the supreme state authority. After the reforms of 1410, the veche was restructured on a model similar to that of Venice, becoming the Commons chamber of the parliament. An upper Senate-like Council of Lords was also created, with title membership for all former city magistrates. Some sources indicate that veche membership may have become full-time, and parliament deputies were now called \"vechniks\". It is recounted that the Novgorod assembly could be summoned by anyone who rung the veche bell, although it is more likely that the common procedure was more complex. This bell was a symbol of republican sovereignty and independence. The whole population of the city—boyars, merchants, and common citizens—then gathered at Yaroslav's Court. Separate assemblies could be held in the districts of Novgorod. In Pskov the veche assembled in the court of the Trinity cathedral.\n\n\"Conciliarism\" or the \"conciliar movement\", was a reform movement in the 14th and 15th century Roman Catholic Church which held that final authority in spiritual matters resided with the Roman Church as corporation of Christians, embodied by a general church council, not with the pope. In effect, the movement sought – ultimately, in vain – to create an All-Catholic Parliament. Its struggle with the Papacy had many points in common with the struggle of parliaments in specific countries against the authority of Kings and other secular rulers.\n\nThe development of the modern concept of parliamentary government dates back to the Kingdom of Great Britain (1707–1800) and the parliamentary system in Sweden during the Age of Liberty (1718–1772).\n\nThe British Parliament is often referred to as the \"Mother of Parliaments\" (in fact a misquotation of John Bright, who remarked in 1865 that \"England is the Mother of Parliaments\") because the British Parliament has been the model for most other parliamentary systems, and its Acts have created many other parliaments. Many nations with parliaments have to some degree emulated the British \"three-tier\" model. Most countries in Europe and the Commonwealth have similarly organised parliaments with a largely ceremonial head of state who formally opens and closes parliament, a large elected lower house and a smaller, upper house.\n\nThe Parliament of Great Britain was formed in 1707 by the Acts of Union that replaced the former parliaments of England and Scotland. A further union in 1801 united the Parliament of Great Britain and the Parliament of Ireland into a Parliament of the United Kingdom.\n\nIn the United Kingdom, Parliament consists of the House of Commons, the House of Lords, and the Monarch. The House of Commons is composed of 650 (soon to be 600) members who are directly elected by British citizens to represent single-member constituencies. The leader of a Party that wins more than half the seats, or less than half but is able to gain the support of smaller parties to achieve a majority in the house is invited by the Monarch to form a government. The House of Lords is a body of long-serving, unelected members: Lords Temporal – 92 of whom inherit their titles (and of whom 90 are elected internally by members of the House to lifetime seats), 588 of whom have been appointed to lifetime seats, and Lords Spiritual – 26 bishops, who are part of the house while they remain in office.\n\nLegislation can originate from either the Lords or the Commons. It is voted on in several distinct stages, called readings, in each house. First reading is merely a formality. Second reading is where the bill as a whole is considered. Third reading is detailed consideration of clauses of the bill.\n\nIn addition to the three readings a bill also goes through a committee stage where it is considered in great detail. Once the bill has been passed by one house it goes to the other and essentially repeats the process. If after the two sets of readings there are disagreements between the versions that the two houses passed it is returned to the first house for consideration of the amendments made by the second. If it passes through the amendment stage Royal Assent is granted and the bill becomes law as an Act of Parliament.\n\nThe House of Lords is the less powerful of the two houses as a result of the Parliament Acts 1911 and 1949. These Acts removed the veto power of the Lords over a great deal of legislation. If a bill is certified by the Speaker of the House of Commons as a money bill (i.e. acts raising taxes and similar) then the Lords can only block it for a month. If an ordinary bill originates in the Commons the Lords can only block it for a maximum of one session of Parliament. The exceptions to this rule are things like bills to prolong the life of a Parliament beyond five years.\n\nIn addition to functioning as the second chamber of Parliament, the House of Lords was also the final court of appeal for much of the law of the United Kingdom—a combination of judicial and legislative function that recalls its origin in the Curia Regis. This changed in October 2009 when the Supreme Court of the United Kingdom opened and acquired the former jurisdiction of the House of Lords.\n\nSince 1999, there has been a Scottish Parliament in Edinburgh, which is a national, unicameral legislature for Scotland. However, the Scottish Parliament does not have complete power over Scottish Politics, as it only holds the powers which were devolved to it by Westminster in 1997. It cannot legislate on defence issues, currency, or national taxation (e.g. VAT, or Income Tax). Additionally, the Scottish Parliament can be dissolved at any given time by the British Parliament without the consent of the devolved government. This applies to all devolved governments within the United Kingdom, a limit on the sovereignty of the devolved governments.\n\nIn Sweden, the half-century period of parliamentary government beginning with Charles XII's death in 1718 and ending with Gustav III's self-coup in 1772 is known as the Age of Liberty. During this period, civil rights were expanded and power shifted from the monarch to parliament.\n\nWhile suffrage did not become universal, the taxed peasantry was represented in Parliament, although with little influence and commoners without taxed property had no suffrage at all.\n\nMany parliaments are part of a parliamentary system of government, in which the executive is constitutionally answerable to the parliament. Some restrict the use of the word \"parliament\" to parliamentary systems, while others use the word for any elected legislative body. Parliaments usually consist of \"chambers\" or \"houses\", and are usually either bicameral or unicameral although more complex models exist, or have existed (\"see Tricameralism).\n\nIn some parliamentary systems, the prime minister is a member of the parliament (e.g. in the United Kingdom), whereas in others they are not (e.g. in the Netherlands). They are commonly the leader of the majority party in the lower house of parliament, but only hold the office as long as the \"confidence of the house\" is maintained. If members of the lower house lose faith in the leader for whatever reason, they can call a vote of no confidence and force the prime minister to resign.\n\nThis can be particularly dangerous to a government when the distribution of seats among different parties is relatively even, in which case a new election is often called shortly thereafter. However, in case of general discontent with the head of government, their replacement can be made very smoothly without all the complications that it represents in the case of a presidential system.\n\nThe parliamentary system can be contrasted with a presidential system, such as the American congressional system, which operates under a stricter separation of powers, whereby the executive does not form part of, nor is it appointed by, the parliamentary or legislative body. In such a system, congresses do not select or dismiss heads of governments, and governments cannot request an early dissolution as may be the case for parliaments. Some states, such as France, have a semi-presidential system which falls between parliamentary and congressional systems, combining a powerful head of state (president) with a head of government, the prime minister, who is responsible to parliament.\n\n\n\nAustralia's States and territories:\n\nIn the federal (bicameral) kingdom of Belgium, there is a curious asymmetrical constellation serving as directly elected legislatures for three \"territorial\" \"regions\"—Flanders (Dutch), Brussels (bilingual, certain peculiarities of competence, also the only region not comprising any of the 10 provinces) and Wallonia (French)—and three cultural \"communities\"—Flemish (Dutch, competent in Flanders and for the Dutch-speaking inhabitants of Brussels), Francophone (French, for Wallonia and for Francophones in Brussels) and German (for speakers of that language in a few designated municipalities in the east of the Walloon Region, living alongside Francophones but under two different regimes):\n\nCanada's provinces and territories:\n\n\n\n\n<maplink latitude=\"21\" longitude=\"78\" zoom=\"5\" width=\"500\" height=\"500\" text=\"Map\" >\n</maplink>\nIndian States and Territories Legislative assemblies:\n\n\nIndian States Legislative councils\n\n\n\n\n\n\n\n\n"}
{"id": "182113", "url": "https://en.wikipedia.org/wiki?curid=182113", "title": "Parliamentary system", "text": "Parliamentary system\n\nA parliamentary system or parliamentary democracy is a system of democratic governance of a state (or subordinate entity) where the executive derives its democratic legitimacy from its ability to command the confidence of the legislature, typically a parliament, and is also held accountable to that parliament. In a parliamentary system, the head of state is usually a person distinct from the head of government. This is in contrast to a presidential system, where the head of state often is also the head of government and, most importantly, the executive does not derive its democratic legitimacy from the legislature.\n\nCountries with parliamentary democracies may be constitutional monarchies, where a monarch is the head of state while the head of government is almost always a member of parliament (such as the United Kingdom, Denmark, Sweden, and Japan), or parliamentary republics, where a mostly ceremonial president is the head of state while the head of government is regularly from the legislature (such as Ireland, Germany, India, and Italy). In a few parliamentary republics, such as Botswana, South Africa, and Suriname, among some others, the head of government is also head of state, but is elected by and is answerable to parliament. In bicameral parliaments, the head of government is generally, though not always, a member of the lower house.\n\nParliamentarianism is the dominant form of government in Europe, with 32 of its 50 sovereign states being parliamentarian. It is also common in the Caribbean, being the form of government of 10 of its 13 island states, and in Oceania. Elsewhere in the world, parliamentary countries are less common, but they are distributed through all continents, most often in former colonies of the British Empire that subscribe to a particular brand of parliamentarianism known as the Westminster system.\n\nSince ancient times, when societies were tribal, there were councils or a headman whose decisions were assessed by village elders. Eventually, these councils have slowly evolved into the modern parliamentary system.\n\nThe first parliaments date back to Europe in the Middle Ages: specifically in 1188 Alfonso IX, King of Leon (Spain) convened the three states in the Cortes of León. An early example of parliamentary government developed in today's Netherlands and Belgium during the Dutch revolt (1581), when the sovereign, legislative and executive powers were taken over by the States General of the Netherlands from the monarch, King Philip II of Spain. The modern concept of parliamentary government emerged in the Kingdom of Great Britain between 1707–1800 and its contemporary, the Parliamentary System in Sweden between 1721–1772.\n\nIn England, Simon de Montfort is remembered as one of the fathers of representative government for convening two famous parliaments. The first, in 1258, stripped the king of unlimited authority and the second, in 1265, included ordinary citizens from the towns. Later, in the 17th century, the Parliament of England pioneered some of the ideas and systems of liberal democracy culminating in the Glorious Revolution and passage of the Bill of Rights 1689.\n\nIn the Kingdom of Great Britain, the monarch, in theory, chaired cabinet and chose ministers. In practice, King George I's inability to speak English led the responsibility for chairing cabinet to go to the leading minister, literally the \"prime\" or first minister, Robert Walpole. The gradual democratisation of parliament with the broadening of the voting franchise increased parliament's role in controlling government, and in deciding whom the king could ask to form a government. By the 19th century, the Great Reform Act of 1832 led to parliamentary dominance, with its choice \"invariably\" deciding who was prime minister and the complexion of the government.\n\nOther countries gradually adopted what came to be called the Westminster Model of government, with an executive answerable to parliament, and exercising, in the name of the head of state, powers nominally vested in the head of state. Hence the use of phrases like \"Her Majesty's government\" or \"His Excellency's government\". Such a system became particularly prevalent in older British dominions, many of which had their constitutions enacted by the British parliament; such as Australia, New Zealand, Canada, the Irish Free State and the Union of South Africa. Some of these parliaments were reformed from, or were initially developed as distinct from their original British model: the Australian Senate, for instance, has since its inception more closely reflected the US Senate than the British House of Lords; whereas since 1950 there is no upper house in New Zealand.\n\nDemocracy and parliamentarianism became increasingly prevalent in Europe in the years after World War I, partially imposed by the democratic victors, the United States, Great Britain and France, on the defeated countries and their successors, notably Germany's Weimar Republic and the new Austrian Republic. Nineteenth-century urbanisation, the Industrial Revolution and modernism had already fuelled the political left's struggle for democracy and parliamentarianism for a long time. In the radicalised times at the end of World War I, democratic reforms were often seen as a means to counter popular revolutionary currents.\n\nA parliamentary system may be either bicameral, with two chambers of parliament (or houses) or unicameral, with just one parliamentary chamber. A bicameral parliament usually consists of a directly elected lower house with the power to determine the executive government, and an upper house which may be appointed or elected through a different mechanism from the lower house.\n\nScholars of democracy such as Arend Lijphart distinguish two types of parliamentary democracies: the Westminster and Consensus systems.\n\n\nImplementations of the parliamentary system can also differ as to how the prime minister and government are appointed and whether the government needs the explicit approval of the parliament, rather than just the absence of its disapproval. Some countries such as India also require the prime minister to be a member of the legislature, though in other countries this only exists as a convention.\n\nFurthermore, there are variations as to what conditions exist (if any) for the government to have the right to dissolve the parliament:\n\nThe parliamentary system can be contrasted with a presidential system which operates under a stricter separation of powers, whereby the executive does not form part of—nor is appointed by—the parliamentary or legislative body. In such a system, parliaments or congresses do not select or dismiss heads of governments, and governments cannot request an early dissolution as may be the case for parliaments. There also exists the semi-presidential system that draws on both presidential systems and parliamentary systems by combining a powerful president with an executive responsible to parliament: for example, the French Fifth Republic.\n\nParliamentarianism may also apply to regional and local governments. An example is the city of Oslo, which has an executive council (Byråd) as a part of the parliamentary system.\n\nA few parliamentary democratic nations such as India, Pakistan, Bangladesh etc. have enacted a law which prohibits floor crossing or switching the party after election process. With this law, the elected representative have to lose their seat in the Parliament House, if they defy the direction of the party in any voting. \n\nIn the UK Parliament, a member is free to cross over to the other side, without being daunted by any disqualification law. In the US, Canada, and Australia, there is no restraint on legislators switching sides.\n\nSupporters generally claim three basic advantages for parliamentary systems:\n\n\nParliamentary systems like that found in the United Kingdom are widely considered to be more flexible, allowing rapid change in legislation and policy as long as there is a stable majority or coalition in parliament, allowing the government to have 'few legal limits on what it can do' Due to the first-past-the-post 'this system produces the classic \"Westminster Model\" with the twin virtues of strong but responsive party government'. This electoral system providing a strong majority in the House of Commons, paired with the fused power system results in a particularly powerful Government able to provide change and 'innovate'.\n\nThe United Kingdom's fused power system is often noted to be advantageous with regards to accountability. The centralised government allows for more transparency as to where decisions originate from, this directly contrasts with the United States' system with former Treasury Secretary C. Douglas Dillon saying \"the president blames Congress, the Congress blames the president, and the public remains confused and disgusted with government in Washington\". Furthermore, ministers of the U.K. cabinet are subject to weekly Question Periods in which their actions/policies are scrutinised, no such regular check on the government exists in the U.S. system.\n\nParliamentary government has attractive features for nations that are ethnically, racially, or ideologically divided. In a presidential system, all executive power is vested in one person, the president, whereas power is more divided in a parliamentary system with its collegial executive. In the 1989 Lebanese Taif Agreement, in order to give Muslims greater political power, Lebanon moved from a semi-presidential system with a powerful president to a system more structurally similar to classical parliamentary government. Iraq similarly disdained a presidential system out of fears that such a system would be tantamount to Shiite domination of the large Sunni minority. Afghanistan's minorities refused to go along with a presidency as strong as the Pashtuns desired.\n\nIt can also be argued that power is more evenly spread out in parliamentary government, as the government and prime minister do not have the power to make unilateral decisions, as the entire government cabinet is answerable and accountable to parliament. Parliamentary systems are less likely to allow celebrity-based politics to fully dominate a society, unlike what often happens in presidential systems, where name-recall and popularity can catapult a celebrity, actor, or popular politician to the presidency despite such candidate's lack of competence and experience.\n\nSome scholars like Juan Linz, Fred Riggs, Bruce Ackerman, and Robert Dahl have found that parliamentary government is less prone to authoritarian collapse. These scholars point out that since World War II, two-thirds of Third Worldcountries establishing parliamentary governments successfully made the transition to democracy. By contrast, no Third World presidential system successfully made the transition to democracy without experiencing coups and other constitutional breakdowns. A 2001 World Bank study found that parliamentary systems are associated with less corruption, which is supported by a separate study that arrived at the same conclusions.\n\nIn his 1867 book \"The English Constitution\", Walter Bagehot praised parliamentary governments for producing serious debates, for allowing for a change in power without an election, and for allowing elections at any time. Bagehot considered the four-year election rule of the United States to be unnatural, as it can potentially allow a president who has disappointed the public with a dismal performance in the second year of his term to continue on until the end of his four-year term. Under a parliamentary system, a prime minister that has lost support in the middle of his term can be easily replaced by his own peers.\n\nAlthough Bagehot praised parliamentary governments for allowing an election to take place at any time, the lack of a definite election calendar can be abused. Previously under some systems, such as the British, a ruling party could schedule elections when it felt that it was likely to retain power, and so avoid elections at times of unpopularity. (Election timing in the UK, however, is now partly fixed under the Fixed-term Parliaments Act 2011.) Thus, by a shrewd timing of elections, in a parliamentary system, a party can extend its rule for longer than is feasible in a functioning presidential system. This problem can be alleviated somewhat by setting fixed dates for parliamentary elections, as is the case in several of Australia's state parliaments. In other systems, such as the Dutch and the Belgian, the ruling party or coalition has some flexibility in determining the election date. Conversely, flexibility in the timing of parliamentary elections can avoid periods of legislative gridlock that can occur in a fixed period presidential system. In any case, voters ultimately have the power to choose whether to vote for the ruling party or someone else.\n\nCritics generally claim these basic disadvantages for parliamentary systems:\n\n\nThe ability for strong parliamentary governments to 'push' legislation through with the ease of fused power systems such as in the United Kingdom, whilst positive in allowing rapid adaptation when necessary e.g. the nationalisation of services during the world wars, does have its drawbacks. The flip-flopping of legislation back and forth as the majority in parliament changed between the Conservatives and Labour over the period 1940-1980, contesting over the nationalisation and privatisation of the British Steel Industry resulted in major instability for the British steel sector.\n\nIn R. Kent Weaver's book \"Are Parliamentary Systems Better?\", he writes that an advantage of presidential systems is their ability to allow and accommodate more diverse viewpoints. He states that because \"legislators are not compelled to vote against their constituents on matters of local concern, parties can serve as organizational and roll-call cuing vehicles without forcing out dissidents.\"\n\n"}
{"id": "206578", "url": "https://en.wikipedia.org/wiki?curid=206578", "title": "Presidential system", "text": "Presidential system\n\nA presidential system is a democratic and republican government in which a head of government leads an executive branch that is separate from the legislative branch. This head of government is in most cases also the head of state, which is called \"president\".\n\nIn presidential countries, the executive is elected and is not responsible to the legislature, which cannot in normal circumstances dismiss it. Such dismissal is possible, however, in uncommon cases, often through impeachment.\n\nThe title \"president\" has persisted from a time when such person personally presided over the governing body, as with the President of the Continental Congress in the early United States, prior to the executive function being split into a separate branch of government.\n\nA presidential system contrasts with a parliamentary system, where the head of government is elected to power through the legislative. There is also a hybrid system called semi-presidentialism.\n\nCountries that feature a presidential or semi-presidential system of government are not the exclusive users of the title of president. Heads of state of parliamentary republics, largely ceremonial in most cases, are called presidents. Dictators or leaders of one-party states, popularly elected or not, are also often called presidents.\n\nPresidentialism is the dominant form of government in the continental Americas, with all of its 22 sovereign states being presidential republics (except for Canada, Belize and Suriname). It is also prevalent in Central and southern West Africa and in Central Asia. There are no presidential republics in Europe (except for Turkey, Belarus, and Cyprus) and Oceania.\n\nIn a full-fledged presidential system, a politician is chosen directly by the public or indirectly by the winning party to be the head of government. Except for Belarus and Kazakhstan, this head of government is also the head of state, and is therefore called \"president\". The post of prime minister (also called premier) may also exist in a presidential system, but unlike in semi-presidential or parliamentary systems, the prime minister answers to the president and not to the legislature.\n\nThe following characteristics apply generally for the numerous presidential governments across the world:\n\nSubnational governments, usually states, may be structured as presidential systems. All of the state governments in the United States use the presidential system, even though this is not constitutionally required. On a local level, many cities use council-manager government, which is equivalent to a parliamentary system, although the post of a city manager is normally a non-political position. Some countries without a presidential system at the national level use a form of this system at a subnational or local level. One example is Japan, where the national government uses the parliamentary system, but the prefectural and municipal governments have governors and mayors elected independently from local assemblies and councils.\n\nSupporters generally claim four basic advantages for presidential systems:\n\nIn most presidential systems, the president is elected by popular vote, although some such as the United States use an electoral college (which is itself directly elected) or some other method. By this method, the president receives a personal mandate to lead the country, whereas in a parliamentary system a candidate might only receive a personal mandate to represent a constituency. That means a president can only be elected independently of the legislative branch.\n\nA presidential system's separation of the executive from the legislature is sometimes held up as an advantage, in that each branch may scrutinize the actions of the other. In a parliamentary system, the executive is drawn from the legislature, making criticism of one by the other considerably less likely. A formal condemnation of the executive by the legislature is often considered a vote of no confidence. According to supporters of the presidential system, the lack of checks and balances means that misconduct by a prime minister may never be discovered. Writing about Watergate, Woodrow Wyatt, a former MP in the UK, said \"don't think a Watergate couldn't happen here, you just wouldn't hear about it.\" (ibid)\n\nCritics respond that if a presidential system's legislature is controlled by the president's party, the same situation exists. Proponents note that even in such a situation a legislator from the president's party is in a better position to criticize the president or his policies should he deem it necessary, since the immediate security of the president's position is less dependent on legislative support. In parliamentary systems, party discipline is much more strictly enforced. If a parliamentary backbencher publicly criticizes the executive or its policies to any significant extent then he/she faces a much higher prospect of losing his/her party's nomination, or even outright expulsion from the party. Even mild criticism from a backbencher could carry consequences serious enough (in particular, removal from consideration for a cabinet post) to effectively muzzle a legislator with any serious political ambitions.\n\nDespite the existence of the no confidence vote, in practice it is extremely difficult to stop a prime minister or cabinet that has made its decision. In a parliamentary system, if important legislation proposed by the incumbent prime minister and his cabinet is \"voted down\" by a majority of the members of parliament then it is considered a vote of no confidence. To emphasize that particular point, a prime minister will often declare a particular legislative vote to be a matter of confidence at the first sign of reluctance on the part of legislators from his or her own party. If a government loses a parliamentary vote of confidence, then the incumbent government must then either resign or call elections to be held, a consequence few backbenchers are willing to endure. Hence, a no confidence vote in some parliamentary countries, like Britain, only occurs a few times in a century. In 1931, David Lloyd George told a select committee: \"Parliament has really no control over the executive; it is a pure fiction.\" (Schlesinger 1982)\n\nBy contrast, if a presidential legislative initiative fails to pass a legislature controlled by the president's party (e.g. the Clinton health care plan of 1993 in the United States), it may damage the president's political standing and that of his party, but generally has no immediate effect on whether or not the president completes his term.\n\nSome supporters of presidential systems claim that presidential systems can respond more rapidly to emerging situations than parliamentary ones. A prime minister, when taking action, needs to retain the support of the legislature, but a president is often less constrained. In \"Why England Slept\", future U.S. president John F. Kennedy argued that British prime ministers Stanley Baldwin and Neville Chamberlain were constrained by the need to maintain the confidence of the Commons.\n\nOther supporters [Who?]\n\nAlthough most parliamentary governments go long periods of time without a no confidence vote, Italy, Israel, and the French Fourth Republic have all experienced difficulties maintaining stability. When parliamentary systems have multiple parties, and governments are forced to rely on coalitions, as they often do in nations that use a system of proportional representation, extremist parties can theoretically use the threat of leaving a coalition to further their agendas.\n\nMany people consider presidential systems more able to survive emergencies. A country under enormous stress may, supporters argue, be better off being led by a president with a fixed term than rotating premierships. France during the Algerian controversy switched to a semi-presidential system as did Sri Lanka during its civil war, while Israel experimented with a directly elected prime minister in 1992. In France and Sri Lanka, the results are widely considered to have been positive. However, in the case of Israel, an unprecedented proliferation of smaller parties occurred, leading to the restoration of the previous system of selecting a prime minister.\n\nThe fact that elections are fixed in a presidential system is considered by supporters a welcome \"check\" on the powers of the executive, contrasting parliamentary systems, which may allow the prime minister to call elections whenever they see fit or orchestrate their own vote of no confidence to trigger an election when they cannot get a legislative item passed. The presidential model is said to discourage this sort of opportunism, and instead forces the executive to operate within the confines of a term they cannot alter to suit their own needs. \n\nProponents of the presidential system also argue that stability extends to the cabinets chosen under the system, compared to a parliamentary system where cabinets must be drawn from within the legislative branch. Under the presidential system, cabinet members can be selected from a much larger pool of potential candidates. This allows presidents the ability to select cabinet members based as much or more on their ability and competency to lead a particular department as on their loyalty to the president, as opposed to parliamentary cabinets, which might be filled by legislators chosen for no better reason than their perceived loyalty to the prime minister. Supporters of the presidential system note that parliamentary systems are prone to disruptive \"cabinet shuffles\" where legislators are moved between portfolios, whereas in presidential system cabinets (such as the United States Cabinet), cabinet shuffles are unusual.\n\nCritics generally claim three basic disadvantages for presidential systems:\n\nA fourth criticism applies specifically to nations with a proportionally elected legislature and a presidency. Where the voters are virtually all represented by their votes in the proportional outcome, the presidency is elected on a winner-take-all basis. Two different electoral systems are therefore in play, potentially leading to conflicts that are based on the natural differences of the systems.\n\nA prime minister without majority support in the legislature must either form a coalition or, if able to lead a minority government, govern in a manner acceptable to at least some of the opposition parties. Even with a majority government, the prime minister must still govern within (perhaps unwritten) constraints as determined by the members of his party—a premier in this situation is often at greater risk of losing his party leadership than his party is at risk of losing the next election. On the other hand, winning the presidency is a winner-take-all, zero-sum game. Once elected, a president might be able to marginalize the influence of other parties and exclude rival factions in his own party as well, or even leave the party whose ticket he was elected under. The president can thus rule without any party support until the next election or abuse his power to win multiple terms, a worrisome situation for many interest groups. Yale political scientist Juan Linz argues that:\n\nConstitutions that only require plurality support are said to be especially undesirable, as significant power can be vested in a person who does not enjoy support from a majority of the population.\n\nSome political scientists say that presidential systems are not constitutionally stable and have difficulty sustaining democratic practices, noting that presidentialism has slipped into authoritarianism in many of the countries in which it has been implemented. According to political scientist Fred Riggs, presidentialism has fallen into authoritarianism in nearly every country it has been attempted. Political sociologist Seymour Martin Lipset pointed out that this has taken place in political cultures not conducive to democracy and that militaries have tended to play a prominent role in most of these countries. On the other hand, an often-cited list of the world's 22 older democracies includes only two countries (Costa Rica and the United States) with presidential systems.\n\nIn a presidential system, the legislature and the president have equal mandates from the public. Conflicts between the branches of government might not be reconciled. When president and legislature disagree and government is not working effectively, there is a strong incentive to use extra-constitutional measures to break the deadlock. Of the three common branches of government, the executive is in the best position to use extra-constitutional measures, especially when the president is head of state, head of government, and commander-in-chief of the military. By contrast, in a parliamentary system where the often-ceremonial head of state is either a constitutional monarch or (in the case of a parliamentary republic) an experienced and respected figure, given some political emergency there is a good chance that even a ceremonial head of state will be able to use emergency reserve powers to restrain a head of government acting in an emergency extra-constitutional manner – this is only possible because the head of state and the head of government are not the same person.\n\nDana D. Nelson, in her 2008 book \"Bad for Democracy\", sees the office of the President of the United States as essentially undemocratic and characterizes \"presidentialism\" as worship of the president by citizens, which she believes undermines civic participation.\n\nSome political scientists speak of the \"failure of presidentialism\" because the separation of powers of a presidential system often creates undesirable long-term political gridlock and instability whenever the president and the legislative majority are from different parties. This is common because the electorate often expects more rapid results than are possible from new policies and switches to a different party at the next election. Critics such as Juan Linz, argue that this inherent political instability can cause democracies to fail, as seen in such cases as Brazil and Chile.\nIn such cases of gridlock, presidential systems are said by critics not to offer voters the kind of accountability seen in parliamentary systems. It is easy for either the president or the legislature to escape blame by shifting it to the other. Describing the United States, former Treasury Secretary C. Douglas Dillon said \"the president blames Congress, the Congress blames the president, and the public remains confused and disgusted with government in Washington\". Years before becoming President, Woodrow Wilson (at the time, a fierce critic of the U.S. system of government) famously wrote \"how is the schoolmaster, the nation, to know which boy needs the whipping?\"\n\nAn example is the increase in the federal debt of the United States that occurred during the presidency of Republican Ronald Reagan. Arguably, the deficits were the product of a bargain between President Reagan and the Democratic Speaker of the House of Representatives, Tip O'Neill. O'Neill agreed to tax cuts favored by Reagan, and in exchange Reagan agreed to budgets that did not restrain spending to his liking. In such a scenario, each side can say they are displeased with the debt, plausibly blame the other side for the deficit, and still claim success.\n\nAnother alleged problem of presidentialism is that it is often difficult to remove a president from office early. Even if a president is \"proved to be inefficient, even if he becomes unpopular, even if his policy is unacceptable to the majority of his countrymen, he and his methods must be endured until the moment comes for a new election\". John Tyler was elected vice president and assumed the presidency because William Henry Harrison died after thirty days in office. Tyler blocked the Whig agenda, was loathed by his nominal party, but remained firmly in control of the executive branch. Most presidential systems provide no legal means to remove a president simply for being unpopular or even for behaving in a manner that might be considered unethical or immoral provided it is not illegal. This has been cited as the reason why many presidential countries have experienced military coups to remove a leader who is said to have lost his mandate.\n\nParliamentary systems can quickly remove unpopular leaders by a vote of no confidence, a procedure that serves as a \"pressure release valve\" for political tension. Votes of no confidence are easier to achieve in minority government situations, but even if the unpopular leader heads a majority government, he or she is often in a less secure position than a president. Usually in parliamentary systems a basic premise is that if a premier's popularity sustains a serious enough blow and the premier does not as a matter of consequence offer to resign prior to the next election, then those members of parliament who would persist in supporting the premier will be at serious risk of losing their seats. Therefore, especially in parliaments with a strong party system, other prominent members of the premier's party have a strong incentive to initiate a leadership challenge in hopes of mitigating damage to their party. More often than not, a premier facing a serious challenge resolves to save face by resigning before being formally removed—Margaret Thatcher's relinquishing of her premiership being a prominent example.\n\nOn the other hand, while removing a president through impeachment is allowed by most constitutions, impeachment proceedings often can be initiated only in cases where the president has violated the constitution or broken the law. Impeachment is often made difficult; by comparison the removal a party leader is normally governed by the (often less formal) rules of the party. Nearly all parties (including governing parties) have a relatively simple process for removing their leaders.\n\nFurthermore, even when impeachment proceedings against a sitting president are successful, whether by causing his removal from office or by compelling his resignation, the legislature usually has little or no discretion in determining the ousted president's successor, since presidential systems usually adhere to a rigid succession process which is enforced the same way regardless of how a vacancy in the presidency comes about. The usual outcome of a presidency becoming vacant is that a vice president automatically succeeds to the presidency. Vice presidents are usually chosen by the president, whether as a running mate who elected alongside the president or appointed by a sitting president, so that when a vice president succeeds to the presidency it is probable that he will continue many or all the policies of the former president. A prominent example of such an accession would be the elevation of Vice President Gerald Ford to the U.S. Presidency after Richard Nixon agreed to resign in the face of virtually certain impeachment and removal, a succession that took place notwithstanding the fact that Ford had only assumed the Vice Presidency after being appointed by Nixon to replace Spiro Agnew, who had also resigned due to scandal. In some cases, particularly when the would-be successor to a presidency is seen by legislators as no better (or even worse) than a president they wish to see removed, there may be a strong incentive to abstain from pursuing impeachment proceedings even if there are legal grounds to do so.\n\nSince prime ministers in parliamentary systems must always retain the confidence of the legislature, in cases where a prime minister suddenly leaves office there is little point in anyone without a reasonable prospect of gaining that legislative confidence attempting to assume the premiership. This ensures that whenever a premiership becomes vacant (or is about to become vacant), legislators from the premier's party will always play a key role in determining the leader's permanent successor. In theory this could be interpreted to support an argument that a parliamentary party ought to have the power to elect their party leader directly, and indeed, at least historically, parliamentary system parties' leadership electoral procedures usually called for the party's legislative caucus to fill a leadership vacancy by electing a new leader directly by and from amongst themselves, and for the whole succession process to be completed within as short a time frame as practical. Today, however, such a system is not commonly practiced and most parliamentary system parties' rules provide for a leadership election in which the general membership of the party is permitted to vote at some point in the process (either directly for the new leader or for delegates who then elect the new leader in a convention), though in many cases the party's legislators are allowed to exercise a disproportionate influence in the final vote.\n\nWhenever a leadership election becomes necessary on account of a vacancy arising suddenly, an interim leader (often informally called the \"interim prime minister\" in cases where this involves a governing party) will be selected by the parliamentary party, usually with the stipulation or expectation that the interim leader will not be a candidate for the permanent leadership. Some parties, such as the British Conservative Party, employ some combination of both aforementioned electoral processes to select a new leader. In any event, a prime minister who is forced to leave office due to scandal or similar circumstance will usually have little if any ability to influence his party on the final selection of a new leader and anyone seen to be having close ties to such a prime minister will have limited if any serious prospect of being elected the new leader. Even in cases when an outgoing prime minister is leaving office voluntarily, it is often frowned on for an outgoing or former premier to engage in any overt attempt to influence the election (for example, by endorsing a candidate in the leadership election), in part because a party in the process of selecting a new leader usually has a strong incentive to foster a competitive leadership election in order to stimulate interest and participation in the election, which in turn encourages the sale of party memberships and support for the party in general.\n\nWalter Bagehot criticized presidentialism because it does not allow a transfer in power in the event of an emergency.\n\nOpponents of the presidential system note that years later, Bagehot's observation came to life during World War II, when Neville Chamberlain was replaced with Winston Churchill.\n\nHowever, supporters of the presidential system question the validity of the point. They argue that if presidents were not able to command some considerable level of security in their tenures, their direct mandates would be worthless. They further counter that republics such as the United States have successfully endured war and other crises without the need to change heads of state. Supporters argue that presidents elected in a time of peace and prosperity have proven themselves perfectly capable of responding effectively to a serious crisis, largely due to their ability to make the necessary appointments to his cabinet and elsewhere in government or by creating new positions to deal with new challenges. One prominent, recent example would be the appointment of a Secretary of Homeland Security following the September 11 attacks in the United States.\n\nSome supporters of the presidential system counter that impediments to a leadership change, being that they are little more than an unavoidable consequence of the direct mandate afforded to a president, are thus a \"strength\" instead of a weakness in times of crisis. In such times, a prime minister might hesitate due to the need to keep parliament's support, whereas a president can act without fear of removal from office by those who might disapprove of his actions. Furthermore, even if a prime minister does manage to successfully resolve a crisis (or multiple crises), that does not guarantee and he or she will possess the political capital needed to remain in office for a similar, future crisis. Unlike what would be possible in a presidential system, a perceived crisis in the parliamentary system might give disgruntled backbenchers or rivals an opportunity to launch a vexing challenge for a prime minister's leadership.\n\nFinally, many have criticized presidential systems for their alleged slowness to respond to their citizens' needs. Often, the checks and balances make action difficult. Walter Bagehot said of the American system, \"the executive is crippled by not getting the law it needs, and the legislature is spoiled by having to act without responsibility: the executive becomes unfit for its name, since it cannot execute what it decides on; the legislature is demoralized by liberty, by taking decisions of others [and not itself] will suffer the effects\".\n\nDefenders of presidential systems argue that a parliamentary system operating in a jurisdiction with strong ethnic or sectarian tensions will tend to ignore the interests of minorities or even treat them with contempt – the first half century of government in Northern Ireland is often cited as an example – whereas presidential systems ensure that minority wishes and rights cannot be disregarded, thus preventing a \"tyranny of the majority\" and vice versa protect the wishes and rights of the majority from abuse by a legislature or an executive that holds a contrary viewpoint especially when there are frequent, scheduled elections. On the other hand, supporters of parliamentary systems contend that the strength and independence of the judiciary is the more decisive factor when it comes to protection of minority rights.\n\nBritish-Irish philosopher and MP Edmund Burke stated that an official should be elected based on \"his unbiased opinion, his mature judgment, his enlightened conscience\", and therefore should reflect on the arguments for and against certain policies before taking positions and then act out on what an official would believe is best in the long run for one's constituents and country as a whole even if it means short-term backlash. Thus defenders of presidential systems hold that sometimes what is wisest may not always be the most popular decision and vice versa.\n\nA number of key theoretical differences exist between a presidential and a parliamentary system:\n\nPresidential systems also have fewer ideological parties than parliamentary systems. Sometimes in the United States, the policies preferred by the two parties have been very similar (but \"see also\" polarization). In the 1950s, during the leadership of Lyndon B. Johnson, the Senate Democrats included the right-most members of the chamber—Harry Byrd and Strom Thurmond, and the left-most members—Paul Douglas and Herbert Lehman. This pattern does not prevail in Latin American presidential democracies.\n\nIn practice, elements of both systems overlap. Though a president in a presidential system does not have to choose a government under the legislature, the legislature may have the right to scrutinize his or her appointments to high governmental office, with the right, on some occasions, to block an appointment. In the United States, many appointments must be confirmed by the Senate, although once confirmed an appointee can only be removed against the president's will through impeachment. By contrast, though answerable \"to\" parliament, a parliamentary system's cabinet may be able to make use of the parliamentary 'whip' (an obligation on party members in parliament to vote with their party) to control and dominate parliament, reducing parliament's ability to control the government.\n\n\"Italics\" indicate states with limited recognition.\n\nThe following countries have presidential systems where a post of prime minister (official title may vary) exists alongside with that of president. Differently from other systems, however, the president is still both the head of state and government and the prime minister's roles are mostly to assist the president. Belarus and Kazakhstan, where the prime minister is effectively the head of government and the president the head of state, are exceptions.\n\n\n"}
{"id": "210282", "url": "https://en.wikipedia.org/wiki?curid=210282", "title": "Semi-presidential system", "text": "Semi-presidential system\n\nA semi-presidential system or dual executive system is a system of government in which a president exists alongside a prime minister and a cabinet, with the latter being responsible to the legislature of the state. It differs from a parliamentary republic in that it has a popularly elected head of state, who is more than a mostly ceremonial figurehead, and from the presidential system in that the cabinet, although named by the president, is responsible to the legislature, which may force the cabinet to resign through a motion of no confidence.\n\nWhile the Weimar Republic (1919–1933) exemplified an early semi-presidential system, the term \"semi-presidential\" was introduced in a 1959 article by journalist Hubert Beuve-Méry and popularized by a 1978 work by political scientist Maurice Duverger, both of which intended to describe the French Fifth Republic (established in 1958).\n\nMaurice Duverger's original definition of semi-presidentialism required that the president be elected, possess significant powers, and serve for a fixed term. Modern definitions merely require that the head of state be elected and that a separate prime minister that is dependent on parliamentary confidence lead the executive.\n\nThere are two separate subtypes of semi-presidentialism: premier-presidentialism and president-parliamentarism.\n\nUnder the premier-presidential system, the prime minister and cabinet are exclusively accountable to parliament. The president may choose the prime minister and cabinet, but only the parliament may approve them and remove them from office with a \"vote of no confidence\". This system is much closer to pure parliamentarism. This subtype is used in Burkina Faso, Cape Verde, East Timor, Lithuania, Madagascar, Mali, Mongolia, Niger, Poland, \nPortugal, Romania, São Tomé and Príncipe, Sri Lanka and Ukraine (since 2014; previously, between 2006 and 2010).\n\nUnder the president-parliamentary system, the prime minister and cabinet are dually accountable to the president and the parliament. The president chooses the prime minister and the cabinet but must have the support of a parliamentary majority for his choice. In order to remove a prime minister or the whole cabinet from power, the president can dismiss them, or the parliament can remove them by a \"vote of no confidence\". This form of semi-presidentialism is much closer to pure presidentialism. It is used in Guinea-Bissau, Mozambique, Namibia, Russia, Senegal and Taiwan. It was also used in Ukraine, first between 1996 and 2005, and again from 2010 to 2014, Georgia between 2004 and 2013, and in Germany during the \"Weimarer Republik\" (Weimar Republic), as the constitutional regime between 1919 and 1933 is called unofficially.\n\nThe powers that are divided between president and prime minister can vary greatly between countries.\n\nIn France, for example, in case of cohabitation, when the president and the prime minister come from opposing parties, the president oversees foreign policy and defense policy (these are generally called \"les prérogatives présidentielles\" (the presidential prerogatives) and the prime minister domestic policy and economic policy. In this case, the division of responsibilities between the prime minister and the president is not explicitly stated in the constitution, but has evolved as a political convention based on the constitutional principle that the prime minister is appointed (with the subsequent approval of a parliament majority) and dismissed by the president. On the other hand, whenever the president is from the same party as the prime minister who leads the \"conseil de gouvernement\" (cabinet), he/she often (if not usually) exercises \"de facto\" control over all fields of policy via the prime minister. It is up to the president to decide how much \"autonomy\" is left to \"their\" prime minister to act on their own.\n\nSemi-presidential systems may sometimes experience periods in which the president and the prime minister are from differing political parties. This is called \"cohabitation\", a term which originated in France when the situation first arose in the 1980s. Cohabitation can create an effective system of checks and balances or a period of bitter and tense stonewalling, depending on the attitudes of the two leaders, the ideologies of themselves or their parties, or the demands of their constituencies.\n\nIn most cases, cohabitation results from a system in which the two executives are not elected at the same time or for the same term. For example, in 1981, France elected both a Socialist president and legislature, which yielded a Socialist premier. But whereas the president's term of office was for seven years, the National Assembly only served for five. When, in the 1986 legislative election, the French people elected a right-of-centre assembly, Socialist President Mitterrand was forced into cohabitation with right wing premier Jacques Chirac.\n\nHowever, in 2000, amendments to the French constitution reduced the length of the French president's term from seven to five years. This has significantly lowered the chances of cohabitation occurring, as parliamentary and presidential elections may now be conducted within a shorter span of each other.\n\nThe incorporation of elements from both presidential and parliamentary republics brings some advantageous elements along with them but, however, it also faces disadvantages related to the confusion from mixed authority patterns.\n\nAdvantages\n\nDisadvantages\n\nIn semi-presidential systems, there is always both a president and a prime minister. In such systems, the president has genuine executive authority, unlike in a parliamentary republic, but the role of a head of government may be exercised by the prime minister. \"Italics\" indicate states with limited recognition. † indicate historic states, that is, those that no longer exist.\n\nThe president chooses the prime minister and cabinet, but only the parliament may remove them from office with a \"vote of no confidence\". The president does not have the power to directly dismiss the prime minister or cabinet. But the president can dissolve the parliament. \n\nThe president chooses the prime minister without the confidence vote from the parliament. In order to remove a prime minister or the whole cabinet from power, the president can dismiss them or the parliament can remove them by a \"vote of no confidence\", but the president can't dissolve the parliament.\n\n\n"}
{"id": "587618", "url": "https://en.wikipedia.org/wiki?curid=587618", "title": "Public administration", "text": "Public administration\n\nPublic administration is the implementation of government policy and also an academic discipline that studies this implementation and prepares civil servants for working in the public service. As a \"field of inquiry with a diverse scope\" whose fundamental goal is to \"advance management and policies so that government can function\". Some of the various definitions which have been offered for the term are: \"the management of public programs\"; the \"translation of politics into the reality that citizens see every day\"; and \"the study of government decision making, the analysis of the policies themselves, the various inputs that have produced them, and the inputs necessary to produce alternative policies.\"\n\nPublic administration is \"centrally concerned with the organization of government policies and programs as well as the behavior of officials (usually non-elected) formally responsible for their conduct\". Many non-elected public servants can be considered to be public administrators, including heads of city, county, regional, state and federal departments such as municipal budget directors, human resources (HR) administrators, city managers, census managers, state mental health directors, and cabinet secretaries. Public administrators are public servants working in public departments and agencies, at all levels of government.\n\nIn the United States, civil servants and academics such as Woodrow Wilson promoted civil service reform in the 1880s, moving public administration into academia. However, \"until the mid-20th century and the dissemination of the German sociologist Max Weber's theory of bureaucracy\" there was not \"much interest in a theory of public administration\". The field is multidisciplinary in character; one of the various proposals for public administration's sub-fields sets out six pillars, including human resources, organizational theory, policy analysis, statistics, budgeting, and ethics.\n\nIn 1947 Paul H. Appleby defined public administration as \"public leadership of public affairs directly responsible for executive action\". In a democracy, it has to do with such leadership and executive action in terms that respect and contribute to the dignity, the worth, and the potentials of the citizen. One year later, Gordon Clapp, then Chairman of the Tennessee Valley Authority defined public administration \"as a public instrument whereby democratic society may be more completely realized.\" This implies that it must \"relate itself to concepts of justice, liberty, and fuller economic opportunity for human beings\" and is thus \"concerned with \"people, with ideas, and with things\". According to James D. Carroll & Alfred M. Zuck, the publication by \"Woodrow Wilson of his essay, \"The Study of Administration\" in 1887 is generally regarded as the beginning of public administration as a specific field of study\".\n\nDrawing on the democracy theme and discarding the link to the executive branch, Patricia M. Shields asserts that public administration \"deals with the stewardship and implementation of the products of a living democracy\". The key term \"product\" refers to \"those items that are constructed or produced\" such as prisons, roads, laws, schools, and security. \"As implementors, public managers engage these products.\" They participate in the doing and making of the \"living\" democracy. A living democracy is \"an environment that is changing, organic\", imperfect, inconsistent and teaming with values. \"Stewardship is emphasized because public administration is concerned \"with accountability and effective use of scarce resources and ultimately making the connection between the doing, the making and democratic values\".\n\nMore recently scholars claim that \"public administration has no generally accepted definition\", because the \"scope of the subject is so great and so debatable that it is easier to explain than define\". Public administration is a field of study (i.e., a discipline) and an occupation. There is much disagreement about whether the study of public administration can properly be called a discipline, largely because of the debate over whether public administration is a sub-field of political science or a sub-field of administrative science\", the latter an outgrowth of its roots in policy analysis and evaluation research. Scholar Donald Kettl is among those who view public administration \"as a sub-field within political science\". According to Lalor a society with a public authority that provides at least one public good can be said to have a public administration whereas the absence of either (or \"a fortiori\" both) a public authority or the provision of at least one public good implies the absence of a public administration. He argues that public administration is the public provision of public goods in which the demand function is satisfied more or less effectively by politics, whose primary tool is rhetoric, providing for public goods, and the supply function is satisfied more or less efficiently by public management, whose primary tools are speech acts, producing public goods. The moral purpose of public administration, implicit in its acceptance of its role, is the maximization of the opportunities of the public to satisfy its wants.\n\nThe North American Industry Classification System definition of the Public Administration (NAICS 91) sector states that public administration \"... comprises establishments primarily engaged in activities of a governmental nature, that is, the enactment and judicial interpretation of laws and their pursuant regulations, and the administration of programs based on them\". This includes \"Legislative activities, taxation, national defense, public order and safety, immigration services, foreign affairs and international assistance, and the administration of government programs are activities that are purely governmental in nature\".\n\nFrom the academic perspective, the National Center for Education Statistics (NCES) in the United States defines the study of public administration as \"A program that prepares individuals to serve as managers in the executive arm of local, state, and federal government and that focuses on the systematic study of executive organization and management. Includes instruction in the roles, development, and principles of public administration; the management of public policy; executive-legislative relations; public budgetary processes and financial management; administrative law; public personnel management; professional ethics; and research methods.\"\n\nSuch neat and prosperous civilisations as Harappa and Mohenjo-daaro must have had a disciplined, benevolent and uncorrupt cadre of public servants. In support of this, there are many references to Brihaspati's works on laws and governance. An interesting extract from Aaine-Akbari [vol.III, tr. by H. S. Barrett, pp217-218] written by Abul Fazl, the famous historian of Akbar's court, mentions a symposium of philosophers of all faiths held in 1578 at Akbar's instance. This sounds credible in the context of Akbar's restless desire to find truth, reflected in his launching a new religion called Din-e-elaahi. The account under advisement is given by the well-known historian Vincent Smith, in his article titled \"The Jain Teachers of Akbar\". Some Charvaka thinkers are said to have participated in the symposium. Under the heading \"Naastika\" Abul Fazl has referred to the good work, judicious administration and welfare schemes that were emphasised by the Charvaka law-makers. Somadeva has also mentioned the Charvaka method of defeating the enemies of the nation. He has referred to thirteen enemies who remain disguised in the kingdom for their selfish interests. They may contain a few relatives of the king and subsidiary rulers, but they should not be spared. They should be rigorously punished like any other such opponent. Kautilya, as already mentioned, has given a detailed scheme to remove the enemies in the garb of friends. The Charvaka stalwart, Brihaspati, is so much more ancient than Kautilya and Somadeva. He appears to be contemporaneous with the Harappa and Mohenjo-daaro culture.\n\nThe central point of traditional religious ritual is to earn ready money for its perpetrators. All unproductive, barren rites designed for various moments in human life starting from several months prior to birth and extending over several years beyond death in the form of the annual sraddha, many of which are current even today, are but channels to feed the priests. They are unreal, imagined and wasteful. While they are unreal, imagined and wasteful; the feeding is real.\n\nThis cunning paradox was realised by the Charvaka for its real worth. They wanted financial causes to produce financial results. Imagined causes only produced imagined results not real ones. \n\nDating back to Antiquity, Pharaohs, kings and emperors have required pages, treasurers, and tax collectors to administer the practical business of government. Prior to the 19th century, staffing of most public administrations was rife with nepotism, favouritism, and political patronage, which was often referred to as a \"spoils system\". Public administrators have long been the \"eyes and ears\" of rulers. In medieval times, the abilities to read and write, add and subtract were as dominated by the educated elite as public employment. Consequently, the need for expert civil servants whose ability to read and write formed the basis for developing expertise in such necessary activities as legal record-keeping, paying and feeding armies and levying taxes. As the European Imperialist age progressed and the militarily powers extended their hold over other continents and people, the need for a sophisticated public administration grew.\n\nThe field of management may well be said to have originated in ancient China, including possibly the first highly centralized bureaucratic state, and the earliest (by the second century BC) example of an administration based on merit through testing. Far in advance of the rest of the world until almost the end of the 18th century, Sinologist Herrlee G. Creel and other scholars find the influence of Chinese administration in Europe by the 12th century, for example, in Fredrick II's promulgations, characterized as the \"birth certificate of modern bureaucracy\".\n\nThough Chinese administration cannot be traced to any one individual, emphasizing a merit system figures of the Fa-Jia like 4th century BC reformer Shen Buhai (400–337 BC) may have had more influence than any other, and might be considered its founder, if not valuable as a rare pre-modern example of abstract theory of administration. Creel writes that, in Shen Buhai, there are the \"seeds of the civil service examination\", and that, if one wishes to exaggerate, it would \"no doubt be possible to translate Shen Buhai's term Shu, or technique, as 'science'\", and argue that he was the first political scientist, though Creel does \"not care to go this far\".\n\nThe eighteenth-century noble, King Frederick William I of Prussia, created professorates in Cameralism in an effort to train a new class of public administrators. The universities of Frankfurt an der Oder and University of Halle were Prussian institutions emphasizing economic and social disciplines, with the goal of societal reform. Johann Heinrich Gottlob Justi was the most well-known professor of Cameralism. Thus, from a Western European perspective, Classic, Medieval, and Enlightenment-era scholars formed the foundation of the discipline that has come to be called public administration.\n\nLorenz von Stein, an 1855 German professor from Vienna, is considered the founder of the science of public administration in many parts of the world. In the time of Von Stein, public administration was considered a form of administrative law, but Von Stein believed this concept too restrictive. Von Stein taught that public administration relies on many preestablished disciplines such as sociology, political science, administrative law and public finance. He called public administration an integrating science, and stated that public administrators should be concerned with both theory and practice. He argued that public administration is a science because knowledge is generated and evaluated according to the scientific method.\n\nModern American public administration is an extension of democratic governance, justified by classic and liberal philosophers of the western world ranging from Aristotle to John Locke to Thomas Jefferson.\nIn the United States of America, Woodrow Wilson is considered the father of public administration. He first formally recognized public administration in an 1887 article entitled \"The Study of Administration\". The future president wrote that \"it is the object of administrative study to discover, first, what government can properly and successfully do, and, secondly, how it can do these proper things with the utmost possible efficiency and at the least possible cost either of money or of energy\". Wilson was more influential to the science of public administration than Von Stein, primarily due to an article Wilson wrote in 1887 in which he advocated four concepts:\n\n\nThe separation of politics and administration has been the subject of lasting debate. The different perspectives regarding this dichotomy contribute to differentiating characteristics of the suggested generations of public administration.\n\nBy the 1920s, scholars of public administration had responded to Wilson's solicitation and thus textbooks in this field were introduced. A few distinguished scholars of that period were, Luther Gulick, Lyndall Urwick, Henri Fayol, Frederick Taylor, and others. Frederick Taylor (1856–1915), another prominent scholar in the field of administration and management also published a book entitled \"The Principles of Scientific Management\" (1911). He believed that scientific analysis would lead to the discovery of the \"one best way\" to do things or carrying out an operation. This, according to him could help save cost and time. Taylor's technique was later introduced to private industrialists, and later into the various government organizations (Jeong, 2007).\n\nTaylor's approach is often referred to as Taylor's Principles or Taylorism. Taylor's scientific management consisted of main four principles (Frederick W. Taylor, 1911):\n\nTaylor had very precise ideas about how to introduce his system (approach):\n'It is only through enforced standardization of methods, enforced adoption of the best implements and working conditions, and enforced cooperation that this faster work can be assured. And the duty of enforcing the adoption of standards and enforcing this cooperation rests with management alone.'\n\nThe American Society for Public Administration (ASPA) the leading professional group for public administration was founded in 1939. ASPA sponsors the journal Public Administration Review, which was founded in 1940.\n\nThe separation of politics and administration advocated by Wilson continues to play a significant role in public administration today. However, the dominance of this dichotomy was challenged by second generation scholars, beginning in the 1940s. Luther Gulick's fact-value dichotomy was a key contender for Wilson's proposed politics-administration dichotomy. In place of Wilson's first generation split, Gulick advocated a \"seamless web of discretion and interaction\".\n\nLuther Gulick and Lyndall Urwick are two second-generation scholars. Gulick, Urwick, and the new generation of administrators built on the work of contemporary behavioural, administrative, and organizational scholars including Henri Fayol, Fredrick Winslow Taylor, Paul Appleby, Frank Goodnow, and Willam Willoughby. The new generation of organizational theories no longer relied upon logical assumptions and generalizations about human nature like classical and enlightened theorists.\n\nGulick developed a comprehensive, generic theory of organization that emphasized the scientific method, efficiency, professionalism, structural reform, and executive control. Gulick summarized the duties of administrators with an acronym; POSDCORB, which stands for planning, organizing, staffing, directing, coordinating, reporting, and budgeting. Fayol developed a systematic, 14-point treatment of private management. Second-generation theorists drew upon private management practices for administrative sciences. A single, generic management theory bleeding the borders between the private and the public sector was thought to be possible. With the general theory, the administrative theory could be focused on governmental organizations. The mid-1940s theorists challenged Wilson and Gulick. The politics-administration dichotomy remained the centre of criticism.\n\nDuring the 1950s, the United States experienced prolonged prosperity and solidified its place as a world leader. Public Administration experienced a kind of heyday due to the successful war effort and successful post war reconstruction in Western Europe and Japan. Government was popular as was President Eisenhower. In the 1960s and 1970s, government itself came under fire as ineffective, inefficient, and largely a wasted effort. The costly American intervention in Vietnam along with domestic scandals including the bugging of Democratic party headquarters (the 1974 Watergate scandal) are two examples of self-destructive government behaviour that alienated citizens.\n\nThere was a call by citizens for efficient administration to replace ineffective, wasteful bureaucracy. Public administration would have to distance itself from politics to answer this call and remain effective. Elected officials supported these reforms. The Hoover Commission, chaired by University of Chicago professor Louis Brownlow, to examine reorganization of government. Brownlow subsequently founded the Public Administration Service (PAS) at the university, an organization which has provided consulting services to all levels of government until the 1970s.\n\nConcurrently, after World War II, the whole concept of public administration expanded to include policymaking and analysis, thus the study of \"administrative policy making and analysis\" was introduced and enhanced into the government decision-making bodies. Later on, the human factor became a predominant concern and emphasis in the study of public administration. This period witnessed the development and inclusion of other social sciences knowledge, predominantly, psychology, anthropology, and sociology, into the study of public administration (Jeong, 2007). Henceforth, the emergence of scholars such as, Fritz Morstein Marx with his book \"The Elements of Public Administration\" (1946), Paul H. Appleby \"Policy and Administration\" (1952), Frank Marini 'Towards a New Public Administration' (1971), and others that have contributed positively in these endeavors.\n\nIn the late 1980s, yet another generation of public administration theorists began to displace the last. The new theory, which came to be called New Public Management, was proposed by David Osborne and Ted Gaebler in their book \"Reinventing Government\". The new model advocated the use of private sector-style models, organizational ideas and values to improve the efficiency and service-orientation of the public sector. During the Clinton Administration (1993–2001), Vice President Al Gore adopted and reformed federal agencies using NPM approaches. In the 1990s, new public management became prevalent throughout the bureaucracies of the US, the UK and, to a lesser extent, in Canada. The original public management theories have roots attributed to policy analysis, according to Richard Elmore in his 1986 article published in the \"Journal of Policy Analysis and Management\".\n\nSome modern authors define NPM as a combination of splitting large bureaucracies into smaller, more fragmented agencies, encouraging competition between different public agencies, and encouraging competition between public agencies and private firms and using economic incentives lines (e.g., performance pay for senior executives or user-pay models). NPM treats individuals as \"customers\" or \"clients\" (in the private sector sense), rather than as citizens.\n\nSome critics argue that the New Public Management concept of treating people as \"customers\" rather than \"citizens\" is an inappropriate borrowing from the private sector model, because businesses see customers as a means to an end (profit), rather than as the proprietors of government (the owners), opposed to merely the customers of a business (the patrons). In New Public Management, people are viewed as economic units not democratic participants which is the hazard of linking an MBA (business administration, economic and employer-based model) too closely with the public administration (governmental, public good) sector. Nevertheless, the NPM model (one of four described by Elmore in 1986, including the \"generic model\") is still widely accepted at multiple levels of government (e.g., municipal, state/province, and federal) and in many OECD nations.\n\nIn the late 1990s, Janet and Robert Denhardt proposed a new public services model in response to the dominance of NPM. A successor to NPM is digital era governance, focusing on themes of reintegrating government responsibilities, needs-based holism (executing duties in cursive ways), and digitalization (exploiting the transformational capabilities of modern IT and digital storage). One example of this is openforum.com.au, an Australian non-for-profit eDemocracy project which invites politicians, senior public servants, academics, business people and other key stakeholders to engage in high-level policy debate.\n\nAnother new public service model is what has been called New Public Governance, an approach which includes a centralization of power; an increased number, role and influence of partisan-political staff; personal-politicization of appointments to the senior public service; and, the assumption that the public service is promiscuously partisan for the government of the day.\n\nIn the mid-1980s, the goal of community programs in the United States was often represented by terms such as independent living, community integration, inclusion, community participation, deinstitutionalization, and civil rights. Thus, the same public policy (and public administration) was to apply to all citizens, inclusive of disability. However, by the 1990s, categorical state systems were strengthened in the United States (Racino, in press, 2014), and efforts were made to introduce more disability content into the public policy curricula with disability public policy (and administration) distinct fields in their own right. Behaviorists have also dominated \"intervention practice\" (generally not the province of public administration) in recent years, believing that they are in opposition to generic public policy (termed ecological systems theory, of the late Urie Bronfenbrenner).\n\nIncreasingly, public policy academics and practitioners have utilized the theoretical concepts of political economy to explain policy outcomes such as the success or failure of reform efforts or the persistence of suboptimal outcomes.\n\nIn academia, the field of public administration consists of a number of sub-fields. Scholars have proposed a number of different sets of sub-fields. One of the proposed models uses five \"pillars\":\n\n\nGiven the array of duties public administrators find themselves performing, the professional administrator might refer to a theoretical framework from which he or she might work. Indeed, many public and private administrative scholars have devised and modified decision-making models.\n\nIn 1971, Professor William Niskanen proposed a rational choice variation which he called the \"budget-maximizing model\". He claimed that rational bureaucrats will universally seek to increase the budgets of their units (to enhance their stature), thereby contributing to state growth and increased public expenditure. Niskanen served on President Reagan's Council of Economic Advisors; his model underpinned what has been touted as curtailed public spending and increased privatization. However, budgeted expenditures and the growing deficit during the Reagan administration is evidence of a different reality. A range of pluralist authors have critiqued Niskanen's universalist approach. These scholars have argued that officials tend also to be motivated by considerations of the public interest.\n\nThe bureau-shaping model, a modification of Niskanen, holds that rational bureaucrats only maximize the part of their budget that they spend on their own agency's operations or give to contractors and interest groups. Groups that are able to organize a \"flowback\" of benefits to senior officials would, according to this theory, receive increased budgetary attention. For instance, rational officials will get no benefit from paying out larger welfare checks to millions of low-income citizens because this does not serve a bureaucrats' goals. Accordingly, one might instead expect a jurisdiction to seek budget increases for defense and security purposes in place programming. If we refer back to Reagan once again, Dunleavy's bureau shaping model accounts for the alleged decrease in the \"size\" of government while spending did not, in fact, decrease. Domestic entitlement programming was financially deemphasized for military research and personnel.\n\nUniversity programs preparing students for careers in public administration typically offer the Master of Public Administration (MPA) degree, although in some universities, an MA in Public Administration is awarded. In the United States, the academic field of public administration draws heavily on political science and administrative law. Some MPA programs include economics courses to give students a background in microeconomic issues (markets, rationing mechanisms, etc.) and macroeconomic issues (e.g., national debt). Scholars such as John A. Rohr write of a long history behind the constitutional legitimacy of government bureaucracy. In Europe (notably in Britain and Germany), the divergence of the field from other disciplines can be traced to the 1720s continental university curriculum. Formally, official academic distinctions were made in the 1910s and 1890s, respectively.\n\nThe goals of the field of public administration are related to the democratic values of improving equality, justice, security, efficiency and effectiveness of public services in a non-profit venue; business administration, on the other hand, is primarily concerned with expanding market share, generating revenue and earning profit. For a field built on concepts (accountability, governance, decentralization and clientele), these concepts are often ill-defined and typologies often ignore certain aspects of these concepts.\n\nThe more specific term \"public management\" refers to ordinary, routine or typical management that aims to achieve public good. In some definitions, \"public management\" refers to private sector, market-driven perspective on the operation of government. This typically involves putting senior executives on performance contracts, rather than tenured positions, instituting pay-for-performance systems for executives, creating revenue-generating agencies and so on. This latter view is often called \"new public management\" (NPM) by its advocates. New Public Management represents a reform attempt that emphasizes the professional nature of public administration. NPM advocates aim to replace the academic, moral or disciplinary emphasis of traditional public administration with a professional focus. Some theorists advocate a \"bright line\" differentiation of the professional field from related academic disciplines like political science and sociology; it remains interdisciplinary in nature.\n\nOne public administration scholar, Donald Kettl, argues that \"public administration sits in a disciplinary backwater\", because \"for the last generation, scholars have sought to save or replace it with fields of study like implementation, public management, and formal bureaucratic theory\". Kettl states that \"public administration, as a subfield within political science...is struggling to define its role within the discipline\". He notes two problems with public administration: it \"has seemed methodologically to lag behind\" and \"the field's theoretical work too often seems not to define it\"-indeed, \"some of the most interesting recent ideas in public administration have come from outside the field\".\n\nPublic administration theory is the domain in which discussions of the meaning and purpose of government, the role of bureaucracy in supporting democratic governments, budgets, governance, and public affairs takes place. In recent years, public administration theory has periodically connoted a heavy orientation toward critical theory and postmodern philosophical notions of government, governance, and power. However, many public administration scholars support a classic definition of the term emphasizing constitutionality, public service, bureaucratic forms of organization, and hierarchical government.\n\nComparative public administration or CPA is defined as the study of administrative systems in a comparative fashion or the study of public administration in other countries. There have been several issues which have hampered the development of comparative public administration, including: the major differences between Western countries and developing countries; the lack of curriculum on this sub-field in public administration programs; and the lack of success in developing theoretical models which can be scientifically tested. Even though CPA is a weakly formed field as a whole, this sub-field of public administration is an attempt at cross-cultural analysis, a \"quest for patterns and regularities of administrative action and behavior.\" CPA is an integral part to the analysis of public administration techniques. The process of comparison allows for more widely applicable policies to be tested in a variety of situations.\n\nComparative public administration emerged during the post-World War II period in order to seek international developmental strategies which aided in the containment of communism during the Cold War. (Riggs 1954, Heady 1960) The developers of this field expanded on a general theory, a research agenda, and generalized \"lessons learned\". (Riggs 1954, Heady 1960) A prominent figure of Public Administration, Woodrow Wilson, commented on the study by saying, \"Like principles of civil liberty are everywhere fostering like methods of government; and if comparative studies of the ways and means of government should enable us to offer suggestions which will practicably combine openness and vigor in the administration of such governments with ready docility to all serious, well-sustained public criticism, they will have approved themselves worthy to be ranked among the highest and most fruitful of the great departments of political study\". As the financial state of the powering countries began to stabilize toward the decline of the Cold War, the field of CPA began to diminish. The resulting decline caused the lack of further expansion of this study making it irrelevant.\n\nComparative public administration lacks curriculum, which has prevented it from becoming a major field of study. This lack of understanding of the basic concepts that build this field's foundation has ultimately led to its lack of use. For example, William Waugh, a professor at Georgia State University has stated \"Comparative studies are difficult because of the necessity to provide enough information on the sociopolitical context of national administrative structures and processes for readers to understand why there are differences and similarities.\" He also asserts, \"Although there is sizable literature on comparative public administration it is scattered and dated.\"\n\nWaugh argues that public administration requires an understanding of different administrative structures and a comparison of different public administration models. The literature to build this base of knowledge is scattered and often hard to obtain. The lack or ill-formed use of comparative public administration has been detrimental for many countries, including the United States. Fred Riggs a political scientist, states that \"comparisons to the United States also can be problematic, because of the tendency of many American scholars to presume the American organizational structures and processes are models for other nations to emulate, which was a failing of early developmental administrative studies.\" In this, he claims the misuse and misapplication of comparative public administration has led to it being underdeveloped.\n\nThe development and better use comparative public administration could lead to better understanding. In 2002, the National Security Strategy was used in the battle of hearts and minds. They tried to assimilate with an Arab and Islamic audience to push American values and democracy in an attempt to stop terrorism, when in fact the lack of comparison on the public level was ineffective and backfired. The lack of willingness to understand their culture led to more tension in the Middle East. In conclusion of these events there are not enough resources directed to the study of comparative public administration. For a basic understanding of sociopolitical structure of a society or culture is a key component of comparative public administration.\n\nDespite all of its set backs there are examples of the application of well-formed Comparative Public Administration working in the world today. One of which is the comparison on the national level David Clark an author in this field states \"In spite of similarities in public management reform rhetoric, it is argued that there is increasing divergence in the philosophy & practice of public service in the two nations, & and these differences reflect regimes that incorporate different ideals of citizenship.\" This highlights the benefit of proper comparison of public administration. By examining patterns that emerge in international public sectors one can identify similarities and differences in many things including ideals of citizenship on the local level. Although the United States failed use of Comparative Public Administration in the Middle East is noted, they did properly incorporate it domestically. \"During the Clinton administration, the focus on residential energy consumption in the United States was elevated to a high level with the inauguration of the Million Solar Roofs initiative, in which the Department of Energy (DOE) sponsored workshops, developed a pool of existing federal lending and financing options, and worked with partners in the solar and building industries to remove market barriers to strengthen grassroots demand for solar technologies\".\n\nThis grassroots demand may have come from the comparative knowledge that concluded \"In the United States, residential and commercial buildings combined now use 71% of all electricity produced and account for 79% of all electricity expenditures. Annual CO emission attributed to electricity consumption in these U.S. buildings constitute 43% of the country's annual total CO emission, which is approximately equivalent to the total CO emission of Japan, France, and the United Kingdom combined. These levels support the claim of the Intergovernmental Panel on Climate Change that energy use in buildings offer more potential for reducing carbon emission than any other single sector in the United States and abroad.\". This example compares CO emission in the United States to other countries and through the buildings sector; the US could cut down on CO2 emission. The field of comparative public administration is often misunderstood for the definition itself is complex and requires layers of understanding. The field will require many more years of collaborative research before it becomes a widely recognized academic study.\n\nSome public administration programs have similarities to business administration programs, in cases where the students from both the Master's in Public Administration (MPA) and Master's in Business Administration (MBA) programs take many of the same courses. In some programs, the MPA (or MAPA) is more clearly distinct from the MBA, in that the MPA often emphasizes substantially different ethical and sociological criteria that pertain to administering government programs for the public good that have not been key criteria for business managers, who typically aim to maximize profit or share price.\n\nThe MPA is related to similar graduate-level government studies programs including Master of Arts (MA) programs in public affairs, public policy, and political science. MPA degrees may be more likely to include program emphases on policy analysis techniques or other topical focuses such as the study of international affairs as opposed to MA degrees, which tend to focus on constitutional issues such as separation of powers, administrative law, contracting with government, problems of governance and power, and participatory democracy. Some MPA degrees may be more oriented towards training students to undertake public service work tasks, whereas some MA programs may have a more academic, theoretical focus. Some universities offer their Masters in public administration as an MA degree (e.g., Carleton University in Ottawa, Canada and the University of Kerala in India).\n\nSome universities offer mid-career Master's programs, sometimes called an MC/MPA, that can be taken part-time (often outside of business hours) by public servants and public service managers who are working full-time. Community programs may offer internships or continuing education credits. One example is the Maxwell School's mid-career Masters at Syracuse University, which was launched by Robert Iversen in the 1970s.\n\nThere are two types of doctoral degrees in public administration: the Doctor of Public Administration (DPA) and the Ph.D. in public administration. The DPA is an applied-research doctoral degree in the field of public administration, focusing on the practice of public administration more than on its theoretical aspects. The DPA requires coursework beyond the Masters level and a thesis, dissertation or other doctoral project. Upon successful completion of the doctoral requirements, the title of \"Doctor\" is awarded and the post-nominals of D.P.A. can be used. Some universities use the Ph.D. as their doctoral degree in public administration (e.g., Carleton University in Ottawa, Canada, and the University Of Kerala in India). The Ph.D. is typically sought by individuals aiming to become professors of public administration or researchers. Individuals pursuing a Ph.D. in public administration often pursue more theoretical dissertation topics than their DPA counterparts.\n\nNotable scholars of public administration have come from a range of fields. In the period before public administration existed as its own independent discipline, scholars contributing to the field came from economics, sociology, management, political science, administrative law, and, other related fields. More recently, scholars from public administration and public policy have contributed important studies and theories.\n\nThere are a number of international public administration organizations. The Commonwealth Association of Public Administration and Management (CAPAM) is diverse, as it includes the 54 member states of the Commonwealth from India and the UK to Nauru. Its biennial conference brings together ministers of public service, top public officials and leading scholars. The oldest organization is the International Institute of Administrative Sciences (IIAS). Based in Brussels, Belgium, the IIAS is a worldwide platform providing a space for exchanges that promote knowledge and good practices to improve the organization and operation of public administration. The IIAS also aims to ensure that public agencies will be in a position to better respond to the current and future expectations and needs of society. The IIAS has set up four entities: the International Association of Schools and Institutes of Administration (IASIA), the European Group for Public Administration (EGPA), The Latin American Group for Public Administration (LAGPA) and the Asian Group for Public Administration (AGPA). IASIA is an association of organizations and individuals whose activities and interests focus on public administration and management. The activities of its members include education and training of administrators and managers. It is the only worldwide scholarly association in the field of public management. EGPA, LAGPA and AGPA are the regional sub-entities of the IIAS. Another body, the International Committee of the US-based Network of Schools of Public Policy, Affairs, and Administration (NASPAA), has developed a number of relationships around the world. They include sub regional and National forums like CLAD, INPAE and NISPAcee, APSA, ASPA.\n\nThe Center for Latin American Administration for Development (CLAD), based in Caracas, Venezuela, this regional network of schools of public administration set up by the governments in Latin America is the oldest in the region. The Institute is a founding member and played a central role in organizing the Inter-American Network of Public Administration Education (INPAE). Created in 2000, this regional network of schools is unique in that it is the only organization to be composed of institutions from North and Latin America and the Caribbean working in public administration and policy analysis. It has more than 49 members from top research schools in various countries throughout the hemisphere.\n\nNISPAcee is a network of experts, scholars and practitioners who work in the field of public administration in central Europe and Eastern Europe, including the Russian Federation and the Caucasus and Central Asia. The US public administration and political science associations like NASPAA, American Political Science Association (APSA) and American Society of Public Administration (ASPA). These organizations have helped to create the fundamental establishment of modern public administration.\n\nEastern Regional Organization for Public Administration (EROPA) is a state-membership based organization, open to other organizations and individuals, headquartered in The Philippines with centres and membership organized around the Asia Pacific region. EROPA organizes annual conferences, and publishes a journal \"Asian Review of Public Administration\" (ARPA). It has a number of centres in the region, and assists in networking experts with its members.\n\n\"Public management\" is an approach to government administration and non-profit administration that resembles or draws on private-sector management and business techniques and approaches. These business approaches often aim to maximize efficiency and effectiveness and provide improved customer service. A contrast is drawn with the study of public administration, which emphasizes the social and cultural drivers of government that many contend (e.g., Graham T. Allison and Charles Goodsell) makes it different from the private sector. Studying and teaching about public management are widely practiced in developed nations.\n\nMany entities study public management in particular, in various countries, including:\n\nComparative public management, through government performance auditing, examines the efficiency and effectiveness of two or more governments.\n\n\n\n\n\n"}
{"id": "153324", "url": "https://en.wikipedia.org/wiki?curid=153324", "title": "Public policy", "text": "Public policy\n\nPublic policy is the process by which governments translate their political vision into programmes and actions to deliver ‘outcomes — desired changes in the real world’. The ‘real world’ is constantly changing and this has resulted in the movement towards greater use of evidence in policy design, making and implementation. Rational choice theory, or now more frequently known as evidence-based policy, argues that focusing on scientific evidence, instead of history and culture, should guide our public policy making. \nThe foundation of public policy is composed of national constitutional laws and regulations. Further substrates include both judicial interpretations and regulations which are generally authorized by legislation. Public policy is considered strong when it solves problems efficiently and effectively, serves and supports governmental institutions and policies, and encourages active citizenship.\n\nIn his book 'Advanced Introduction to Public Policy', B. Guy Peters defines public policy as \"the set of activities that governments engage in for the purpose of changing their economy and society\", effectively saying that public policy is legislation brought in with the aim of benefiting or impacting the electorate in some way. In another definition, author B. Dente in his book 'Understanding Policy Decisions' explains public policy as \"a set of actions that affect the solution of a policy problem, i.e. a dissatisfaction regarding a certain need, demand or opportunity for public intervention. Its quality is measured by the capacity to create public value.\" \n\nOther scholars define public policy as a system of \"courses of action, regulatory measures, laws, and funding priorities concerning a given topic promulgated by a governmental entity or its representatives.\" Public policy is commonly embodied in \"constitutions, legislative acts, and judicial decisions.\"\n\nPublic policy focuses on the decisions that create the outputs of a political system, such as transport policies, the management of a public health service, the administration of a system schooling and the organization of a defence force.\n\nIn the United States, this concept refers not only to the result of policies, but more broadly to the decision-making and analysis of governmental decisions. As an academic discipline, public policy is studied by professors and students at public policy schools of major universities throughout the country. The U.S. professional association of public policy practitioners, researchers, scholars, and students is the Association for Public Policy Analysis and Management.\n\nMuch of public policy is concerned with evaluating decision-making in governments and public bureaucracies.\n\nPublic policy making can be characterized as a dynamic, complex, and interactive system through which public problems are identified and resolved by creating new public policy or by reforming existing public policy.\n\nPublic problems can originate in endless ways and require different policy responses (such as regulations, subsidies, import quotas, and laws) on the local, national, or international level. The public problems that influence public policy making can be of economic, social, or political nature\n\nThe Government holds a legal monopoly to initiate or threaten physical force to achieve its ends when necessary. For instance, in times of chaos when quick decision making is needed..\nPublic policy making is an exhausting and time-consuming 'policy cycle'. The basic stages of policy cycle are as follows; a problem is identified, a policy response is formulated, the preferred solution is then selected and implemented, and finally the policy is evaluated. However, the evaluation stage takes an in depth look into what can be learnt from the process as a whole, whether the original problem has been solved, and if not, what is recommended as an alternative course of action. Thus, returning policy makers to the very first step; the identification.\n\nEach system is influenced by different public problems and issues, and has different stakeholders; as such, each requires different public policy.\n\nIn public policy making, numerous individuals, corporations, non-profit organizations and interest groups compete and collaborate to influence policymakers to act in a particular way.\n\nThe large set of actors in the public policy process, such as politicians, civil servants, lobbyists, domain experts, and industry or sector representatives, use a variety of tactics and tools to advance their aims, including advocating their positions publicly, attempting to educate supporters and opponents, and mobilizing allies on a particular issue. \n\nMany actors can be important in the public policy process, but government officials ultimately choose public policy in response to the public issue or problem at hand. In doing so, government officials are expected to meet public sector ethics and take the needs of all project stakeholders into account.\n\nIt is however worth noting that what public policy is put forward can be influenced by the political stance of the party in power. Following the 2008/2009 financial crisis, David Cameron's Conservative party looked to implement a policy of austerity in 2010 after winning the General Election that year, to shore up the economy and diminish the UK's national debt. Whilst the Conservatives saw reducing the national debt as an absolute priority, the Labour Party, since the effects of Conservative austerity became apparent, have slated the policy for its 'needless' pressure on the working classes and those reliant on welfare, their 2019 election manifesto stating \"Tory cuts [have] pushed our public services to breaking point” and that  “the Conservatives have starved our education system of funding”. This is a good example of how varying political beliefs can impact what is perceived as paramount for the electorate. \n\nSince societies have changed in the past decades, the public policy making system changed too. In the 2010s, public policy making is increasingly goal-oriented, aiming for measurable results and goals, and decision-centric, focusing on decisions that must be taken immediately. \n\nFurthermore, mass communications and technological changes such as the widespread availability of the Internet have caused the public policy system to become more complex and interconnected. The changes pose new challenges to the current public policy systems and pressures leaders to evolve to remain effective and efficient.\n\nPublic policies come from all governmental entities and at all levels: legislatures, courts, bureaucratic agencies, and executive offices at national, local and state levels. On the federal level, public policies are laws enacted by Congress, executive orders issued by the president, decisions handed down by the US Supreme Court, and regulations issued by bureaucratic agencies.\n\nOn the local, public policies include city ordinances, fire codes, and traffic regulations. They also take the form of written rules and regulations of city governmental departments: the police, fire departments, street repair, or building inspection. On the state level, public policies involve laws enacted by the state legislatures, decisions made by state courts, rules developed by state bureaucratic agencies, and decisions made by governors.\n\nData-driven policy is a policy designed by a government based on existing data, evidence, rational analysis and use of information technology to crystallize problems and highlight effective solutions. Data-driven policy making aims to make use of data and collaborate with citizens to co-create policy. Policy makers can now make use of new data sources and technological developments like Artificial Intelligence to gain new insights and make policy decisions which contribute to societal development.\n\nEvidence-based policy is a term now commonly used to refer to Rational Choice Theory. This transition occurred because political actors did not want to carry the connotations that rational choice theory does. Therefore, the theory has simply become associated with evidence, what rational choice theory admires most. This approach to public policy attempts to ensure that every decision is of high-quality and focused on logic. \nSome have promoted particular types of evidence as 'best' for policymakers to consider, including scientifically rigorous evaluation studies such as randomized controlled trials to identify programs and practices capable of improving policy-relevant outcomes. However, some areas of policy-relevant knowledge are not well served by quantitative research, leading to debate about the methods and instruments that are considered critical for the collection of relevant evidence. For instance, policies that are concerned with human rights, public acceptability, or social justice may require other evidence than what randomized trials provide, or may require moral philosophical reasoning in addition to considerations of evidence of intervention effect (which randomised trials are principally designed to provide ). Good data, analytical skills and political support to the use of scientific information, as such, are typically seen as the important elements of an evidence-based approach.\n\nAlthough evidence-based policy can be traced as far back as the fourteenth century, it was more recently popularized by the Blair Government in the United Kingdom. The Blair Government said they wanted to end the ideological led-based decision making for policy making. For example, a UK Government white paper published in 1999 (\"Modernising Government\") noted that Government must \"produce policies that really deal with problems, that are forward-looking and shaped by evidence rather than a response to short-term pressures; that tackle causes not symptoms\".\n\nEvidence-based policy is associated with Adrian Smith because in his 1996 presidential address to the Royal Statistical Society, Smith questioned the current process of policy making and urged for a more “evidence-based approach” commenting that it has “valuable lessons to offer”.\n\nSome policy scholars now avoid using the term \"evidence-based\" policy, using others such as \"evidence informed\". This language shift allows continued thinking about the underlying desire to \"improve\" evidence use in terms of its rigor or quality, while avoiding some of the key limitations or reductionist ideas at times seen with the evidence-based language. Still, the language of evidence-based policy is widely used and, as such, can be interpreted to reflect a desire for evidence to be used well or appropriately in one way or another – such as by ensuring systematic consideration of rigorous and high quality policy relevant evidence, or by avoiding biased and erroneous applications of evidence for political ends.\n\nUnlike the UK, the USA has a largely devolved government, with power at local, state and federal level. Due to these various levels of governance it can often be difficult to coordinate passing bills and legislation, and there is often disagreement. Despite this, the system allows for citizens to be relatively involved in inputting legislation. Furthermore, each level of government is set up in a similar way with similar rules, and all pump money into creating what is hoped to be effective legislation. Policy creation in America is often seen as unique to other states.\n\nAs an academic discipline, public policy brings in elements of many social science fields and concepts, including economics, sociology, political economy, social policy, program evaluation, policy analysis, and public management, all as applied to problems of governmental administration, management, and operations. At the same time, the study of public policy is distinct from political science or economics, in its focus on the application of theory to practice. While the majority of public policy degrees are master's and doctoral degrees, there are several universities that offer undergraduate education in public policy.\n\nTraditionally, the academic field of public policy focused on domestic policy. However, the wave of economic globalization that occurred in the late 20th and early 21st centuries created a need for a subset of public policy that focused on global governance, especially as it relates to issues that transcend national borders such as climate change, terrorism, nuclear proliferation, and economic development. Consequently, many traditional public policy schools had to adjust their curricula to better suit this new policy landscape, as well as develop entirely new curricula altogether.\n\n\n"}
{"id": "16685964", "url": "https://en.wikipedia.org/wiki?curid=16685964", "title": "Roman Senate", "text": "Roman Senate\n\nThe Roman Senate () was a political institution in ancient Rome. It was one of the most enduring institutions in Roman history, being established in the first days of the city of Rome (traditionally founded in 753 BC). It survived the overthrow of the kings in 509 BC, the fall of the Roman Republic in the 1st century BC, the division of the Roman Empire in 395 AD, the fall of the Western Roman Empire in 476 AD, and the barbarian rule of Rome in the 5th, 6th, and 7th centuries.\n\nDuring the days of the kingdom, most of the time the Senate was little more than an advisory council to the king, but it also elected new Roman kings. The last king of Rome, Lucius Tarquinius Superbus, was overthrown following a coup d'état led by Lucius Junius Brutus, who founded the Roman Republic. During the early Republic, the Senate was politically weak, while the various executive magistrates were quite powerful. Since the transition from monarchy to constitutional rule was most likely gradual, it took several generations before the Senate was able to assert itself over the executive magistrates. By the middle Republic, the Senate had reached the apex of its republican power. The late Republic saw a decline in the Senate's power, which began following the reforms of the tribunes Tiberius and Gaius Gracchus.\n\nAfter the transition of the Republic into the Principate, the Senate lost much of its political power as well as its prestige. Following the constitutional reforms of the Emperor Diocletian, the Senate became politically irrelevant. When the seat of government was transferred out of Rome, the Senate was reduced to a purely municipal body. This decline in status was reinforced when the emperor Constantine the Great created an additional senate in Constantinople.\n\nAfter Romulus Augustulus was deposed in 476, the Senate in the West functioned under the rule of Odovacer (476–489) and during Ostrogothic rule (489–535). It was restored after the reconquest of Italy by Justinian I, but ultimately disappeared after AD 603, the date of its last recorded public act. The title \"senator\" continued into the Middle Ages as a largely meaningless honorific. The Eastern Senate survived in Constantinople through the 14th century.\n\nThe senate was a political institution in the ancient Roman Kingdom. The word \"senate\" derives from the Latin word \"senex\", which means \"old man\"; the word thus means \"assembly of elders\". The prehistoric Indo-Europeans who settled Italy in the centuries before the legendary founding of Rome in 753 BC were structured into tribal communities, and these communities often included an aristocratic board of tribal elders.\n\nThe early Roman family was called a \"gens\" or \"clan\", and each clan was an aggregation of families under a common living male patriarch, called a \"pater\" (the Latin word for \"father\"). When the early Roman \"gentes\" were aggregating to form a common community, the \"patres\" from the leading clans were selected for the confederated board of elders that would become the Roman senate. Over time, the \"patres\" came to recognize the need for a single leader, and so they elected a king (\"rex\"), and vested in him their sovereign power. When the king died, that sovereign power naturally reverted to the \"patres\".\n\nThe senate is said to have been created by Rome's first king, Romulus, initially consisting of 100 men. The descendants of those 100 men subsequently became the patrician class. Rome's fifth king, Lucius Tarquinius Priscus, chose a further 100 senators. They were chosen from the minor leading families, and were accordingly called the \"patres minorum gentium\".\n\nRome's seventh and final king, Lucius Tarquinius Superbus, executed many of the leading men in the senate, and did not replace them, thereby diminishing their number. However, in 509 BC Rome's first and third consuls, Lucius Junius Brutus and Publius Valerius Publicola chose from amongst the leading equites new men for the senate, these being called \"conscripti\", and thus increased the size of the senate to 300.\n\nThe senate of the Roman Kingdom held three principal responsibilities: It functioned as the ultimate repository for the executive power, it served as the king's council, and it functioned as a legislative body in concert with the people of Rome. During the years of the monarchy, the senate's most important function was to elect new kings. While the king was nominally elected by the people, it was actually the senate who chose each new king.\n\nThe period between the death of one king and the election of a new king was called the \"interregnum\", during which time the Interrex nominated a candidate to replace the king. After the senate gave its initial approval to the nominee, he was then formally elected by the people, and then received the senate's final approval. At least one king, Servius Tullius, was elected by the senate alone, and not by the people.\n\nThe senate's most significant task, outside regal elections, was to function as the king's council, and while the king could ignore any advice it offered, its growing prestige helped make the advice that it offered increasingly difficult to ignore. Only the king could make new laws, although he often involved both the senate and the curiate assembly (the popular assembly) in the process.\n\nWhen the Republic began, the Senate functioned as an advisory council. It consisted of 300–500 senators, who were initially patrician and served for life. Before long, plebeians were also admitted, although they were denied the senior magistracies for a longer period.\n\nSenators were entitled to wear a toga with a broad purple stripe, maroon shoes, and an iron (later gold) ring.\n\nThe Senate of the Roman Republic passed decrees called \"senatus consulta\", which in form constituted \"advice\" from the senate to a magistrate. While these decrees did not hold legal force, they usually were obeyed in practice.\n\nIf a \"senatus consultum\" conflicted with a law (\"lex\") that was passed by an assembly, the law overrode the \"senatus consultum\" because the \"senatus consultum\" had its authority based in precedent and not in law. A \"senatus consultum\", however, could serve to interpret a law.\n\nThrough these decrees, the senate directed the magistrates, especially the Roman Consuls (the chief magistrates) in their prosecution of military conflicts. The senate also had an enormous degree of power over the civil government in Rome. This was especially the case with regard to its management of state finances, as only it could authorize the disbursal of public funds from the treasury. As the Roman Republic grew, the senate also supervised the administration of the provinces, which were governed by former consuls and praetors, in that it decided which magistrate should govern which province.\n\nSince the 3rd century BC the senate also played a pivotal role in cases of emergency. It could call for the appointment of a dictator (a right resting with each consul with or without the senate's involvement). However, after 202 BC, the office of dictator fell out of use (and was revived only two more times) and was replaced with the \"senatus consultum ultimum\" (\"ultimate decree of the senate\"), a senatorial decree which authorised the consuls to employ any means necessary to solve the crisis.\n\nWhile senate meetings could take place either inside or outside the formal boundary of the city (the \"pomerium\"), no meeting could take place more than a mile (1 km) outside it. The senate operated while under various religious restrictions. For example, before any meeting could begin, a sacrifice to the gods was made, and a search for divine omens (the \"auspices\") was taken. The senate was only allowed to assemble in places dedicated to the gods.\n\nMeetings usually began at dawn, and a magistrate who wished to summon the senate had to issue a compulsory order. The senate meetings were public and directed by a presiding magistrate (usually a consul). While in session, the senate had the power to act on its own, and even against the will of the presiding magistrate if it wished. The presiding magistrate began each meeting with a speech, then referred an issue to the senators, who would discuss it in order of seniority.\n\nSenators had several other ways in which they could influence (or frustrate) a presiding magistrate. For example, every senator was permitted to speak before a vote could be held, and since all meetings had to end by nightfall, a dedicated group or even a single senator could talk a proposal to death (a filibuster or \"diem consumere\"). When it was time to call a vote, the presiding magistrate could bring up whatever proposals he wished, and every vote was between a proposal and its negative.\n\nWith a dictator as well as a senate, the senate could veto any of the dictator's decisions. At any point before a motion passed, the proposed motion could be vetoed, usually by a tribune. If there were no veto, and the matter were of minor importance, it could be put to either a voice vote or a show of hands. If there were no veto and no obvious majority, and the matter were of a significant nature, there was usually a physical division of the house, with senators voting by taking a place on either side of the chamber.\n\nSenate membership was controlled by the censors. By the time of Augustus, ownership of property worth at least one million sesterces was required for membership. The ethical requirements of senators were significant. In contrast to members of the Equestrian order, senators could not engage in banking or any form of public contract. They could not own a ship that was large enough to participate in foreign commerce, they could not leave Italy without permission from the rest of the senate and they were not paid a salary. Election to magisterial office resulted in automatic senate membership.\n\nAfter the fall of the Roman Republic, the constitutional balance of power shifted from the Roman senate to the Roman Emperor. Though retaining its legal position as under the republic, in practice, however, the actual authority of the imperial senate was negligible, as the emperor held the true power in the state. As such, membership in the senate became sought after by individuals seeking prestige and social standing, rather than actual authority.\n\nDuring the reigns of the first emperors, legislative, judicial, and electoral powers were all transferred from the Roman assemblies to the senate. However, since the emperor held control over the senate, the senate acted as a vehicle through which he exercised his autocratic powers.\nThe first emperor, Augustus, reduced the size of the senate from 900 members to 600, even though there were only about 100 to 200 active senators at one time. After this point, the size of the senate was never again drastically altered. Under the empire, as was the case during the late republic, one could become a senator by being elected \"quaestor\" (a magistrate with financial duties), but only if one were already of senatorial rank. In addition to quaestors, elected officials holding a range of senior positions were routinely granted senatorial rank by virtue of the offices that they held.\n\nIf an individual were not of senatorial rank, there were two ways for him to become a senator. Under the first method, the emperor manually granted that individual the authority to stand for election to the quaestorship, while under the second method, the emperor appointed that individual to the senate by issuing a decree. Under the empire, the power that the emperor held over the senate was absolute.\n\nThe two consuls were a part of the senate, but had more power than the senators. During senate meetings, the emperor sat between the two consuls, and usually acted as the presiding officer. Senators of the early empire could ask extraneous questions or request that a certain action be taken by the senate. Higher ranking senators spoke before those of lower rank, although the emperor could speak at any time.\n\nBesides the emperor, consuls and praetors could also preside over the senate. Since no senator could stand for election to a magisterial office without the emperor's approval, senators usually did not vote against bills that had been presented by the emperor. If a senator disapproved of a bill, he usually showed his disapproval by not attending the senate meeting on the day that the bill were to be voted on.\n\nWhile the Roman assemblies continued to meet after the founding of the empire, their powers were all transferred to the senate, and so senatorial decrees (\"senatus consulta\") acquired the full force of law. The legislative powers of the imperial senate were principally of a financial and an administrative nature, although the senate did retain a range of powers over the provinces.\n\nDuring the early Roman Empire, all judicial powers that had been held by the Roman assemblies were also transferred to the senate. For example, the senate now held jurisdiction over criminal trials. In these cases, a consul presided, the senators constituted the jury, and the verdict was handed down in the form of a decree (\"senatus consultum\"), and, while a verdict could not be appealed, the emperor could pardon a convicted individual through a veto. The emperor Tiberius transferred all electoral powers from the assemblies to the senate, and, while theoretically the senate elected new magistrates, the approval of the emperor was always needed before an election could be finalized.\n\nAround 300 AD, the emperor Diocletian enacted a series of constitutional reforms. In one such reform, he asserted the right of the emperor to take power without the theoretical consent of the senate, thus depriving the senate of its status as the ultimate depository of supreme power. Diocletian's reforms also ended whatever illusion had remained that the senate had independent legislative, judicial, or electoral powers. The senate did, however, retain its legislative powers over public games in Rome, and over the senatorial order.\n\nThe senate also retained the power to try treason cases, and to elect some magistrates, but only with the permission of the emperor. In the final years of the western empire, the senate would sometimes try to appoint their own emperor, such as in the case of Eugenius, who was later defeated by forces loyal to Theodosius I. The senate remained the last stronghold of the traditional Roman religion in the face of the spreading Christianity, and several times attempted to facilitate the return of the Altar of Victory (first removed by Constantius II) to the senatorial curia.\n\nAccording to the \"Historia Augusta\" (\"Elagabalus\" 4.2 and 12.3) emperor Elagabalus had his mother or grandmother take part in Senate proceedings. \"And Elagabalus was the only one of all the emperors under whom a woman attended the senate like a man, just as though she belonged to the senatorial order\" (David Magie's translation). According to the same work, Elagabalus also established a women's senate called the \"senaculum\", which enacted rules to be applied to matrons, regarding clothing, chariot riding, the wearing of jewelry etc. (\"Elagabalus\" 4.3 and \"Aurelian\" 49.6). Before this, Agrippina the Younger, mother of Nero, had been \"listening\" to Senate proceedings, concealed behind a curtain, according to Tacitus (\"Annales\", 13.5).\n\nAfter the fall of the Western Roman Empire, the senate continued to function under the Germanic chieftain Odoacer, and then under Ostrogothic rule. The authority of the senate rose considerably under barbarian leaders, who sought to protect the institution. This period was characterized by the rise of prominent Roman senatorial families, such as the Anicii, while the senate's leader, the princeps senatus, often served as the right hand of the barbarian leader. It is known that the senate successfully installed Laurentius as pope in 498, despite the fact that both King Theodoric and Emperor Anastasius supported the other candidate, Symmachus.\n\nThe peaceful coexistence of senatorial and barbarian rule continued until the Ostrogothic leader Theodahad found himself at war with Emperor Justinian I and took the senators as hostages. Several senators were executed in 552 as revenge for the death of the Ostrogothic king, Totila. After Rome was recaptured by the imperial (Byzantine) army, the senate was restored, but the institution (like classical Rome itself) had been mortally weakened by the long war. Many senators had been killed and many of those who had fled to the east chose to remain there, thanks to favorable legislation passed by Emperor Justinian, who, however, abolished virtually all senatorial offices in Italy. The importance of the Roman senate thus declined rapidly.\n\nIn 578 and again in 580, the senate sent envoys to Constantinople. They delivered 3000 pounds (=1,360 kilograms) of gold as a gift to the new emperor, Tiberius II Constantinus, along with a plea for help against the Lombards, who had invaded Italy ten years earlier. Pope Gregory I, in a sermon from 593, lamented the almost complete disappearance of the senatorial order and the decline of the prestigious institution.\n\nIt is not clearly known when the Roman senate disappeared in the West, but it is known from the Gregorian register that the senate acclaimed new statues of Emperor Phocas and Empress Leontia in 603, and that was also the last time the senate was mentioned. In 630, the house of the Senate, Curia Julia, was transformed into a church by Pope Honorius I, probably with the permission of the Emperor Heraclius.\n\nIn later medieval times, the title \"senator\" was still in occasional use, but it had become a meaningless adjunct title of nobility and no longer implied membership in an organized governing body.\nIn 1144, the Commune of Rome attempted to establish a government modeled on the old Roman Republic in opposition to the temporal power of the higher nobles and the pope. This included setting up a senate along the lines of the ancient one. The revolutionaries divided Rome into fourteen \"regions\", each electing four senators for a total of 56 (although one source, often repeated, gives a total of 50). These senators elected as their leader Giordano Pierleoni, son of the Roman consul Pier Leoni, with the title patrician, since \"consul\" was also a deprecated noble styling.\n\nThis renovated form of government was constantly embattled. By the end of the 12th century, it had undergone a radical transformation, with the reduction of the number of senators to a single one – \"Summus Senator\" – being thereafter the title of the head of the civil government of Rome. In modern terms, for example, this is comparable to the reduction of a board of commissioners to a single commissioner, such as the political head of the police department of New York City. Between 1191 and 1193, this was a certain Benedetto called \"Carus homo\" or \"carissimo\".\n\nThe senate continued to exist in Constantinople however, although it evolved into an institution that differed in some fundamental forms from its predecessor. Designated in Greek as \"synkletos\", or assembly, the Senate of Constantinople was made up of all current or former holders of senior ranks and official positions, plus their descendants. At its height during the 6th and 7th centuries, the Senate represented the collective wealth and power of the Empire, on occasion nominating and dominating individual emperors.\nIn the second half of the 10th century a new office, \"proëdrus\" (), was created as head of the senate by Emperor Nicephorus Phocas. Up to the mid-11th century, only eunuchs could become proëdrus, but later this restriction was lifted and several proëdri could be appointed, of which the senior proëdrus, or \"protoproëdrus\" (), served as the head of the senate. There were two types of meetings practised: \"silentium\", in which only magistrates currently in office participated and \"conventus\", in which all syncletics (, senators) could participate. The Senate in Constantinople existed until at least the beginning of the 13th century, its last known act being the election of Nicolas Canabus as emperor in 1204 during the Fourth Crusade.\n\n\n\n"}
{"id": "298608", "url": "https://en.wikipedia.org/wiki?curid=298608", "title": "State of emergency", "text": "State of emergency\n\nA state of emergency is a situation in which a government is empowered to perform actions or impose policies that it would normally not be permitted to undertake. A government can declare such a state during a disaster, civil unrest, or armed conflict. Such declarations alert citizens to change their normal behavior and orders government agencies to implement emergency plans. \"Justitium\" is its equivalent in Roman law—a concept in which the senate could put forward a final decree (senatus consultum ultimum) that was not subject to dispute.\n\nStates of emergency can also be used as a rationale or pretext for suspending rights and freedoms guaranteed under a country's constitution or basic law. The procedure for and legality of doing so vary by country.\n\nUnder international law, rights and freedoms may be suspended during a state of emergency; for example, a government can detain persons and hold them without trial. All rights that can be derogated from are listed in the International Covenant for Civil and Political Rights. Non-derogable rights cannot be suspended. Non-derogable rights are listed in Article 4 of the ICCPR; they include right to life, the rights to freedom from arbitrary deprivation of liberty, slavery, torture, and ill-treatment.\n\nSome countries have made it illegal to modify emergency law or the constitution during the emergency; other countries have the freedom to change any legislation or rights based constitutional frameworks at any time that the legislative chooses to do so. Constitutions are contracts between the government and the private individuals of that country. The International Covenant for Civil and Political Rights (ICCPR) is an international law document signed and ratified by states. Therefore, the Covenant applies to only those persons acting in an official capacity, not private individuals. However, States Parties to the Covenant are expected to integrate it into national legislation. The state of emergency (within the ICCPR framework) must be publicly declared and the Secretary-General of the United Nations and all other States Parties to the Covenant must be notified immediately, to declare the reason for the emergency, the date on which the emergency is to start, the derogations that may take place, with the timeframe of the emergency and the date in which the emergency is expected to finish. Although this is common protocol stipulated by the ICCPR, its monitoring Committee of experts has no sanction power and its recommendations are therefore not always strictly followed; enforcement is therefore better regulated by the American and European Conventions and Courts on human rights.\n\nThough fairly uncommon in democracies, dictatorial regimes often declare a state of emergency that is prolonged indefinitely for the life of the regime, or for extended periods of time so that derogations can be used to override human rights of their citizens usually protected by the International Covenant on Civil and political rights. In some situations, martial law is also declared, allowing the military greater authority to act. In other situations, emergency is not declared and de facto measures taken or decree-law adopted by the government. Ms. Nicole Questiaux (France) and Mr. Leandro Despouy (Argentina), two consecutive United Nations Special Rapporteurs, have recommended to the international community to adopt the following \"principles\" to be observed during a state or de facto situation of emergency : Principles of Legality, Proclamation, Notification, Time Limitation, Exceptional Threat, Proportionality, Non-Discrimination, Compatibility, Concordance and Complementarity of the Various Norms of International Law (cf. \"Question of Human Rights and State of Emergency\", E/CN.4/Sub.2/1997/19, at Chapter II; see also \"\").\n\nArticle 4 to the International Covenant on Civil and Political Rights (ICCPR), permits states to derogate from certain rights guaranteed by the ICCPR in \"time of public emergency\". Any measures derogating from obligations under the Covenant, however, must be to only the extent required by the exigencies of the situation, and must be announced by the State Party to the Secretary-General of the United Nations. The European Convention on Human Rights and American Convention on Human Rights have similar derogatory provisions. No derogation is permitted to the International Labour Conventions.\n\nSome political theorists, such as Carl Schmitt, have argued that the power to decide the initiation of the state of emergency defines sovereignty itself. In \"State of Exception\" (2005), Giorgio Agamben criticized this idea, arguing that the mechanism of the state of emergency deprives certain people of their civil and political rights, producing his interpretation of \"homo sacer\".\n\nIn many democratic states there are a selection of legal definitions for specific states of emergency, when the constitution of the State is partially in abeyance depending on the nature of the perceived threat to the general public. In order of severity these may include:\n\nSometimes, the state of emergency can be abused by being invoked. An example would be to allow a state to suppress internal opposition without having to respect human rights. An example was the August 1991 attempted coup in the Soviet Union (USSR) where the coup leaders invoked a state of emergency; the failure of the coup led to the dissolution of the Soviet Union.\n\nDerogations by states having ratified or acceded to binding international agreements such as the ICCPR, the American and European Conventions on Human Rights and the International Labour Conventions are monitored by independent expert committees, regional Courts and other State Parties.\n\nThe Constitution, which has been amended several times, has always allowed for a state of emergency (literally \"estado de sitio\", \"state of siege\"), to be declared if the constitution or the authorities it creates are endangered by internal unrest or foreign attack. This provision was much abused during dictatorships, with long-lasting states of siege giving the government a free hand to suppress opposition ( a state of emergency had been declared 52 times by democratic and dictatorial governments, starting in 1854 shortly after the constitution came into force). The American Convention on Human Rights (Pacto de San José de Costa Rica), adopted in 1969 but ratified by Argentina only in 1984 immediately after the end of the National Reorganization Process, restricts abuse of the state of emergency by requiring any signatory nation declaring such a state to inform the other signatories of its circumstances and duration, and what rights are affected.\n\nState-of-emergency legislation differs in each state of Australia.\n\nIn Victoria, the premier can declare a state of emergency if there is a threat to employment, safety or public order. The declaration expires after 30 days, and a resolution of either the upper or lower House of Parliament may revoke it earlier. Under the Public Safety Preservation Act, a declared state of emergency allows the premier to immediately make any desired regulations to secure public order and safety. However, these regulations expire if Parliament does not agree to continue them within 7 days. Also, under the Essential Services Act, the premier (or delegate) may operate or prohibit operation of, as desired, any essential service (e.g., transport, fuel, power, water, gas).\n\nIn regards to Emergency Management, regions (usually on a local government area basis) that have been affected by a natural disaster are the responsibility of the state, until that state declares a State of Emergency where access to the Federal Emergency Fund becomes available to help respond to and recover from natural disasters. A State of Emergency does not apply to the whole state, but rather districts or shires, where essential services may have been disrupted.\n\nSee also, Exceptional circumstances; a term most commonly used in Australia with regard to emergency relief payments.\n\nA state of emergency was declared in New South Wales in December 2019 in response to the 2019 bushfires. At that time 7.8m acres of NSW bush land had been lost, resulting in over 600 homes lost and 8 fatalities including those of two volunteer firefighters who died after their tanker was hit by a falling tree while en route to the Gospers mountain fire. As of the 27th of December 2019, 827 homes have been lost, 8 people have lost their lives and 9,600,000 acres (3,900,000 hectares) have been lost with unmeasurable consequence for Australia's flora and fauna.\n\nExtreme act that, in Brazil (\"Estado de Sítio\" or \"Estado de Exceção\", in Portuguese), can be declared on the following circumstances:\n\nThe state of emergency could last for 30 days, being possible to extend it for more days in case of persistence of the reasons of exceptionality.\n\nOnly the President is able to declare or prorogate this State; after receiving formal authorization from National Congress and after consultation with the National Security Council or the Council of the Republic.\n\nThe federal government of Canada can use the Emergencies Act to invoke a state of emergency. A national state of emergency automatically expires after 90 days, unless extended by the Governor-in-Council. There are different levels of emergencies: Public Welfare Emergency, Public Order Emergency, International Emergency, and War Emergency.\n\nThe Emergencies Act replaced the War Measures Act in 1988. The War Measures Act was invoked three times in Canadian history, most controversially during the 1970 October Crisis, and also during World War I (from 1914 to 1920, against threat of Communism) and World War II (from 1942 to 1945, against perceived threat from Japanese Canadians following Imperial Japan's attack on Pearl Harbor).\n\nUnder the current Emergency Act a state of emergency can also be declared by provincial, territorial, and municipal governments. In addition Canada's federal government and any of its provincial governments can suspend, for five years at a time, Charter rights to fundamental freedoms in section 2, to legal rights in sections 7 through 14, and to equality rights in section 15 by legislation which invokes the notwithstanding clause, section 33, and therefore emergency powers can effectively be created even without using the Emergency Act.\n\nThe police chief in a district can impose a zone in which people can be body searched without a specific suspicion. Such an order must be issued in writing, published, and imposed for a limited period. The police law (article 6) regulates this area. The normal procedure calls for assisting the suspect to a private area and stripping them. The police can also impose a zone in where specific crimes such as violence, threats, blackmailing and vandalism can be punished with a double penalty length. The zone can only be imposed if there is an extraordinary crime development and the zone can only last up to three months unless the extraordinary crime development still applies.\n\nIf the police feel that a situation involving a crowd of people can get out of hand, they can order the assembly to be dissolved and \"pass the street\" in the name of the king. People that after three such warnings are still part of the crowd can then without further warning be subjugated to mass arrest. All people arrested can then be detained for 24 hours without charging them or taking them for a judge. This is called a precluding arrest.\n\nEgyptians lived under an Emergency Law (Law No. 162 of 1958) from 1967 to 2012, except for an 18-month break in 1980 and 1981. The emergency was imposed during the 1967 Arab-Israeli War, and reimposed following the assassination of President Anwar Sadat. The law continuously extended every three years since 1981. Under the law, police powers were extended, constitutional rights suspended and censorship was legalized. The law sharply circumscribed any non-governmental political activity: street demonstrations, non-approved political organizations, and unregistered financial donations were formally banned. Some 17,000 people were detained under the law, and estimates of political prisoners run as high as 30,000. The emergency rule expired on May 31, 2012, and was put back in place in January 2013. Egypt declared a month-long national emergency on 14 August 2013.\n\nThe Egyptian presidency announced a one-month state of emergency across the country on August 14, 2013 and ordered the armed forces to help the Interior Ministry enforce security. The announcement made on state TV followed deadly countrywide clashes between supporters of deposed President Mohammed Morsi and the security forces.\n\nThree main provisions concern various kind of \"state of emergency\" in France: Article 16 of the Constitution of 1958 allows, in time of crisis, \"extraordinary powers\" to the president. Article 36 of the same constitution regulates \"state of siege\" (\"\"). Finally, the Act of 3 April 1955 allows the proclamation, by the Council of Ministers, of the \"state of emergency\" (\"\"). The distinction between article 16 and the 1955 Act concerns mainly the distribution of powers: whereas in article 16, the executive power basically suspend the regular procedures of the Republic, the 1955 Act permits a twelve-day state of emergency, after which a new law extending the emergency must be voted by the Parliament. These dispositions have been used at various times, in 1955, 1958, 1961, 1988, 2005, and 2015.\n\nThe Weimar Republic constitution (1919–1933) allowed states of emergency under Article 48 to deal with rebellions. Article 48 was often invoked during the 14-year life of the Republic, sometimes for no reason other than to allow the government to act when it was unable to obtain a parliamentary majority.\n\nAfter the February 27, 1933, Reichstag fire, an attack blamed on the communists, Adolf Hitler declared a state of emergency using Article 48, and then had President von Hindenburg sign the Reichstag Fire Decree, which suspended some of the basic civil liberties provided by the Weimar Constitution (such as habeas corpus, freedom of expression, freedom of the speech, the freedom to assemble or the privacy of communications) for the whole duration of the Third Reich. On March 23, the Reichstag enacted the Enabling Act of 1933 with the required two-thirds majority, which enabled Chancellor Adolf Hitler and his cabinet to enact laws without the participation of the legislative. (The Weimar Constitution was never actually repealed by Nazi Germany, but it effectively became ineffective after the passage of the Enabling Act.) These two laws implemented the \"Gleichschaltung\", the Nazis' institution of totalitarianism.\n\nIn the postwar Federal Republic of Germany the \"Emergency Acts\" state that some of the basic constitutional rights of the Basic Law may be limited in case of a state of defence, a state of tension, or an internal state of emergency or disaster (catastrophe). These amendments to the constitution were passed on May 30, 1968, despite fierce opposition by the so-called \"extra-parliamentary opposition\" (see German student movement for details).\n\nDuring the state of war, or turmoil which threatens national security or unity, and the Standing Committee of the National People's Congress believes is beyond the control of the local government, it can invoke Article 18 of the Hong Kong Basic Law and declare a \"State of Emergency\" in Hong Kong, thus the Central People's Government can selectively implement national laws not normally allowed in Hong Kong. Deployment of troops from the People's Liberation Army Hong Kong Garrison under the \"Law of the People's Republic of China on Garrisoning the Hong Kong Special Administrative Region\" can happen.\n\nThe Chief Executive of Hong Kong along with the Executive Council can prohibit public gatherings, issue curfew orders, prohibit the movement of vessels or aircraft, delegate authority, and other listed powers, under \"Cap. 245 Public Order Ordinance\".\n\nAlthough the People's Liberation Army Hong Kong Garrison may not interfere in internal Hong Kong affairs, however, the Hong Kong Special Administrative Region Government may invoke Article 14 of the Hong Kong Basic Law and request the Central People's Government permission to have the garrison assist in \"maintenance of public order or disaster relief\".\n\nSince 1997, State of Emergency have never been declared. However, emergency measures have been used in varying degrees over the years during British rule and after the establishment of the Special Administrative Region. A few notable mentions are as follow:\n\n\nOn 4 October 2019, Carrie Lam, the Chief Executive of Hong Kong S.A.R., invoked Section 2(1) within \"Cap. 241 Emergency Regulations Ordinance\" implemented since 1922 and last amended by the Legislative Council in 1999, which allow the government to implement the new, \"Cap. 241K Prohibition on Face Covering Regulation\". The new regulation forbid public assembly participants from wearing masks or obscure faces during such events without reasonable excuses. The permitted excuses are: pre-existing medical or health reasons, religious reasons, and if the person uses the face covering for physical safety while performing an activity connected with their profession or employment. Any person defying the new regulation face possible criminal prosecution. The government's motive in doing so is to end months of social unrest and riots, however, did not declare a \"State of Emergency\". The new regulation took effect at 00:00 HKT on 5 October 2019. Offenders risked a maximum of one-year imprisonment or a fine of HK$25,000 (US$3,200).\n\nThe High Court of Hong Kong denied an application for a judicial injunction of the anti-mask law, on the same night shortly before the new regulation took effect. A subsequent attempt by pro-democrats to halt the new regulation also failed, however, the court recommended a judicial review at a later date.\n\nOn 18 November 2019, the High Court ruled the \"Cap. 241 Emergency Regulations Ordinance\" is \"incompatible with the Basic Law\", however, the court \"leaves open the question of the constitutionality of the ERO insofar as it relates to any occasion of emergency.\" The court also held the ordinance meets the \"prescribed by law\" requirement. However, the court deemed s3(1)(b), (c), (d) and s5 of the regulation do not meet the proportionality test as they impose restrictions on fundamental rights that goes beyond what is necessary in furthering its intended goals.\n\nOn 22 November 2019, the High Court made the following remark:\n\n\"Nevertheless, we recognise that our Judgment is only a judgment at first instance, and will soon be subject to an appeal to the Court of Appeal. In view of the great public importance of the issues raised in this case, and the highly exceptional circumstances that Hong Kong is currently facing, we consider it right that we should grant a short interim suspension order so that the respondents may have an opportunity to apply to the Court of Appeal, if so advised, for such interim relief as may be appropriate. Accordingly, we shall grant an interim temporary suspension order to postpone the coming into operation of the declarations of invalidity for a period of 7 days up to the end of 29 November 2019, with liberty to apply.\"\n\nOn 26 November 2019, the High Court announced hearing for the government appeal against the judgement is on 9 January 2020.\n\nOn 27 November 2019, the Court of Appeal extended the interim suspension of the judgment until 10 December 2019.\n\nOn 10 December 2019, the Court of Appeal refused to suspend the \"unconstitutional\" ruling by the Court of First Instance on the anti-mask regulation. As scheduled, a full hearing will commence on 9 January 2020.\n\nAccording to the Hungarian Constitution, the National Assembly of Hungary can declare state of emergency in case of armed rebellion or natural or industrial disaster. It expires after 30 days, but can be extended. Most civil rights can be suspended, but basic human rights (such as the right to life, the ban of torture, and freedom of religion) cannot.\n\nDuring state of emergency, the Parliament cannot be disbanded.\n\nThe Icelandic constitution provides no mechanism for the declaration of war, martial law nor state of emergency.\n\nThe State of Emergency can be proclaimed by the President of India, when he/she perceives grave threats to the nation, albeit through the advice of the cabinet of ministers. Part XVIII of the Constitution of India gives the President the power to overrule many provisions, including the ones guaranteeing fundamental rights to the citizens of India\n\nIn India, a state of emergency was declared twice:\n\n\nThe first internal Emergency was declared by the president, Fakhruddin Ali Ahmed on advice of the then Prime Minister, Indira Gandhi The provisions of the Constitution allows the Prime Minister to rule by decree.\n\nIn Ireland declaring a state of \"national emergency\" involves Article 28.3.3° of the 1937 Constitution of Ireland, which states that:\nIn addition, during a \"war or armed rebellion\", military tribunals may try civilians, and the Defence Forces are not bound by habeas corpus.\n\nThe First Amendment of the Constitution of 1939 allows an emergency to be declared during wars in which the state is a non-belligerent, subject to resolutions by the houses of the Oireachtas. By the 2nd Amendment of 1941, an emergency ends, not automatically when the war does, but only by Oireachtas resolutions. The 21st Amendment of 2002 prevents the reintroduction of capital punishment during an emergency.\n\nThe first amendment was rushed through the Oireachtas after the outbreak of the Second World War, in which the state remained neutral. Immediately after, the required resolution was passed, in turn enabling the passage of the Emergency Powers Act 1939 (EPA), which granted the government and its ministers sweeping powers to issue statutory orders termed \"Emergency Powers Orders\" (EPOs). (The period in Ireland was and is referred to as \"The Emergency\".) The EPA expired in 1946, although some EPOs were continued under the Supplies and Services (Temporary Provisions) Act 1946 until as late as 1957. Rationing continued until 1951.\n\nThe 1939 state of emergency was not formally ended until a 1976 resolution, which also declared a new state of emergency in relation to the Troubles in Northern Ireland and in particular the recent assassination of the British ambassador to Ireland, Christopher Ewart Biggs. The Emergency Powers Act 1976 was then passed to increase the Garda Síochána powers to arrest, detain, and question those suspected of offences against the state. President Cearbhall Ó Dálaigh referred the bill under Article 26 of the Constitution to the Supreme Court, which upheld its constitutionality. The referral was condemned by minister Paddy Donegan as a \"thundering disgrace\", causing Ó Dálaigh to resign in protest. The 1976 EPA expired after one year, but the state of emergency persisted until 1995, when as part of the Northern Ireland peace process it was rescinded as a \"confidence building measure\" to satisfy physical force republicans after the Provisional IRA's 1994 ceasefire.\n\nThe Offences against the State Act does not require a state of emergency under Article 28.3.3°. Part V of the Act, which provides for a non-jury Special Criminal Court (SCC), is permitted under Article 38.3.1°. Part V is activated by a declaration from the government that it is \"necessary to secure the preservation of public peace and order\", and it can be rescinded by vote of Dáil Éireann. Provision for internment is similarly activated and rescinded (originally by Part VI of the 1939 act, later by Part II of a 1940 amending act). Parts V and VI were both activated during the Second World War and the IRA's late 1950s Border Campaign; Part V has been continually active since 1972.\n\nSeveral official reviews of the Constitution and the Offences Against the State Acts have recommended a time limit within which the operation of Article 28.3.3° or Article 38.3.1° must either be explicitly renewed by resolution or else lapse.\n\nIsrael's Emergency Defence Regulations are older than the state itself, having been passed under the British Mandate for Palestine in 1945. A repeal was briefly considered in 1967 but cancelled following the Six-Day War. The regulations allow Israel, through its military, to control movements and prosecute suspected terrorists in occupied territories, and to censor publications that are deemed prejudicial to national defense.\n\nThe Standing Committee of the National People's Congress can declare a state of emergency and deploy troops from the People's Liberation Army Macau Garrison under the Article 14 of Macau's Basic Law on the defence of the Macau Special Administrative Region.\n\nThe Chief Executive of Macau can use the Macau national security law to prohibit public gatherings, issue curfew orders, prohibit other activities perceived to be a threat against the Region or China.\n\nSince 1999 no emergency measure have been enacted. Prior to 1999 emergency measures have been used for 1 major incident:\n\nIn Malaysia, if the Yang di-Pertuan Agong (Monarch) is satisfied that a grave emergency exists whereby the security, or the economic life, or public order in the Federation or any part thereof is threatened, he may issue a Proclamation of Emergency making therein a declaration to that effect.\n\nIn the history of Malaysia, a state of emergency was declared by the then-colonial government of Britain. The state of emergency lasted from 1948 until 1960 to deal with the communists led by Chin Peng.\n\nStates of emergency were also declared during the \"Konfrontasi\" in 1962, the 1966 Sarawak constitutional crisis and 1977 Kelantan Emergency.\n\nWhen a race riot broke out on May 13, 1969, a state of emergency was declared.\n\nOn August 11, 2005 a state of emergency was announced for the world's 13th largest port, Port Klang and the district of Kuala Selangor after air pollution there reached dangerous levels (defined as a value greater than 500 on the Air Pollution Index or API).\n\nThiery Rommel, the European Commission's envoy to Malaysia, told Reuters by telephone on November 13, 2007 (the last day of his mission) that, \"Today, this country still lives under (a state of) emergency.\" Although not officially proclaimed as a state of emergency, the Emergency Ordinance and the Internal Security Act had allowed detention for years without trial.\n\nOn June 23, 2013 a state of emergency was declared by Prime Minister Najib Abdul Razak for Muar and Ledang, Johor as smoke from land-clearing fires in Indonesia pushed air pollution index to above 750. This was the first time in years that air quality had dipped to a hazardous level with conditions worsening as dry weather persisted and fires raged in Sumatra.\n\nA state of emergency was declared on December 26, 2004, following the 2004 Indian Ocean Earthquake and Tsunami. The resulting tsunamis caused extensive damage to the country's infrastructure, cutting off communications from large swathes of the nation, decimating islands and forcing the closure of a number of resorts due to the damage.\n\nOn February 5, 2018, a state of emergency was declared by Maldives's President Abdulla Yameen for 15 days and ordered security forces into the supreme court and arrested a former president Maumoon Abdul Gayoom and the Chief Justice of Honorable Supreme court of Maldives.\n\nNamibia declared last a State of Emergency due to an ongoing drought in 2016.\n\nThe Civil Defence Emergency Management Act 2002 gives the government and local city council the power to issue a state of emergency, either over the entire country or within a specific region. This may suspend ordinary work and essential services if need be. The state of emergency in New Zealand expires on the commencement of the seventh day after the date on which it was declared, unless it is extended. However, the minister of civil defence or local mayor may lift the state of emergency after an initial review of the region's status.\n\nIn Nigeria, a state of emergency is usually declared in times of great civil unrest. In recent years, it has specifically been implemented in reaction to terrorist attacks on Nigerians by the Islamic jihadist group Boko Haram.\n\nOn 14 May 2013, Goodluck Jonathan declared a state of emergency for the entire northeastern states of Borno, Yobe and Adamawa. A more limited state of emergency had been declared on 31 December 2011 in parts of Yobe, Borno, Plateau and Niger states. This earlier declaration included the temporary shutdown of the international borders in those regions.\n\nIn Pakistan, a state of emergency was declared five times in its history:\n\nThe first three were regarded as the imposition of direct martial law.\n\nIn Romania, there are two types of states of emergency, each designed for a different type of situation.\n\nThe most well-known event in which the state of emergency has been enforced was because of 1977 Vrancea earthquake.\n\nThe last instance in which the \"special zone of public safety\" was enforced was in December 8, 2013-ongoing, in Pungești, Vaslui following civil unrest in Pungești from Chevron's plans to begin exploring shale-gas in the village. According to police officials, the special security zone will be maintained as long as there is conflict in the area that poses a threat to Chevron's operations. This special security zone has faced domestic and international criticism for alleged human-rights abuses.\n\nSierra Leone declared, on 7 February 2019, a State of Emergency due to ongoing rape and sexual violence in the country.\n\nStates of emergency in South Africa are governed by section 37 of the Constitution and by the State of Emergency Act, 1997. The President may declare a state of emergency only when \"the life of the nation is threatened by war, invasion, general insurrection, disorder, natural disaster or other public emergency\" and if the ordinary laws and government powers are not sufficient to restore peace and order. The declaration is made by proclamation in the \"Government Gazette\" and may only apply from the time of publication, not retroactively. It can only continue for 21 days unless the National Assembly grants an extension, which may be for at most three months at a time. The High Courts have the power, subject to confirmation by the Constitutional Court, to determine the validity of the declaration of a state of emergency.\n\nDuring a state of emergency the President has the power to make emergency regulations \"necessary or expedient\" to restore peace and order and end the emergency. This power can be delegated to other authorities. Emergency measures can violate the Bill of Rights, but only to a limited extent. Some rights are inviolable, including amongst others the rights to life and to human dignity; the prohibition of discrimination on the grounds of race, sex or religion; the prohibition of torture or inhuman punishment; and the right of accused people to a fair trial. Any violation of a constitutional right must be strictly required by the emergency. Emergency measures may not indemnify the government or individuals for illegal actions. They may impose criminal penalties, but not exceeding three years' imprisonment. They may not require military service beyond that required by the ordinary laws governing the defence force. An emergency measure may be disapproved by the National Assembly, in which case it lapses, and no emergency measure may interfere with the elections, powers or sittings of Parliament or the provincial legislatures. The courts have the power to determine the validity of any emergency measure.\n\nThe constitution places strict limits on any detention without trial during a state of emergency. A friend or family member of the detainee must be informed, and the name and place of detention must be published in the \"Government Gazette\". The detainee must have access to a doctor and a legal representative. He or she must be brought before a court within at most ten days, for the court to determine whether the detention is necessary, and if not released may demand repeated review every ten days. At the court review the detainee must be allowed legal representation and must be allowed to appear in person. The provisions on detention without trial do not apply to prisoners of war in an international conflict; instead they must be treated in accordance with the Geneva Conventions and other international law.\n\nIn Spain, there are three degrees of state of emergency (\"estado de emergencia\" in Spanish): \"alarma\" (alarm or alert), \"excepción\" (exception[al circumstance]) and \"sitio\" (siege). They are named by the constitution, which limits which rights may be suspended, but regulated by the \"Ley Orgánica 4/1981\" (Organic Law).\n\nOn December 4, 2010, the first state of alert was declared following the air traffic controllers strike. It was the first time since the Francisco Franco's regime that a state of emergency was declared.\n\nIn Sri Lanka, the President is able to proclaim emergency regulations under the \"Public Security Ordinance\" in the constitution in order to preserve public security and public order; suppression of mutiny, riot or civil commotion; or maintenance of supplies and services essential to the life of the community. These regulations last for one month unless confirmed otherwise by Parliament.\n\nAccording to Art. 185 of the Swiss Federal Constitution The Federal Council (Bundesrat) can call up in their own competence military personnel of maximum 4000 militia for three weeks to safeguard inner or outer security (called Federal Intervention or Federal Execution, respectively). A larger number of soldiers or of a longer duration is subject to parliamentary decision. For deployments within Switzerland the principle of subsidiarity rules: as a first step, unrest has to be overcome with the aid of cantonal police units.\n\nAn emergency prevailed in Syria from 1962 to 2011. Originally predicated on the conflict with Israel, the emergency acted to centralize authority in the presidency and the national security apparatus while silencing public dissent. The emergency was terminated in response to protests that preceded the Syrian Civil War. Under the 2012 constitution, the president may pass an emergency decree with a 2/3 concurrence of his ministers, provided that he presents it to the legislature for constitutional review.\n\nA state of emergency was declared in 1970 during the Black Power Revolution by then Prime Minister Eric Williams. During the attempted state coup by the Jamaat al Muslimeen against the NAR government of the then Prime Minister A. N. R. Robinson, a state of emergency was declared during the coup attempt and for a period after the coup.\n\nOn August 4, 1995, a state of emergency was declared to remove the Speaker of the House Occah Seepaul by Prime Minister Patrick Manning during a constitutional crisis. The government had attempted to remove the speaker via a no-confidence motion, which failed. The state of emergency was used to remove the speaker using the emergency powers granted.\n\nThe Prime Minister Kamla Persad-Bissessar announced a state of emergency on 22 August 2011 at 8:00 pm in an attempt to crack down on the trafficking of illegal drugs and firearms, in addition to gangs. The decision of the President, George Maxwell Richards, to issue the proclamation for the state of emergency was debated in the country's Parliament as required by the Constitution on September 2, 2011 and passed by the required simple majority of the House of Representatives. On September 4 the Parliament extended the state of emergency for a further 3 months. It ended in December 2011.\n\nSince the foundation of the Republic of Turkey in 1923 the military conducted three \"coups d'état\" and announced martial law. Martial law between 1978 and 1983 was replaced by a state of emergency that lasted until November 2002.\nThe latest state of emergency was declared by President Erdoğan on 20 July 2016 following a failed coup attempt on 15 July 2016 by a faction of the country's armed forces. It was lifted in 2018.\n\nIn the United Kingdom, only the British Sovereign, on the advice of the Privy Council, or a Minister of the Crown in exceptional circumstances, has the power to introduce emergency regulations under the Civil Contingencies Act 2004, in case of an emergency, broadly defined as war or attack by a foreign power, terrorism which poses a threat of serious damage to the security of the UK, or events which threaten serious damage to human welfare or the environment of a place in the UK. The duration of these regulations is limited to thirty days, but may be extended by Parliament.\nA state of emergency was last invoked in 1974 by Prime Minister Edward Heath in response to increasing industrial action.\n\nThe act grants wide-ranging powers to central and local government in the event of an emergency. It allows the modification of primary legislation by emergency regulation, with the exception of the Human Rights Act 1998 and Part 2 of the Civil Contingencies Act 2004.\n\nThe United States Constitution implicitly provides some emergency powers in the article about the executive power :\n\nAside from these, many provisions of law exist in various jurisdictions, which take effect only upon an executive declaration of emergency; some 500 federal laws take effect upon a presidential declaration of emergency. The National Emergencies Act regulates this process at the federal level. It requires the President to specifically identify the provisions activated and to renew the declaration annually so as to prevent an arbitrarily broad or open-ended emergency.\nPresidents have occasionally taken action justified as necessary or prudent because of a state of emergency, only to have the action struck down in court as unconstitutional.\n\nA state governor or local mayor may declare a state of emergency within his or her jurisdiction. This is common at the state level in response to natural disasters. The Federal Emergency Management Agency maintains a system of assets, personnel and training to respond to such incidents. For example, on December 10, 2015, Washington state Governor Jay Inslee declared a state of emergency due to flooding and landslides caused by heavy rains.\n\nThe 1977 International Emergency Economic Powers Act allows the government to freeze assets, limit trade and confiscate property in response to an \"unusual and extraordinary threat\" to the United States that originates substantially outside of it. As of 2015 more than twenty emergencies under the IEEPA remain active regarding various subjects, the oldest of which was declared in 1979 with regard to the government of Iran. Another ongoing national emergency, declared after the September 11 attacks, authorizes the president to retain or reactivate military personnel beyond their normal term of service.\n\n\n\n\n\n\n"}
{"id": "99860", "url": "https://en.wikipedia.org/wiki?curid=99860", "title": "Voting", "text": "Voting\n\nVoting is a method for a group, such as a meeting or an electorate, in order to make a collective decision or express an opinion usually following discussions, debates or election campaigns. Democracies elect holders of high office by voting. Residents of a place represented by an elected official are called \"constituents\", and those constituents who cast a ballot for their chosen candidate are called \"voters\". There are different systems for collecting votes.\n\nIn a democracy, a government is chosen by voting in an election: a way for an electorate to elect, i.e. choose, among several candidates for rule. In a representative democracy \"voting\" is the method by which the electorate appoints its representatives in its government.\nIn a direct democracy, \"voting\" is the method by which the electorate directly make decisions, turn bills into laws, etc.\n\nA vote is a formal expression of an individual's choice for or against some motion (for example, a proposed resolution); for or against some ballot question; or for a certain candidate, selection of candidates, or political party. Many countries use a secret ballot, a practice to prevent voters from being intimidated and to protect their political privacy.\n\nVoting often takes place at a polling station; it is voluntary in some countries, compulsory in others, such as Australia.\n\nDifferent voting systems use different types of votes. Plurality voting does not require the winner to achieve a vote majority, or more than fifty percent of the total votes cast. In a voting system that uses a single vote per race, when more than two candidates run, the winner may commonly have less than fifty percent of the vote.\n\nA side effect of a single vote per race is vote splitting, which tends to elect candidates that do not support centrism, and tends to produce a two-party system. An alternative to a single-vote system is approval voting.\n\nTo understand why a single vote per race tends to favor less centric candidates, consider a simple lab experiment where students in a class vote for their favorite marble. If five marbles are assigned names and are placed \"up for election\", and if three of them are green, one is red, and one is blue, then a green marble will rarely win the election. The reason is that the three green marbles will split the votes of those who prefer green. In fact, in this analogy, the only way that a green marble is likely to win is if more than sixty percent of the voters prefer green. If the same percentage of people prefer green as those who prefer red and blue, that is to say if 33 percent of the voters prefer green, 33 percent prefer blue, and 33 percent prefer red, then each green marble will only get eleven percent of the vote, while the red and blue marbles will each get 33 percent, putting the green marbles at a serious disadvantage. If the experiment is repeated with other colors, the color that is in the majority will still rarely win. In other words, from a purely mathematical perspective, a single-vote system tends to favor a winner that is different from the majority. If the experiment is repeated using approval voting, where voters are encouraged to vote for as many candidates as they approve of, then the winner is much more likely to be any one of the five marbles, because people who prefer green will be able to vote for every one of the green marbles.\nA development on the 'single vote' system is to have two-round elections, or repeat first-past-the-post. This system is most common around the world. In most cases, the winner must receive a majority, which is more than half. and if no candidate obtains a majority at the first round, then the two candidates with the largest plurality are selected for the second round. Variants exist on these two points: the requirement for being elected at the first round is sometimes less than 50%, and the rules for participation in the runoff may vary.\n\nAn alternative to the Two-round voting system is the single round instant-runoff voting system (Also referred to as \"Alternative vote\" or \"Preferential voting\") as used in some elections in Australia, Ireland and the USA. Voters rank each candidate in order of preference (1,2,3 etc.). Votes are distributed to each candidate according to the preferences allocated. If no single candidate has 50% of the vote, then the candidate with the fewest votes is excluded and their votes redistributed according to the voters nominated order of preference. The process repeating itself until a candidate has 50% or more votes. The system is designed to produce the same result as an exhaustive ballot but using only a single round of voting.\n\nIn a voting system that uses a \"multiple vote\", the voter can vote for any subset of the alternatives. So, a voter might vote for Alice, Bob, and Charlie, rejecting Daniel and Emily. Approval voting uses such multiple votes.\n\nIn a voting system that uses a \"ranked vote\", the voter has to rank the alternatives in order of preference. For example, they might vote for Bob in first place, then Emily, then Alice, then Daniel, and finally Charlie. Ranked voting systems, such as those famously used in Australia, use a ranked vote.\n\nIn a voting system that uses a \"scored vote\" (or \"range vote\"), the voter gives each alternative a number between one and ten (the upper and lower bounds may vary). See cardinal voting systems.\n\nSome \"multiple-winner\" systems may have a single vote or one vote per elector per available position. In such a case the elector could vote for Bob and Charlie on a ballot with two votes. These types of systems can use ranked or unranked voting, and are often used for at-large positions such as on some city councils.\n\nMost of the time, when the citizens of a country are invited to vote, it is for an election. However, people can also vote in referendums and initiatives. Since the end of the eighteenth century, more than five hundred national referendums (including initiatives) were organised in the world; among them, more than three hundred were held in Switzerland. Australia ranked second with dozens of referendums.\n\nResults may lead at best to confusion, at worst to violence and even civil war, in the case of political rivals. Many alternatives may fall in the latitude of indifference—they are neither accepted nor rejected. Avoiding the choice that most people strongly reject may sometimes be at least as important as choosing the one that they most favour.\n\nThere are social choice theory definitions of seemingly reasonable criteria that are a measure of the fairness of certain aspects of voting, including non-dictatorship, unrestricted domain, non-imposition, Pareto efficiency, and independence of irrelevant alternatives but Arrow's impossibility theorem states that no voting system can meet all these standards.\n\nTo ensure fair voting and preventing the misuse of the microblogging platform, Twitter announced for adding a feature for users to report content that misleads voters. This announcement came when general elections are going to be held in India and some other countries.\n\nNegative voting allows a vote that expresses disapproval of a candidate. For explanatory purposes, consider a hypothetical voting system that uses negative voting. In this system, one vote is allowed, with the choice of either for a candidate, or against a candidate. Each positive vote adds one to a candidate's overall total, while a negative vote subtracts one, arriving at a net favorability. The candidate with the highest net favorability is the winner. Note that not only is a negative total possible, but also, a candidate may even be elected with 0 votes if enough negative votes are cast against their opponents.\n\nUnder this implementation, negative voting is no different from a positive voting system, when only two candidates are on the ballot. However, in the case of three or more candidates, each negative vote for a candidate counts positively towards all of the other candidates.\n\nConsider the following example:\n\nThree candidates are running for the same seat. Two hypothetical election results are given, contrasting positive and negative voting. Both polling accuracy and voter turnout are assumed to be 100 percent.\n\nElection results with positive voting:\n\nA-voters, with the clear advantage of 40%, logically vote for Candidate A. B-voters, unconfident of their candidate's chances, split their votes exactly in half, giving both Candidates A and C 15% each. C-voters, also logically vote for their candidate. A is the winner with 55%, C at 45% and B 0%.\n\nElection results with negative voting:\n\nA-voters again, with the clear advantage of 40%, logically vote for Candidate A. B-voters again, split exactly in half. Each B-voter decides to vote negatively against their least favorite candidate, with the reasoning that this negative vote allows them to express approval for the two other candidates. C-voters also decide to vote negatively against Candidate A, reasoning along similar lines. Candidate B is the winner with 0 votes. Enough negative votes were cast against Candidate B's opponents, resulting in negative totals. Candidate A, despite having polled at 40%, winds up with -5%, offset due to the aggregate 45% of negative votes cast by B and C voters. Candidate C ends up with -15%.\n\nProxy voting is the type of voting where a registered citizen who is able to vote passes on his or her vote to a different voter or electorate legitimately.\n\nIn South Africa, there is a strong presence of anti-voting campaigns by poor citizens. They make the structural argument that no political party truly represents them. For instance, this resulted in the \"No Land! No House! No Vote!\" Campaign which becomes very prominent each time the country holds elections. The campaign is prominent among three of South Africa's largest social movements: the Western Cape Anti-Eviction Campaign, Abahlali baseMjondolo, and the Landless Peoples Movement.\n\nOther social movements in other parts of the world also have similar campaigns or non-voting preferences. These include the Zapatista Army of National Liberation and various anarchist-oriented movements.\n\nIt is possible to make a blank vote, carrying out the act of voting, which may be compulsory, without selecting any candidate or option, often as an act of protest. In some jurisdictions, there is an official none of the above option and it is counted as a valid vote. Usually, blank and null votes are counted (together or separately) but are not considered valid.\n\nModern political science has questioned whether average citizens have sufficient political information to cast meaningful votes. A series of studies coming out of the University of Michigan in the 1950s and 1960s argued that voters lack a basic understanding of current issues, the liberal–conservative ideological dimension, and the relative ideological dilemma.\n\nStudies from other institutions have suggested that the physical appearance of candidates is a criterion upon which voters base their decision.\n\nChristadelphians, Jehovah's Witnesses, Old Order Amish, Rastafarians, the Assemblies of Yahweh, and some other religious groups, have a policy of not participating in politics through voting. Rabbis from all Jewish denominations encourage voting; some even consider it a religious obligation.\n\nWhenever several people who do not all agree need to make some decision, voting is a very common way of reaching a decision peacefully. The right to vote is usually restricted to certain people. Members of a society or club, or shareholders of a company, but not outsiders, may elect its officers, or adopt or change its rules, in a similar way to the election of people to official positions. A panel of judges, either formal judicial authorities or, say, judges of a competition, may make decisions by voting. A group of friends or members of a family may decide which film to see by voting. The method of voting can range from formal submission of written votes, through show of hands, voice voting or audience response systems, to informally noting which outcome seems to be preferred by more people.\n\nAccording to Robert's Rules of Order, a widely used guide to parliamentary procedure, the bases for determining the voting result consist of two elements: (1) the percentage of votes that are required for a proposal to be adopted or for a candidate to be elected (e.g. more than half, two-thirds, three-quarters, etc.); and (2) the set of members to which the proportion applies (e.g. the members present and voting, the members present, the entire membership of the organization, the entire electorate, etc.). An example is a majority vote of the members present and voting.\n\nThe voting result could also be determined using a plurality, or the most votes among the choices.\n\nIn addition, a decision could be made without a formal vote by using unanimous consent.\n\nA voting method is the way in which people cast their votes in an election or referendum. There are several different methods in use around the world.\n\nDeliberative assemblies—bodies that use parliamentary procedure to arrive at decisions—use several methods of voting on motions (formal proposal by a member or members of a deliberative assembly that the assembly take certain action). The regular methods of voting in such bodies are a voice vote, a rising vote, and a show of hands. Additional forms of voting include a recorded vote and balloting. The assembly could decide on the voting method by adopting a motion on it. Different legislatures may have their own voting methods.\n\nThe most common voting method uses paper ballots on which voters mark their preferences. This may involve marking their support for a candidate or party listed on the ballot, or a write-in, where they write out the name of their preferred candidate if it is not listed.\nAn alternative paper-based system known as ballot letters is used in Israel, where polling booths contain a tray with ballots for each party contesting the elections; the ballots are marked with the letter(s) assigned to that party. Voters are given an envelope into which they put the ballot of the party they wish to vote for, before placing the envelope in the ballot box. The same system is also implemented in Latvia.\n\nMachine voting uses voting machines, which may be manual (e.g. lever machines) or electronic. In Brazil, voters type in the number of the candidate they wish to vote for and then confirm their vote when the candidate's photo is displayed on screen.\n\nIn some countries people are allowed to vote online. Estonia was one of the first countries to use online voting: it was first used in the 2005 local elections.\n\nMany countries allow postal voting, where voters are sent a ballot and return it by post.\n\nIn contrast to a secret ballot, an open ballot takes place in public and is commonly done by a show of hands. An example is the Landsgemeinde system in Switzerland, which is still in use in the cantons of Appenzell Innerrhoden, Glarus, Grisons, and Schwyz.\n\nIn The Gambia, voting is carried out using marbles, a method introduced in 1965 to deal with illiteracy. Polling stations contain metal drums painted in party colours and emblems with candidates' photos attached to them. Voters are given a marble to place in the drum of their chosen candidate; when dropped into the drum, a bell sounds to register the vote. To avoid confusion, bicycles are banned near polling booths on election day. If the marble is left on top of the drum rather than placed in it, the vote is deemed invalid.\n\nA similar system used in social clubs sees voters given a white ball to indicate support and a black ball to indicate opposition. This led to the coining of the term blackballing.\n\nSome votes are carried in person if all the people eligible to vote are present. This could be by a show of hands or keypad polling.\n\n"}
{"id": "29066482", "url": "https://en.wikipedia.org/wiki?curid=29066482", "title": "Electoral system", "text": "Electoral system\n\nAn electoral system is a set of rules that determine how elections and referendums are conducted and how their results are determined. Political electoral systems are organized by governments, while non-political elections may take place in business, non-profit organisations and informal organisations. These rules govern all aspects of the voting process: when elections occur, who is allowed to vote, who can stand as a candidate, how ballots are marked and cast, how the ballots are counted (electoral method), limits on campaign spending, and other factors that can affect the outcome. Political electoral systems are defined by constitutions and electoral laws, are typically conducted by election commissions, and can use multiple types of elections for different offices.\n\nSome electoral systems elect a single winner to a unique position, such as prime minister, president or governor, while others elect multiple winners, such as members of parliament or boards of directors. There are many variations in electoral systems, but the most common systems are first-past-the-post voting, the two-round (runoff) system, proportional representation and ranked voting. Some electoral systems, such as mixed systems, attempt to combine the benefits of non-proportional and proportional systems.\n\nThe study of formally defined electoral methods is called social choice theory or voting theory, and this study can take place within the field of political science, economics, or mathematics, and specifically within the subfields of game theory and mechanism design. Impossibility proofs such as Arrow's impossibility theorem demonstrate that when voters have three or more alternatives, it is not possible to design a ranked voting electoral system that reflects the preferences of individuals in a global preference of the community, present in countries with proportional representation and plurality voting.\n\nPlurality voting is a system in which the candidate(s) with the highest number of votes wins, with no requirement to get a majority of votes. In cases where there is a single position to be filled, it is known as first-past-the-post; this is the second most common electoral system for national legislatures, with 58 countries using it to elect their legislatures, the vast majority of which are current or former British or American colonies or territories. It is also the second most common system used for presidential elections, being used in 19 countries.\n\nIn cases where there are multiple positions to be filled, most commonly in cases of multi-member constituencies, plurality voting is referred to as block voting or plurality-at-large. This takes two main forms: in one form voters have as many votes as there are seats and can vote for any candidate, regardless of party – this is used in eight countries. There are variations on this system such as limited voting, where voters are given fewer votes than there are seats to be filled (Gibraltar is the only territory where this system is in use) and single non-transferable vote (SNTV), in which voters can vote for only one candidate in a multi-member constituency, with the candidates receiving the most votes declared the winners; this system is used in Afghanistan, Kuwait, the Pitcairn Islands and Vanuatu. In the other main form of block voting, also known as party block voting, voters can only vote for the multiple candidates of a single party. This is used in five countries as part of mixed systems.\n\nThe Dowdall system, a multi-member constituency variation on the Borda count, is used in Nauru for parliamentary elections and sees voters rank the candidates depending on how many seats there are in their constituency. First preference votes are counted as whole numbers; the second preference votes divided by two, third preferences by three; this continues to the lowest possible ranking. The totals achieved by each candidate determine the winners.\n\nMajoritarian voting is a system in which candidates have to receive a majority of the votes to be elected, although in some cases only a plurality is required in the last round of counting if no candidate can achieve a majority. There are two main forms of majoritarian systems, one using a single round of ranked voting and the other using two or more rounds. Both are primarily used for single-member constituencies.\n\nMajoritarian voting can take place in a single round using instant-runoff voting (IRV), whereby voters rank candidates in order of preference; this system is used for parliamentary elections in Australia and Papua New Guinea. If no candidate receives a majority of the vote in the first round, the second preferences of the lowest-ranked candidate are then added to the totals. This is repeated until a candidate achieves over 50% of the number of valid votes. If not all voters use all their preference votes, then the count may continue until two candidates remain, at which point the winner is the one with the most votes. A modified form of IRV is the contingent vote where voters do not rank all candidates, but have a limited number of preference votes. If no candidate has a majority in the first round, all candidates are excluded except the top two, with the highest remaining preference votes from the votes for the excluded candidates then added to the totals to determine the winner. This system is used in Sri Lankan presidential elections, with voters allowed to give three preferences.\n\nThe other main form of majoritarian system is the two-round system, which is the most common system used for presidential elections around the world, being used in 88 countries. It is also used in 20 countries for electing the legislature. If no candidate achieves a majority of votes in the first round of voting, a second round is held to determine the winner. In most cases the second round is limited to the top two candidates from the first round, although in some elections more than two candidates may choose to contest the second round; in these cases the second round is decided by plurality voting. Some countries use a modified form of the two-round system, such as Ecuador where a candidate in the presidential election is declared the winner if they receive 40% of the vote and are 10% ahead of their nearest rival, or Argentina (45% plus 10% ahead), where the system is known as ballotage.\n\nAn exhaustive ballot is not limited to two rounds, but sees the last-placed candidate eliminated in each round of voting. Due to the potentially large number of rounds, this system is not used in any major popular elections, but is used to elect the Speakers of parliament in several countries and members of the Swiss Federal Council. In some formats there may be multiple rounds held without any candidates being eliminated until a candidate achieves a majority, a system used in the United States Electoral College.\n\nProportional representation is the most widely used electoral system for national legislatures, with the parliaments of over eighty countries elected by various forms of the system.\n\nParty-list proportional representation is the single most common electoral system and is used by 80 countries, and involves voters voting for a list of candidates proposed by a party. In closed list systems voters do not have any influence over the candidates put forward by the party, but in open list systems voters are able to both vote for the party list and influence the order in which candidates will be assigned seats. In some countries, notably Israel and the Netherlands, elections are carried out using 'pure' proportional representation, with the votes tallied on a national level before assigning seats to parties. However, in most cases several multi-member constituencies are used rather than a single nationwide constituency, giving an element of geographical representation; but this can result in the distribution of seats not reflecting the national vote totals. As a result, some countries have leveling seats to award to parties whose seat totals are lower than their proportion of the national vote.\n\nIn addition to the electoral threshold (the minimum percentage of the vote that a party must obtain to win seats), there are several different ways to allocate seats in proportional systems. There are two main types of system: highest average and largest remainder. Highest average systems involve dividing the votes received by each party by a series of divisors, producing figures that determine seat allocation; for example the D'Hondt method (of which there are variants including Hagenbach-Bischoff) and the Webster/Sainte-Laguë method. Under largest remainder systems, parties' vote shares are divided by the quota (obtained by dividing the total number of votes by the number of seats available). This usually leaves some seats unallocated, which are awarded to parties based on the largest fractions of seats that they have remaining. Examples of largest remainder systems include the Hare quota, Droop quota, the Imperiali quota and the Hagenbach-Bischoff quota.\n\nSingle transferable vote (STV) is another form of proportional representation; in STV, voters rank candidates in a multi-member constituency rather than voting for a party list; it is used in Malta and the Republic of Ireland. To be elected, candidates must pass a quota (the Droop quota being the most common). Candidates that pass the quota on the first count are elected. Votes are then reallocated from the least successful candidates, as well as surplus votes from successful candidates, until all seats have been filled by candidates who have passed the quota.\n\nIn several countries, mixed systems are used to elect the legislature. These include parallel voting and mixed-member proportional representation.\n\nIn parallel voting systems, which are used in 20 countries, there are two methods by which members of a legislature are elected; part of the membership is elected by a plurality or majority vote in single-member constituencies and the other part by proportional representation. The results of the constituency vote have no effect on the outcome of the proportional vote.\n\nMixed-member proportional representation, in use in eight countries, also sees the membership of the legislature elected by constituency and proportional methods, but in this case the results of the proportional vote are adjusted to balance the seats won in the constituency vote in order to ensure that parties have a number of seats proportional to their vote share. This may result in overhang seats, where parties win more seats in the constituency system than they would be entitled to based on their vote share. Variations of this include the Additional Member System and Alternative Vote Plus, in which voters cast votes for both single-member constituencies and multi-member constituencies; the allocation of seats in the multi-member constituencies is adjusted to achieve an overall seat total proportional to parties' vote share by taking into account the number of seats won by parties in the single-member constituencies. A form of mixed-member proportional representation, Scorporo, was used in Italy from 1993 until 2006.\n\nSome electoral systems feature a majority bonus system to either ensure one party or coalition gains a majority in the legislature, or to give the party receiving the most votes a clear advantage in terms of the number of seats. In Greece the party receiving the most votes is given an additional 50 seats, San Marino has a modified two-round system, which sees a second round of voting featuring the top two parties or coalitions if there is no majority in the first round. The winner of the second round is guaranteed 35 seats in the 60-seat Grand and General Council.\n\nIn Uruguay, the President and members of the General Assembly are elected by on a single ballot, known as the double simultaneous vote. Voters cast a single vote, voting for the presidential, Senatorial and Chamber of Deputies candidates of that party. This system was also previously used in Bolivia and the Dominican Republic.\n\nPrimary elections are a feature of some electoral systems, either as a formal part of the electoral system or informally by choice of individual political parties as a method of selecting candidates, as is the case in Italy. Primary elections limit the risk of vote splitting by ensuring a single party candidate. In Argentina they are a formal part of the electoral system and take place two months before the main elections; any party receiving less than 1.5% of the vote is not permitted to contest the main elections. In the United States, there are both partisan and non-partisan primary elections.\n\nSome elections feature an indirect electoral system, whereby there is either no popular vote, or the popular vote is only one stage of the election; in these systems the final vote is usually taken by an electoral college. In several countries, such as Mauritius or Trinidad and Tobago, the post of President is elected by the legislature. In others like India, the vote is taken by an electoral college consisting of the national legislature and state legislatures. In the United States, the president is indirectly elected using a two-stage process; a popular vote in each state elects members to the electoral college that in turn elects the President. This can result in a situation where a candidate who receives the most votes nationwide does not win the electoral college vote, as most recently happened in 2000 and 2016.\n\nIn addition to the various electoral systems in use in the political sphere, there are numerous others, some of which are proposals and some of which have been adopted for usage in business (such as electing corporate board members) or for organisations but not for public elections.\n\nRanked systems include Bucklin voting, the various Condorcet methods (Copeland's, Dodgson's, Kemeny-Young, Maximal lotteries, Minimax, Nanson's, Ranked pairs, Schulze), the Coombs' method and positional voting. There are also several variants of single transferable vote, including CPO-STV, Schulze STV and the Wright system. Dual-member proportional representation is a proposed system with two candidates elected in each constituency, one with the most votes and one to ensure proportionality of the combined results. Biproportional apportionment is a system whereby the total number of votes is used to calculate the number of seats each party is due, followed by a calculation of the constituencies in which the seats should be awarded in order to achieve the total due to them.\n\nCardinal electoral systems allow voters to score candidates independently. The complexity ranges from approval voting where voters simply state whether they approve of a candidate or not to range voting, where a candidate is scored from a set range of numbers. Other cardinal systems include proportional approval voting, sequential proportional approval voting, satisfaction approval voting, majority judgment and the Janeček Method where voters can cast positive and negative votes.\n\nHistorically, weighted voting systems were used in some countries. These allocated a greater weight to the votes of some voters than others, either indirectly by allocating more seats to certain groups (such as the Prussian three-class franchise), or by weighting the results of the vote. The latter system was used in colonial Rhodesia for the 1962 and 1965 elections. The elections featured two voter rolls (the 'A' roll being largely European and the 'B' roll largely African); the seats of the House Assembly were divided into 50 constituency seats and 15 district seats. Although all voters could vote for both types of seats, 'A' roll votes were given greater weight for the constituency seats and 'B' roll votes greater weight for the district seats. Weighted systems are still used in corporate elections, with votes weighted to reflect stock ownership.\n\nIn addition to the specific method of electing candidates, electoral systems are also characterised by their wider rules and regulations, which are usually set out in a country's constitution or electoral law. Participatory rules determine candidate nomination and voter registration, in addition to the location of polling places and the availability of online voting, postal voting, and absentee voting. Other regulations include the selection of voting devices such as paper ballots, machine voting or open ballot systems, and consequently the type of vote counting systems, verification and auditing used.\n\nElectoral rules place limits on suffrage and candidacy. Most countries's electorates are characterised by universal suffrage, but there are differences on the age at which people are allowed to vote, with the youngest being 16 and the oldest 21 (although voters must be 25 to vote in Senate elections in Italy). People may be disenfranchised for a range of reasons, such as being a serving prisoner, being declared bankrupt, having committed certain crimes or being a serving member of the armed forces. Similar limits are placed on candidacy (also known as passive suffrage), and in many cases the age limit for candidates is higher than the voting age. A total of 21 countries have compulsory voting, although in some there is an upper age limit on enforcement of the law. Many countries also have the none of the above option on their ballot papers.\n\nIn systems that use constituencies, apportionment or districting defines the area covered by each constituency. Where constituency boundaries are drawn has a strong influence on the likely outcome of elections in the constituency due to the geographic distribution of voters. Political parties may seek to gain an advantage during redistricting by ensuring their voter base has a majority in as many constituencies as possible, a process known as gerrymandering. Historically rotten and pocket boroughs, constituencies with unusually small populations, were used by wealthy families to gain parliamentary representation.\n\nSome countries have minimum turnout requirements for elections to be valid. In Serbia this rule caused multiple re-runs of presidential elections, with the 1997 election re-run once and the 2002 elections re-run three times due insufficient turnout in the first, second and third attempts to run the election. The turnout requirement was scrapped prior to the fourth vote in 2004. Similar problems in Belarus led to the 1995 parliamentary elections going to a fourth round of voting before enough parliamentarians were elected to make a quorum.\n\nReserved seats are used in many countries to ensure representation for ethnic minorities, women, young people or the disabled. These seats are separate from general seats, and may be elected separately (such as in Morocco where a separate ballot is used to elect the 60 seats reserved for women and 30 seats reserved for young people in the House of Representatives), or be allocated to parties based on the results of the election; in Jordan the reserved seats for women are given to the female candidates who failed to win constituency seats but with the highest number of votes, whilst in Kenya the Senate seats reserved for women, young people and the disabled are allocated to parties based on how many seats they won in the general vote. Some countries achieve minority representation by other means, including requirements for a certain proportion of candidates to be women, or by exempting minority parties from the electoral threshold, as is done in Poland, Romania and Serbia.\n\nIn ancient Greece and Italy, the institution of suffrage already existed in a rudimentary form at the outset of the historical period. In the early monarchies it was customary for the king to invite pronouncements of his people on matters in which it was prudent to secure its assent beforehand. In these assemblies the people recorded their opinion by clamouring (a method which survived in Sparta as late as the 4th century BCE), or by the clashing of spears on shields.\n\nVoting has been used as a feature of democracy since the 6th century BC, when democracy was introduced by the Athenian democracy. However, in Athenian democracy, voting was seen as the least democratic among methods used for selecting public officials, and was little used, because elections were believed to inherently favor the wealthy and well-known over average citizens. Viewed as more democratic were assemblies open to all citizens, and selection by lot (known as sortition), as well as rotation of office.\n\nGenerally, the taking of votes was effected in the form of a poll. The practice of the Athenians, which is shown by inscriptions to have been widely followed in the other states of Greece, was to hold a show of hands, except on questions affecting the status of individuals: these latter, which included all lawsuits and proposals of ostracism, in which voters chose the citizen they most wanted to exile for ten years, were determined by secret ballot (one of the earliest recorded elections in Athens was a plurality vote that it was undesirable to win, namely an ostracism vote). At Rome the method which prevailed up to the 2nd century BCE was that of division (). But the system became subject to intimidation and corruption. Hence a series of laws enacted between 139 and 107 BCE prescribed the use of the ballot (), a slip of wood coated with wax, for all business done in the assemblies of the people. \nFor the purpose of carrying resolutions a simple majority of votes was deemed sufficient. As a general rule equal value was made to attach to each vote; but in the popular assemblies at Rome a system of voting by groups was in force until the middle of the 3rd century BCE by which the richer classes secured a decisive preponderance.\n\nMost elections in the early history of democracy were held using plurality voting or some variant, but as an exception, the state of Venice in the 13th century adopted approval voting to elect their Great Council.\nThe Venetians' method for electing the Doge was a particularly convoluted process, consisting of five rounds of drawing lots (sortition) and five rounds of approval voting. By drawing lots, a body of 30 electors was chosen, which was further reduced to nine electors by drawing lots again. An electoral college of nine members elected 40 people by approval voting; those 40 were reduced to form a second electoral college of 12 members by drawing lots again. The second electoral college elected 25 people by approval voting, which were reduced to form a third electoral college of nine members by drawing lots. The third electoral college elected 45 people, which were reduced to form a fourth electoral college of 11 by drawing lots. They in turn elected a final electoral body of 41 members, who ultimately elected the Doge. Despite its complexity, the method had certain desirable properties such as being hard to game and ensuring that the winner reflected the opinions of both majority and minority factions. This process, with slight modifications, was central to the politics of the Republic of Venice throughout its remarkable lifespan of over 500 years, from 1268 to 1797.\n\nJean-Charles de Borda proposed the Borda count in 1770 as a method for electing members to the French Academy of Sciences. His method was opposed by the Marquis de Condorcet, who proposed instead the method of pairwise comparison that he had devised. Implementations of this method are known as Condorcet methods. He also wrote about the Condorcet paradox, which he called the \"intransitivity of majority preferences\". However, recent research has shown that the philosopher Ramon Llull devised both the Borda count and a pairwise method that satisfied the Condorcet criterion in the 13th century. The manuscripts in which he described these methods had been lost to history until they were rediscovered in 2001.\n\nLater in the 18th century, apportionment methods came to prominence due to the United States Constitution, which mandated that seats in the United States House of Representatives had to be allocated among the states proportionally to their population, but did not specify how to do so. A variety of methods were proposed by statesmen such as Alexander Hamilton, Thomas Jefferson, and Daniel Webster. Some of the apportionment methods devised in the United States were in a sense rediscovered in Europe in the 19th century, as seat allocation methods for the newly proposed method of party-list proportional representation. The result is that many apportionment methods have two names; \"Jefferson's method\" is equivalent to the D'Hondt method, as is \"Webster's method\" to the Sainte-Laguë method, while \"Hamilton's method\" is identical to the Hare largest remainder method.\n\nThe single transferable vote (STV) method was devised by Carl Andræ in Denmark in 1855 and in the United Kingdom by Thomas Hare in 1857. STV elections were first held in Denmark in 1856, and in Tasmania in 1896 after its use was promoted by Andrew Inglis Clark. Party-list proportional representation began to be used to elect European legislatures in the early 20th century, with Belgium the first to implement it for its 1900 general elections. Since then, proportional and semi-proportional methods have come to be used in almost all democratic countries, with most exceptions being former British colonies.\n\nPerhaps influenced by the rapid development of multiple-winner electoral systems, theorists began to publish new findings about single-winner methods in the late 19th century. This began around 1870, when William Robert Ware proposed applying STV to single-winner elections, yielding instant-runoff voting (IRV). Soon, mathematicians began to revisit Condorcet's ideas and invent new methods for Condorcet completion; Edward J. Nanson combined the newly described instant runoff voting with the Borda count to yield a new Condorcet method called Nanson's method. Charles Dodgson, better known as Lewis Carroll, proposed the straightforward Condorcet method known as Dodgson's method as well as a proportional multiwinner method based on proxy voting.\n\nRanked voting electoral systems eventually gathered enough support to be adopted for use in government elections. In Australia, IRV was first adopted in 1893, and continues to be used along with STV today. In the United States in the early-20th-century progressive era, some municipalities began to use Bucklin voting, although this is no longer used in any government elections, and has even been declared unconstitutional in Minnesota.\n\nThe use of game theory to analyze electoral systems led to discoveries about the effects of certain methods. Earlier developments such as Arrow's impossibility theorem had already shown the issues with Ranked voting systems. Research led Steven Brams and Peter Fishburn to formally define and promote the use of approval voting in 1977. Political scientists of the 20th century published many studies on the effects that the electoral systems have on voters' choices and political parties, and on political stability. A few scholars also studied which effects caused a nation to switch to a particular electoral system. One prominent current voting theorist is Nicolaus Tideman, who formalized concepts such as strategic nomination and the spoiler effect in the independence of clones criterion. Tideman also devised the ranked pairs method, a Condorcet method that is not susceptible to clones.\n\nThe study of electoral systems influenced a new push for electoral reform beginning around the 1990s, when proposals were made to replace plurality voting in governmental elections with other methods. New Zealand adopted mixed-member proportional representation for the 1993 general elections and STV for some local elections in 2004. After plurality voting was a key factor in the contested results of the 2000 presidential elections in the United States, various municipalities in the United States began to adopt IRV, although some of them subsequently returned to their prior method. However, attempts at introducing more proportional systems were not always successful; in Canada there were two referendums in British Columbia in 2005 and 2009 on adopting an STV method, both of which failed. In the United Kingdom, a 2011 referendum on adopting Instant-runoff voting saw the proposal rejected.\n\nIn other countries there were calls for the restoration of plurality or majoritarian systems or their establishment where they have never been used; a referendum was held in Ecuador in 1994 on the adoption the two round system, but the idea was rejected. In Romania a proposal to switch to a two-round system for parliamentary elections failed only because voter turnout in the referendum was too low. Attempts to reintroduce single-member constituencies in Poland (2015) and two-round system in Bulgaria (2016) via referendums both also failed due to low turnout.\n\nElectoral systems can be compared by different means. Attitudes towards systems are highly influenced by the systems' impact on groups that one supports or opposes, which can make the objective comparison of voting systems difficult. There are several ways to address this problem:\n\nOne approach is to define criteria mathematically, such that any electoral system either passes or fails. This gives perfectly objective results, but their practical relevance is still arguable.\n\nAnother approach is to define ideal criteria that no electoral system passes perfectly, and then see how often or how close to passing various methods are over a large sample of simulated elections. This gives results which are practically relevant, but the method of generating the sample of simulated elections can still be arguably biased.\n\nA final approach is to create imprecisely defined criteria, and then assign a neutral body to evaluate each method according to these criteria. This approach can look at aspects of electoral systems which the other two approaches miss, but both the definitions of these criteria and the evaluations of the methods are still inevitably subjective.\n\nArrow's and Gibbard's theorems prove that no system using ranked voting or cardinal voting, can meet all such criteria simultaneously. Instead of debating the importance of different criteria, another method is to simulate many elections with different electoral systems, and estimate the typical overall happiness of the population with the results, their vulnerability to strategic voting, their likelihood of electing the candidate closest to the average voter, etc.\n\n"}
{"id": "222839", "url": "https://en.wikipedia.org/wiki?curid=222839", "title": "Welfare state", "text": "Welfare state\n\nThe welfare state is a form of government in which the state protects and promotes the economic and social well-being of the citizens, based upon the principles of equal opportunity, equitable distribution of wealth, and public responsibility for citizens unable to avail themselves of the minimal provisions for a good life. Sociologist T. H. Marshall described the modern welfare state as a distinctive combination of democracy, welfare, and capitalism.\n\nAs a type of mixed economy, the welfare state funds the governmental institutions for healthcare and education along with direct benefits given to individual citizens. Modern welfare states include Germany, France, Belgium, and the Netherlands, as well as the Nordic countries, which employ a system known as the Nordic model. \n\nThe German term \"sozialstaat\" (\"social state\") has been used since 1870 to describe state support programs devised by German \"sozialpolitiker\" (\"social politicians\") and implemented as part of Bismarck's conservative reforms. In Germany, the term \"wohlfahrtsstaat\", a direct translation of the English \"welfare state\", is used to describe Sweden's social insurance arrangements.\n\nThe literal English equivalent \"social state\" did not catch on in Anglophone countries. However, during the Second World War, Anglican Archbishop William Temple, author of the book \"Christianity and the Social Order\" (1942), popularized the concept using the phrase \"welfare state\". Bishop Temple's use of \"welfare state\" has been connected to Benjamin Disraeli's 1845 novel \"Sybil: or the Two Nations\" (in other words, the rich and the poor), where he writes \"power has only one duty — to secure the social welfare of the PEOPLE\". At the time he wrote \"Sybil\", Disraeli (later a prime minister) belonged to Young England, a conservative group of youthful Tories who disagreed with how the Whigs dealt with the conditions of the industrial poor. Members of Young England attempted to garner support among the privileged classes to assist the less fortunate and to recognize the dignity of labor that they imagined had characterized England during the Feudal Middle Ages.\n\nThe Swedish welfare state is called \"folkhemmet\" (\"the people's home\") and goes back to the 1936 compromise, as well as to another important contract made in 1938 between Swedish trade unions and large corporations. Even though the country is often rated comparably economically free, Sweden's mixed economy remains heavily influenced by the legal framework and continual renegotiations of union contracts, a government-directed and municipality-administered system of social security, and a system of universal health care that is run by the more specialized and in theory more politically isolated county councils of Sweden.\n\nThe Italian term \"stato sociale\" (\"social state\") and the Turkish term \"sosyal devlet\" reproduces the original German term. In French, the concept is expressed as \"l'État-providence\". Spanish and many other languages employ an analogous term: \"estado del bienestar\" – literally, \"state of well-being\". In Portuguese, two similar phrases exist: \"estado de bem-estar social\", which means \"state of social well-being\", and \"estado de providência\" – \"providing state\", denoting the state's mission to ensure the basic well-being of the citizenry. In Brazil the concept is referred to as \"previdência social\", or \"social providence\".\n\nModern welfare programs are chiefly distinguished from earlier forms of poverty relief by their universal, comprehensive character. The institution of social insurance in Germany under Bismarck was an influential example. Some schemes were based largely in the development of autonomous, mutualist provision of benefits. Others were founded on state provision. In a highly influential essay, \"Citizenship and Social Class\" (1949), British sociologist T. H. Marshall identified modern welfare states as a distinctive combination of democracy, welfare, and capitalism, arguing that citizenship must encompass access to social, as well as to political and civil rights. Examples of such states are Germany, all of the Nordic countries, the Netherlands, France, Uruguay and New Zealand and the United Kingdom in the 1930s. Since that time, the term welfare state applies only to states where social rights are accompanied by civil and political rights.\n\nChanged attitudes in reaction to the worldwide Great Depression, which brought unemployment and misery to millions, were instrumental in the move to the welfare state in many countries. During the Great Depression, the welfare state was seen as a \"middle way\" between the extremes of communism on the left and unregulated \"laissez-faire\" capitalism on the right. In the period following World War II, some countries in Western Europe moved from partial or selective provision of social services to relatively comprehensive \"cradle-to-grave\" coverage of the population. Other Western European states did not, such as the United Kingdom, Ireland, Spain and France.\n\nThe activities of present-day welfare states extend to the provision of both cash welfare benefits (such as old-age pensions or unemployment benefits) and in-kind welfare services (such as health or childcare services). Through these provisions, welfare states can affect the distribution of wellbeing and personal autonomy among their citizens, as well as influencing how their citizens consume and how they spend their time.\n\nEmperor Ashoka of India put forward his idea of a welfare state in the 3rd century BCE. He envisioned his \"dharma\" (religion or path) as not just a collection of high-sounding phrases. He consciously tried to adopt it as a matter of state policy; he declared that \"all men are my children\" and \"whatever exertion I make, I strive only to discharge debt that I owe to all living creatures.\" It was a totally new ideal of kingship. Ashoka renounced war and conquest by violence and forbade the killing of many animals. Since he wanted to conquer the world through love and faith, he sent many missions to propagate Dharma. Such missions were sent to places like Egypt, Greece, and Sri Lanka. The propagation of Dharma included many measures of people's welfare. Centers of the treatment of men and beasts founded inside and outside of the empire. Shady groves, wells, orchards and rest houses were laid out. Ashoka also prohibited useless sacrifices and certain forms of gatherings which led to waste, indiscipline and superstition. To implement these policies he recruited a new cadre of officers called Dharmamahamattas. Part of this group's duties was to see that people of various sects were treated fairly. They were especially asked to look after the welfare of prisoners.\n\nThe Roman Republic intervened sporadically to distribute free or subsidized grain to its population, through the program known as \"Cura Annonae.\" The city of Rome grew rapidly during the Roman Republic and Empire, reaching a population approaching one million in the second century AD. The population of the city grew beyond the capacity of the nearby rural areas to meet the food needs of the city.\n\nRegular grain distribution began in 123 BC with a grain law proposed by Gaius Gracchus and approved by the Roman Plebeian Council (popular assembly). The numbers of those receiving free or subsidized grain expanded to a high of an estimated 320,000 people at one point. In the 3rd century AD, the dole of grain was replaced by bread, probably during the reign of Septimius Severus (193-211 AD). Severus also began providing olive oil to residents of Rome, and later the emperor Aurelian (270-275) ordered the distribution of wine and pork. The doles of bread, olive oil, wine, and pork apparently continued until near the end of the Western Roman Empire in 476 AD. The dole in the early Roman Empire is estimated to account for 15 to 33 percent of the total grain imported and consumed in Rome.\n\nIn addition to food, the Roman Republic also supplied free entertainment, through \"ludi\" (public games). Public money was allocated for the staging of \"ludi,\" but the presiding official increasingly came to augment the splendor of his games from personal funds as a form of public relations. The sponsor was able to cultivate the favor of the people of Rome.\n\nThe concept of states taxing for the welfare budget was introduced in early 7th century Islamic law. \"Zakat\" is one of the five pillars of Islam and is a mandatory form of 2.5% income tax to be paid by all individuals earning above a basic threshold to provide for the needy. Umar (584-644), leader of the Rashidun Caliphate (empire), established a welfare state through the Bayt al-mal (treasury), which for instance was used to stockpile food in every region of the Islamic Empire for disasters and emergencies.\n\nOtto von Bismarck established the first welfare state in a modern industrial society, with social-welfare legislation, in 1880's Imperial Germany. Bismarck extended the privileges of the Junker social class to ordinary Germans. His 17 November 1881 Imperial Message to the Reichstag used the term \"practical Christianity\" to describe his program.\n\nGerman laws from this era also insured workers against industrial risks inherent in the workplace.\n\nIn Switzerland, the Swiss Factory Act of 1877 limited working hours for everyone, and gave maternity benefits.\nThe Swiss welfare state also arose in the late 19th century; its existence and depth varied individually by canton. Some of the programs first adopted within the Cantons of Switzerland were emergency relief, elementary schools, and homes for the elderly and children.\n\nIn the Austro-Hungarian Empire, a version was set up by Count Eduard von Taaffe a few years after Bismarck in Germany. Legislation to help the working class in Austria emerged from Catholic conservatives. Von Taffe used Swiss and German models of social reform, including the Swiss Factory Act of 1877 German laws that insured workers against industrial risks inherent in the workplace to create the 1885 Trade Code Amendment.\n\nHistorian of the 20th Century fascist movement, Robert Paxton, observes that the provisions of the welfare state were enacted in the 19th Century by religious conservatives to counteract appeals from trade unions and socialism.\n\nLater, Paxton writes \"All the modern twentieth-century European dictatorships of the right, both fascist and authoritarian, were welfare states...They all provided medical care, pensions, affordable housing, and mass transport as a matter of course, in order to maintain productivity, national unity, and social peace.\" Adolf Hitler’s National Socialist German Workers' Party expanded the welfare state to the point where over 17 million German citizens were receiving assistance under the auspices of the National Socialist People's Welfare by 1939.\n\nWhen socialist parties abandoned Marxism after World War II, they fully accepted the welfare state as their ultimate goal.\n\nPrior to 1900 in Australia, charitable assistance from benevolent societies, sometimes with financial contributions from the authorities, was the primary means of relief for people not able to support themselves. The 1890s economic depression and the rise of the trade unions and the Labor parties during this period led to a movement for welfare reform.\n\nIn 1900, the states of New South Wales and Victoria enacted legislation introducing non-contributory pensions for those aged 65 and over. Queensland legislated a similar system in 1907 before the federal labor government led by Andrew Fisher introduced a national aged pension under the Invalid and Old-Aged Pensions Act 1908. A national invalid disability pension was started in 1910, and a national maternity allowance was introduced in 1912.\n\nDuring the Second World War, Australia under a labor government created a welfare state by enacting national schemes for: child endowment in 1941; a widows' pension in 1942; a wife’s allowance in 1943; additional allowances for the children of pensioners in 1943; and unemployment, sickness, and special benefits in 1945.\n\nCanada's welfare programs are funded and administered at all levels of government (with 13 different provincial/territorial systems), and include medicare, public education (through graduate school), social housing and social services. Social support is given through programs including Social Assistance, Guaranteed Income Supplement, Child Tax Benefit, Old Age Security, Employment Insurance, Workers’ Compensation, and the Canada/Quebec Pension Plans.\n\nAfter 1830 in France Liberalism and economic modernization were key goals. While liberalism was individualistic and laissez-faire in Britain and the United States, in France liberalism was based instead on a solidaristic conception of society, following the theme of the French Revolution, \"Liberté, égalité, fraternité\" (\"liberty, equality, fraternity\"). In the Third Republic, especially between 1895 and 1914 “Solidarité” [\"solidarism\"] was the guiding concept of a liberal social policy, whose chief champions were the prime ministers Leon Bourgeois (1895–96) and Pierre Waldeck-Rousseau (1899-1902). The French welfare state expanded when it tried to follow some of Bismarck's policies. Poor relief was the starting point. More attention was paid to industrial labour in the 1930s during a short period of socialist political ascendency, with the Matignon Accords and the reforms of the Popular Front. Paxton points out these reforms were paralleled and even exceeded by measures taken by the Vichy regime in the 1940s.\n\nOtto von Bismarck, the powerful Chancellor of Germany (in office 1871–90), developed the first modern welfare state by building on a tradition of welfare programs in Prussia and Saxony that had begun as early as in the 1840s. The measures that Bismarck introduced – old-age pensions, accident insurance, and employee health insurance – formed the basis of the modern European welfare state. His paternalistic programs aimed to forestall social unrest and to undercut the appeal of the new Social Democratic Party, and to secure the support of the working classes for the German Empire, as well as to reduce emigration to the United States, where wages were higher but welfare did not exist. Bismarck further won the support of both industry and skilled workers through his high-tariff policies, which protected profits and wages from American competition, although they alienated the liberal intellectuals who wanted free trade. During the 12 years of rule by Adolf Hitler’s National Socialist German Workers' Party the welfare state was expanded and extended to the point where over 17 million German citizens were receiving assistance under the auspices of the National Socialist People's Welfare (NSV) by 1939, an agency that projected a powerful image of caring and support.\nSome policies enacted to enhance social welfare in Germany were Health Insurance 1883, Accident Insurance 1884, Old Age Pensions 1889, and National Unemployment Insurance 1927.\n\nWelfare states in Latin America have been considered as \"welfare states in transition\" or \"emerging welfare states\". Mesa-Lago has classified the countries taking into account the historical experience of their welfare systems. The pioneers were Uruguay, Chile and Argentina, as they started to develop the first welfare programs in the 1920s following a bismarckian model. Other countries such as Costa Rica developed a more universal welfare system (1960s–1970s) with social security programs based on the Beveridge model. Researchers such as Martinez-Franzoni and Barba-Solano have examined and identified several welfare regime models based on the typology of Esping-Andersen. Other scholars such as Riesco and Cruz-Martinez have examined the welfare state development in the region.\n\nAccording to Alex Segura-Ubiergo: Latin American countries can be unequivocally divided into two groups depending on their 'welfare effort' levels. The first group, which for convenience we may call welfare states, includes Uruguay, Argentina, Chile, Costa Rica, and Brazil. Within this group, average social spending per capita in the 1973–2000 period was around $532, while as a percentage of GDP and as a share of the budget, social spending reached 51.6 and 12.6 percent, respectively. In addition, between approximately 50 and 75 percent of the population is covered by the public health and pension social security system. In contrast, the second group of countries, which we call non-welfare states, has welfare-effort indices that range from 37 to 88. Within this second group, social spending per capita averaged $96.6, while social spending as a percentage of GDP and as a percentage of the budget averaged 5.2 and 34.7 percent, respectively. In terms of the percentage of the population actually covered, the percentage of the active population covered under some social security scheme does not even reach 10 percent.\n\nSaudi Arabia, Kuwait, and Qatar have become welfare states exclusively for their own citizens.\n\nThe Nordic welfare model refers to the welfare policies of the Nordic countries, which also tie into their labor market policies. The Nordic model of welfare is distinguished from other types of welfare states by its emphasis on maximizing labor force participation, promoting gender equality, egalitarian and extensive benefit levels, the large magnitude of income redistribution and liberal use of the expansionary fiscal policy.\n\nWhile there are differences among the Nordic countries, they all share a broad commitment to social cohesion, a universal nature of welfare provision in order to safeguard individualism by providing protection for vulnerable individuals and groups in society and maximizing public participation in social decision-making. It is characterized by flexibility and openness to innovation in the provision of welfare. The Nordic welfare systems are mainly funded through taxation.\n\nChina traditionally relied on the extended family to provide welfare services. The one-child policy introduced in 1978 has made that unrealistic, and new models have emerged since the 1980s as China has rapidly become richer and more urban. Much discussion is underway regarding China's proposed path toward a welfare state. Chinese policies have been incremental and fragmented in terms of social insurance, privatization, and targeting. In the cities, where the rapid economic development has centered, lines of cleavage have developed between state-sector and non-state-sector employees, and between labor-market insiders and outsiders.\n\nHistorian Derek Fraser tells the British story in a nutshell:\n\nThe modern welfare state in the United Kingdom began operations with the Liberal welfare reforms of 1906–1914 under Liberal Prime Minister H. H. Asquith. These included the passing of the Old-Age Pensions Act in 1908, the introduction of free school meals in 1909, the 1909 Labour Exchanges Act, the Development Act 1909, which heralded greater Government intervention in economic development, and the enacting of the National Insurance Act 1911 setting up a national insurance contribution for unemployment and health benefits from work.\n\nThe minimum wage was introduced in the United Kingdom in 1909 for certain low-wage industries and expanded to numerous industries, including farm labour, by 1920. However, by the 1920s, a new perspective was offered by reformers to emphasize the usefulness of family allowance targeted at low-income families was the alternative to relieving poverty without distorting the labour market. The trade unions and the Labour Party adopted this view. In 1945, family allowances were introduced; minimum wages faded from view. Talk resumed in the 1970s, but in the 1980s the Thatcher administration made it clear it would not accept a national minimum wage. Finally, with the return of Labour, the National Minimum Wage Act 1998 set a minimum of ₤3.60 per hour, with lower rates for younger workers. It largely affected workers in high turnover service industries such as fast food restaurants, and members of ethnic minorities.\n\nDecember 1942 saw the publication of the \"Report of the Inter-Departmental Committee on Social Insurance and Allied Services\", commonly known as the Beveridge Report after its chairman, Sir William Beveridge. The Beveridge Report proposed a series of measures to aid those who were in need of help, or in poverty and recommended that the government find ways of tackling what the report called \"the five giants\": Want, Disease, Ignorance, Squalor, and Idleness. It urged the government to take steps to provide citizens with adequate income, adequate health care, adequate education, adequate housing, and adequate employment, proposing that \"All people of working age should pay a weekly National Insurance contribution. In return, benefits would be paid to people who were sick, unemployed, retired, or widowed.\"\n\nThe Beveridge Report assumed that:\n\nThe report stressed the lower costs and efficiency of universal benefits. Beveridge cited miners' pension schemes as examples of some of the most efficient available and argued that a universal state scheme would be cheaper than a myriad of individual friendly societies and private insurance schemes and also less expensive to administer than a means-tested government-run welfare system for the poor.\n\nThe Liberal Party, the Conservative Party, and then the Labour Party all adopted the Beveridge Report's recommendations. Following the Labour election victory in the 1945 general election many of Beveridge's reforms were implemented through a series of Acts of Parliament. On 5 July 1948, the National Insurance Act, National Assistance Act and National Health Service Act came into force, forming the key planks of the modern UK welfare state. In 1949, the Legal Aid and Advice Act was passed, providing the \"fourth pillar\" of the modern welfare state, access to advice for legal redress for all.\n\nBefore 1939, most health care had to be paid for through non-government organisations – through a vast network of friendly societies, trade unions, and other insurance companies, which counted the vast majority of the UK working population as members. These organizations provided insurance for sickness, unemployment, and disability, providing an income to people when they were unable to work. As part of the reforms, the Church of England also closed down its voluntary relief networks and passed the ownership of thousands of church schools, hospitals and other bodies to the state.\n\nWelfare systems continued to develop over the following decades. By the end of the 20th-century parts of the welfare system had been restructured, with some provision channelled through non-governmental organizations which became important providers of social services.\n\nThe United States developed a limited welfare state in the 1930s. The earliest and most comprehensive philosophical justification for the welfare state was produced by an American, the sociologist Lester Frank Ward (1841–1913), whom the historian Henry Steele Commager called \"the father of the modern welfare state\".\n\nWard saw social phenomena as amenable to human control. \"It is only through the artificial control of natural phenomena that science is made to minister to human needs\" he wrote, \"and if social laws are really analogous to physical laws, there is no reason why social science should not receive practical application such as have been given to physical science.\" Ward wrote:\n\nThe charge of paternalism is chiefly made by the class that enjoys the largest share of government protection. Those who denounce it are those who most frequently and successfully invoke it. Nothing is more obvious today than the single inability of capital and private enterprise to take care of themselves unaided by the state; and while they are incessantly denouncing \"paternalism,\" by which they mean the claim of the defenseless laborer and artisan to a share in this lavish state protection, they are all the while besieging legislatures for relief from their own incompetency, and \"pleading the baby act\" through a trained body of lawyers and lobbyists. The dispensing of national pap to this class should rather be called \"maternalism,\" to which a square, open, and dignified paternalism would be infinitely preferable.\nWard's theories centred around his belief that a universal and comprehensive system of education was necessary if a democratic government was to function successfully. His writings profoundly influenced younger generations of progressive thinkers such as Theodore Roosevelt, Thomas Dewey, and Frances Perkins (1880–1965), among others.\n\nThe United States was the only industrialized country that went into the Great Depression of the 1930s with no social insurance policies in place. In 1935 Franklin D. Roosevelt's New Deal instituted significant social insurance policies. In 1938 Congress passed the Fair Labor Standards Act, limiting the work week to 40 hours and banning child labor for children under 16, over stiff congressional opposition from the low-wage South.\n\nThe Social Security law was very unpopular among many groups – especially farmers, who resented the additional taxes and feared they would never be made good. They lobbied hard for exclusion. Furthermore, the Treasury realized how difficult it would be to set up payroll deduction plans for farmers, for housekeepers who employed maids, and for non-profit groups; therefore they were excluded. State employees were excluded for constitutional reasons (the federal government in the United States cannot tax state governments). Federal employees were also excluded.\n\nBy 2013, the U.S. remained the only major industrial state without a uniform national sickness program. American spending on health care (as a percent of GDP) is the highest in the world, but it is a complex mix of federal, state, philanthropic, employer and individual funding. The US spent 16% of its GDP on health care in 2008, compared to 11% in France in second place.\n\nSome scholars, such as Gerard Friedman, argue that labor-union weakness in the Southern United States undermined unionization and social reform throughout the United States as a whole, and is largely responsible for the anemic U.S. welfare state. Sociologists Loïc Wacquant and John L. Campbell contend that since the rise of neoliberal ideology in the late 1970s and early 1980s, an expanding carceral state, or government system of mass incarceration, has largely supplanted the increasingly retrenched social welfare state, which has been justified by its proponents with the argument that the citizenry must take on personal responsibility. Scholars assert that this transformation of the welfare state to a post-welfare punitive state, along with neoliberal structural adjustment policies and the globalization of the U.S. economy, have created more extreme forms of \"destitute poverty\" in the U.S. which must be contained and controlled by expanding the criminal justice system into every aspect of the lives of the poor.\n\nOther scholars such as Esping-Andersen argue that the welfare state in the United States has been characterized by private provision because such a state would better reflect the racial and sexual biases within the private sector. The disproportionate number of racial and sexual minorities in private sector jobs with weaker benefits, he argues, is evidence that the American welfare state is not necessarily intended to improve the economic situation of such groups.\n\nBroadly speaking, welfare states are either universal – with provisions that cover everybody, or selective – with provisions covering only those deemed most needy. In his 1990 book, \"The Three Worlds of Welfare Capitalism\", Danish sociologist Gøsta Esping-Andersen further identified three subtypes of welfare state models.\n\nSince the building of the decommodification index is limited and the typology is debatable, these 18 countries could be ranked from most purely social-democratic (Sweden) to the most liberal (the United States). Ireland represents a near-hybrid model whereby two streams of unemployment benefit exist: contributory and means-tested. However, payments can begin immediately and are theoretically available to all Irish citizens even if they have never worked, provided they are habitually resident.\n\nSocial stigma varies across the three conceptual welfare states. Particularly, it is highest in liberal states, and lowest in social democratic states. Espring-Anderson proposes that the universalist nature of social democratic states eliminate the duality between beneficiaries and non-recipients, whereas in means-tested liberal states there is resentment towards redistribution efforts. That is to say, the lower the percent of GDP spent on welfare, the higher the stigma of the welfare state.\n\nEsping-Anderson also argues that welfare states set the stage for post-industrial employment evolution in terms of employment growth, structure, and stratification. He uses Germany, Sweden, and the United States to provide examples of the differing results of each of the three welfare states.\n\nAccording to the Swedish political scientist Bo Rothstein, in non-universal welfare states, the state is primarily concerned with directing resources to \"the people most in need\". This requires tight bureaucratic control in order to determine who is eligible for assistance and who is not. Under universal models such as Sweden, on the other hand, the state distributes welfare to all people who fulfill easily established criteria (e.g. having children, receiving medical treatment, etc.) with as little bureaucratic interference as possible. This, however, requires higher taxation due to the scale of services provided. This model was constructed by the Scandinavian ministers Karl Kristian Steincke and Gustav Möller in the 1930s and is dominant in Scandinavia.\n\nSociologist Lane Kenworthy argues that the Nordic experience demonstrates that the modern social democratic model can \"promote economic security, expand opportunity, and ensure rising living standards for all ... while facilitating freedom, flexibility and market dynamism.\"\n\nAmerican political scientist Benjamin Radcliff has also argued that the universality and generosity of the welfare state (i.e. the extent of decommodification) is the single most important societal-level structural factor affecting the quality of human life, based on the analysis of time serial data across both the industrial democracies and the American States. He maintains that the welfare state improves life for everyone, regardless of social class (as do similar institutions, such as pro-worker labor market regulations and strong labor unions).\n\nEmpirical evidence suggests that taxes and transfers considerably reduce poverty in most countries whose welfare states constitute at least a fifth of GDP.\n\nResearchers have found very little correlation between economic performance and social expenditure. They also see little evidence that social expenditures contribute to losses in productivity; economist Peter Lindert of the University of California, Davis attributes this to policy innovations such as the implementation of \"pro-growth\" tax policies in real-world welfare states.\n\nNor have social expenses contributed significantly to public debt.According to the OECD, social expenditures in its 34 member countries rose steadily between 1980 and 2007, but the increase in costs was almost completely offset by GDP growth. More money was spent on welfare because more money circulated in the economy and because government revenues increased. In 1980, the OECD averaged social expenditures equal to 16 percent of GDP. In 2007, just before the financial crisis kicked into full gear, they had risen to 19 percent – a manageable increase.\n\nA Norwegian study covering the period 1980 to 2003 found welfare state spending correlated negatively with student achievement. However, many of the top-ranking OECD countries on the 2009 PISA tests are considered welfare states.\n\nThe table below shows social expenditure as a percentage of GDP for OECD member states in 2018:\n\nEarly conservatives, under the influence of Thomas Malthus, opposed every form of social insurance \"root and branch\". They argued, according to economist Brad DeLong, that it would \"make the poor richer, and they would become more fertile. As a result, farm sizes would drop (as the land was divided among ever more children), labor productivity would fall, and the poor would become even poorer. Social insurance was not just pointless; it was counterproductive.\" Malthus, a clergyman for whom birth control was anathema, believed that the poor needed to learn the hard way to practice frugality, self-control and chastity. Traditional conservatives also protested that the effect of social insurance would be to weaken private charity and loosen traditional social bonds of family, friends, religious and non-governmental welfare organisations.\n\nOn the other hand, Karl Marx opposed piecemeal reforms advanced by middle-class reformers out of a sense of duty. In his \"Address of the Central Committee to the Communist League\", written after the failed revolution of 1848, he warned that measures designed to increase wages, improve working conditions and provide social insurance were merely bribes that would temporarily make the situation of working classes tolerable to weaken the revolutionary consciousness that was needed to achieve a socialist economy. Nevertheless, Marx also proclaimed that the Communists had to support the bourgeoisie wherever it acted as a revolutionary progressive class because \"bourgeois liberties had first to be conquered and then criticised\".\n\nIn the 20th century, opponents of the welfare state have expressed apprehension about the creation of a large, possibly self-interested, bureaucracy required to administer it and the tax burden on the wealthier citizens that this entailed.\n\nPolitical historian Alan Ryan points out that the modern welfare state stops short of being an \"advance in the direction of socialism... its egalitarian elements are more minimal than either its defenders or its critics think\". It does not entail advocacy for social ownership of industry. The modern welfare state, Ryan writes, does not set out\nto make the poor richer and the rich poorer, which is a central element in socialism, but to help people to provide for themselves in sickness while they enjoy good health, to put money aside to cover unemployment while they are in work, and to have adults provide for the education of their own and other people's children, expecting those children's future taxes to pay in due course for the pensions of their parents’ generation. These are devices for shifting income across different stages in life, not for shifting income across classes. Another distinct difference is that social insurance does not aim to transform work and working relations; employers and employees pay taxes at a level they would not have done in the nineteenth century, but owners are not expropriated, profits are not illegitimate, cooperativism does not replace hierarchical management.\n\nHistorian Walter Scheidel has commented that the establishment of welfare states in the West in the early 20th century could be partly a reaction by elites to the Bolshevik Revolution and its violence against the bourgeoisie, which feared violent revolution in its own backyard. They were diminished decades later as the perceived threat receded:\n\nIt's a little tricky because the US never really had any strong leftist movement. But if you look at Europe, after 1917 people were really scared about communism in all the Western European countries. \"You have all these poor people, they might rise up and kill us and take our stuff.\" That wasn't just a fantasy because it was happening next door. And that, we can show, did trigger steps in the direction of having more welfare programs and a rudimentary safety net in response to fear of communism. Not that they [the communists] would invade, but that there would be homegrown movements of this sort. American populism is a little different because it's more detached from that. But it happens roughly at the same time, and people in America are worried about communism, toonot necessarily very reasonably. But that was always in the background. And people have only begun to study systematically to what extent the threat, real or imagined, of this type of radical regime really influenced policy changes in Western democracies. You don't necessarily even have to go out and kill rich peopleif there was some plausible alternative out there, it would arguably have an impact on policy making at home. That's certainly there in the 20s, 30s, 40s, 50s, and 60s. And there's a debate, right, because it becomes clear that the Soviet Union is really not in very good shape, and people don't really like to be there, and all these movements lost their appeal. That's a contributing factor, arguably, that the end of the Cold War coincides roughly with the time when inequality really starts going up again, because elites are much more relaxed about the possibility of credible alternatives or threats being out there.\n\n\n\n"}
{"id": "27052", "url": "https://en.wikipedia.org/wiki?curid=27052", "title": "Administrative division", "text": "Administrative division\n\nAn administrative division, unit, entity, area or region, also referred to as a subnational entity, constituent unit, or country subdivision, is a portion of a country or other region delineated for the purpose of administration. Administrative divisions are granted a certain degree of autonomy and are usually required to manage themselves through their own local governments. Countries are divided up into these smaller units to make managing their land and the affairs of their people easier. A country may be divided into provinces, states, counties, cantons or other sub-units, which, in turn, may be divided in whole or in part into municipalities, counties or others.\n\nAdministrative divisions are conceptually separate from dependent territories, with the former being an integral part of the state and the other being only under some lesser form of control. However, the term \"administrative division\" can include dependent territories as well as accepted administrative divisions (for example, in geographical databases).\n\nFor clarity and convenience the standard neutral reference for the largest administrative subdivision of a country is called the \"first-level administrative division\" or \"first administrative level\". Next smaller is called \"second-level administrative division\" or \"second administrative level\".\n\nIn many of the following terms originating from British cultural influence, areas of relatively low mean population density might bear a title of an entity one would expect to be either larger or smaller. There is no fixed rule, for \"all politics is local\" as is perhaps well demonstrated by their relative lack of systemic order. In the realm of self-government, any of these can and does occur along a stretch of road—which for the most part is passing through rural unsettled countryside. Since the terms are administrative political subdivisions of the local regional government their exact relationship and definitions are subject to home rule considerations, tradition, as well as state statute law and local governmental (administrative) definition and control. In British cultural legacy, some territorial entities began with fairly expansive counties which encompass an appreciably large area, but were divided over time into a number of smaller entities.\nWithin those entities are the large and small cities or towns, which may or may not be the county seat. Some of the world's larger cities culturally, if not officially, span several counties, and those crossing state or provincial boundaries have much in common culturally as well, but are rarely incorporated within the same municipal government. Many sister cities share a water boundary, which quite often serves as a border of both cities and counties. For example, Cambridge and Boston, Massachusetts appear to the casual traveler as one large city, while locally they each are quite culturally different and occupy different counties.\n\n\"General terms for these incorporated places include \"municipality,\" \"settlement,\" \"locality,\" and \"populated place.\"\"\n\n\nDue to variations in their use worldwide, consistency in the translation of terms from non-English to English is sometimes difficult to maintain.\n\n\n\n"}
{"id": "64501", "url": "https://en.wikipedia.org/wiki?curid=64501", "title": "Arrondissement", "text": "Arrondissement\n\nAn arrondissement (, , ) is any of various administrative divisions of France, Belgium, Haiti, certain other Francophone countries, and the Netherlands.\n\nThe 101 French departments are divided into 342 \"arrondissements\", which may be roughly translated into English as districts. The capital of an arrondissement is called a subprefecture. When an arrondissement contains the prefecture (capital) of the department, that prefecture is the capital of the arrondissement, acting both as a prefecture and as a subprefecture. Arrondissements are further divided into cantons and communes.\n\nA municipal arrondissement (, pronounced ), is a subdivision of the commune, used in the three largest cities: Paris, Lyon, and Marseille. It functions as an even lower administrative division, with its own mayor. Although usually referred to simply as an \"arrondissement,\" they should not be confused with departmental arrondissements, which are groupings of communes within one \"département\". The official translation into English is \"district\".\n\nBelgium is a federalized country which geographically consists of three regions, of which only Flanders (Flemish Region) and Wallonia (Walloon Region) are subdivided into five provinces each; the Brussels-Capital Region is neither a province nor is it part of one.\n\nIn Belgium, there are administrative, judicial and electoral arrondissements. These may or may not relate to identical geographical areas.\n\n\nIn the Netherlands an \"arrondissement\" is a judicial jurisdiction.\n\nSubdivisions of the canton of Bern include districts since 2010, which are called \"arrondissements administratifs\" in French.\n\nIn some post-Soviet states, there are cities that are divided into municipal raioni similarly to how some French cities are divided into municipal arrondissements (see e.g. Raions of cities in Ukraine, Municipal divisions of Russia, Administrative divisions of Minsk).\n\nMost nations in Africa who have been colonised by the French have retained the arrondissement administrative structure. These are normally subunits of a Department, and may either contain or be coequal with Communes (towns). In Mali the arrondissement is a subunit of a Cercle, while in some places arrondissements are essentially subdistricts of large cities.\n\n\nAs of 2015, Haiti's ten departments are sub-divided into 42 arrondissements.\n\nIn the Canadian province of Quebec, eight cities are divided into arrondissements, known as boroughs in English. In Quebec, boroughs are provincially organized and recognized sub-municipal entities that have mayors and councillors.\n"}
{"id": "804036", "url": "https://en.wikipedia.org/wiki?curid=804036", "title": "Caliphate", "text": "Caliphate\n\nA caliphate ( ') is an Islamic state under the leadership of an Islamic steward with the title of caliph (; ', ), a person considered a political-religious successor to the Islamic prophet Muhammad and a leader of the entire ummah (Muslim community). Historically, the caliphates were polities based in Islam which developed into multi-ethnic trans-national empires. During the medieval period, three major caliphates succeeded each other: the Rashidun Caliphate (632–661), the Umayyad Caliphate (661–750) and the Abbasid Caliphate (750–1258). In the fourth major caliphate, the Ottoman Caliphate, the rulers of the Ottoman Empire claimed caliphal authority from 1517. During the history of Islam, a few other Muslim states, almost all hereditary monarchies, have claimed to be caliphates.\n\nPrior to the rise of Muhammad, Arab tribes followed a pre-Islamic Arab polytheism and lived as self-governing sedentary and nomadic tribal communities. Following the early Muslim conquests by Muhammad, the region became politically unified under Islam.\n\nThe first caliphate, the Rashidun Caliphate, immediately succeeded Muhammad after his death in 632. The four Rashidun caliphs were chosen through \"shura, \"a process of community consultation that some consider to be an early form of Islamic democracy. The fourth caliph, Ali, who, unlike the prior three, was from the same clan as Muhammad (Banu Hashim), is considered by Shia Muslims to be the first rightful caliph and Imam after Muhammad. Ali reigned during the First Fitna (656–661), a civil war between supporters of Ali and supporters of the assassinated previous caliph, Uthman, from Banu Umayya, as well as rebels in Egypt; the war led to the establishment of the Umayyad Caliphate under Muawiyah I in 661.\n\nThe second caliphate, the Umayyad Caliphate, was ruled by Banu Umayya, a Meccan clan descended from Umayya ibn Abd Shams. The caliphate continued the Arab conquests, incorporating the Caucasus, Transoxiana, Sindh, the Maghreb and the Iberian Peninsula (Al-Andalus) into the Muslim world. The caliphate had considerable acceptance of the Christians within its territory, necessitated by their large numbers, especially in the region of Syria. Following the Abbasid Revolution from 746–750, which primarily arose from non-Arab Muslim disenfranchisement, the Abbasid Caliphate was established in 750.\n\nThe third caliphate, the Abbasid Caliphate was ruled by the Abbasids, a dynasty of Meccan origin descended from Hashim, a great-grandfather of Muhammad, via Abbas, an uncle of Muhammad. Caliph al-Mansur founded its second capital of Baghdad in 762, which became a major scientific, cultural and art centre, as did the territory as a whole, during the period known as the Islamic Golden Age. From the 10th century, Abbasid rule became confined to an area around Baghdad and saw several occupations from foreign powers. In 1258, the Mongol Empire sacked Baghdad, ending the Abbasid Caliphate, and in 1261 the Mamluks in Egypt re-established the Abbasid Caliphate in Cairo. Though lacking in political power, the Abbasid dynasty continued to claim authority in religious matters until the Ottoman conquest of Mamluk Egypt in 1517.\n\nThe fourth major caliphate, the Ottoman Caliphate, was established after their conquest of Mamluk Egypt in 1517. The conquest gave the Ottomans control over the holy cities of Mecca and Medina, previously controlled by the Mamluks. The Ottomans gradually came to be viewed as the \"de facto\" leaders and representatives of the Muslim world. Following their defeat in World War I, their empire was partitioned by the United Kingdom and French Third Republic. The Turkish Republic was proclaimed on 29 October 1923, and as part of the reforms of its first president, Mustafa Kemal Atatürk, the Grand National Assembly of Turkey constitutionally abolished the institution of the caliphate on 3 March 1924.\n\nA few other states that existed through history have called themselves caliphates, including the Isma'ili Fatimid Caliphate in Northeast Africa (909–1171), the Umayyad Caliphate of Córdoba in Iberia (929–1031), the Berber Almohad Caliphate in Morocco (1121–1269) and the Fula Sokoto Caliphate in present-day northern Nigeria (1804–1903).\n\nThe Sunni branch of Islam stipulates that, as a head of state, a caliph was a selected or elected position. Followers of Shia Islam, however, believe a caliph should be an Imam chosen by God from the \"Ahl al-Bayt\" (the \"Family of the House\", Muhammad's direct descendants). In simpler terms, the Sunni favour election while the Shia favour bloodline.\n\nIn the early 21st century, following the failure of the Arab Spring and defeat of the self-proclaimed \"Islamic State\", there has been seen \"a broad mainstream embrace of a collective Muslim identity\" by young Muslims, and the appeal of a caliphate as an \"idealized future Muslim state\" has grown stronger.\n\nBefore the advent of Islam, Arabian monarchs traditionally used the title \"malik \"(King, ruler), or another from the same root.\n\nThe term \"caliph\" (), derives from the Arabic word \"\" (, ), which means \"successor\", \"steward\", or \"deputy\" and has traditionally been considered a shortening of \"Khalīfat Rasūl Allāh\" (\"successor of the messenger of God\"). However, studies of pre-Islamic texts suggest that the original meaning of the phrase was \"successor \"selected by\" God\".\n\nIn the immediate aftermath of the death of Muhammad, a gathering of the Ansar (natives of Medina) took place in the \"Saqifah\" (courtyard) of the Banu Sa'ida clan. The general belief at the time was that the purpose of the meeting was for the Ansar to decide on a new leader of the Muslim community among themselves, with the intentional exclusion of the Muhajirun (migrants from Mecca), though this has later become the subject of debate.\n\nNevertheless, Abu Bakr and Umar, both prominent companions of Muhammad, upon learning of the meeting became concerned of a potential coup and hastened to the gathering. Upon arriving, Abu Bakr addressed the assembled men with a warning that an attempt to elect a leader outside of Muhammad's own tribe, the Quraysh, would likely result in dissension as only they can command the necessary respect among the community. He then took Umar and another companion, Abu Ubaidah ibn al-Jarrah, by the hand and offered them to the Ansar as potential choices. He was countered with the suggestion that the Quraysh and the Ansar choose a leader each from among themselves, who would then rule jointly. The group grew heated upon hearing this proposal and began to argue amongst themselves. Umar hastily took Abu Bakr's hand and swore his own allegiance to the latter, an example followed by the gathered men.\n\nAbu Bakr was near-universally accepted as head of the Muslim community (under the title of Caliph) as a result of Saqifah, though he did face contention as a result of the rushed nature of the event. Several companions, most prominent among them being Ali ibn Abi Talib, initially refused to acknowledge his authority. Ali may have been reasonably expected to assume leadership, being both cousin and son-in-law to Muhammad. The theologian Ibrahim al-Nakhai stated that Ali also had support among the Ansar for his succession, explained by the genealogical links he shared with them. Whether his candidacy for the succession was raised during Saqifah is unknown, though it is not unlikely. Abu Bakr later sent Umar to confront Ali to gain his allegiance, resulting in an altercation which may have involved violence. However, after six months the group made peace with Abu Bakr and Ali offered him his fealty.\n\nAbu Bakr nominated Umar as his successor on his deathbed. Umar, the second caliph, was killed by a Persian named Piruz Nahavandi. His successor, Uthman, was elected by a council of electors (majlis). Uthman was killed by members of a disaffected group. Ali then took control but was not universally accepted as caliph by the governors of Egypt and later by some of his own guard. He faced two major rebellions and was assassinated by Abd-al-Rahman ibn Muljam, a Khawarij. Ali's tumultuous rule lasted only five years. This period is known as the Fitna, or the first Islamic civil war. The followers of Ali later became the Shi'a (\"shiaat Ali\", partisans of Ali.) minority sect of Islam and reject the legitimacy of the first 3 caliphs. The followers of all four Rashidun Caliphs (Abu Bakr, Umar, Uthman and Ali) became the majority Sunni sect.\n\nUnder the Rashidun each region (Sultanate, Wilayah, or Emirate) of the Caliphate had its own governor (Sultan, Wāli or Emir). Muawiyah, a relative of Uthman and governor (\"Wali\") of Syria, succeeded Ali as Caliph. Muawiyah transformed the caliphate into a hereditary office, thus founding the Umayyad dynasty.\n\nIn areas which were previously under Sasanian Empire or Byzantine rule, the Caliphs lowered taxes, provided greater local autonomy (to their delegated governors), greater religious freedom for Jews, and some indigenous Christians, and brought peace to peoples demoralised and disaffected by the casualties and heavy taxation that resulted from the decades of Byzantine-Persian warfare.\n\nAli's reign was plagued by turmoil and internal strife. The Persians, taking advantage of this, infiltrated the two armies and attacked the other army causing chaos and internal hatred between the companions at the Battle of Siffin. The battle lasted several months, resulting in a stalemate. In order to avoid further bloodshed, Ali agreed to negotiate with Mu'awiyah. This caused a faction of approximately 4,000 people, who would come to be known as the Kharijites, to abandon the fight. After defeating the Kharijites at the Battle of Nahrawan, Ali was later assassinated by the Kharijite Ibn Muljam. Ali's son Hasan was elected as the next caliph, but abdicated in favor of Mu'awiyah a few months later to avoid any conflict within the Muslims. Mu'awiyah became the sixth caliph, establishing the Umayyad Dynasty, named after the great-grandfather of Uthman and Mu'awiyah, Umayya ibn Abd Shams.\n\nBeginning with the Umayyads, the title of the caliph became hereditary. Under the Umayyads, the Caliphate grew rapidly in territory, incorporating the Caucasus, Transoxiana, Sindh, the Maghreb and most of the Iberian Peninsula (Al-Andalus) into the Muslim world. At its greatest extent, the Umayyad Caliphate covered 5.17 million square miles (13,400,000 km), making it the largest empire the world had yet seen and the sixth-largest ever to exist in history.\n\nGeographically, the empire was divided into several provinces, the borders of which changed numerous times during the Umayyad reign. Each province had a governor appointed by the caliph. However, for a variety of reasons, including that they were not elected by Shura and suggestions of impious behaviour, the Umayyad dynasty was not universally supported within the Muslim community. Some supported prominent early Muslims like Al-Zubayr; others felt that only members of Muhammad's clan, the Banu Hashim, or his own lineage, the descendants of Ali, should rule.\n\nThere were numerous rebellions against the Umayyads, as well as splits within the Umayyad ranks (notably, the rivalry between Yaman and Qays). At the command of Yazid son of Muawiya, an army led by Umar ibn Saad, a commander by the name of Shimr Ibn Thil-Jawshan killed Ali's son Hussein and his family at the Battle of Karbala in 680, solidifying the Shia-Sunni split. Eventually, supporters of the Banu Hashim and the supporters of the lineage of Ali united to bring down the Umayyads in 750. However, the \"Shi‘at ‘Alī\", \"the Party of Ali\", were again disappointed when the Abbasid dynasty took power, as the Abbasids were descended from Muhammad's uncle, ‘Abbas ibn ‘Abd al-Muttalib and not from Ali.\n\nIn 750, the Umayyad dynasty was overthrown by another family of Meccan origin, the Abbasids. Their time represented a scientific, cultural and religious flowering. Islamic art and music also flourished significantly during their reign. Their major city and capital Baghdad began to flourish as a center of knowledge, culture and trade. This period of cultural fruition ended in 1258 with the sack of Baghdad by the Mongols under Hulagu Khan. The Abbasid Caliphate had however lost its effective power outside Iraq already by c. 920. By 945, the loss of power became official when the Buyids conquered Baghdad and all of Iraq. The empire fell apart and its parts were ruled for the next century by local dynasties.\n\nIn the 9th century, the Abbasids created an army loyal only to their caliphate, composed predominantly of Turkic Cuman, Circassian and Georgian slave origin known as Mamluks. By 1250 the Mamluks came to power in Egypt. The Mamluk army, though often viewed negatively, both helped and hurt the caliphate. Early on, it provided the government with a stable force to address domestic and foreign problems. However, creation of this foreign army and al-Mu'tasim's transfer of the capital from Baghdad to Samarra created a division between the caliphate and the peoples they claimed to rule. In addition, the power of the Mamluks steadily grew until Ar-Radi (934–41) was constrained to hand over most of the royal functions to Muhammad ibn Ra'iq.\n\nIn 1261, following the Mongol conquest of Baghdad, the Mamluk rulers of Egypt tried to gain legitimacy for their rule by declaring the re-establishment of the Abbasid caliphate in Cairo. The Abbasid caliphs in Egypt had little political power; they continued to maintain the symbols of authority, but their sway was confined to religious matters. The first Abbasid caliph of Cairo was Al-Mustansir (r. June–November 1261). The Abbasid caliphate of Cairo lasted until the time of Al-Mutawakkil III, who ruled as caliph from 1508 to 1516, then he was deposed briefly in 1516 by his predecessor Al-Mustamsik, but was restored again to the caliphate in 1517.\n\nThe Ottoman Great Sultan Selim I defeated the Mamluk Sultanate and made Egypt part of the Ottoman Empire in 1517. Al-Mutawakkil III was captured together with his family and transported to Constantinople as a prisoner where he had a ceremonial role. He died in 1543, following his return to Cairo.\n\nThe Abbasid dynasty lost effective power over much of the Muslim realm by the first half of the tenth century\n\nThe Umayyad dynasty, which had survived and come to rule over Al-Andalus, reclaimed the title of Caliph in 929, lasting until it was overthrown in 1031.\n\nThe Fatimid Caliphate was an Isma'ili Shi'i caliphate, originally based in Tunisia, that extended its rule across the Mediterranean coast of Africa and ultimately made Egypt the centre of its caliphate. At its height, in addition to Egypt, the caliphate included varying areas of the Maghreb, Sicily, the Levant and the Hejaz.\n\nThe Fatimids established the Tunisian city of Mahdia and made it their capital city, before conquering Egypt and building the city of Cairo there in 969. Thereafter, Cairo became the capital of the caliphate, with Egypt becoming the political, cultural and religious centre of the state. Islam scholar Louis Massignon dubbed the 4th century AH /10th century CE as the \"Ismaili century in the history of Islam\".\n\nThe term \"Fatimite\" is sometimes used to refer to the citizens of this caliphate. The ruling elite of the state belonged to the Ismaili branch of Shi'ism. The leaders of the dynasty were Ismaili Imams and had a religious significance to Ismaili Muslims. They are also part of the chain of holders of the office of the Caliphate, as recognised by some Muslims. Therefore, this constitutes a rare period in history in which the descendants of Ali (hence the name Fatimid, referring to Ali's wife Fatima) and the Caliphate were united to any degree, excepting the final period of the Rashidun Caliphate under Ali himself.\n\nThe caliphate was reputed to exercise a degree of religious tolerance towards non-Ismaili sects of Islam as well as towards Jews, Maltese Christians and Copts.\n\nThe Shiʻa Ubayd Allah al-Mahdi Billah of the Fatimid dynasty, who claimed descent from Muhammad through his daughter, claimed the title of Caliph in 909, creating a separate line of caliphs in North Africa. Initially controlling Algeria, Tunisia and Libya, the Fatimid caliphs extended their rule for the next 150 years, taking Egypt and Palestine, before the Abbasid dynasty was able to turn the tide, limiting Fatimid rule to Egypt. The Fatimid dynasty finally ended in 1171.\n\nDuring the Umayyad dynasty, the Iberian Peninsula was an integral province of the Umayyad Caliphate ruling from Damascus. The Umayyads lost the position of Caliph in Damascus in 750, and Abd al-Rahman I became Emir of Córdoba in 756 after six years in exile. Intent on regaining power, he defeated the existing Islamic rulers of the area who defied Umayyad rule and united various local fiefdoms into an emirate.\n\nRulers of the emirate used the title \"emir\" or \"sultan\" until the 10th century, when Abd al-Rahman III was faced with the threat of invasion by the Fatimid Caliphate. To aid his fight against the invading Fatimids, who claimed the caliphate in opposition to the generally recognised Abbasid Caliph of Baghdad, Al-Mu'tadid, Abd al-Rahman III claimed the title of caliph himself. This helped Abd al-Rahman III gain prestige with his subjects, and the title was retained after the Fatimids were repulsed. The rule of the Caliphate is considered as the heyday of Muslim presence in the Iberian peninsula, before it fragmented into various taifas in the 11th century. This period was characterised by a flourishing in technology, trade and culture; many of the buildings of al-Andalus were constructed in this period.\n\nThe Almohad Caliphate (, from Arabic \"\", \"the Monotheists\" or \"the Unifiers\") was a Moroccan Berber Muslim movement founded in the 12th century.\n\nThe Almohad movement was started by Ibn Tumart among the Masmuda tribes of southern Morocco. The Almohads first established a Berber state in Tinmel in the Atlas Mountains in roughly 1120. The Almohads succeeded in overthrowing the Almoravid dynasty in governing Morocco by 1147, when Abd al-Mu'min (r. 1130–1163) conquered Marrakech and declared himself Caliph. They then extended their power over all of the Maghreb by 1159. Al-Andalus followed the fate of Africa and all Islamic Iberia was under Almohad rule by 1172.\n\nThe Almohad dominance of Iberia continued until 1212, when Muhammad al-Nasir (1199–1214) was defeated at the Battle of Las Navas de Tolosa in the Sierra Morena by an alliance of the Christian princes of Castile, Aragon, Navarre and Portugal. Nearly all of the Moorish dominions in Iberia were lost soon after, with the great Moorish cities of Córdoba and Seville falling to the Christians in 1236 and 1248, respectively.\n\nThe Almohads continued to rule in northern Africa until the piecemeal loss of territory through the revolt of tribes and districts enabled the rise of their most effective enemies, the Marinid dynasty, in 1215. The last representative of the line, Idris al-Wathiq, was reduced to the possession of Marrakesh, where he was murdered by a slave in 1269; the Marinids seized Marrakesh, ending the Almohad domination of the Western Maghreb.\n\nAfter the Umayyad campaigns in India and the conquest on small territories of the western part of the Indian peninsula, early Indian Muslim dynasties founded by the Ghurid dynasty Ghaznavids did not extensively strive for a caliphate since the Ottoman Empire was already observing the caliphate. Although the Mughal Empire is not recognised as an Islamic caliphate, its sixth emperor Muhammad Alamgir Aurangzeb, however, has been often regarded as one of the few Islamic caliphs to have ruled the Indian peninsula. He received support from Ottoman Sultans such as Suleiman II and Mehmed IV. As a memorizer of Quran, Aurangzeb fully established sharia in South Asia via his Fatwa Alamgiri. He re-introduced jizya and banned islamically unlawful activities. However, Aurangzeb's personal expenses were covered by his own incomes, which included the sewing of caps and trade of his written copies of the Quran. Thus he has been compared to the 2nd Caliph Umar bin Khattab and Kurdish-Arab conqueror Saladin. Other notable rulers such as Muhammad bin Bakhtiyar Khalji, Alauddin Khilji, Firuz Shah Tughlaq, Shamsuddin Ilyas Shah, Babur, Sher Shah Suri, Tipu Sultan, and the Nawabs of Bengal also attempted to wholly establish caliphates in the land of India and Bengal.\n\nThe caliphate was claimed by the sultans of the Ottoman Empire beginning with Murad I (reigned 1362 to 1389), while recognising no authority on the part of the Abbasid caliphs of the Mamluk-ruled Cairo. Hence the seat of the caliphate moved to the Ottoman capital of Edirne. In 1453, after Mehmed the Conqueror's conquest of Constantinople, the seat of the Ottomans moved to Constantinople, present-day Istanbul. In 1517, the Ottoman sultan Selim I defeated and annexed the Mamluk Sultanate of Cairo into his empire. Through conquering and unifying Muslim lands, Selim I became the defender of the Holy Cities of Mecca and Medina, which further strengthened the Ottoman claim to the caliphate in the Muslim world. Ottomans gradually came to be viewed as the \"de facto\" leaders and representatives of the Islamic world. However, the earlier Ottoman caliphs did not officially bear the title of caliph in their documents of state, inscriptions, or coinage. It was only in the late eighteenth century that the claim to the caliphate was discovered by the sultans to have a practical use, since it allowed them to counter Russian claims to protect Ottoman Christians with their own claim to protect Muslims under Russian rule.\n\nAccording to Barthold, the first time the title of \"caliph\" was used as a political instead of symbolic religious title by the Ottomans was the Treaty of Küçük Kaynarca with the Russian Empire in 1774, when the Empire retained moral authority on territory whose sovereignty was ceded to the Russian Empire.\n\nThe British supported and propagated the view that the Ottomans were Caliphs of Islam among Muslims in British India and the Ottoman Sultans helped the British by issuing pronouncements to the Muslims of India telling them to support British rule from Sultan Ali III and Sultan Abdülmecid I.\n\nThe outcome of the Russo-Turkish War of 1768–74 was disastrous for the Ottomans. Large territories, including those with large Muslim populations, such as Crimea, were lost to the Russian Empire. However, the Ottomans under Abdul Hamid I claimed a diplomatic victory by being allowed to remain the religious leaders of Muslims in the now-independent Crimea as part of the peace treaty; in return Russia became the official protector of Christians in Ottoman territory.\n\nAround 1880 Sultan Abdul Hamid II reasserted the title as a way of countering Russian expansion into Muslim lands. His claim was most fervently accepted by the Muslims of British India. By the eve of World War I, the Ottoman state, despite its weakness relative to Europe, represented the largest and most powerful independent Islamic political entity. The sultan also enjoyed some authority beyond the borders of his shrinking empire as caliph of Muslims in Egypt, India and Central Asia.\n\nIn 1899 John Hay, U.S. Secretary of State, asked the American ambassador to Ottoman Turkey, Oscar Straus, to approach Sultan Abdul Hamid II to use his position as caliph to order the Tausūg people of the Sultanate of Sulu in the Philippines to submit to American suzerainty and American military rule; the Sultan obliged them and wrote the letter which was sent to Sulu via Mecca. As a result, the \"Sulu Mohammedans ... refused to join the insurrectionists and had placed themselves under the control of our army, thereby recognizing American sovereignty.\"\n\nAfter the Armistice of Mudros of October 1918 with the military occupation of Constantinople and Treaty of Versailles (1919), the position of the Ottomans was uncertain. The movement to protect or restore the Ottomans gained force after the Treaty of Sèvres (August 1920) which imposed the partitioning of the Ottoman Empire and gave Greece a powerful position in Anatolia, to the distress of the Turks. They called for help and the movement was the result. The movement had collapsed by late 1922.\n\nOn 3 March 1924, the first President of the Turkish Republic, Mustafa Kemal Atatürk, as part of his reforms, constitutionally abolished the institution of the caliphate. Its powers within Turkey were transferred to the Grand National Assembly of Turkey, the parliament of the newly formed Turkish Republic. The title was then claimed by Hussein bin Ali, Sharif of Mecca and Hejaz, leader of the Arab Revolt, but his kingdom was defeated and annexed by ibn Saud in 1925.\n\nEgyptian scholar Ali Abdel Raziq published his 1925 book \"Islam and the Foundations of Governance\". The argument of this book has been summarized as \"Islam does not advocate a specific form of government\". He focussed his criticism both at those who use religious law as contemporary political proscription and at the history of rulers claiming legitimacy by the caliphate. Raziq wrote that past rulers spread the notion of religious justification for the caliphate \"so that they could use religion as a shield protecting their thrones against the attacks of rebels\".\n\nA summit was convened at Cairo in 1926 to discuss the revival of the caliphate, but most Muslim countries did not participate and no action was taken to implement the summit's resolutions. Though the title \"Ameer al-Mumineen\" was adopted by the King of Morocco and by Mohammed Omar, former head of the Taliban of Afghanistan, neither claimed any legal standing or authority over Muslims outside the borders of their respective countries.\n\nSince the end of the Ottoman Empire, occasional demonstrations have been held calling for the re-establishment of the caliphate. Organisations which call for the re-establishment of the caliphate include Hizb ut-Tahrir and the Muslim Brotherhood.\n\nThe Bornu Caliphate, which was headed by the Bornu Emperors, began in 1472. A rump state of the larger Kanem-Bornu Empire, its rulers held the title of Caliph until 1893, when it was absorbed into the British Colony of Nigeria and Northern Cameroones Protectorate. The British recognized them as the 'Sultans of Bornu,' one step down in Muslim royal titles. After Nigeria became independent, its rulers became the 'Emirs of Bornu,' another step down.\n\nThe Indonesian Sultan of Yogyakarta historically used \"Khalifatullah\" (Caliph of God) as one of his many titles. In 2015 sultan Hamengkubuwono X renounced any claim to the Caliphate in order to facilitate his daughter's inheritance of the throne, as the theological opinion of the time was that a woman may hold the secular office of sultan but not the spiritual office of caliph.\n\nThe Sokoto Caliphate was an Islamic state in what is now Nigeria led by Usman dan Fodio. Founded during the Fulani War in the early 19th century, it controlled one of the most powerful empires in sub-Saharan Africa prior to European conquest and colonisation. The caliphate remained extant through the colonial period and afterwards, though with reduced power. The current head of the Sokoto Caliphate is Sa'adu Abubakar.\n\nThe Toucouleur Empire, also known as the Tukular Empire, was one of the Fulani jihad states in sub-saharan Africa. It was eventually pacified and annexed by the French Republic, being incorporated into French West Africa.\n\nThe Khilafat Movement was launched by Muslims in British India in 1920 to defend the Ottoman Caliphate at the end of the First World War and it spread throughout the British colonial territories. It was strong in British India where it formed a rallying point for some Indian Muslims as one of many anti-British Indian political movements. Its leaders included Mohammad Ali Jouhar, his brother Shawkat Ali and Maulana Abul Kalam Azad, Dr. Mukhtar Ahmed Ansari, Hakim Ajmal Khan and Barrister Muhammad Jan Abbasi. For a time it was supported by Mohandas Karamchand Gandhi, who was a member of the Central Khilafat Committee. However, the movement lost its momentum after the abolition of the Caliphate in 1924. After further arrests and flight of its leaders, and a series of offshoots splintered off from the main organisation, the Movement eventually died down and disbanded.\n\nThe Sharifian Caliphate () was an Arab caliphate proclaimed by the Sharifian rulers of Hejaz in 1924 previously known as Vilayet Hejaz, declaring independence from the Ottoman Caliphate. The idea of the Sharifian Caliphate had been floating around since at least the 15th century. Toward the end of the 19th century, it started to gain importance due to the decline of the Ottoman Empire, which was heavily defeated in the Russo-Turkish War of 1877–78. There is little evidence, however, that the idea of a Sharifian Caliphate ever gained wide grassroots support in the Middle East or anywhere else for that matter.\n\nThough non-political, some Sufi orders and the Ahmadiyya movement define themselves as caliphates. Their leaders are thus commonly referred to as \"khalifas\" (caliphs).\n\nIn Sufism, tariqas (orders) are led by spiritual leaders (khilafah ruhaniyyah), the main khalifas, who nominate local khalifas to organize zaouias.\n\nSufi caliphates are not necessarily hereditary. Khalifas are aimed to serve the \"silsilah\" in relation to spiritual responsibilities and to propagate the teachings of the tariqa.\nThe Ahmadiyya Muslim Community is a self-proclaimed Islamic revivalist movement founded in 1889 by Mirza Ghulam Ahmad of Qadian, India, who claimed to be the promised Messiah and Mahdi, awaited by Muslims. He also claimed to be a follower-prophet subordinate to Muhammad, the prophet of Islam. The group are traditionally shunned by the majority of Muslims.\n\nAfter Ahmad's death in 1908, his first successor, Hakeem Noor-ud-Din, became the caliph of the community and assumed the title of \"Khalifatul Masih\" (Successor or Caliph of the Messiah). After Hakeem Noor-ud-Din, the first caliph, the title of the Ahmadiyya caliph continued under Mirza Mahmud Ahmad, who led the community for over 50 years. Following him were Mirza Nasir Ahmad and then Mirza Tahir Ahmad who were the third and fourth caliphs respectively. The current caliph is Mirza Masroor Ahmad, who lives in London\n\nThe Quran uses the term \"khalifa\" twice. First, in al-Baqara, 30, it refers to God creating humanity as his \"khalifa\" on Earth. Second, in Sad, 26, it addresses King David as God's \"khalifa \"and reminds him of his obligation to rule with justice.\n\nIn addition, the following excerpt from the Quran, known as the 'Istikhlaf Verse', is used by some to argue for a Quranic basis for Caliphate:\nIn the above verse, the word \"Khulifa\" (the plural of \"Khalifa\") has been variously translated as \"successors\" and \"ones who accede to power\".\n\nSeveral schools of jurisprudence and thought within Sunni Islam argue that to govern a state by Sharia is, by definition, to rule via the Caliphate and use the following verses to sustain their claim.\n\nThe following \"hadith\" from Musnad Ahmad ibn Hanbal can be understood to prophesy two eras of Caliphate (both on the lines/precepts of prophethood).\n\nIn the above, the first era of Caliphate is commonly accepted by Muslims to be that of the Rashidun Caliphate.\n\nNafi'a reported saying:\n\nHisham ibn Urwah reported on the authority of Abu Saleh on the authority of Abu Hurairah that Muhammad said:\n\nMuslim narrated on the authority of al-A'araj, on the authority of Abu Hurairah, that Muhammad said:\n\nMuslim reported on the authority of Abdel Aziz al-Muqrin, who said:\n\nMany Islamic texts, including several ahadith, state that the Mahdi will be elected caliph and rule over a caliphate. A number of Islamic figures titled themselves both \"caliph\" and \"al-Mahdi\", including the first Abbasid caliph As-Saffah.\n\nAl-Habbab Ibn ul-Munthir said, when the Sahaba met in the wake of the death of Muhammad, (at the thaqifa hall) of Bani Sa’ida: Upon this Abu Bakr replied: \n\nThen he got up and addressed the Muslims.\n\nIt has additionally been reported that Abu Bakr went on to say on the day of Al-Saqifa: \n\nThe Sahaba agreed to this and selected Abu Bakr as their first Khaleef. Habbab ibn Mundhir who suggested the idea of two Ameers corrected himself and was the first to give Abu Bakr the Bay'ah. This indicates an Ijma as-Sahaba of all of the Sahaba. Ali ibni abi Talib, who was attending the body of Muhammad at the time, also consented to this.\n\nImam Ali whom the Shia revere said: \n\nScholars like Al-Mawardi, Ibn Hazm, Ahmad al-Qalqashandi, and Al-Sha`rani stated that the global Muslim community can have only one leader at any given time. Al-Nawawi and Abd al-Jabbar ibn Ahmad declared it impermissible to give oaths of loyalty to more than one leader.\n\nAl-Joziri said: \n\nShia scholars have expressed similar opinions. However, the Shia school of thought states that the leader must not be appointed by the Islamic ummah, but must be appointed by God.\n\nAl-Qurtubi said that the caliph is the \"pillar upon which other pillars rest\", and said of the Quranic verse, \"Indeed, man is made upon this earth a Caliph\":\nAn-Nawawi said:\n\nAl-Ghazali when writing of the potential consequences of losing the Caliphate said:\n\nIbn Taymiyyah said:\n\nOnce the subject of intense conflict and rivalry amongst Muslim rulers, the caliphate lay dormant and largely unclaimed since the 1920s. For the vast majority of Muslims the caliph, as leader of the ummah, \"is cherished both as memory and ideal\" as a time when Muslims \"enjoyed scientific and military superiority globally\". The Islamic prophet Muhammad is reported to have prophesied:\n\nThe group Tanzim Qaidat al-Jihad fi Bilad al-Rafidayn (Al-Qaeda in Iraq) formed as an affiliate of Al-Qaeda network of Islamist militants during the Iraq War. The group eventually expanded into Syria and rose to prominence as the Islamic State of Iraq and the Levant (ISIL) during the Syrian Civil War. In the summer of 2014, the group launched the Northern Iraq offensive, seizing the city of Mosul. The group declared itself a caliphate under Abu Bakr al-Baghdadi on 29 June 2014 and renamed itself as the \"Islamic State\". ISIL's claim to be the highest authority of Muslims has been widely rejected. No prominent Muslim scholar has supported its declaration of caliphate; even Salafi-jihadist preachers accused the group of engaging in political showmanship and bringing disrepute to the notion of Islamic state.\n\nISIL has been at war with armed forces including the Iraqi Army, the Syrian Army, the Free Syrian Army, Al-Nusra Front, Syrian Democratic Forces, and Iraqi Kurdistan's \"Peshmerga\" and People's Protection Units (YPG) along with a 60 nation coalition in its efforts to establish a \"de facto\" state on Iraqi and Syrian territory.\n\nThe members of the Ahmadiyya community believe that the Ahmadiyya Caliphate (Arabic: \"Khilāfah\") is the continuation of the Islamic caliphate, first being the \"Rāshidūn\" (rightly guided) Caliphate (of Righteous Caliphs). This is believed to have been suspended with Ali, the son-in-law of Muhammad and re-established with the appearance of Mirza Ghulam Ahmad (1835–1908, the founder of the movement) whom Ahmadis identify as the Promised Messiah and Mahdi.\n\nAhmadis maintain that in accordance with Quranic verses (such as ) and numerous ahadith on the issue, \"Khilāfah\" can only be established by God Himself and is a divine blessing given to \"those who believe and work righteousness\" and uphold the unity of God, therefore any movement to establish the \"Khilāfah\" centered on human endeavours alone is bound to fail, particularly when the condition of the people diverges from the ‘precepts of prophethood’ and they are as a result disunited, their inability to establish a \"Khilāfah\" caused fundamentally by the lack of righteousness in them. Although the khalifa is elected it is believed that God himself directs the hearts of believers towards an individual. Thus the khalifa is designated neither necessarily by right (i.e. the rightful or competent one in the eyes of the people at that time) nor merely by election but primarily by God.\n\nAccording to Ahmadiyya thought, a khalifa need not be the head of a state; rather the Ahmadiyya community emphasises the spiritual and organisational significance of the Khilāfah. It is primarily a religious/spiritual office, with the purpose of upholding, strengthening and spreading Islam and of maintaining the high spiritual and moral standards within the global community established by Muhammad – who was not merely a political leader but primarily a religious leader. If a khalifa does happen to bear governmental authority as a head of state, it is incidental and subsidiary in relation to his overall function as khalifa which is applicable to believers transnationally and not limited to one particular state.\n\nAhmadi Muslims believe that God has assured them that this Caliphate will endure to the end of time, depending on their righteousness and faith in God. The Khalifa provides unity, security, moral direction and progress for the community. It is required that the Khalifa carry out his duties through consultation and taking into consideration the views of the members of the \"Shura\" (consultative body). However, it is not incumbent upon him to always accept the views and recommendations of the members. The Khalifatul Masih has overall authority for all religious and organisational matters and is bound to decide and act in accordance with the Qur'an and sunnah.\n\nA number of Islamist political parties and mujahideen called for the restoration of the caliphate by uniting Muslim nations, either through political action (e.g., Hizb ut-Tahrir), or through force (e.g., al-Qaeda). Various Islamist movements gained momentum in recent years with the ultimate aim of establishing a Caliphate. In 2014, ISIL/ISIS made a claim to re-establishing the Caliphate. Those advocating the re-establishment of a Caliphate differed in their methodology and approach. Some were locally oriented, mainstream political parties that had no apparent transnational objectives.\n\nAbul A'la Maududi believed the caliph was not just an individual ruler who had to be restored, but was man's representation of God's authority on Earth:\nThe Muslim Brotherhood advocates pan-Islamic unity and the implementation of Islamic law. Founder Hassan al-Banna wrote about the restoration of the Caliphate.\n\nOne transnational group whose ideology was based specifically on restoring the caliphate as a pan-Islamic state is Hizb ut-Tahrir (literally, \"Party of Liberation\"). It is particularly strong in Central Asia and Europe and is growing in strength in the Arab world. It is based on the claim that Muslims can prove that God exists and that the Qur'an is the word of God. Hizb ut-Tahrir's stated strategy is a non-violent political and intellectual struggle.\n\nIn Southeast Asia, groups such as Jemaah Islamiyah aimed to establish a Caliphate across Indonesia, Malaysia, Brunei and parts of Thailand, the Philippines and Cambodia.\n\nAl-Qaeda has as one of its clearly stated goals the re-establishment of a caliphate. Its former leader, Osama bin Laden, called for Muslims to \"establish the righteous caliphate of our umma\". Al-Qaeda chiefs released a statement in 2005, under which, in what they call \"phase five\" there will be \"an Islamic state, or caliphate\". Al-Qaeda has named its Internet newscast from Iraq \"The Voice of the Caliphate\". According to author and Egyptian native Lawrence Wright, Ayman al-Zawahiri, bin Laden's mentor and al-Qaeda's second-in-command until 2011, once \"sought to restore the caliphate... which had formally ended in 1924 following the dissolution of the Ottoman Empire but which had not exercised real power since the thirteenth century.\" Zawahiri believes that once the caliphate is re-established, Egypt would become a rallying point for the rest of the Islamic world, leading the \"jihad\" against the West. \"Then history would make a new turn, God willing\", Zawahiri later wrote, \"in the opposite direction against the empire of the United States and the world's Jewish government\".\n\nScholar Olivier Roy writes that \"early on, Islamists replace the concept of the caliphate ... with that of the emir.\" There were a number of reasons including \"that according to the classical authors, a caliph must be a member of the tribe of the Prophet (the Quraysh) ... moreover, caliphs ruled societies that the Islamists do not consider to have been Islamic (the Ottoman Empire).\" This is not the view of the majority of Islamist groups, as both the Muslim Brotherhood and Hizb ut-Tahrir view the Ottoman state as a caliphate.\n\nIn his book \"The Early Islamic Conquests\" (1981), Fred Donner argues that the standard Arabian practice during the early Caliphates was for the prominent men of a kinship group, or tribe, to gather after a leader's death and elect a leader from amongst themselves, although there was no specified procedure for this shura, or consultative assembly. Candidates were usually from the same lineage as the deceased leader, but they were not necessarily his sons. Capable men who would lead well were preferred over an ineffectual direct heir, as there was no basis in the majority Sunni view that the head of state or governor should be chosen based on lineage alone. Since the Umayyads, all Caliphates have been dynastic.\n\nTraditionally, Sunni Muslim madhhabs all agreed that a Caliph must be a descendant of the Quraysh. Al-Baqillani has said that the leader of the Muslims simply should be from the majority.\n\nFollowing the death of Muhammad, a meeting took place at Saqifah. At that meeting, Abu Bakr was elected caliph by the Muslim community. Sunni Muslims developed the belief that the caliph is a temporal political ruler, appointed to rule within the bounds of Islamic law (Sharia). The job of adjudicating orthodoxy and Islamic law was left to mujtahids, legal specialists collectively called the Ulama. Many Muslims call the first four caliphs the Rashidun, meaning the Rightly-Guided, because they are believed to have followed the Qur'an and the sunnah (example) of Muhammad.\n\nWith the exception of Zaidis, Shi'ites believe in the Imamate, a principle by which rulers are Imams who are divinely chosen, infallible and sinless and must come from the \"Ahl al-Bayt\" regardless of majority opinion, shura or election. They claim that before his death, Muhammad had given many indications, in the hadith of the pond of Khumm in particular, that he considered Ali, his cousin and son-in-law, as his successor. For the Twelvers, Ali and his eleven descendants, the twelve Imams, are believed to have been considered, even before their birth, as the only valid Islamic rulers appointed and decreed by God. Shia Muslims believe that all the Muslim caliphs following Muhammad's death to be illegitimate due to their unjust rule and that Muslims have no obligation to follow them, as the only guidance that was left behind, as ordained in the hadith of the two weighty things, was the Islamic holy book, the Quran and Muhammad's family and offspring, who are believed to be infallible, therefore able to lead society and the Muslim community with complete justice and equity. The Prophet's own grandson, and third Shia Imam, Hussain ibn Ali led an uprising against injustice and the oppressive rule of the Muslim caliph at the time at the Battle of Karbala. Shia Muslims emphasise that values of social justice, and speaking out against oppression and tyranny are not merely moral values, but values essential to a persons religiosity.\nAfter these Twelve Imams, the potential Caliphs, had passed, and in the absence of the possibility of a government headed by their Imams, some Twelvers believe it was necessary that a system of Shi'i Islamic government based on the Guardianship of the Islamic Jurist be developed, due to the need for some form of government, where an Islamic jurist or faqih rules Muslims, suffices. However this idea, developed by the marja' Ayatollah Ruhollah Khomeini and established in Iran, is not universally accepted among the Shia.\n\nIsmailis believe in the Imamate principle mentioned above, but they need not be secular rulers as well.\n\nThe Majlis al-Shura (literally \"consultative assembly\") was a representation of the idea of consultative governance. The importance of this is premised by the following verses of the Qur'an:\n\nThe majlis is also the means to elect a new caliph. Al-Mawardi has written that members of the majlis should satisfy three conditions: they must be just, have enough knowledge to distinguish a good caliph from a bad one and have sufficient wisdom and judgement to select the best caliph. Al-Mawardi also said that in emergencies when there is no caliphate and no majlis, the people themselves should create a majlis and select a list of candidates for caliph; then the majlis should select a caliph from the list of candidates.\n\nSome Islamist interpretations of the role of the Majlis al-Shura are the following:\nIn an analysis of the shura chapter of the Qur'an, Islamist author Sayyid Qutb argues that Islam only requires the ruler to consult with some of the representatives of the ruled and govern within the context of the Sharia. Taqiuddin al-Nabhani, the founder of a transnational political movement devoted to the revival of the Caliphate, writes that although the Shura is an important part of \"the ruling structure\" of the Islamic caliphate, \"(it is) not one of its pillars\", meaning that its neglect would not make a Caliph's rule un-Islamic such as to justify a rebellion. However, the Muslim Brotherhood, the largest Islamic movement in Egypt, has toned down these Islamist views by accepting in principle that in the modern age the Majlis al-Shura is democracy but during its governance of Egypt in 2013, the Muslim Brotherhood did not put that principle into practice.\n\nAl-Mawardi said that if the rulers meet their Islamic responsibilities to the public the people must obey their laws, but a Caliph or ruler who becomes either unjust or severely ineffective must be impeached via the Majlis al-Shura. Al-Juwayni argued that Islam is the goal of the ummah, so any ruler who deviates from this goal must be impeached. Al-Ghazali believed that oppression by a caliph is sufficient grounds for impeachment. Rather than just relying on impeachment, Ibn Hajar al-Asqalani stated that the people have an obligation to rebel if the caliph begins to act with no regard for Islamic law. Ibn Hajar al-Asqalani said that to ignore such a situation is \"haraam\" and those who cannot revolt from inside the caliphate should launch a struggle from outside. Al-Asqalani used two ayahs from the Qur'an to justify this:\nIslamic lawyers commented that when the rulers refuse to step down after being impeached through the Majlis, becoming dictators through the support of a corrupt army, if the majority is in agreement they have the option to launch a revolution. Many noted that this option is to be exercised only after factoring in the potential cost of life.\n\nThe following hadith establishes the principle of rule of law in relation to nepotism and accountability\n\nVarious Islamic lawyers, however, place multiple conditions and stipulations on the execution of such a law, making it difficult to implement. For example, the poor cannot be penalised for stealing out of poverty, and during a time of drought in the Rashidun caliphate, capital punishment was suspended until the effects of the drought passed.\n\nIslamic jurists later formulated the concept that all classes were subject to the law of the land, and no person is above the law; officials and private citizens alike have a duty to obey the same law. Furthermore, a Qadi (Islamic judge) was not allowed to discriminate on the grounds of religion, race, colour, kinship or prejudice. In a number of cases, Caliphs had to appear before judges as they prepared to render their verdict.\n\nAccording to Noah Feldman, a law professor at Harvard University, the system of legal scholars and jurists responsible for the rule of law was replaced by the codification of Sharia by the Ottoman Empire in the early 19th century:\n\nDuring the Muslim Agricultural Revolution, the Caliphate understood that real incentives were needed to increase productivity and wealth and thus enhance tax revenues. A social transformation took place as a result of changing land ownership giving individuals of any gender, ethnic or religious background the right to buy, sell, mortgage and inherit land for farming or any other purpose. Signatures were required on contracts for every major financial transaction concerning agriculture, industry, commerce and employment. Copies of the contract were usually kept by both parties involved.\n\nThere are similarities between Islamic economics and leftist or socialist economic policies. Islamic jurists have argued that privatization of the origin of oil, gas and other fire-producing fuels, as well as lakes, waterways, and grazing land is forbidden. Some have even claimed that \"Pasture\" might be applied to all agricultural land, though they are in the minority. The principle of public or joint ownership has been drawn by Muslim jurists from the following hadith of Muhammad: \"The Muslims are partners in three, water, pastures and fire\" Islamic jurists hold that \"in water, pastures and fire\" includes other natural resources as well, including petroleum, and they specify that \"pastures\" means land that is not privately owned, where people graze their animals. It does not include privately owned farm land, orchards, groves, etc., as it is a well known fact that the Companions of Prophet Muhammad, held privately owned orchards and farm lands in the first Islamic state at Medina. They also make exceptions in the case of processing, packaging, and selling water, as long as there is no dire need for it by the people. The legal ruling by the majority of ulema is that water is public property, while it is still in the lake, river, etc., but when it is put into a container, it becomes the property of the owner of the vessel. According to Saleh Al-Fawzan, \"If a person has collected water in his vessel or in his pond, then he has taken possession of it and it is permissible for him to sell it, because he has collected it and it has come into his possession, and he has expended effort to acquire it, so it has become his property.\" However, if the Muslim community is in dire need of water, then it must be shared, regardless of whether it came from public waterways or a private well.\nAside from similarities to socialism, early forms of proto-capitalism and free markets were present in the Caliphate, since an early market economy and early form of merchant capitalism developed between the 8th and 12th centuries, which some refer to as \"Islamic capitalism\". A vigorous monetary economy developed based on the circulation of a stable high-value currency (the dinar) and the integration of previously independent monetary areas. Business techniques and forms of business organisation employed during this time included early contracts, bills of exchange, long-distance international trade, early forms of partnership (\"mufawada\") such as limited partnerships (\"mudaraba\") and early forms of credit, debt, profit, loss, capital (\"al-mal\"), capital accumulation (\"nama al-mal\"), circulating capital, capital expenditure, revenue, cheques, promissory notes, trusts (\"waqf\"), startup companies, savings accounts, transactional accounts, pawning, loaning, exchange rates, bankers, money changers, ledgers, deposits, assignments, the double-entry bookkeeping system, and lawsuits. Organisational enterprises similar to corporations independent from the state also existed in the medieval Islamic world. Many of these concepts were adopted and further advanced in medieval Europe from the 13th century onwards.\n\nEarly Islamic law included collection of \n\"Zakat\" (charity), one of the Five Pillars of Islam, since the time of the first Islamic State, established by Allah's Messenger at Medina. The taxes (including \"Zakat\" and \"Jizya\") collected in the treasury (\"Bayt al-mal\") of an Islamic government were used to provide income for the needy, including the poor, elderly, orphans, widows and the disabled.\nDuring the Caliphate of Abu Bakr, a number of the Arab tribes, who had accepted Islam at the hand of The Prophet Muhammad, rebelled and refused to continue to pay the Zakat, leading to the Ridda Wars. Caliph Umar added to the duties of the state an allowance, paid on behalf of every man woman and child, starting at birth, creating the world's first state run social welfare program.\nMaya Shatzmiller states that the demographic behavior of medieval Islamic society varied in some significant aspects from other agricultural societies. Nomadic groups within places like the deserts of Egypt and Morocco maintained high birth rates compared to rural and urban populations, though periods of extremely high nomadic birth rates seem to have occurred in occasional \"surges\" rather than on a continuous basis. Individuals living in large cities had much lower birth rates, possibly due to the use of birth control methods and political or economic instability. This led to population declines in some regions. While several studies have shown that Islamic scholars enjoyed a life expectancy of 59–75 years between the eleventh and thirteenth centuries, the overall life expectancy of men in the same societies was lower. Factoring in infant mortality, Lawrence Conrad estimates the average lifespan in the early Islamic caliphate to be above 35 years for the general population, compared to around 40 years for the population of Classical Greece and 31 years for the population of thirteenth century England.\n\nThe early Islamic Empire also had the highest literacy rates among pre-modern societies, alongside the city of classical Athens in the 4th century BC, and later, China after the introduction of printing from the 10th century. One factor for the relatively high literacy rates in the early Islamic Empire was its parent-driven educational marketplace, as the state did not systematically subsidize educational services until the introduction of state funding under Nizam al-Mulk in the 11th century. Another factor was the diffusion of paper from China, which led to an efflorescence of books and written culture in Islamic society; thus papermaking technology transformed Islamic society (and later, the rest of Afro-Eurasia) from an oral to scribal culture, comparable to the later shifts from scribal to typographic culture, and from typographic culture to the Internet. Other factors include the widespread use of paper books in Islamic society (more so than any other previously existing society), the study and memorisation of the Qur'an, flourishing commercial activity and the emergence of the Maktab and Madrasah educational institutions.\n\n\n\n\n\n"}
{"id": "139176", "url": "https://en.wikipedia.org/wiki?curid=139176", "title": "City-state", "text": "City-state\n\nA city-state is a sovereign microstate that usually consists of a single city and its dependent territories. Historically, this included cities such as Rome, Athens, Carthage, and the Italian city-states during the Renaissance. , only a handful of sovereign city-states exist, with some disagreement as to which are city-states. A great deal of consensus exists that the term properly applies currently to Monaco, Singapore, and Vatican City. \n\nA number of other small states share similar characteristics, and therefore are sometimes also cited as modern city-states—namely, Qatar, Brunei, Kuwait, Bahrain, and Malta, which each have an urban center comprising a significant proportion of the population, though all have several distinct settlements and a designated or \"de facto\" capital city. Occasionally, other small states with high population densities, such as San Marino, are also cited, despite lacking a large urban centre characteristic of traditional city-states.\n\nSeveral non-sovereign cities enjoy a high degree of autonomy, and are sometimes considered city-states. Hong Kong and Macau, along with independent members of the United Arab Emirates, most notably Dubai and Abu Dhabi, are often cited as such.\n\nHistorical city-states included Sumerian cities such as Uruk and Ur; Ancient Egyptian city-states, such as Thebes and Memphis; the Phoenician cities (such as Tyre and Sidon); the five Philistine city-states; the Berber city-states of the Garamantes; the city-states of ancient Greece (the poleis such as Athens, Sparta, Thebes, and Corinth); the Roman Republic (which grew from a city-state into a great power); the Mayan and other cultures of pre-Columbian Mesoamerica (including cities such as Chichen Itza, Tikal, Copán and Monte Albán); the central Asian cities along the Silk Road; the city-states of the Swahili coast; Venice; Ragusa; states of the medieval Russian lands such as Novgorod and Pskov; and many others. Danish historian Poul Holm has classed the Viking colonial cities in medieval Ireland, most importantly Dublin, as city-states.\n\nIn Cyprus, the Phoenician settlement of Kition (in present-day Larnaca) was a city-state that existed from around 800 BC until the end of the 4th century BC.\n\nSome of the most well-known examples of city-state culture in human history are the ancient Greek city-states and the merchant city-states of Renaissance Italy, which organised themselves as small independent centers. The success of small regional units coexisting as autonomous actors in loose geographical and cultural unity, as in Italy and Greece, often prevented their amalgamation into larger national units. However, such small political entities often survived only for short periods because they lacked the resources to defend themselves against incursions by larger states (such as Roman conquest of Greece). Thus they inevitably gave way to larger organisations of society, including the empire and the nation-state.\n\nIn the history of mainland Southeast Asia, aristocratic groups, Buddhist leaders, and others organized settlements into autonomous or semi-autonomous city-states. These were referred to as \"mueang\", and were usually related in a tributary relationship now described as mandala or as \"over-lapping sovereignty\", in which smaller city-states paid tribute to larger ones that paid tribute to still larger ones—until reaching the apex in cities like Ayutthaya, Bagan, Bangkok and others that served as centers of Southeast Asian royalty. The system existed until the 19th century, when colonization by European powers occurred. Siam, a regional power at the time, needed to define their territories for negotiation with the European powers so the Siamese government established a nation-state system, incorporated their tributary cities (Lan Xang, Cambodia and some Malay cities) into their territory and abolished the mueang and the tributary system.\n\nIn early Philippine history, the barangay was a complex sociopolitical unit which scholars have historically considered the dominant organizational pattern among the various peoples of the Philippine archipelago. These sociopolitical units were sometimes also referred to as barangay states, but are more properly referred to using the technical term \"polity\". Evidence suggests a considerable degree of independence as city states ruled by Datus, Rajahs and Sultans. Early chroniclers record that the name evolved from the term \"balangay\", which refers to a plank boat widely used by various cultures of the Philippine archipelago prior to the arrival of European colonizers.\n\nIn the Holy Roman Empire (962–1806) over 80 Free Imperial Cities came to enjoy considerable autonomy in the Middle Ages and in early modern times, buttressed legally by international law following the Peace of Westphalia of 1648. Some, like three of the earlier Hanseatic cities – Bremen, Hamburg and Lübeck – pooled their economic relations with foreign powers and were able to wield considerable diplomatic clout. Individual cities often made protective alliances with other cities or with neighbouring regions, including the Hanseatic League (1358 – 17th century), the Swabian League of Cities (1331–1389), the Décapole (1354–1679) in the Alsace, or the Old Swiss Confederacy ( 1300 – 1798). The Swiss cantons of Zürich, Bern, Lucerne, Fribourg, Solothurn, Basel, Schaffhausen, and Geneva originated as city-states.\n\nAfter the dissolution of the Holy Roman Empire in 1806, some cities – then members of different confederacies – officially became sovereign city-states, such as the Free Hanseatic City of Bremen (1806–11 and again 1813–71), the Free City of Frankfurt upon Main (1815–66), the Free and Hanseatic City of Hamburg (1806–11 and again 1814–71), the Free and Hanseatic City of Lübeck (1806–11 and again 1813–71), and the Free City of Kraków (1815–1846). Under Habsburg rule the city of Fiume had the status of a \"corpus separatum\" (1779–1919), which – while falling short of an independent sovereignty – had many attributes of a city-state.\n\nIn the 20th century West Berlin, though lacking sovereignty, functioned from 1948 until 1990 as a state legally not belonging to any other state, but ruled by the Western Allies. They allowed – notwithstanding their overlordship as occupant powers – its internal organisation as one state simultaneously being a city, officially called Berlin (West). Though West Berlin maintained close ties to the West German Federal Republic of Germany, it never legally formed a part of it.\n\n\"Examples: Republic of Venice, Republic of Genoa, Republic of Amalfi, Republic of Florence, Duchy of Milan, Papal States\"\n\nThe Free City of Danzig was a semi-autonomous city-state that existed between 1920 and 1939, consisting of the Baltic Sea port of Danzig (now Gdańsk, Poland) and nearly 200 towns in the surrounding areas. It was created on 15 November 1920 under the terms of Article 100 (Section XI of Part III) of the 1919 Treaty of Versailles after the end of World War I.\n\nAfter a prolonged period where the city of Fiume enjoyed considerable autonomy under Habsburg rule (see Corpus separatum (Fiume)), The Free State of Fiume was proclaimed as a fully independent free state which existed between 1920 and 1924. Its territory of 28 km (11 sq mi) comprised the city of Fiume (now in Croatia and, since the end of World War II, known as Rijeka) and rural areas to its north, with a corridor to its west connecting it to Italy.\n\nThe Shanghai International Settlement (1845–1943) was an international zone with its own legal system, postal service, and currency.\n\nThe international zone within the city of Tangier, in North Africa was approximately 373 km (144 sq mi). It was at first under the joint administration of France, Spain, and the United Kingdom, plus later Portugal, Italy, Belgium, the Netherlands, Sweden, and the United States. The international zone was initially attached to Morocco. It then became a French-Spanish protectorate from 1923 until the 29 of October 1956 when it was reintegrated into the state of Morocco.\n\nThe Klaipėda Region or Memel Territory was defined by the Treaty of Versailles in 1920 when it was put under the administration of the Council of Ambassadors. The Memel Territory was to remain under the control of the League of Nations until a future day when the people of the region would be allowed to vote on whether the land would return to Germany or not. The then predominantly ethnic German Memel Territory (Prussian Lithuanians and Memellanders constituted the other ethnic groups), situated between the river and the town of that name, was occupied by Lithuania in the Klaipėda Revolt of 1923.\n\nThe Free Territory of Trieste was an independent territory situated in Central Europe between northern Italy and Yugoslavia, facing the north part of the Adriatic Sea, under direct responsibility of the United Nations Security Council in the aftermath of World War II, from 1947 to 1954. The UN attempted to make the Free Territory of Trieste into a city state, but it never gained real independence and in 1954 its territory was divided between Italy and Yugoslavia.\n\nUnder the United Nations Partition Plan for Palestine of 1947, Mandatory Palestine was to be partitioned into three states: a Jewish state of Israel, an Arab state of Palestine, and a \"corpus separatum\" (Latin for \"separated body\") consisting of a Jerusalem city-state under the control of United Nations Trusteeship Council. Although the plan had some international support and the UN accepted this proposal (and still officially holds the stance that Jerusalem should be held under this regime), implementation of the plan failed as the 1948 Palestine war broke out with the 1947–48 Civil War in Mandatory Palestine, ultimately resulting in Jerusalem being split into West Jerusalem and East Jerusalem. Israel would eventually gain control of East Jerusalem in the Six-Day War in 1967.\n\nThe Principality of Monaco is an independent city-state. Monaco-Ville (the ancient fortified city) and Monaco's well-known area Monte Carlo are districts of a continuous urban zone, not distinct cities, though they were three separate municipalities (\"communes\") until 1917. The Principality of Monaco and the city of Monaco (each having specific powers) govern the same territory. Though they maintain a small military, they would still have to rely on France for defence in the face of an aggressive world power.\n\nSingapore is an island city-state in Southeast Asia. About 5.6 million people live and work within , making Singapore the 2nd-most-densely populated country in the world after Monaco. Singapore was part of Malaysia before it was expelled from the federation in 1965, becoming an independent republic, a city and a sovereign country. \"The Economist\" refers to the nation as the \"world's only fully functioning city-state\". In particular, it has its own currency and a full armed forces for deterrence to safeguard the nation's sovereignty against potential aggressors.\n\nUntil September 1870, the city of Rome had been controlled by the pope as part of his Papal States. When King Victor Emmanuel II seized the city in 1870, Pope Pius IX refused to recognize the newly formed Kingdom of Italy.\n\nBecause he could not travel without effectively acknowledging the authority of the king, Pius IX and his successors each claimed to be a \"Prisoner in the Vatican\", unable to leave the papal enclave once they had ascended the papal thrones.\n\nThe impasse was resolved in 1929 by the Lateran Treaties negotiated by the Italian dictator Benito Mussolini between King Victor Emmanuel III and Pope Pius XI. Under this treaty, the Vatican was recognized as an independent state, with the Pope as its head. The Vatican City State has its own citizenship, diplomatic corps, flag, and postage stamps. With a population of less than 1,000 (mostly clergymen), it is by far the smallest sovereign country in the world.\n\nSome cities or urban areas, while not sovereign states, may nevertheless enjoy such a high degree of autonomy that they function as \"city-states\" within the context of the sovereign state to which they belong. Historian Mogens Herman Hansen describes this aspect of self-government as: \"The city-state is a self-governing, but not necessarily independent political unit.\"\n\nTwo cities in Germany, namely Berlin and Hamburg, are considered city-states (German: \"Stadtstaaten\"). Additionally, the state of Bremen is often called a city-state although it consists of the two cities of Bremen and Bremerhaven, which are separated by the state of Lower Saxony. Together with thirteen area states (German: \"Flächenländer\") they form the sixteen federal states of Germany. Hamburg and Bremen are \"Free and Hanseatic Cities\".\n\nGenerally, the city-states have no other rights or duties than the other states. Through the financial redistribution system of Equalization Payments in Germany (German: \"Länderfinanzausgleich\"), they do receive more money because of their demographic characteristics. The city-states are most distinctive due to the names of their state organs: their governments are called Senate, the prime ministers 'mayor' (Governing Mayor in Berlin and First Mayor in Hamburg) or President of the Senate (in Bremen) and also the expressions for their state parliaments differ from the other states.\n\nIn the 18th century many German cities were free imperial cities (German: \"Reichsstädte\"), without a principality between them and the imperial level. After the Napoleonic era, in 1815, four were still city-states: Hamburg, Bremen and Lübeck in Northern Germany, and Frankfurt where the Federal Convention was located. Frankfurt was incorporated by Prussia in 1866, and Lübeck became a part of Prussia during the national socialist regime in 1937 (Greater Hamburg Law). After 1945, Berlin was a divided city, and the Western part became a German quasi-state under (Western) Allied supervision. Since 1990/1991, the reunited Berlin is an ordinary German state among others.\n\n\n"}
{"id": "6916", "url": "https://en.wikipedia.org/wiki?curid=6916", "title": "Colony", "text": "Colony\n\nA colony is a territory under the immediate complete political control and occupied by settlers of a state, distinct from the home territory of the sovereign. For colonies in antiquity, city-states would often found their own colonies. Some colonies were historically countries, while others were territories without definite statehood from their inception.\n\nThe metropolitan state is the state that rules the colony. In Ancient Greece, the city that founded a colony was known as the metropolis. \"Mother country\" is a reference to the metropolitan state from the point of view of citizens who live in its colony. There is a United Nations list of Non-Self-Governing Territories.\n\nUnlike a puppet state or satellite state, a colony has no independent international representation, and its top-level administration is under direct control of the metropolitan state.\n\nThe term \"informal colony\" is used by some historians to refer to a country under the \"de facto\" control of another state, although this term is often contentious.\n\nThe word \"colony\" comes from the Latin word \"colōnia\". This in turn derives from the word \"colōnus\", which means colonist but also implies a farmer. Cologne is an example of a settlement preserving this etymology. Other, less obvious settlements that began as Roman colonia include cities from Belgrade to York. A tell-tale sign of a settlement once being a Roman colony is a city centre with a grid pattern. The terminology is taken from architectural analogy, where a column pillar is beneath the (often stylized) head capital, which is also a biological analog of the body as subservient beneath the controlling head (with 'capital' coming from the Latin word \"caput\", meaning 'head'). So colonies are not independently self-controlled, but rather are controlled from a separate entity that serves the capital function.\n\nRoman colonies first appeared when the Romans conquered neighbouring Italic peoples. These were small farming settlements that appeared when the Romans had subdued an enemy in war. A colony could take many forms, as a trade outpost or a military base in enemy territory. Its original definition as a settlement created by people migrating from a central region to an outlying one became the modern definition.\n\n\n\nThe Special Committee on Decolonization maintains the United Nations list of Non-Self-Governing Territories, which identifies areas the United Nations (though not without controversy) believes are colonies. Given that dependent territories have varying degrees of autonomy and political power in the affairs of the controlling state, there is disagreement over the classification of \"colony\".\n\n\n\n"}
