{"id": "172180", "url": "https://en.wikipedia.org/wiki?curid=172180", "title": "French Directory", "text": "French Directory\n\nThe Directory (also called Directorate, ) was the five-member committee that governed France from 2 November 1795 until 9 November 1799, when it was overthrown by Napoleon Bonaparte in the Coup of 18 Brumaire, and replaced by the Consulate. It gave its name to the final four years of the French Revolution. Mainstream historiography also uses the term in reference to the period from the dissolution of the National Convention on 26 October 1795 (4 Brumaire) to Napoleon's coup d’état. \n\nThe Directory was continually at war with foreign coalitions which at different times included Britain, Austria, Prussia, the Kingdom of Naples, Russia and the Ottoman Empire. It annexed Belgium and the left bank of the Rhine, while Bonaparte conquered a large part of Italy. The Directory established 196 short-lived sister republics modelled after France, in Italy, Switzerland and the Netherlands. The conquered cities and states were required to send to France huge amounts of money, as well as art treasures, which were used to fill the new Louvre museum in Paris. An army led by Bonaparte tried to conquer Egypt and marched as far as Saint-Jean-d'Acre in Syria. The Directory defeated a resurgence of the War in the Vendée, the royalist-led civil war in the Vendée region, but failed in its venture to support the Irish Rebellion of 1798 and create an Irish Republic.\n\nThe French economy was in continual crisis during the Directory. At the beginning, the treasury was empty; the paper money, the Assignat, had fallen to a fraction of its value, and prices soared. The Directory stopped printing assignats and restored the value of the money, but this caused a new crisis; prices and wages fell, and economic activity slowed to a standstill.\n\nIn its first two years, the Directory concentrated on ending the excesses of the Jacobin Reign of Terror; mass executions stopped, and measures taken against exiled priests and royalists were relaxed. The Jacobin political club was closed and the government crushed an armed uprising planned by the Jacobins and an early socialist revolutionary, François-Noël Babeuf, known as \"\"Gracchus\" Babeuf\". However, following the discovery of a royalist conspiracy including a prominent general, Jean-Charles Pichegru, the Jacobins took charge of the new Councils and hardened the measures against the Church and émigrés. The Jacobins took two additional seats in the Directory, hopelessly dividing it.\n\nIn 1799, after several defeats, French victories in the Netherlands and Switzerland restored the French military position, but the Directory had lost the support of all the political factions. Bonaparte returned from Egypt in October, and was engaged by the Abbé Sieyès and others to carry out a parliamentary coup d'état on 8–9 November 1799. The coup abolished the Directory, and replaced it with the French Consulate led by Bonaparte.\n\nOn 27 July 1794, members of the French Convention, the revolutionary parliament of France, rose up against its leader Maximilien Robespierre, who was in the midst of executing thousands of suspected enemies of the Revolution. Robespierre and his leading followers were declared outside the law, and on 28 July were arrested and guillotined the same day. The Revolutionary Tribunal, which had sent thousands to the guillotine, ceased meeting and its head, Fouquier-Tinville, was arrested and imprisoned, and after trial was himself sentenced to death. More than five hundred suspected counter-revolutionaries awaiting trial and execution were immediately released.\n\nIn July 1794, the members of the Convention began planning a new form of government and drafting a new Constitution, which would become the Constitution of the Year III (August 1795). An important aim was to prevent too much power from becoming concentrated in the hands of one man. One of the authors of the new Constitution, François Antoine de Boissy d'Anglas, wrote to the Convention:\nWe propose to you to compose an executive power of five members, renewed with one new member each year, called the Directory. This executive will have a force concentrated enough that it will be swift and firm, but divided enough to make it impossible for any member to even consider becoming a tyrant. A single chief would be dangerous. Each member will preside for three months; he will have during this time the signature and seal of the head of state. By the slow and gradual replacement of members of the Directory, you will preserve the advantages of order and continuity and will have the advantages of unity without the inconveniences.\n\nThe Constitution of the Year III (22 August 1795) began with the 1789 Declaration of the Rights of Man and of the Citizen and declared that \"the Rights of Man in society are liberty, equality, security, and property\". It guaranteed freedom of religion, freedom of the press, and freedom of labour, but forbade armed assemblies and even public meetings of political societies. Only individuals or public authorities could tender petitions.\n\nThe judicial system was reformed, and judges were given short terms of office: two years for justices of the peace, five for judges of department tribunals. They were elected, and could be re-elected, to assure their independence from the other branches of government.\n\nThe new legislature had two houses, a Council of Five Hundred and a Council of Ancients with two hundred fifty members. Electoral assemblies in each \"canton\" of France, which brought together a total of thirty thousand qualified electors, chose representatives to an electoral assembly in each department, which then elected the members of both houses. The members of this legislature had a term of three years, with one-third of the members renewed every year. The Ancients could not initiate new laws, but could veto those proposed by the Council of Five Hundred.\n\nThe Constitution established a unique kind of executive, a five-man Directory chosen by the legislature. It required the Council of Five Hundred to prepare, by secret ballot, a list of candidates for the Directory. The Council of Ancients then chose, again by secret ballot, the Directors from that provided list. The Constitution required that Directors be at least forty years old. To assure gradual but continual change, one Director, chosen by lot, was replaced each year. Ministers for the various departments of State aided the Directors. These ministers did not form a council or cabinet and had no general powers of government.\n\nThe new Constitution sought to create a separation of powers; the Directors had no voice in legislation or taxation, nor could Directors or Ministers sit in either house. To assure that the Directors would have some independence, each would be elected by one portion of the legislature, and they could not be removed by the legislature unless they violated the law.\n\nUnder the new Constitution of 1795, to be eligible to vote in the elections for the Councils, voters were required to meet certain minimum property and residency standards. In towns with over six thousand population, they had to own or rent a property with a revenue equal to the standard income for at least one hundred fifty or two hundred days of work, and to have lived in their residence for at least a year. This ruled out a large part of the French population.\n\nThe big loser under the new system was the City of Paris, which had dominated events in the first part of the Revolution. On 24 August 1794, the committees of the sections of Paris, the bastions of the Jacobins, which had provided most of the manpower for demonstrations and invasions of the Convention, were abolished. Shortly afterwards, on 31 August, the municipality of Paris, which had been the domain of Danton and Robespierre, was abolished, and the city placed under direct control of the national government. When the Law of 19 Vendémiaire Year IV (11 October 1795), in application of the new Constitution, created the first twelve \"arrondissements\" of Paris, it established twelve new committees, one for each \"arrondissement\". The city became a new department, the department of the Seine, replacing the former department of Paris created in 1790.\n\nMeanwhile, the leaders of the still ruling National Convention tried to meet challenges from both neo-Jacobins on the left and royalists on the right. On 21 September 1794, the remains of Jean-Paul Marat, whose furious articles had promoted the Reign of Terror, were placed with great ceremony in the Panthéon, while on the same day, the moderate Convention member Merlin de Thionville described the Jacobins as \"A hangout of outlaws\" and the \"knights of the guillotine\". Young men known as Muscadins, largely from middle-class families, attacked the Jacobin and radical clubs. The new freedom of the press saw the appearance of a host of new newspapers and pamphlets from the left and the right, such as the royalist \"L'Orateur du peuple\" edited by Stanislas Fréron, an extreme Jacobin who had moved to the extreme right, and at the opposite end of the spectrum, the \"Tribun du peuple\", edited by Gracchus Babeuf, a former priest who advocated an early version of socialism. On 5 February 1795, the semi-official newspaper \"Le Moniteur Universel\" (\"Le Moniteur\") attacked Marat for encouraging the bloody extremes of the Reign of Terror. Marat's remains were removed from the Panthéon two days later. The surviving Girondin deputies, whose leaders had been executed during the Reign of Terror, were brought back into the Convention on 8 March 1795.\n\nThe Convention tried to bring a peaceful end to the Catholic and royalist uprising in the Vendée. The Convention signed an amnesty agreement, promising to recognize the freedom of religion and allowing territorial guards to keep their weapons if the \"Vendéens\" would end their revolt. On a proposal from Boissy d'Anglas, on 21 February 1795 the Convention formally proclaimed the freedom of religion and the separation of church and state.\n\nBetween July 1794 and the October 1795 elections for the new-style Parliament, the government tried to obtain international peace treaties and secure French gains. In January 1795 General Pichegru took advantage of an extremely cold winter and invaded the Dutch Republic. He captured Utrecht on 18 January, and on 14 February units of French cavalry captured the Dutch fleet, which was trapped in the ice at Den Helder. The Dutch government asked for peace, conceding Dutch Flanders, Maastricht and Venlo to France. On 9 February, after a French offensive in the Alps, the Grand Duke of Tuscany signed a treaty with France. Soon afterwards, on 5 April, France signed a peace treaty, the Peace of Basel, with Prussia, where King Frederick William II was tired of the war; Prussia recognized the French occupation of the western bank of the Rhine. On 22 July 1795, a peace agreement, the \"Treaty of Basel\", was signed with Spain, where the French army had marched as far as Bilbao. By the time the Directory was chosen, the coalition against France was reduced to Britain and Austria, which hoped that Russia might be brought in on its side.\n\nOn 20 May 1795 (1 Prairial Year III), the Jacobins attempted to seize power in Paris. Following the model of Danton's seizure of the National Assembly in June, 1792, a mob of sans-culottes invaded the meeting hall of the Convention at the Tuileries, killed one deputy, and demanded that a new government be formed. This time the army moved swiftly to clear the hall. Several deputies who had taken the side of the invaders were arrested. The uprising continued the following day, as the sans-culottes seized the \"Hôtel de Ville\" as they had done in earlier uprisings, but with little effect; crowds did not move to support them. On the third day, 22 May, the army moved into and occupied the working-class neighborhood of the faubourg Saint-Antoine. The sans-culottes were disarmed and their leaders were arrested. In the following days the surviving members of the Committee of Public Safety, the committee that had been led by Robespierre, were arrested, with the exception of Carnot and two others. Six of the deputies who had participated in the uprising and had been sentenced to death committed suicide before they were taken to the guillotine.\n\nOn 23 June 1795, the Chouans, royalist and Catholic rebels in Brittany, formed an army of 14,000 men near Quiberon. With the assistance of the British navy, a force of two thousand royalists was landed at Quiberon. The French army under General Hoche reacted swiftly, forcing the royalists to take refuge on the peninsula and then to withdraw. They surrendered on 21 July; 748 of the rebels were executed by firing squad.\n\nThe new Constitution of the Year III was presented to the Convention and debated between 4 July – 17 August 1795, and was formally adopted on 22 August 1795. It was a long document, with 377 articles, compared with 124 in the first French Constitution of 1793. Even before it took effect, however, the members of the Convention took measures to assure they would still have dominance in the legislature over the government. They required that in the first elections, two hundred and fifty new deputies would be elected, while five hundred members of the old Convention would remain in place until the next elections. A national referendum of eligible voters was then held. The total number of voters was low; of five million eligible voters, 1,057,390 electors approved the Constitution, and 49,978 opposed it. The proposal that two thirds of members of the old Convention should remain in place was approved by a much smaller margin, 205,498 to 108,754. \n\nThe new Constitution of the Year III was officially proclaimed in force on 23 September 1795, but the new Councils had not yet been elected, and the Directors had not yet been chosen. The leaders of the royalists and constitutional monarchists chose this moment to try to seize power. They saw that the vote in favor of the new Constitution was hardly overwhelming. Paris voters were particularly hostile to the idea of keeping two-thirds of the old members of the Convention in the new Councils. A central committee was formed, with members from the wealthier neighborhoods of Paris, and they began planning a march on the center of the city and on the Tuileries, where the Convention still met.\n\nThe members of the Convention, very much experienced with conspiracies, were well aware that the planning was underway. A group of five republican deputies, led by Paul Barras, had already formed an unofficial directory, in anticipation of the creation of the real one. They were concerned about the national guard members from western Paris, and were unsure about the military commander of Paris, General Menou. Barras decided to turn to military commanders in his entourage who were known republicans, particularly Bonaparte, whom he had known when Bonaparte was successfully fighting the British in Toulon. Bonaparte, at this point a general of second rank in the Army of the Interior, was ordered to defend the government buildings on the right bank.\n\nThe armed royalist insurgents planned a march in two columns along both the right bank and left bank of the Seine toward the Tuileries. There on October 5, 1795, the royalists were met by the artillery of General Joachim Murat at the Sablons and by Bonaparte's soldiers and artillery in front of the church of Saint-Roch. The whiff of grapeshot of Bonaparte's cannons and gunfire of his soldiers brutally mowed down advancing columns, killing some four hundred insurgents, and ended the rebellion. Bonaparte was promoted to General of Division on 16 October, and General in Chief of the Army of the Interior on 26 October. It was the last uprising to take place in Paris during the French Revolution.\n\nBetween 12 and 21 October 1795, immediately after the suppression of royalist uprising in Paris, the elections for the new Councils decreed by the new Constitution took place. Three-hundred seventy nine members of the old Convention, for the most part moderate republicans, were elected to the new legislature. To assure that the Directory did not abandon the Revolution entirely, the Council required that all of the members of the Directory be former members of the Convention and regicides, those who had voted for the execution of Louis XVI.\n\nDue to the rules established by the Convention, a majority of members of the new legislature, 381 of 741 deputies, had served in the Convention and were ardent republicans, but a large part of the new deputies elected were royalists, 118 versus 11 from the left. The members of the upper house, the Council of Ancients, were chosen by lot from among all of the deputies.\n\nOn 31 October 1795, the Council of Ancients chose the first Directory from a list of candidates submitted by the Council of Five Hundred. One person elected, the Abbé Sieyès, refused to take the position, saying it didn't suit his interests or personality. A new member, Lazare Carnot, was elected in his place.\n\nThe members elected to the Directory were the following:\n\nThe following day, the members of the new government took over their offices in the Luxembourg Palace, which had previously been occupied by the Committee of Public Safety. Nothing had been prepared, and the rooms had no furniture: they managed to find firewood to heat the room, and a table in order to work. Each member took charge of a particular sector: Rewbell diplomacy; Carnot and Le Tourneur military affairs, La Révellière-Lépeaux religion and public instruction, and Barras internal affairs.\n\nThe Council of Ancients was attributed the building at the Tuileries Palace formerly occupied by the Convention, while the Council of Five Hundred deliberated in the \"Salle du Manège\", the former riding school west of the palace in the Tuileries Garden. One of the early decisions of the new parliament was to designate uniforms for both houses: the Five Hundred wore long white robes with a blue belt, a scarlet cloak and a hat of blue velour, while members of the Ancients wore a robe of blue-violet, a scarlet sash, a white mantle, and a violet hat.\nThe new Director overseeing financial affairs, La Réveillière-Lépeaux, gave a succinct description of the financial state of France when the Directory took power: \"The national Treasury was completely empty; not a single sou remained. The assignats were almost worthless; the little value which remained drained away each day with accelerated speed. One could not print enough money in one night to meet the most pressing needs of the next day... The public revenues were nonexistent; citizens had lost the habit of paying taxes. [...] All public credit was dead and all confidence lost. [...] The depreciation of the \"assignats\", the frightening speed of the fall, reduced the salary of all public employees and functionaries to a value which was purely nominal.\"\n\nThe drop in value in the money was accompanied by extraordinary inflation. The \"Louis d'or\" (gold coin), which was worth 2000 \"livres\" in paper money at the beginning of the Directory, increased to 3000 and then 5000 \"livres\". The price of a liter of wine increased from 50 sous in October 1795 to ten francs and then thirty francs. A measure of flour worth two \"livres\" in 1790 was worth 225 \"livres\" in October 1794.\n\nThe new government continued to print \"assignats\", which were based on the value of property confiscated from the Church and the aristocracy, but it could not print them fast enough; even when it printed one hundred million in a day, it covered only one-third of the government's needs. To fill the treasury, the Directory resorted in December 1795 to a forced loan of 600 million \"livres\" from wealthy citizens, who were required to pay between 50 and 6000 \"livres\" each.\n\nTo fight inflation, the government began minting more coins of gold and silver, which had real value; the government had little gold but large silver reserves, largely in the form of silverware, candlesticks and other objects confiscated from the churches and the nobility. It minted 72 million écus, and when this silver supply ran low, it obtained much more gold and silver through military campaigns outside of France, particularly from Bonaparte's army in Italy. Bonaparte demanded gold or silver from each city he conquered, threatening to destroy the cities if they did not pay.\n\nThese measures reduced the rate of inflation. On 19 February 1796, the government held a ceremony in the Place Vendôme to destroy the printing presses which had been used to produce huge quantities of \"assignats\". This success produced a new problem: the country was still flooded with more than two billion four hundred million (2.400.000.000) \"assignats\", claims on confiscated properties, which now had some value. Those who held \"assignats\" were able to exchange them for state mandates, which they could use to buy châteaux, church buildings and other biens nationaux (state property) at extremely reduced prices. Speculation became rampant, and property in Paris and other cities could change hands several times a day.\n\nAnother major problem faced by the Directory was the enormous public debt, the same problem that had led to the Revolution in the first place. In September–December 1797, the Directory attacked this problem by declaring bankruptcy on two-thirds of the debt, but assured payment on the other third. This resulted in the ruin of those who held large quantities of government bonds, but stabilized the currency. To keep the treasury full, the Directory also imposed new taxes on property owners, based on the number of fireplaces and chimneys, and later on the number of windows, of their residences. It refrained from adding more taxes on wine and salt, which had helped cause the 1789 revolution, but added new taxes on gold and silver objects, playing cards, tobacco, and other luxury products. Through these means, the Directory brought about a relative stability of finances which continued through the Directory and Consulate. \n\nThe food supply for the population, and particularly for the Parisians, was a major economic and political problem before and during the Revolution; it had led to food riots in Paris and attacks on the Convention. To assure the supply of food to the sans-culottes in Paris, the base of support of the Jacobins, the Convention had strictly regulated grain distribution and set maximum prices for bread and other essential products. As the value of the currency dropped, the fixed prices soon did not cover the cost of production, and supplies dropped. The Convention was forced to abolish the maximum on 24 December 1794, but it continued to buy huge quantities of bread and meat which it distributed at low prices to the Parisians. This Paris food distribution cost a large part of the national budget, and was resented by the rest of the country, which did not have that benefit. By early 1796, the grain supply was supplemented by deliveries from Italy and even from Algeria. Despite the increased imports, the grain supply to Paris was not enough. The Ministry of the Interior reported on 23 March 1796 that there was only enough wheat to make bread for five days, and there were shortages of meat and firewood. The Directory was forced to resume deliveries of subsidized food to the very poor, the elderly, the sick, and government employees. The food shortages and high prices were one factor in the growth of discontent and the Gracchus Babeuf's uprising, the Conspiracy of the Equals, in 1796. The harvests were good in the following years and the food supplies improved considerably, but the supply was still precarious in the north, the west, the southeast, and the valley of the Seine.\n\nIn 1795, the Directory faced a new threat from the left, from the followers of François Noël Babeuf, a talented political agitator who took the name \"Gracchus\" and was the organizer of what became known as the Conspiracy of the Equals. Babeuf had, since 1789, been drawn to the Agrarian Law, an agrarian reform preconized by the ancient Roman brothers, Tiberius and Gaius Gracchus, of sharing goods in common, as means of achieving economic equality. By the time of the fall of Robespierre, he had abandoned this as an impractical scheme and was moving towards a more complex plan. Babeuf did not call for the abolition of private property, and wrote that peasants should own their own plots of land, but he advocated that all wealth should be shared equally: all citizens who were able would be required to work, and all would receive the same income. Babeuf did not believe that the mass of French citizens was ready for self-government; accordingly, he proposed a dictatorship under his leadership until the people were educated enough to take charge. \"People!\", Babeuf wrote. \"Breathe, see, recognize your guide, your defender... Your tribune presents himself with confidence.\" \n\nAt first, Babeuf's following was small; the readers of his newspaper, \"Le Tribun du peuple\" (\"The Tribune of the People\"), were mostly middle-class far-left Jacobins who had been excluded from the new government. However, his popularity increased in the working-class of the capital with the drop in value of the \"assignats\", which rapidly resulted in the decrease of wages and the rise of food prices. Beginning in October 1795, he allied himself with the most radical Jacobins, and on 29 March 1796 formed the \"Directoire secret des Égaux\" (\"Secret Directory of Equals\"), which proposed to \"revolutionize the people\" through pamphlets and placards, and eventually to overthrow the government. He formed an alliance of utopian socialists and radical Jacobins, including Félix Lepeletier, Pierre-Antoine Antonelle, Sylvain Marechal, Jean-Pierre-André Amar and Jean-Baptiste Robert Lindet. The Conspiracy of Equals was organized in a novel way: in the center was Babeuf and the Secret Directory, who hid their identities, and shared information with other members of the Conspiracy only via trusted intermediaries. This conspiratorial structure was later adopted by Marxist movements. Despite his precautions, the Directory infiltrated an agent into the conspiracy, and was fully informed of what he was doing. Bonaparte, the newly named commander of the Army of the Interior, was ordered to close the Panthéon Club, the major meeting place for the Jacobins in Paris, which he did on 27 February 1796. The Directory took other measures to prevent an uprising; the Legion of Police (\"légion de police\"), a local police force dominated by Jacobins, was forced to become a part of the Army, and the Army organized a mobile column to patrol the neighborhoods and stop uprisings.\n\nBefore Babeuf and his conspiracy could strike, he was betrayed by a police spy and arrested in his hiding place on 10 May 1796. Though he was a talented agitator, he was a very poor conspirator; with him in his hiding place were the complete records of the conspiracy, with all of the names of the conspirators. Despite this setback, the conspiracy went ahead with its plans. On the night of 9–10 September 1796, between 400 and 700 Jacobins went to the 21st Regiment of Dragoons (\"21e régiment de dragons\") army camp at Grenelle and tried to incite an armed rebellion against the Convention. At the same time a column of militants was formed in the working-class neighborhoods of Paris to march on the Luxembourg Palace, headquarters of the Directory. Director Carnot had been informed the night before by the commander of the camp, and a unit of dragoons was ready. When the attack began at about ten o'clock, the dragoons appeared suddenly and charged. About twenty Jacobins were killed, and the others arrested. The column of militants, learning what had happened, disbanded in confusion. The widespread arrest of Babeuf's militants and Jacobins followed. The practice of arresting suspects at their homes at night, stopped after the downfall of Robespierre, was resumed on this occasion.\n\nDespite his arrest, Babeuf, in jail, still felt he could negotiate with the government. He wrote to the Directory: \"Citizen Directors, why don't you look above yourselves and treat with me as with an equal power? You have seen now the vast confidence of which I am the center... this view makes you tremble.\" Several attempts were made by Babeuf's followers to free him from prison. He was finally moved to Vendôme for trial. The Directory did not tremble. The accused Jacobins were tried by military courts between 19 September and 27 October. Thirty Jacobins, including three former deputies of the Convention, were convicted and guillotined. Babeuf and his principal followers were tried in Vendôme between 20 February and 26 May 1797. The two principal leaders, Babeuf and Darthé, were convicted. They both attempted suicide, but failed and were guillotined on 27 May 1797. However, in the following months, the Directory and Councils gradually turned away from the royalist right and tried to find new allies on the left.\n\nThe major preoccupation of the Directory during its existence was the war against the coalition of Britain and Austria. The military objective set by the Convention in October 1795 was to enlarge France to what were declared its natural limits: the Pyrenees, the Rhine and the Alps, the borders of Gaul at the time of the Roman Empire. In 1795, Prussia, Spain and the Dutch Republic quit the War of the First Coalition and the war and made peace with France, but Great Britain refused to accept the French annexation of Belgium. Beside Britain and Austria, the only enemies remaining for France were the kingdom of Sardinia and several small Italian states. Austria proposed a European congress to settle borders, but the Directory refused, demanding direct negotiations with Austria instead. Under British pressure, Austria agreed to continue the war against France.\n\nLazare Carnot, the Director who oversaw military affairs, planned a new campaign against Austria, using three armies: General Jourdan's Army of Sambre-et-Meuse on the Rhine and General Moreau's Army of the Rhine and Moselle on the Danube would march to Vienna and dictate a peace. A third army, the Army of Italy under General Bonaparte, who had risen in rank with spectacular speed due to his defense of the government from a royalist uprising, would carry out a diversionary operation against Austria in northern Italy. Jourdan's army captured Mayence and Frankfurt, but on 14 August 1796 was defeated by the Austrians at the Battle of Amberg and again on 3 September 1796 at the Battle of Würzburg, and had to retreat back to the Rhine. General Moreau, without the support of Jourdan, was also forced to retreat.\n\nThe story was much different in Italy. Bonaparte, though he was only twenty-eight years old, was named commander of the Army of Italy on 2 March 1796, through the influence of Barras, his patron in the Directory. Bonaparte faced the combined armies of Austria and Sardinia, which numbered seventy thousand men. Bonaparte slipped his army between them and defeated them in a series of battles, culminating at the Battle of Mondovi where he defeated the Sardinians on 22 April 1796, and the Battle of Lodi, where he defeated the Austrians on 10 May. The king of Sardinia and Savoy was forced to make peace in May 1796 and ceded Nice and Savoy to France.\n\nAt the end of 1796, Austria sent two new armies to Italy to expel Bonaparte, but Bonaparte outmaneuvered them both, winning a first victory at the Battle of Arcole on 17 November 1796, then at the Battle of Rivoli on 14 January 1797. He forced Austria to sign the Treaty of Campo Formio (October 1797), whereby the emperor ceded Lombardy and the Austrian Netherlands to the French Republic in exchange for Venice and urged the Diet to surrender the lands beyond the Rhine.\n\nThe Directory was eager to form a coalition with Spain to block British commerce with the continent and to close the Mediterranean Sea to British ships. By the Treaty of San Ildefonso, concluded in August 1796, Spain became the ally of France, and on 5 October, it declared war on Britain. The British fleet under Admiral Jervis defeated the Spanish fleet at the Cape St Vincent, keeping the Mediterranean open to British ships, but the United Kingdom was brought into such extreme peril by the mutinies in its fleet that it offered to acknowledge the French conquest of the Netherlands and to restore the French colonies.\n\nThe Directory also sought a new way to strike British interests and to repay the 1707-1800 Kingdom of Great Britain for the support it gave to royalist insurgents in Brittany, France. A French fleet of 44 vessels departed Brest on 15 December 1796, carrying an expeditionary force of 14,000 soldiers, led by General Hoche to Ireland, where they hoped to join forces with Irish rebels to expel the British from the 1542-1800 Kingdom of Ireland. However, the fleet was separated by storms off the Irish coast and, being unable to land on Ireland, had to return to home port with 31 vessels and 12,000 surviving soldiers.\n\nThe first elections held after the formation of the Directory were held in March and April 1797, in order to replace one-third of the members of the Councils. The elections were a crushing defeat for the old members of the Convention; 205 of the 216 were defeated. Only eleven former deputies from the Convention were reelected, several of whom were royalists. The elections were a triumph for the royalists, particularly in the south and in the west; after the elections there were about 160 royalist deputies, divided between those who favored a return to an absolute monarchy, and those who wished a constitutional monarchy on the British model. The constitutional monarchists elected to the Council included Pierre Samuel du Pont de Nemours, who later emigrated to the United States with his family, and whose son, Éleuthère Irénée du Pont, founded the \"E. I. du Pont de Nemours and Company\", now known as DuPont. In Paris and other large cities, the candidates of the left dominated. General Jean-Charles Pichegru, a former Jacobin and ordinary soldier who had become one of the most successful generals of the Revolution, was elected president of the new Council of Five Hundred. François Barbé-Marbois, a diplomat and future negotiator of the sale of Louisiana to the United States, was elected president of the Council of Ancients.\nRoyalism was not strictly legal, and deputies could not announce themselves as such, but royalist newspapers and pamphlets soon appeared, there were pro-monarchy demonstrations in theaters, and royalists wore identifying clothing items, such as black velvet collars, in show of mourning for the execution of Louis XVI. The parliamentary royalists demanded changes in the government fiscal policies, and a more tolerant position toward religion. During the Convention, churches had been closed and priests required to take an oath to the government. Priests who had refused to take the oath were expelled from the country, on pain of the death penalty if they returned. Under the Directory, many priests had quietly returned, and many churches around the country had re-opened and were discreetly holding services. When the Directory proposed moving the ashes of the celebrated mathematician and philosopher René Descartes to the \"Panthéon\", one deputy, Louis-Sébastien Mercier, a former Girondin and opponent of the Jacobins, protested that the ideas of Descartes had inspired the Reign of Terror of the Revolution and destroyed religion in France. Descartes' ashes were not moved. \"Émigrés\" who had left during the Revolution had been threatened by the Convention with the death penalty if they returned; now, under the Directory, they quietly began to return.\n\nParallel with the parliamentary royalists, but not directly connected with them, a clandestine network of royalists existed, whose objective was to place Louis XVIII, then in exile in Germany, on the French throne. They were funded largely by Britain, through the offices of William Wickham, the British spymaster who had his headquarters in Switzerland. These networks were too divided and too closely watched by the police to have much effect on politics. However, Wickham did make one contact that proved to have a decisive effect on French politics: through an intermediary, he had held negotiations with General Pichegru, then commander of the Army of the Rhine.\n\nThe Directory itself was divided. Carnot, Letourneur and La Révellière Lépeaux were not royalists, but favored a more moderate government, more tolerant of religion. Though Carnot himself had been a member of the Committee of Public Safety led by Robespierre, he declared that the Jacobins were ungovernable, that the Revolution could not go on forever, and that it was time to end it. A new member, François-Marie, marquis de Barthélemy, a diplomat, had joined the Directory; he was allied with Carnot. The royalists in the Councils immediately began to demand more power over the government and particularly over the finances, threatening the position of Barras.\n\nBarras, the consummate intriguer, won La Révellière Lépeaux over to his side, and began planning the downfall of the royalists. From letters taken from a captured royalist agent, he was aware of the contacts that General Pichegru made with the British and that he had been in contact with the exiled Louis XVIII. He presented this information to Carnot, and Carnot agreed to support his action against the Councils. General Hoche, the new Minister of War, was directed to march the Army of Sambre-et-Meuse through Paris on its way to Brest, on the pretext that they would be embarked for a new expedition to Ireland. Hoche himself resigned as Minister of War on 22 July. General Pierre Augereau, a close subordinate and ally of Bonaparte, and his troops arrived in Paris on 7 August, though it was a violation of the Constitution for soldiers to be within twelve leagues of the city without permission of the Councils. The royalist members of the Councils protested, but could do nothing to send them away.\n\nOn 4 September 1797, with the army in place, the \"Coup d'état\" of 18 Fructidor, Year V was set in motion. General Augereau's soldiers arrested Pichegru, Barthélemy, and the leading royalist deputies of the Councils. The next day, the Directory annulled the elections of about two hundred deputies in 53 departments. Sixty-five deputies were deported to Guiana, 42 royalist newspapers were closed, and 65 journalists and editors were deported. Carnot and Barthélemy were removed from the Directory. Carnot went into exile in Switzerland; he later returned and became, for a time, Bonaparte's minister of war. Barthélemy and Pichegru both were sent to exile in French Guiana (penal colony of Cayenne). In June 1798, they both escaped, and went first to the United States and then to England. During the Consulate, Pichegru returned secretly to Paris, where he was captured on 28 February 1804. He died in prison on 6 April 1804, either strangled or having committed suicide.\n\nThe coup was followed by a scattering of uprisings by royalists in Aix-en-Provence, Tarascon and other towns, particularly in the southwest and west. A commissioner of the Directory was assassinated in Lyon, and on 22 October counter-revolutionaries seized the city government of Carpentras for twenty-four hours. These brief uprisings served only to justify a wave of repression from the new government.\n\nWith Carnot and Barthélemy gone from the Directory, and the royalists expelled from the Councils, the Jacobins were once again in control of the government. The two vacant places in the Directory were filled by Merlin de Douai, a lawyer who had helped write the Law of Suspects during the Reign of Terror; and François de Neufchâteau, a poet and expert in industry inland navigation, who served only a few months. Eight of the twelve Directors and ministers of the new government were regicides, who as deputies of the Convention had voted for the execution of Louis XVI, and were now determined to continue the Revolution. \n\nThe central administration and city governments were quickly purged of suspected royalists. The next target was the wave of noble \"émigrés\" and priests who had begun to return to France. The Jacobins in the Councils demanded that the law of 1793 be enforced; \"émigrés\" were ordered to leave France within fifteen days. If they did not, they were to be judged by a military commission, and, on simple proof of their identity, were to be executed within twenty-four hours. Military commissions were established throughout the country to judge not only returning \"émigrés\", but also rebels and conspirators. Between 4 September 1797 and the end of the Directory in 1799, 160 persons were condemned to death by the military tribunals, including 41 priests and several women. \n\nOn 16 October 1797, the Council of Five Hundred considered a new law which banned political activities by nobles, who were to be considered as foreigners, and had to apply for naturalization in order to take part in politics. A certain number, listed by names, were to be banned permanently from political activity, were to have their property confiscated, and were to be required to leave immediately. The law called for certain exemptions for those in the government and military (Director Barras and General Bonaparte were both from minor noble families). In the end, resistance to the law was so great that it was not adopted.\n\nThe Jacobin-dominated councils also demanded the deportation of priests who refused to take an oath to the government, and an oath declaring their hatred of royalty and anarchy. 267 priests were deported to the French penal colony of Cayenne in French Guiana, of whom 111 survived and returned to France. 920 were sent to a prison colony on the Île de Ré, and 120, a large part of them Belgians, to another colony on the Île d'Oléron. The new government continued the anti-religious policy of the Convention. Several churches, including the cathedral Notre Dame de Paris and the church of Saint-Sulpice, were converted Theophilanthropic temples, a new religion based on the belief in the existence of God and the immortality of the human spirit. Religious observations were forbidden on Sunday; they were allowed only on the last day of the 10-day week (\"décade\") of the French Republican Calendar. Other churches remained closed, and were forbidden to ring their bells, although many religious services took place in secret in private homes. The National Guard was mobilized to search rural areas and forests for priests and nobles in hiding. As during the Reign of Terror, lists were prepared of suspects, who would be arrested in the event of attempted uprisings. \n\nThe new Jacobin-dominated Directory and government also targeted the press. Newspaper publishers were required to submit copies of their publications to the police for official approval. On 17 December 1797, seventeen Paris newspapers were closed by order of the Directory. The Directory also imposed a substantial tax on all newspapers or magazines distributed by mail, although Jacobin publications, as well as scientific and art publications, were excluded. Books critical of the Jacobins were censored; Louis-Marie Prudhomme's six-volume \"Histoire générale et impartiale des erreurs, des fautes et des crimes commis pendant la Révolution française\" (\"General and impartial history of the errors, faults and crimes committed during the French Revolution\") was seized by the police. The Directory also authorized the opening and reading of letters coming from outside of France.\nDespite all these security measures, there was a great increase in brigandage and robbery in the French countryside; travelers were frequently stopped on roads and robbed; the robberies were often blamed on royalist bands. On 18 January 1798, the Councils passed a new law against highwaymen and bandits, calling for them to be tried by military tribunals, and authorizing the death penalty for robbery or attempted robbery on the roads of France.\n\nThe political repression and terror under the Directory were real, but they were on a much smaller scale than the Reign of Terror under Robespierre and the Convention, and the numbers of those repressed declined during the course of the Directory. After 1798, no further political prisoners were sent to French Guiana, and, in the final year of the Directory, only one person was executed for a political offense.\n\nIn the spring of 1798, not only a new third of the legislature had to be chosen, but the places of the members expelled by the revolution of had to be filled. 437 seats were open, out of 750. The elections took place between 9 and 18 April. The royalists had been disqualified, and the moderates were in disarray, while the radical Jacobins made a strong showing. Before the new deputies could take their seats, Barras and the other Directors, more moderate than the new Jacobins, organized a commission to review the elections, and disqualified many of the more extreme Jacobin candidates, replacing them with moderates. They sent the list of candidates for Director to the Councils, excluding any radicals. François de Neufchåteau was chosen by a drawing of lots to leave the Directory and Barras proposed only moderate Jacobins to replace him: the choice fell on Jean-Baptiste Treilhard, a lawyer. These political maneuvers secured the power of the Directory, but widened further the gap between the moderate Directory and the radical Jacobin majority in the Councils.\n\nOn 17 October 1797, General Bonaparte and the Austrians signed the Treaty of Campoformio. It was a triumph for France. France received the left bank of the Rhine as far south of Cologne, Belgium, and the islands in the Ionian Sea that had belonged to Venice. Austria in compensation was given the territories of Venice up to the Aegean Sea. In late November and December, he took part in negotiations with the Holy Roman Empire and Austria, at the Second Congress of Rastatt, to redraw the borders of Germany. He was then summoned back to Paris to take charge of an even more ambitious project, the invasion of Britain, which had been proposed by Director Carnot and General Hoche. But an eight-day inspection of the ports where the invasion fleet was being prepared convinced Bonaparte that the invasion had little chance of success: the ships were in poor condition, the crews poorly trained, and funds and logistics were lacking. He privately told his associate Marmont his view of the Directory: \"Nothing can be done with these people. They don't understand anything of greatness. We need to go back to our projects for the East. It is only there that great results can be achieved.\" The invasion of England was cancelled, and a less ambitious plan to support an Irish uprising was proposed instead (see below).\n\nThe grand plan of the Directory in 1798, with the assistance of its armies, was the creation of \"Sister Republics\" in Europe which would share the same revolutionary values and same goals, and would be natural allies of France. In the Dutch Republic (Republic of the Seven United Netherlands), the French army installed the Batavian Republic with the same system of a Directory and two elected Councils. In Milan, the Cisalpine Republic was created, which was governed jointly by a Directory and Councils and by the French army. General Berthier, who had replaced Bonaparte as the commander of the Army of Italy, imitated the actions of the Directory in Paris, purging the new republic's legislature of members whom he considered too radical. The Ligurian Republic was formed in Genoa. Piedmont was also turned by the French army into a sister republic, the Piedmontese Republic. In Turin, King Charles-Emmanuel IV, (whose wife, Clotilde was Louis XVI's youngest sister), fled French dominance and sailed, protected by the British fleet, to Sardinia. In Savoy, General Joubert did not bother to form a sister republic, he simply made the province a department of France.\n\nThe Directory also directly attacked the authority of Pope Pius VI, who governed Rome and the Papal States surrounding it. Shortly after Christmas on 28 December 1797, anti-French riots took place in Rome, and a French Army brigadier general, Duphot, was assassinated. Pope Pius VI moved quickly and formally apologized to the Directory on 29 December 1797, but the Directory refused his apology. Instead, Berthier's troops entered Rome and occupied the city on 10 February 1798. Thus the Roman Republic was also proclaimed on 10 February 1798. Pius VI was arrested and confined in the Grand Duchy of Tuscany before being taken to France in 1799. The Vatican treasury of thirty million francs was sent to Paris, where it helped finance Bonaparte's expedition to Egypt, and five hundred cases of paintings, statues, and other art objects were sent to France and added to the collections of the Louvre.\n\nA French army under General Guillaume Brune occupied much of Switzerland. The Helvetian Republic was proclaimed on 12 April 1798. On 26 August 1798, Geneva was detached from the new republic and made part of France. The treasury of Bern was seized, and, like the treasury of the Vatican, was used to finance Bonaparte's expedition to Egypt.\n\nThe new military campaigns required thousands of additional soldiers. The Directory approved the first permanent law of conscription, which was unpopular in the countryside, and particularly in Belgium, which had formally become part of France. Riots and peasant uprisings took place in the Belgian countryside. Blaming the unrest on Belgian priests, French authorities ordered the arrest and deportation of several thousands of them.\n\nThe idea of a French military expedition to Egypt had been proposed by Talleyrand in a memoir to the French Institute as early as 3 July 1797, and in a letter the following month from Talleyrand to Bonaparte. The Egyptian expedition had three objectives: to cut the shortest route from England to British India by occupying he Isthmus of Suez; to found a colony which could produce cotton and sugar cane, which were in short supply in France due to the British blockade; and to provide a base for a future French attack on British India. It also had several personal advantages for Bonaparte: it allowed him to keep a distance from the unpopular Directory, while at the same time staying in the public eye.\nThe Directory itself was not enthusiastic about the idea, which would take its most successful general and his army far from Europe just at the time that a major new war was brewing. Director La Révellière-Lépeaux wrote: \"The idea never came from the Directory or any of its members. The ambition and pride of Bonaparte could no longer support the idea of not being visible, and of being under the orders of the Directory.\"\n\nThe idea presented two other problems: Republican French policy was opposed to colonization, and France was not at war with the Ottoman Empire, to which Egypt belonged. Therefore, the expedition was given an additional scientific purpose: \"to enlighten the world and to obtain new treasures for science.\" A large team of prominent scientists was added to the expedition; twenty-one mathematicians, three astronomers, four architects, thirteen naturalists and an equal number of geographers, plus painters, a pianist and the poet François-Auguste Parseval-Grandmaison\n\nOn 19 May 1798, two hundred ships carrying Bonaparte, and 35,000 men comprising the \"Armée d'Orient\", most of them veterans of Bonaparte's Army of Italy, sailed from Toulon. The British fleet under Nelson, expecting a French expedition toward Constantinople, was not in position to stop them. The French fleet stopped briefly at Malta, capturing the island, the government of which offered little resistance. Bonaparte's army landed in the bay of Alexandria on 1 July, and captured that city on 2 July, with little opposition. He wrote a letter to the Pascha of Egypt, claiming that his purpose was to liberate Egypt from the tyranny of the Mamluks. His army marched across the desert, despite extreme heat, and defeated the Mameluks at the Battle of the Pyramids on 21 July 1798. A few days later, however, on 1 August, the British fleet under Admiral Nelson arrived off the coast; the French fleet was taken by surprise and destroyed in the Battle of the Nile. Only four French ships escaped. Bonaparte and his army were prisoners in Egypt. .\n\nAnother attempt to support an Irish uprising was made on 7 August 1798. A French fleet sailed from Rochefort-sur-Mer (Rochefort) carrying an expeditionary force led by General Jean Joseph Amable Humbert. The attack was intended to support an uprising of Irish nationalists led by Wolfe Tone. Tone had several meetings with Bonaparte in France to coordinate the timing, but the uprising within the Kingdom of Ireland began early and was suppressed on 14 July 1798 before the French fleet arrived. The French force landed at Killala, in northwest Ireland, on 22 August. It defeated British troops in two small engagements on 24 and 27 August, and Humbert declared the formation of an Irish Republic at Castlebar on 27 August, but the French forces were defeated at the Battle of Ballinamuck on 8 September 1798 by the troops of Lord Cornwallis, British Commander-in-chief in Ireland. A second part of the French expeditionary force, not knowing that the first had surrendered, left Brest on 16 September. It was intercepted by the British Navy in the bay of Donegal, and six of the French warships were captured.\n\nTensions between the United States and France developed into the Quasi-War, an undeclared naval war. France complained the United States was ignoring the 1778 Treaty of Alliance that had brought the French into the American Revolutionary War. The United States insisted on taking a neutral stance in the war between France and Britain. After the Jay Treaty with Britain went into effect in 1795, France began to side against the United States and by 1797 had seized over 300 American merchant ships. Federalists favored Britain while Jeffersonian Republicans favored France. Federalist President John Adams built up the United States Navy, finishing three frigates, approving funds to build three more and sending diplomats to Paris to negotiate. They were insulted by Foreign Minister Talleyrand (who demanded bribes before talking). The XYZ Affair told Americans about the negotiations and angered American public opinion. The war was fought almost entirely at sea, mostly between privateers and merchant ships. In 1800, the Convention of 1800 (Treaty of Mortefontaine) ended the conflict.\n\nBritain and Austria had been alarmed by the French creation of Sister Republics. Austria first demanded that France hand over a share of the territory of the new Republics to it. When the Directory refused, Austria began searching for partners for a new military alliance against France. The new Czar of Russia, Paul I of Russia, was extremely hostile to French republican ideas, sympathetic to the exiled Louis XVIII, and willing to join a new coalition against France. The Czar offered an army of 20,000 men, sent by sea to Holland on his Baltic fleet. He sent another army of 60,000 men, veterans of fighting in Poland and Turkey, under his best general, Alexander Suvorov, to join the Austrian forces in northern Italy.\n\nThe King of Prussia, Frederick-William III, had carefully preserved neutrality in order to profit from both sides. The Directory made the error of sending one of the most prominent revolutionaries of 1789, the Abbé Sieyés, who had voted for the death of Louis XVI, as ambassador to Berlin, where his ideas appalled the arch-conservative and ultra-monarchist king. Frederick William maintained his neutrality, refusing to support either side, a setback for France.\n\nBy the end of 1798, the coalition could count on 300,000 soldiers, and would be able to increase the number to 600,000. The best French army, headed by Bonaparte, was stranded in Egypt. General Brune had 12,000 men in Holland; Bernadotte, 10,000 men on the Rhine; Jourdan, 40,000 men in the army of the Danube; Massena, 30,000 soldiers in Switzerland; Scherer, 40,000 men on the Adige river in northern Italy; and 27,000 men under Macdonald were based in Naples: a total of 170,000 men. To try to match the coalition forces, the Directory ordered a new call up of young men between the ages of twenty and twenty five to the army, seeking to add two hundred thousand new soldiers. \n\nOn 10 November 1798, the British and Austrian governments had agreed on a common goal of suppressing the five new sister republics and forcing France back into its 1789 borders. Then on 29 November 1798, on the first day of the War of the Second Coalition, the King of Naples launched an attack on Rome, which was lightly defended by French soldiers. A British fleet landed three thousand Neapolitan soldiers in Tuscany. However, the French army of General Championnet responded quickly, defeating the Neapolitan army at the Battle of Civita Castellana at Civita Castellana on 5 December. The next day, 6 December 1798, French soldiers also forced the King of Sardinia to remove his soldiers from Piedmont and to retreat to his island of Sardinia, his last possession. The French army marched to the Kingdom of Naples, obliging the King of Naples to leave his City of Naples on a British warship on 23 December 1798. Naples was then occupied on 23 January 1799, and a new Neapolitan republic, the so-called Parthenopean Republic, the sixth under French protection, was proclaimed on 26 January.\n\nPeace negotiations with Austria went nowhere in the spring of 1799, and the Directory decided to launch a new offensive into Germany, but the arrival of a Russian army under Alexander Suvorov and fresh Austrian forces under the Archduke Charles for a time changed the balance of power. Jourdan's Army of the Danube crossed the Rhine on 6 March but was defeated by the Archduke Charles, first at the Battle of Ostrach and then at the Battle of Stockach on 25 March 1799. Jourdan's army withdrew while Jourdan himself returned to Paris to plea for more soldiers.\n\nThe forces of the Second Coalition invaded French-occupied Italy, and after five earlier battles, a joint Russian-Austrian army under Suvorov's command defeated Moreau at the Battle of Cassano on 27 April 1799 and thus occupied Turin and Milan and thereby took back the Cisalpine Republic from France. Suvorov then defeated the French Army on the Terrivva. To redress the situation, Joubert was named the new head of the Army of Italy on 5 July, but his army suffered defeat by the Russians at the Battle of Novi, on 15 August; Joubert himself was shot through the heart when the battle began, and his army was routed. The Sister Republics established by the French in Italy quickly collapsed, leaving only Genoa under French control. \n\nIn August, the Russians and British opened a new front in the Netherlands. A British army was landed at Helder on 27 August, and was joined by a Russian army. On 31 August, the Dutch fleet, allied with France, was defeated by the Royal Navy. Seeing the French army and government in a crisis, the leaders of the royalist rebellions in the Vendée and Brittany came together on 15 September to prepare a renewed uprising.\n\nThe surviving leaders of the royalist rebellions in the Vendée and Brittany, which had long been dormant, saw a new opportunity for success and met to plan strategy on 15 September 1799. The royalist commander Louis de Frotté, in exile in England, returned to France to command the new uprising.\n\nWhile the French armies in Italy and Switzerland tried to preserve the Sister Republics, Bonaparte pursued his own campaign in Egypt. He explained in a letter to the Directory that Egyptian venture was just the beginning of a broader campaign \"to create a formidable diversion in the campaign of Republican France versus monarchic Europe. Egypt would be the base of something much larger than the original project, and at the same time a lever which will aid in the creation of a general uprising of the Muslim world.\" This uprising, he believed, would lead to the collapse of British power from the Middle East to India. With this goal in mind, he left Cairo and marched his army across the Sinai desert into Syria, where he laid siege to the port of Saint-Jean-d'Acre of the Ottoman Empire, which was defended by a local army and supplied by a British fleet offshore. His long siege and attempts to storm the city were a failure; his army was ravaged by disease, it was down to 11,000 men, and he learned that an Ottoman army was to be embarked by the British fleet to sail to Cairo to recapture the city. On 17 May, he abandoned the siege and was back in Cairo by 4 June. The British fleet landed the Ottoman army, but as soon as they were ashore they were decisively defeated by Bonaparte at the Battle of Aboukir on 25 July 1799.\n\nDue to the British blockade of Egypt, Bonaparte had received no news from France for six months. He sent one of his military aides to meet with Turkish government officials and to try to get news from France, but the officer was intercepted by the British navy. The British admiral and naval commander in the eastern Mediterranean, Sir Sidney Smith, who had lived in Paris and knew France well, gave the officer a packet of recent French newspapers and sent him back to Bonaparte. Bonaparte spent the night reading the newspapers, learning about the political and military troubles in France. His orders permitted him to return home any time he chose. The next day he decided to return to France immediately. He handed over command of the army to General Kléber and left Egypt with a small party of senior officers aboard the frigate \"La Muiron\". He escaped the British blockade but did not reach France until 9 October.\n\nThe military position of France, which seemed disastrous during the summer, improved greatly in September. On 19 September, General Brune won a victory over the British-Russian army in the Netherlands at Castricum. On 18 October, besieged by Brune at Alkmaar, the British-Russian forces under the Duke of York agreed to withdraw. In Switzerland, a Russian Empire Army had split into two. On 25–26 September, the French army in Switzerland, led by André Masséna, defeated one part of the Russian army under Alexander Rimsky-Korsakov at the Second Battle of Zurich, and forced the rest of the Russian army, under Suvorov, into disastrous retreat across the Alps to 'Italy'. Suvorov was furious at the Austrians, blaming them for not supporting his troops, and he urged the Czar to withdraw his forces from the war. \nThe royalist uprising in the west of France, planned to accompany the British-Russian-Austrian offensive, was also a failure. The Chouans briefly seized Le Mans on 14 October and Nantes on 19 October, but they were quickly driven out by the French Army, and the rebellion had collapsed by 29 October.\n\nSince the beginning of the Revolution, the nation suffered from rampant inflation. By the time of the Directory, the paper money, the \"assignat\", based on the value of goods confiscated from the church and nobility, had already lost most of its value. Prices had soared, and the government could not print money fast enough to meet its expenses. The value of the assignat had dropped drastically against the value of the livre, a coin from the old regime which contained silver. In 1790, at the beginning the Revolution, an assignat with a face value of 1000 livres could be exchanged for nine hundred real livres containing silver. In January, 1795, The Convention decided to issue assignats worth thirty billion livres, without any additional backing by gold. By March 1795, an assignat with a value of one thousand livres could buy only eighty livres containing metal. In February 1796, the Directory decided to abolish the assignat, and held a public ceremony to destroy the printing plates. The assignat was replaced by a new note, the \"Mandat territorial\". But since this new paper money also lacked any substantial backing, its value also plummeted; by February 1797 the \"Mandat\" was worth only one percent of its original value. The Directory decided to return to the use of gold or silver coins, which kept their value. One hundred livres of Mandats was exchanged for twenty sous of metallic money. The difficulty was that the Directory had only enough gold and silver to produce three hundred million livres. The result of the shortage of money in circulation was a drastic deflation and drop in prices, which was accompanied by a drop in investment, and a drop in wages. It led to a drop in economic activity, and unemployment.\n\nNew elections to elect 315 members of the Councils were held between 21 March – 9 April 1799. The royalists had been discredited and were gone; the major winners were the neo-Jacobins, who wanted to continue and strengthen the Revolution. The new members of the Council included Lucien Bonaparte, the younger brother of Napoleon, just twenty-four years old. On the strength of his name, he was elected the President of the Council of Five Hundred.\n\nThis time the Directors did not try to disqualify the Jacobins but looked for other ways to keep control of the government. It was time to elect a new member of the Directory, as Rewbell had been designated by the drawing of lots to step down. Under the Constitution, the selection of a new member of the Directory was voted by the old members of the Councils, not the newly elected ones. The candidate selected to replace him was the Abbé Sieyés, one of the major leaders of the revolution in 1789, who had been serving as Ambassador to Berlin. Sieyés had his own project in mind: He had devised a new doctrine that the power of government should be limited in order to protect the rights of the citizens. His idea was to adopt a new Constitution with a supreme court, on the American model, to protect individual rights. He privately saw his primary mission as preventing a return of Reign of Terror of 1793, a new constitution, and bringing the Revolution to a close as soon as possible, by whatever means. \n\nOnce the elections were complete, the Jacobin majority immediately demanded that the Directory be made more revolutionary. The Councils began meeting on 20 May, and on 5 June they began their offensive to turn the Directors to the left. They declared the election of the Director Treilhard illegal on technical grounds and voted to replace him with Louis-Jérôme Gohier, a lawyer who had been Minister of Justice during the Convention, and who had overseen the arrest of the moderate Girondin deputies. The Jacobins in the Council then went a step further and demanded the resignation of two moderate Directors, La Revelliere and Merlin. They were replaced by two new members, Roger Ducos, a little-known lawyer who had been a member of the Committee of Public Safety, and was an ally of Barras, and an obscure Jacobin general, Jean-François-Auguste Moulin. The new Ministers named by the Directors were for the most part reliable Jacobins, though Sieyés arranged the appointment of one of his allies, Joseph Fouché, as the new Minister of Police. \n\nThe Jacobin members immediately began proposing laws that were largely favorable to the sans-culottes and working class, but which alarmed the upper and middle classes. The Councils imposed a forced loan of one hundred million francs, to be paid immediately according to a graduated scale by all who paid a property tax of over three hundred francs. Those who did not pay would be classified the same as émigré nobles and would lose all civil rights. The Councils also passed a new law that called for making hostages of the fathers, mothers and grandparents of emigre nobles whose children had emigrated or were serving in rebel bands or armies. These hostages were subject to large fines or deportation in the event of assassinations or property damage caused by royalist soldiers or bandits. On June 27 General Jourdan, a prominent Jacobin member of the Councils, proposed a mass draft of all eligible young men between twenty and twenty-five to raise two hundred thousand new soldiers for the army. This would be the first draft since 1793.\n\nThe new Jacobins opened a new political club, the Club du Manege, on the model of Jacobin clubs of the Convention. It opened on 6 July and soon had three thousand members, with 250 deputies, including many alumni of the Jacobins during the Reign of Terror, as well as former supporters of the ultra-revolutionary François Babeuf. One prominent member, General Jourdan, greeted the members at the club's banquet of July 14 with the toast, \"to a return of the pikes'\", referring to the weapons used by the sans-culottes to parade the heads of executed nobles. The club members also were not afraid to attack the Directory itself, complaining of its lavish furnishings and the luxurious coaches used by Directory members. The Directory soon responded to the provocations; Sieyés denounced the club members as a return of Robespierre's reign of terror. The Minister of Police, Fouché, closed the Club on 13 August.\n\nThe rule that Directors must to be at least forty years old became one justification for the Coup of 18 Brumaire: the coup d'état took place on 9 November 1799, when Bonaparte was thirty years old. Bonaparte returned to France, landing at the fishing village of Saint-Raphaël on 9 October 1799, and made a triumphal progression northward to Paris. His victory over the Ottoman Turks at the Battle of Aboukir had been widely reported, and overshadowed the other French victories at the Second Battle of Zurich and the Battle of Bergen. Between Avignon and Paris, he was welcomed by large, enthusiastic crowds, who saw him as a saviour of the Republic from foreign enemies and the corruption of the Directory. Upon his arrival in Paris he was elected to the Institut de France for the scientific accomplishments of his expedition to Egypt. He was welcomed by royalists because he was from a minor noble family in Corsica, and by the Jacobins because he had suppressed the attempted royalist \"coup d'état\" at the beginning of the Directory. His brother Lucien, though only twenty-four years old, became a prominent figure in the Council of Five Hundred because of his name.\n\nBonaparte's first ambition was to be appointed to the Directory, but he was not yet forty years old, the minimum age set by the Constitution, and the Director Gohier, a strict legalist, blocked that avenue. His earliest ally had been the Director Barras, but he disliked Barras because his wife Joséphine had been his mistress before she married Bonaparte, and because of charges of corruption that surrounded Barras and his allies. Bonaparte wrote later that the Jacobin director, General Moulin, approached Bonaparte and suggested that he lead a \"coup d'état\", but he declined; he wished to end the Revolution, not continue it. Sieyés, who had been looking for a war hero and general to assist in a \"coup d'état\", had originally in mind General Joubert, but Joubert had been killed at the Battle of Novi in August 1799. He then approached General Moreau, but Moreau was not interested. The first meeting between Sieyés and Bonaparte, on 23 October 1799, went badly; the two men each had enormous egos and instantly disliked each other. Nonetheless, they had a strong common interest and, on 6 November 1799, they formalized their plan.\n\nThe \"coup d'état\" was carefully planned by Sieyès and Bonaparte, with the assistance of Bonaparte's brother Lucien, the diplomat and consummate intriguer Talleyrand, the Minister of Police Fouché, and the Commissioner of the Directory, Pierre François Réal. The plan called for three Directors to suddenly resign, leaving the country without an Executive. The Councils would then be told that a Jacobin conspiracy threatened the Nation; the Councils would be moved for their own security to the Château de Saint-Cloud, some west of Paris, safe from the mobs of the French capital. Bonaparte would be named head of government to defend the Republic against the conspiracy; the Councils would be dissolved, and a new Constitution would be written. If the coup went well, it was simply a parliamentary maneuver; it would be perfectly legal. Bonaparte would provide security and take the part of convincing the Deputies. Fouché and Réal would assure that there would be no interference from the police or the city of Paris. Fouché proposed that the leading Jacobin deputies be arrested at the start of the coup, but Bonaparte said it would not be necessary, which later proved to be an error. Shortly before the coup, Bonaparte met with the principal army commanders: Jourdan, Bernadotte, Augereau and Moreau, and informed them of the impending coup. They did not all support it, but agreed not to stand in his way. The president of the Council of Ancients was also brought into the coup, so he could play his part, and Bonaparte's brother Lucien would manage the Council of Five Hundred. On the evening of 6 November, the Councils held a banquet at the former church of Saint-Sulpice. Bonaparte attended, but seemed cold and distracted, and departed early. \n\nEarly in the morning of 9 November, army units began taking positions in Paris, and the members of the Council of Ancients were awakened and instructed to come to the Tuileries Palace for an emergency meeting. When they gathered at seven-thirty, they were told that a Jacobin conspiracy to overthrow the government had been discovered and that they should transfer their meeting the next day to the \"Château de Saint-Cloud\", where they would be in safety. The members were asked to approve a decree to move the meeting site, and to appoint Bonaparte as commander of troops in Paris to assure their security. Alarmed, they quickly approved the decree. Bonaparte himself appeared with his staff and told them, \"Citizen representatives, the Republic was about to perish. You learned of it and your decree has just saved it\". At eleven in the morning, the members of the Council of Five Hundred met at the Palais Bourbon and were given the same message. They agreed to move their meeting the following day to Saint-Cloud.\n\nAs planned, by the afternoon Sieyés and Roger Ducos had given their resignations. Talleyrand was assigned to win the resignation of Barras. Talleyrand was supplied with a large amount of money to offer Barras to quit; historians differ on whether he gave the money to Barras or kept it for himself. Barras, seeing the movements of soldiers outside and being assured that he could keep the great wealth he had acquired as a Director, readily agreed to leave the Directory. With three members gone, the Directory could not legally meet. The Jacobin directors Moulin and Gohier were arrested and confined to the Luxembourg Palace under the guard of General Moreau. The first day of the coup had gone exactly as planned.\nOn 10 November, the members of both councils were taken in a procession of carriages with a strong military escort to Saint-Cloud. 6,000 soldiers had already been assembled at the château; because their pay had repeatedly been delayed, they were particularly hostile to the members of the Chambers. Bonaparte spoke first to the Council of the Ancients, assembled in the \"Orangerie\" of the domain of Saint-Cloud, and explained that the Directory was no more. Bonaparte was received coldly, but the Council did not offer any opposition. He then moved to the Council of Five Hundred, which was already meeting under the presidency of his brother Lucien. Here he received a far more hostile reception from the Jacobin deputies. He was questioned, jeered, insulted, shouted down, and jostled. His brother was unable to restore calm, and some of the Jacobin deputies began to demand that Bonaparte be declared outside the law, as Robespierre had been. If the Council voted him outside the law, Bonaparte could be arrested and executed immediately without trial. While the deputies raged and argued, Bonaparte and his brother, escorted by a handful of soldiers, left the \"Orangerie\", approached the unit of grenadiers of General Murat waiting impatiently outside, and told them that the deputies had tried to assassinate Bonaparte with their pens. The grenadiers charged into the hall and quickly emptied it of deputies.\n\nBonaparte wrote his own official version of what happened, which was published in all newspapers and posted on placards on walls all over France; it vividly described how he had narrowly escaped death from the hands of \"twenty Jacobin assassins\" and concluded: \"The majority returned freely and peacefully to the meeting hall, listened to the propositions which had been made for assuring the public safety, deliberated and prepared a beneficial resolution which should become the new law and basis of the Republic.\"\n\nWith that event, the Directory was finished. A new government, the Consulate, was founded. According to most historians, the French Revolution was over.\n\nDespite wars and social turmoil, the population of France continued to grow during the Directory. It was 27,800,000 in 1796, before the Directory, and had grown to 27,900,000 by 1801. Annual population growth had dropped from 16 percent in 1785, before the Revolution, to zero in 1790; but it then rebounded to 36 percent in 1795, then down to 12 percent in 1800. Part of the drop in birthrate during the Directory is attributed to the simplification of divorce, and the change in inheritance laws, which granted equal shares to all descendants. The number of young men killed in the wars during the Directory numbered 235,000 between 1795 and 1799. The high birth rate before the Revolution – together with conscription from conquered and allied states – allowed Napoleon to fill the ranks of his \"Grande Armée\" during the Empire between 1804 and 1815.\n\nBy the time of the Directory, French society had been dramatically restructured. Nobles and clergy, the two classes which had held most of the power before the Revolution, had disappeared. An estimated one percent of the population, mostly nobles and priests, but also many members of the upper middle class who had supported the monarchy, had emigrated. The number was even higher in border regions, such as Bas-Rhin, where 4.5 percent of the population had left.\n\nUnder the Directory the middle and upper classes took a dominant position in Paris society, replacing the nobility. Enormous fortunes were made, often by providing supplies to the army or by speculation on real estate. Some parts of the middle and upper classes suffered: the abolition of the old professional guilds of lawyers and doctors brought the ruin of many members, who faced competition from anyone who wanted to use those titles. The merchants and shipowners in Bordeaux, Nantes, Marseille and other ports were ruined by the British naval blockade. Bankers took on a more prominent role, when investment was scarce.\n\nTwo new groups gained importance during the Directory. The number of government officials of all levels increased dramatically. The writer Louis-Sébastien Mercier in his \"Paris pendant la Révolution (1789–1798), ou Le nouveau Paris\", published in 1800, wrote: \"There is no one who has not complained of the insolence, or the ignorance, of the multitude of government officials employed in the bureaus to sharpen their pens and to obstruct the course of affairs. New has the bureaucracy been carried to a point so so exaggerated, so costly, to exhausting.\"\n\nGenerals and other military officers also grew greatly in importance during the Directory and became a caste independent of the political structure. The Directory had abolished the Jacobin system of political commissioners who supervised and could overrule the military commanders. Generals like Bonaparte in Italy, Hoche in Germany and Pichegru in Alsace directed entire provinces according to their own ideas and wishes, with little interference from Paris. The soldiers of these generals were often more loyal to their generals than to the Directory, as the soldiers of Bonaparte showed during the 1799 coup d'état that ended the Directory.\n\nThe working class and poor in Paris and other large cities suffered particularly from the high inflation during the first part of the Directory, which brought higher prices for bread, meat, wine, firewood and other basic commodities. In the last two years of the Directory, the problem was the opposite: with the suppression of the \"assignats\", the money became scarce, the economy slowed, and unemployment grew. The Directory distributed scarce food items, such as cooking oil, butter and eggs, to government employees and to members of the Councils. Before the Revolution, taking care of the poor had been the responsibility of the Church. During the Directory, the government, particularly in Paris and other large cities, was forced to take over this role. To feed the Parisians and prevent food riots, the government bought flour in the countryside at market prices with its silver coins, then gave it to the bakeries, which sold it at the traditional market price of four sous a pound, which was virtually nothing. The subsidies were reduced in the last years of the Directory, paying only for bread, but they were an enormous expense for the Directory. At the beginning, the government tried to provide the standard minimum of one pound of bread a day per person, but the shortage of money reduced the daily ration to sixty grams of bread a day. The government also tried giving rice as a substitute for bread, but the poor lacked firewood to cook it.\n\nEconomic problems led to a large increase in crime under The Directory, particularly in the countryside. Bands of the unemployed became beggars and turned to robbery, and brigands robbed travelers along the highways. Some of the brigands were former royalists turned highwaymen. They were later celebrated in the novel of Alexander Dumas, \"Les Compagnons de Jéhu\" (\"The Companions of Jehu\"). The government did not have the money to hire more police, and the great majority of the army was occupied fighting in Italy, Switzerland and Egypt. The growing insecurity on the roads seriously harmed commerce in France. The problem of brigands and highwaymen was not seriously addressed until after a serious wave of crimes on the roads in the winter of 1797–98. The Councils passed a law calling for the death penalty for any robbery committed on the main highways or against a public vehicle, such as a coach, even if nothing was taken. If the crime was committed by more than one person, the robbers were tried by a military tribunal rather than a civilian court. The wave of highway robberies was finally stopped by Bonaparte and the Consulate, which employed special tribunals even swifter and more severe than the Directory tribunals.\n\nCorruption was another serious problem, particularly with the businessmen who provided supplies to the army and government. In one case, the Chevalier enterprise received a contract to build three large warships and two frigates at Rochefort; the company was paid in national property seized from the aristocracy and the Church, but it never constructed the ships, or even bought the materials. Huge contracts for government supplies were passed from the furnishers to sub-contractors, who each paid the furnisher a fee. Sometimes contractors demanded to be paid for their services in advance in silver. They were paid, but never delivered the services, and then reimbursed the government with nearly-worthless assignats. The Directors themselves were accused of receiving money from contractors. The Minister of Finance of the Directory, Dominique-Vincent Ramel-Nogaret, was offered 100,000 francs for a bribe to give a contract by a furnisher named Langlois. Ramel refused and turned Langlois over to the police; however, some ministers and Directors, like Barras, left the government with large fortunes. The Directory was unable to escape the accusations of widespread corruption.\n\nBorn in reaction against the strict codes of behavior established during the Convention and the Reign of Terror, the \"Muscadins\" were fashionable young men who carried canes and sometimes, in groups, attacked sans-culottes. Following soon afterwards, the Directory had its own fashion reflecting the new social behavior and carried out by young Parisians of both sexes, from middle and upper-class families, often survivors of the excesses of the Revolution, who had lost parents and family members to the guillotine. They were called \"Incroyables and Merveilleuses\" and dressed in extravagant costumes. The men, the \"Incroyables\", wore long hair to their shoulders, round hats with broad brims, short coats and silk culottes. Their female counterparts, the \"Merveilleuses\", wore flowing, high-breasted transparent dresses reminiscent of the Greco-Roman era. They frequented balls called \"Bals des victimes\" and spoke in their own particular accent and vocabulary, avoiding to pronounce the letter \"R\", as it was the first letter of the word \"Revolution\"\n\nDuring the Directory, almost all the structures and rules of Paris society had been swept away, but no new structures and rules had yet been created to replace them. The brothers Goncourt meticulously described the period on their \"Histoire de la société française pendant le Directoire\". Caste and rank mattered far less; all the old titles and forms of address had disappeared, along with old customs and social conventions. Men no longer took off their hats when talking to women, and people of different ranks spoke to each other as equals. Society no longer met in private, in the houses of the nobility, but in public, at balls, restaurants and public gardens. As the Goncourts said, \"social anarchy\" reigned in Paris: \"everyone met with everyone.\" Government ministers could be seen walking or dining with actresses, bankers with courtesans.\n\"Liaisons were easy\", the Goncourts reported, \"marriage less so.\" The old system of marriages arranged between families based on fortune, profession, and social condition was less common. Marriages were no longer controlled by the church, but by the new civil code, which described marriage as \"nature in action.\" Marriage was seen as a temporary, not a permanent state. Children born outside of marriage were given equal status concerning inheritance and other legal matters as those born to married couples. Divorce was much simpler, and could be requested by either the husband or wife. In one period of fifteen months, 5,994 civil law divorces were granted in Paris, of which 3,886 were requested by the wife. Of 1,148 divorces granted on the grounds of \"incompatibility of humor\", 887 were requested by the wife. The new system also led to a large increase in the number of children born outside of marriage and not wanted; in 1795 four thousand unwanted children in the Department of the Seine were turned over to founding hospitals.\n\nThe breakdown of the old system of arranged marriages led to the creation of the first newspaper where men and women could advertise themselves for suitable spouses, called the \"Indicateur des marriages\". it also led to the establishment of the first marriage bureaus. A businessman named Liardot rented a large former mansion, brought in selected eligible young women as paying guests, and invited men seeking wives to meet them at balls, concerts and card games each given at the house each evening. The men were screened by their profession and education.\n\nAlthough balls were not banned during the Reign of Terror, after the death of Robespierre and the fall of the Jacobins, the city experienced a frenzy of dancing that lasted throughout the period of the French Directory. The Goncourt brothers reported that 640 balls took place in 1797 alone. Several former monasteries were turned into ballrooms, including the Noviciate of the Jesuits, the \"Monastère des Carmes\" (turned into a prison where 191 members of the Catholic Church (bishops, priests, monks) were massacred on 2 September 1792, the \"Séminaire Saint-Sulpice\", and even in the former Saint-Sulpice cemetery. Some of the former palatial townhouses of the nobility were rented and used for ballrooms; the \"Hôtel de Longueville\" near the Louvre put on enormous spectacles, with three hundred couples dancing, in thirty circles of sixteen dancers each, the women in nearly transparent dresses, styled after Roman tunics. In the public balls, everyone danced with everyone; merchants, clerks, artisans and workers danced with shop women and seamstresses. In the more popular public balls, the \"cavaliers\" were charged 80 sous for admission, while women paid 12 sous. At more exclusive balls, admission was five livres. Aristocrats who had survived or returned from exile held their own balls in their houses in the Faubourg Saint-Germain, where \"Bals des victimes\" (\"Balls of the victims\") were attended by invitees who had lost at least one parent to the guillotine.\n\nThe formal dancing of the minuet was replaced by a much more passionate new dance, the waltz, which was introduced to Paris during this time from Germany. For summer evening entertainment, Parisians began to abandon the Tuileries Gardens and the gardens of the Palais-Royal and went to the new pleasure gardens which appeared in the neighborhood between the Grands boulevards and the Palais-Royal. The most famous was the \"Jardin de Tivoli\", also known as \"Folie Boutin\" or \"Grand Tivoli\", located on rue Saint-Lazare. It had belonged to an aristocrat named Boutin, who was guillotined during the Reign of Terror. It was a vast garden covering 40 \"arpents\" (13,675 hectares), and could hold as many as ten thousand persons. It had alleys filled with promenaders, greenhouses, illuminations, an orchestra, dancing, a café, and fireworks at night. Other new gardens competed by adding spectacles and pageants. The \"Jardin des Champs-Élysées\" offered a pageant of costumed soldiers on horseback performing elaborate maneuvers and firing weapons. The \"Mousseau\" (now \"Parc Monceau\") had performers dressed as American Indians dancing and fighting battles. The former \"Pavillon de Hanovre\", which had been part of Cardinal Richelieu's residential complex, featured a terrace for dancing and dining decorated with Turkish tents, Chinese kiosks and lanterns.\n\nMany new restaurants and cafés, usually close to the twenty-three theaters, appeared in and around the Palais-Royal and the new boulevards. A new café, the \"Tortoni\", specializing in ice creams, opened in 1795 at the corner of the boulevard des Italiens and \"rue Taitbout\". The new restaurants in the Palais-Royal were often run by the former chefs of archbishops and aristocrats who had gone into exile. The restaurant \"Méot\" offered a menu with over one hundred dishes. Beside the \"Méot\" and \"Beauvilliers\", under the arcades of the Palais-Royal were the restaurants and cafés such as \"Naudet\", \"Robert\", \"Véry\", \"Foy\", \"Huré\" , \"Berceau\", \"Lyrique\", \"Liberté conquise\", \"de Chartres\" (now \"Le Grand Véfour\"), and \"du Sauvage\" (the last owned by the former coachman of Robespierre). In the cellars of the Palais-Royal were more popular cafés, usually with music, smaller menus at more reasonable prices. One of those, the \"Postal\" , offered a menu for just 36 sous. Many of the cafés in the cellars had orchestras; the most famous was the \"Café des Aveugles\", with an orchestra of four blind musicians.\n\nAfter the Reign of Terror had ended, dining hours for upper-class Parisians returned gradually to what they had been before the Revolution, with \"déjeuner\" at midday, dinner at 6 or 7 in the evening, and supper at 2 in the morning. When the theater performances ended at 10 PM, the spectators went to the nearby cafés on the boulevards.\n\nThe Roman Catholic Church was one of the greatest status quo losers of over the course of the 1789-1799 French Revolution. Priests, who refused to take an oath to the Civil Constitution of the Clergy emigrated or were expelled from France under a penalty of death. Church property, from cathedrals to candlesticks, was seized and sold. Church ceremonies were banned, causing clandestine religious services to be conducted in private homes. During the Reign of Terror, at Robespierre's urging, the National Convention, on 7 May 1794, proclaimed a new religion, the Cult of the Supreme Being, which in a little over a year led to the Thermidorian Reaction, and Robespierre's downfall and execution. The Roman Catholic Church had been the official state religion during the monarchy, and the Directors were all anti-religious republicans, but the Directory, with a few exceptions, did not try to impose any particular religious views, and its policy toward priests and religious institutions changed depending upon political events. After the fall of Robespierre, the repression against the Church eased and, although the policy of repression remained, many churches, especially in the provinces, re-opened, and exiled priests began to quietly return.\n\nIn November 1797, working with the new decimal-based Republican Calendar, the week of which has ten days, the Directory replaced Sundays and religious holidays with republican celebrations. The tenth day of the week, called \"decadi\", was designated to replace Sunday. The churches still functioning with Constitutional priests were instructed to have mass on \"decadi\", rather than on the day that would have been Sunday in the previous calendar, and \"decadi\" became the official non-working day: government employees were off, and schools, shops and markets were closed. To replace saints' and religious days, a whole series of secular holidays was created, in addition to the patriotic celebrations already in place, such as 14 of July and important dates of the French Revolution. There were also special days, such as, \"the day of the sovereignty of the people\"; \"the day of youth\"; \"the day of spouses\"; \"the day of agriculture\" and \"the day of the elderly\". Certain churches were given new names: the cathedral Notre Dame de Paris was renamed \"Temple of the Supreme Being\", Saint-Étienne-du-Mont became the \"Temple of Filial piety\". On \"decadi\", the constitutional priests who performed services were required to share the space with other republican religions and associations who wanted to use the buildings. Large churches were divided into sections for use by various religions.\n\nA new religion, Theophilanthropy, had been founded in 1796 by a Freemason printer-bookseller named Jean-Baptiste Chemin-Dupontès (1760–1852?). It was encouraged by the Director La Révellière-Lépeaux and the Ministry of the Interior, with the state paying for its newspaper. Members believed in God and in the immortality of the soul, but not in the original sin. The sect was similar in form to Calvinism, with readings aloud of texts, hymns and sermons. With the support of the Directory, the sect was given four churches in Paris, including Saint-Roch, Saint-Sulpice and, in April 1798, Notre-Dame de Paris, as well as churches in Dijon, Poitiers and Bordeaux. Members of the sect included some prominent figures, such as General Hoche, the industrialist Éleuthère Irénée du Pont, the painter Jean-Baptiste Regnault, and the American philosopher and political activist Thomas Paine. Beginning in May 1798, however, the Directory began to withdraw support from the newly established deistic sect, which it considered too close to the Jacobins. The sect still had eighteen churches in 1799, but in 1801 it was abolished by Bonaparte. \n\nIn Italy, the French army attacked the papal states governed by the Roman Catholic Church in Italy. In February 1797, Bonaparte occupied Ancona to force Pope Pius VI to negotiate. The pope was obliged to cede Ancona and the northern part of his states to the new French-sponsored Cispadane Republic. The gold and silver in the treasury of the Vatican was taken to France to help support the French currency. Following anti-French riots in Rome in December 1797, a French army under Berthier entered Rome and proclaimed a Roman Republic. Pius VI was taken prisoner by the French Army, and transferred to Valence in France, where he was kept prisoner until his death in 1801.\n\nMany of the economic, social and political woes during the Directory were results of the breakdown of the financial system. The principal problem was a great shortage of money with real value; that is, coins made of silver, and an excess of paper money, the value of which shrank as more and more was printed. The Directory produced only 32 million livres worth of silver-based coins in its first two years. Much of this money was hoarded, since, unlike the paper money, it had and retained real value. As a consequence, the government, to cover its costs, was forced to print millions of notes, first called \"assignats\" and then \"mandats\", which were based on the value of property seized from the Church and the clergy. These notes declined in value as more and more were printed. When the notes became nearly worthless, the Directory first devalued them and finally gave up and stopped printing paper money. The shortage of real money in the second part of the Directory led to a new problem: shortage of credit; interest rates rose to about ten percent, double what they had been in 1789. The consequence in the last two years of the Directory was a decline in economic activity, and in wages, while prices rose. The population lost confidence in the money and in the Directory's management.\n\nThe lack of credit led to the creation of a number of new private banks, and the growing importance of banks and bankers in the economy. The \"Caisse des comptes courants\", created in June 1796, had some of the most important industrialists and financiers in France as its founders, who would later become the founders of the \"Banque de France\". Several other new private banks followed, which concentrated the wealth of France even more in Paris. Since the nobility had gone into exile, the bankers became the new nobility of France. In other words, bankers became the new aristocrats.\n\nThe transportation system within France was another handicap to the economy. The roads and canals had not been improved or maintained since the overthrow of the monarchy. Major canals that had been started in Burgundy and in the north were unfinished. Maritime commerce was in an even worse situation as a result of the war and blockade of French ports by Britain. During the Directory, the number of French ships of more than two hundred tons was one tenth of what it had been in 1789. The conquest of Belgium, the Netherlands and Italy improved the situation somewhat: French goods could be transported on the neutral ships of these countries, and maritime traffic on the Baltic Sea to Germany became an important trade route for France. However, the British navy largely cut off the trade with the French colonies in the Caribbean, which earlier had provided sugar, cotton, indigo and coffee to France; and the entry of the fleet of Admiral Nelson into the Mediterranean Sea cut off the trade routes there. The major ports of Nantes and Marseille saw their commerce and trade routes disappear.\n\nThe continual wars and fiscal crises greatly limited the expansion of French industry. The Industrial Revolution had only just begun in France. Production during the Directory had fallen below what it was in 1789. The number of workers in the silk industry in Lyon had dropped from 12,000 before 1787 to 6,500. The cotton textile industry was more successful due to the embargo against British products caused by the war. New factories and new technologies, such as mechanical looms, were introduced in Normandy and in Alsace. However, the technologies were still primitive; the steam engine had not yet arrived in French factories. The chemical industry was also advancing rapidly during the Directory; the chemist and entrepreneur Jean Antoine Claude Chaptal built a chemical factory in Montpellier, which he soon moved to Chaillot, a village west of Paris. The most effective promoter of French industry was François de Neufchâteau, who was Minister of the Interior before becoming a Director in 1797. He planned a new canal system, began work on a new road across the Pyrenees, and organized the first national industrial exposition in Paris, which opened with great success in October 1798. Once he became Consul, Bonaparte copied the idea of the industrial exposition. Despite this bright spot, French industry was primitive: without steam power, most factories in France depended upon water power, and the metallurgy industry still melted iron with wood fires, not oil.\n\nAgriculture was another weak spot of the French economy. While the country was essentially rural, the methods of farming had not been changed in centuries. The vast majority of farmers had small plots of land, sold little and worked essentially to produce enough food for their families. The price of grain was freed from government control under the Directory in 1797, and farmers could sell their grain at whatever price they could get. Following the Revolution in 1789, the forests had been taken away from the nobles and opened to everyone; as a result, large areas of the forests were immediately cut down, and no new trees planted to replace them. The land of the nobility and Church was taken and redistributed to peasants, but under the new inheritance laws, which gave equal shares to all sons, the size of the farm plots became smaller and smaller. Small plots were not consolidated into larger fields, as was taking place in England at the same time. Most farmers were reluctant to try new methods; they did not want to leave fields idle to recover productivity, or to grow forage crops to feed cattle. Furthermore, during the endless wars of the Directory, thousands of farmers were taken into the army, and thousands of horses and mules needed for farming were taken by the Army for the use of the cavalry and transport. Under these conditions, food shortages and famines occurred regularly in France until the time of Napoleon III.\n\nThe education system of France was in a chaotic state at the beginning of the Directory. The College of Sorbonne and most other colleges of the University of Paris, had been closed because of their close association with the Catholic Church, and did not reopen until 1808. The schools run by the Catholic Church had also been closed, and any kind of religious instruction forbidden. The government of Jacobins during the Convention created several new scientific institutions, but had concentrated on primary education, which it decreed should be obligatory and free for all young people, but there were few teachers available. By forbidding religious education, seizing the property of the Church and chasing out the clergy, they effectively closed the largest part of the educational system of the country.\n\nAt the beginning of the period, the Directory reversed the policy of obligatory and free education for all, largely because of the lack of money to pay teachers. The Directory began to create a system of central schools, with the goal of one in each department, which boys could attend from the age of twelve, with a full curriculum of sciences, history and literature. The state paid a part of the cost, while each student also paid the professor a fee. The new schools had libraries (mostly confiscated from the nobility), small botanical gardens, and museums of natural history. For the first time in French schools, French instead of Latin was the basis of education. Three of these schools were organized in Paris; two of them later became the famous Lycée Henri-IV and Lycée Charlemagne. But by the end of the Directory there were only 992 students in the three Paris schools.\n\nFor primary education, each \"arrondissement\" in Paris had one school for boys and another for girls, and each commune in the country was supposed to have the same. Since the state lacked money, teachers were paid by the commune or by the students. Once a student learned to read, write, and count, he or she was graduated. In villages, the school was often located in the former church, and teachers were expected, as part of their duties, to carry water, clean the church, ring the bells, and, when needed, dig graves in the churchyard cemetery.\n\nThe choices were greater for the children of the middle and upper middle class, as these families had tutors, or sent their children to private schools, but for much of the population, schooling was minimal. There were 56 public schools in the Seine department, which by the population should have had at least 20,000 students; but they had only between 1100 and 1200.\n\nThe continual wars during the Directory also had their effect on education. Beginning in October 1797, boys in public schools were required to take part in periodic military exercises, and the Directory established five military schools, called \"Écoles de Mars\", for a total of 15,000 students. Attendance was a requirement for entry into the higher schools of engineering and public works.\n\nThe Directory focused its attention on secondary education and especially on creating specialized higher schools for training managers, judges, doctors and engineers, for which there was an immediate and pressing need. The \"École Polytechnique\" had been founded by a member of the Directory, Lazare Carnot and the mathematician Gaspard Monge, in 1794. The school became the most prestigious engineering and public works school in France. However, by the end of the Directory there were still no law schools, and only two schools of medicine outside of Paris.\n\nThe \"Institut de France\" was also founded in 1795 by Lazare Carnot and Monge, to bring together the scientists and researchers, who previously had worked in separate academies, to share knowledge and ideas. It was divided into three large sections: physical sciences and mathematics; moral and political science; and literature and the fine arts. It organized the large party of scientists and scholars who accompanied Napoleon to Egypt, which discovered such treasures as the Rosetta Stone, which allowed the deciphering of Egyptian hieroglyphs. One of the \"Institut de France\" first members and speakers was Napoléon Bonaparte, who took the place of Carnot after the latter had been removed from the Directory and left France.\n\nThe artists of Paris were in a difficult situation during the Directory, as their most important patrons, the aristocracy, had been executed or had emigrated; however a new wealthy class was just being formed. Before the Revolution a half-figure portrait could be commissioned from a less-known artist for three hundred livres. During the Directory, the price fell to forty-eight livres. Nonetheless, the \"Salon\" took place in the Louvre in 1795 as it had since 1725, before the Revolution, and each year thereafter. The most prominent artist of the Revolution, Jacques-Louis David, closely connected with the Jacobins, was in seclusion in his studio inside the Louvre. At the end of the period, in 1799, he produced one important work, the \"Intervention of the Sabine Women\". However, a new generation of artists, inspired by David, showed their works; François Gérard; Anne-Louis Girodet, a pupil of David, renown for his romantic paintings, particularly a 1797 painting of the prominent actress Mademoiselle Lange as Venus; Carle Vernet, the son and father of famous painters; the portrait painter and miniaturist Jean-Baptiste Isabey, known as the \"painter of the kings\" or \"portraitist of Europe\", who painted queen Marie-Antoinette and empress Joséphine, and remained active until the Second Empire; the genre painter Louis-Léopold Boilly; Antoine-Jean Gros, a young history and landscape painter, who soon achieved fame and a government position in 1796 with a heroic portrait of Bonaparte at the battle of Arcole; the romantic landscapes of Hubert Robert; Pierre-Paul Prud'hon, whose work combined Neoclassicism and Romanticism; and a major neoclassical sculptor from the earlier generation, Jean-Antoine Houdon, famous for his busts of George Washington and Voltaire.\n\nMaking the Louvre into an art museum had first been proposed in 1747 by Étienne La Font de Saint-Yenne and supported by Diderot in 1765 in the article on the Louvre in the \"Encyclopédie\". The idea was accepted by Louis XVI who, in 1789, began work on the \"Grande Galerie\" of the Louvre. The Revolution intervened, and on 27 July 1793 the Convention decreed the creation of a Museum of the Republic (\"Musée de la République française\"), which opened on 10 August 1793, the first anniversary of the storming of the Tuileries.\n\nIn 1797, at the end of Bonaparte's triumphant first Italian campaign, convoys of wagons began arriving in Paris, carrying bronze horses, Greek antiquities, tapestries, marble statues, paintings and other works of art taken from Italian cities under the terms of peace agreed by the Austrians. They included works by Raphael, Leonardo da Vinci, Titian, Paolo Veronese and other masters. Other convoys arrived from the Netherlands and Flanders with more art from the Spanish provinces. The more famous works were displayed on wagons in a festive victory parade through the center of Paris. The rest was crammed, unwrapped, into the corridors, galleries and stairways of the Louvre. Work began to rebuild the \"Galerie d'Apollon\" and other galleries to provide a home for the 'newly acquired' art.\n\nThe Directory had no public money to spend on architecture, but the newly-wealthy upper class had abundant money to buy châteaux and town houses, and to redecorate them. The style of interior decoration, known as the \"Directoire style\", was one of the notable contributions of the period. It was a transitional style, a compromise between the Louis XVI style and French neoclassicism. Riesener, the famous furniture designer for Louis XVI, did not die until 1806, though his clientele changed from the nobility to the wealthy new upper class. The Directory saw the first widespread use of mahogany, an imported tropical wood used in the making of furniture.\n\nThe Directory period produced a small number of important literary works, often very critical of the excesses of Revolution. These included \"Essay on the Revolutions\" by Chateaubriand, published in 1797, which called for a return to Christian values. At the complete opposite end of the literary scale was the last major work of the Marquis de Sade, \"The New Justine\", published in 1797. Sade also wrote a satirical brochure mocking thinly-disguised characters resembling Bonapartre and Josephine. Shortly after the end Directory, on 6 March 1801, Sade was arrested for \"Justine\" and its sequel and ended his days in the insane asylum of Charenton. \n\nHistorians generally have not been kind to the 'Age of the Directory'. Adolphe Thiers, later twice the Prime Minister and the first president of the Third Republic, wrote the first major history in French of the Revolution, in ten volumes, published between 1823 and 1827. He described the Directory this way:\n\nThiers blamed Barras, the only Director who served from the beginning to the end of the Directory, for its failure.\n\nThe most celebrated and vivid description of French society under the Directory was written by the Goncourt brothers, Edmond and Jules, published in 1864, which described the mores, daily life, culture and preoccupations of the Parisians. Its final chapter contained the lines:\n\nThe shortest and simplest description of the entire period, from the Convention to the Empire, was given by Honoré de Balzac in 1837–43 in his novel \"Illusions perdues\". The Spanish Jesuit diplomat Carlos Herrera tells Lucien de Rubempré: \"In 1793 the French invented government by the people, which ended with an absolute emperor. So much for your national history\".\n\nIn 1909, Pyotr Kropotkin wrote:\n\nIn 1971, Robert Roswell Palmer wrote:\n\nIn 1971, the American historians Jerome Blum, Rondo Cameron and Thomas G. Barnes wrote:\n\nIn the 1970s, other historians wrote that the achievements of the Directory, were minor, though it did establish administrative procedures and financial reforms that worked out well when Napoleon started using them. It was blamed for creating chronic violence, ambivalent forms of justice, and repeated recourse to heavy-handed repression.\n\nIn 1994, Isser Woloch wrote:\nIn 2007, Howard Brown wrote: \n\nThe Directory was officially led by a president, as stipulated by Article 141 of the Constitution of the Year III. An entirely ceremonial post, the first presidency was held by Rewbell who was chosen by lot on 2 Nov 1795. The directors conducted their elections privately, and appointed a new president every three months. The last president was Gohier, who resigned during Brumaire after his arrest by troops under the Bonapartist general Jean Victor Marie Moreau.\n\nThe following table displays all \"Directeurs\" and their dates of service:\n\nThe ministers under the Directory were:\n\n\n"}
{"id": "12607", "url": "https://en.wikipedia.org/wiki?curid=12607", "title": "Glasnost", "text": "Glasnost\n\nIn the Russian language the word Glasnost (; , ) has several general and specific meanings. It has been used in Russian to mean \"openness and transparency\" since at least the end of the eighteenth century.\n\nIn the Russian Empire of the late-19th century, the term was particularly associated with reforms of the judicial system, ensuring that the press and the public could attend court hearings and that the sentence was read out in public. In the mid-1980s, it was popularised by Mikhail Gorbachev as a political slogan for increased government transparency in the Soviet Union.\n\n\"For centuries\", human rights activist Lyudmila Alexeyeva has explained, the word \"glasnost\" has been in the Russian language: \"It was in the dictionaries and lawbooks as long as there had been dictionaries and lawbooks. It was an ordinary, hardworking, non-descript word that was used to refer to a process, any process of justice or governance, being conducted in the open.\" In the mid-1960s, however, as Alexeyeva recounts, it acquired a new and topical importance.\n\nOn 5 December 1965, a key event in the emergence of the Soviet civil rights movement, often known as the Glasnost rally, took place in Moscow when protesters on Pushkin Square led by Alexander Yesenin-Volpin demanded access to the closed trial of Yuly Daniel and Andrei Sinyavsky. They specifically asked for \"glasnost\", i.e. the admission of the public, independent observers and foreign journalists, to the trial, something that was required in the newly issued, but not widely available, Code of Criminal Procedure. With a few specified exceptions, Article 111 of the Code stated that judicial hearings in the USSR should be held in public.\n\nSuch protests against closed trials continued throughout the post-Stalin era. Andrei Sakharov, famously, did not travel to Oslo to receive his Nobel Peace Prize because he was standing outside a court building in Vilnius (Lithuania), demanding access to the 1976 trial of Sergei Kovalev, an editor of the \"Chronicle of Current Events\" and prominent rights activist.\n\nIn 1986, aware of the term's historical and more recent resonance, Mikhail Gorbachev and his advisers adopted \"glasnost\" as a political slogan, together with the obscure \"perestroika\".\n\nGlasnost was taken to mean increased openness and transparency in government institutions and activities in the Soviet Union (USSR). \"Glasnost\" apparently reflected a commitment to getting Soviet citizens to discuss publicly the problems of their system and seek solutions. Gorbachev encouraged popular scrutiny and criticism of leaders, as well as a certain level of exposure by the mass media. Some critics, especially among legal reformers and dissidents, regarded the Soviet authorities' new slogans as vague and limited alternatives to more basic liberties.\n\nAlexei Simonov, president of the Glasnost Defence Foundation, would define the term as follows: \"Glasnost is a tortoise crawling towards Freedom of Speech\".\n\nBetween 1986 and 1991, during an era of reforms in the USSR, glasnost was frequently linked with other generalised concepts such as perestroika (literally: restructuring or regrouping) and demokratizatsiya (democratisation). Gorbachev often appealed to glasnost when promoting policies aimed at reducing corruption at the top of the Communist Party and the Soviet government, and moderating the abuse of administrative power in the Central Committee.\n\nThe ambiguity of \"glasnost\" defines the distinctive five-year period (1986–1991) at the end of the USSR's existence. There was decreasing pre-publication and pre-broadcast censorship and greater freedom of information.\n\nThe \"Era of Glasnost\" saw greater contact between Soviet citizens and the Western world, particularly the United States: restrictions on travel were loosened for many, allowing increased business and cultural interchange.\n\nGorbachev's interpretation of \"glasnost\" can best be summarized, translated, and explained in English as \"openness\". While associated with freedom of speech, the main goal of this policy was to make the country's management transparent, and circumvent the narrow circle of bureaucrats who previously exercised complete control of the economy.\n\nDuring Glasnost, Soviet history under Stalin was re-examined; censored literature in the libraries was made more widely available; and there was a greater freedom of speech for citizens and openness in the media.\n\nPropaganda about the supposedly higher quality of consumer goods and quality of life in the United States and Western Europe began to be transmitted to the Soviet population, along with western popular culture.\n\nThe outright prohibition of censorship was enshrined in Article 29 of the new 1993 Constitution of the Russian Federation. This did not end attempts by officials to restrict access to information in post-Soviet Russia or pressure by the authorities on media outlets not to publicise or discuss certain events or subjects. Monitoring of the infringement of media rights in the years from 2004 to 2013 would find that instances of censorship were the most commonly reported type of violation.\n\nThere were also periodic concerns about the extent of glasnost in court proceedings, as restrictions were placed on access to certain cases for the media and for the public.\n\n\n"}
{"id": "190192", "url": "https://en.wikipedia.org/wiki?curid=190192", "title": "Leviathan (Hobbes book)", "text": "Leviathan (Hobbes book)\n\nLeviathan or The Matter, Forme and Power of a Common-Wealth Ecclesiasticall and Civil, commonly referred to as Leviathan, is a book written by Thomas Hobbes (1588–1679) and published in 1651 (revised Latin edition 1668). Its name derives from the biblical Leviathan. The work concerns the structure of society and legitimate government, and is regarded as one of the earliest and most influential examples of social contract theory. \"Leviathan\" ranks as a classic Western work on statecraft comparable to Machiavelli's \"The Prince\". Written during the English Civil War (1642–1651), \"Leviathan\" argues for a social contract and rule by an absolute sovereign. Hobbes wrote that civil war and the brute situation of a state of nature (\"the war of all against all\") could only be avoided by strong, undivided government.\n\nThe title of Hobbes's treatise alludes to the Leviathan mentioned in the Book of Job. Unlike the more informative titles usually given to works of early modern political philosophy, such as John Locke's \"Two Treatises of Government\" or Hobbes's own earlier work \"The Elements of Law\", Hobbes selected a more poetic name for this more provocative treatise. Lexicographers in the early modern period believed that the term \"leviathan\" was associated with the Hebrew words \"lavah\", meaning \"to couple, connect, or join\", and \"thannin\", meaning \"a serpent or dragon\". In the Westminster Assembly's annotations on the Bible, the interpreters believed that the creature was named using these root words “because by his bignesse he seemes not one single creature, but a coupling of divers together; or because his scales are closed, or straitly compacted together.” Samuel Mintz suggests that these connotations lend themselves to Hobbes's understanding of political force since both \"Leviathan and Hobbes's sovereign are unities compacted out of separate individuals; they are omnipotent; they cannot be destroyed or divided; they inspire fear in men; they do not make pacts with men; theirs is the dominion of power.\"\n\nAfter lengthy discussion with Thomas Hobbes, the Parisian Abraham Bosse created the etching for the book's famous frontispiece in the \"géometrique\" style which Bosse himself had refined. It is similar in organisation to the frontispiece of Hobbes' \"De Cive\" (1642), created by Jean Matheus. The frontispiece has two main elements, of which the upper part is by far the more striking.\n\nIn it, a giant crowned figure is seen emerging from the landscape, clutching a sword and a crosier, beneath a quote from the Book of Job—\"Non est potestas Super Terram quae Comparetur ei. Iob. 41 . 24\" (\"There is no power on earth to be compared to him. Job 41 . 24\")—further linking the figure to the monster of that book. (Due to disagreements over the precise location of the chapters and verses when they were divided in the Late Middle Ages, the verse Hobbes quotes is usually given as Job 41:33 in modern Christian translations into English, Job 41:25 in the Masoretic text, Septuagint, and the Luther Bible; it is Iob 41:24 in the Vulgate.) The torso and arms of the figure are composed of over three hundred persons, in the style of Giuseppe Arcimboldo; all are facing inwards with just the giant's head having visible features. (A manuscript of \"Leviathan\" created for Charles II in 1651 has notable differences – a different main head but significantly the body is also composed of many faces, all looking outwards from the body and with a range of expressions.)\n\nThe lower portion is a triptych, framed in a wooden border. The centre form contains the title on an ornate curtain. The two sides reflect the sword and crosier of the main figure – earthly power on the left and the powers of the church on the right. Each side element reflects the equivalent power – castle to church, crown to mitre, cannon to excommunication, weapons to logic, and the battlefield to the religious courts. The giant holds the symbols of both sides, reflecting the union of secular, and spiritual in the sovereign, but the construction of the torso also makes the figure the state.\n\nHobbes begins his treatise on politics with an account of human nature. He presents an image of man as matter in motion, attempting to show through example how everything about humanity can be explained materialistically, that is, without recourse to an incorporeal, immaterial soul or a faculty for understanding ideas that are external to the human mind.\nHobbes proceeds by defining terms clearly and unsentimentally. Good and evil are nothing more than terms used to denote an individual's appetites and desires, while these appetites and desires are nothing more than the tendency to move toward or away from an object. Hope is nothing more than an appetite for a thing combined with an opinion that it can be had. He suggests that the dominant political theology of the time, Scholasticism, thrives on confused definitions of everyday words, such as \"incorporeal substance\", which for Hobbes is a contradiction in terms.\n\nHobbes describes human psychology without any reference to the \"summum bonum\", or greatest good, as previous thought had done. Not only is the concept of a \"summum bonum\" superfluous, but given the variability of human desires, there could be no such thing. Consequently, any political community that sought to provide the greatest good to its members would find itself driven by competing conceptions of that good with no way to decide among them. The result would be civil war.\n\nHowever, Hobbes states that there is a \"summum malum\", or greatest evil. This is the fear of violent death. A political community can be oriented around this fear.\n\nSince there is no \"summum bonum\", the natural state of man is not to be found in a political community that pursues the greatest good. But to be outside of a political community is to be in an anarchic condition. Given human nature, the variability of human desires, and need for scarce resources to fulfill those desires, the state of nature, as Hobbes calls this anarchic condition, must be a war of all against all. Even when two men are not fighting, there is no guarantee that the other will not try to kill him for his property or just out of an aggrieved sense of honour, and so they must constantly be on guard against one another. It is even reasonable to preemptively attack one's neighbour.\n\nThe desire to avoid the state of nature, as the place where the \"summum malum\" of violent death is most likely to occur, forms the polestar of political reasoning. It suggests a number of laws of nature, although Hobbes is quick to point out that they cannot properly speaking be called \"laws,\" since there is no one to enforce them. The first thing that reason suggests is to seek peace, but that where peace cannot be had, to use all of the advantages of war. Hobbes is explicit that in the state of nature nothing can be considered just or unjust, and every man must be considered to have a right to all things. The second law of nature is that one ought to be willing to renounce one's right to all things where others are willing to do the same, to quit the state of nature, and to erect a commonwealth with the authority to command them in all things. Hobbes concludes Part One by articulating an additional seventeen laws of nature that make the performance of the first two possible and by explaining what it would mean for a sovereign to represent the people even when they disagree with the sovereign.\n\nThe purpose of a commonwealth is given at the start of Part II: \n\nThe commonwealth is instituted when all agree in the following manner: \"I authorise and give up my right of governing myself to this man, or to this assembly of men, on this condition; that thou give up, thy right to him, and authorise all his actions in like manner.\"\n\nThe sovereign has twelve principal rights:\n\n\nHobbes explicitly rejects the idea of \"Separation of Powers\". In item 6 Hobbes is explicitly in favour of censorship of the press and restrictions on the rights of free speech should they be considered desirable by the sovereign to promote order.\n\nThere are three (monarchy, aristocracy and democracy):\n\nAnd only three; since unlike Aristotle he does not sub-divide them into \"good\" and \"deviant\":\n\nAnd monarchy is the best, on practical grounds:\n\nThe right of succession always lies with the sovereign. Democracies and aristocracies have easy succession; monarchy is harder:\n\nBecause in general people haven't thought carefully. However, the succession is definitely in the gift of the monarch:\n\nBut, it is not always obvious who the monarch has appointed:\n\nHowever, the answer is:\n\nAnd this means:\n\nNote that (perhaps rather radically) this does not have to be any blood relative:\n\nHowever, practically this means:\n\nIn \"Leviathan\", Hobbes explicitly states that the sovereign has authority to assert power over matters of faith and doctrine and that if he does not do so, he invites discord. Hobbes presents his own religious theory but states that he would defer to the will of the sovereign (when that was re-established: again, \"Leviathan\" was written during the Civil War) as to whether his theory was acceptable. Hobbes materialistic presuppositions also led him to hold a view which was considered highly controversial at the time. Hobbes rejected the idea of incorporeal substances and subsequently argued that even God himself was a corporeal substance. Although Hobbes never explicitly stated he was an atheist, many allude to the possibility that he is.\n\nThomas Hobbes also touched upon the sovereign's ability to tax in \"Leviathan\", although he is not as widely cited for his economic theories as he is for his political theories. Hobbes believed that equal justice includes the equal imposition of taxes. The equality of taxes doesn't depend on equality of wealth, but on the equality of the debt that every man owes to the commonwealth for his defence and the maintenance of the rule of law. Hobbes also championed public support for those unable to maintain themselves by labour, which would presumably be funded by taxation. He advocated public encouragement of works of Navigation etc. to usefully employ the poor who could work.\n\nIn Part III Hobbes seeks to investigate the nature of a \"Christian\" commonwealth. This immediately raises the question of which scriptures we should trust, and why. If any person may claim supernatural revelation superior to the civil law, then there would be chaos, and Hobbes' fervent desire is to avoid this. Hobbes thus begins by establishing that we cannot infallibly know another's personal word to be divine revelation:\n\nThis is good, but if applied too fervently would lead to all the Bible being rejected. So, Hobbes says, we need a test: and the true test is established by examining the books of scripture, and is:\n\n\"Seeing therefore miracles now cease\" means that only the books of the Bible can be trusted. Hobbes then discusses the various books which are accepted by various sects, and the \"question much disputed between the diverse sects of Christian religion, from whence the Scriptures derive their authority\". To Hobbes, \"it is manifest that none can know they are God's word (though all true Christians believe it) but those to whom God Himself hath revealed it supernaturally\". And therefore \"The question truly stated is: by what authority they are made law?\"\n\nUnsurprisingly, Hobbes concludes that ultimately there is no way to determine this other than the civil power:\n\nHe discusses the Ten Commandments, and asks \"who it was that gave to these written tables the obligatory force of laws. There is no doubt but they were made laws by God Himself: but because a law obliges not, nor is law to any but to them that acknowledge it to be the act of the sovereign, how could the people of Israel, that were forbidden to approach the mountain to hear what God said to Moses, be obliged to obedience to all those laws which Moses propounded to them?\" and concludes, as before, that \"making of the Scripture law, belonged to the civil sovereign.\"\n\nFinally: \"We are to consider now what office in the Church those persons have who, being civil sovereigns, have embraced also the Christian faith?\" to which the answer is: \"Christian kings are still the supreme pastors of their people, and have power to ordain what pastors they please, to teach the Church, that is, to teach the people committed to their charge.\"\n\nThere is an enormous amount of biblical scholarship in this third part. However, once Hobbes' initial argument is accepted (that no-one can know for sure anyone else's divine revelation) his conclusion (the religious power is subordinate to the civil) follows from his logic. The very extensive discussions of the chapter were probably necessary for its time. The need (as Hobbes saw it) for the civil sovereign to be supreme arose partly from the many sects that arose around the civil war, and to quash the \"Pope of Rome's challenge\", to which Hobbes devotes an extensive section.\n\nHobbes named Part IV of his book Kingdom of Darkness. By this, Hobbes does not mean Hell (he did not believe in Hell or Purgatory) but the darkness of ignorance as opposed to the light of true knowledge. Hobbes' interpretation is largely unorthodox and so sees much darkness in what he sees as the misinterpretation of Scripture.\n\nHobbes enumerates four causes of this darkness.\n\nThe first is by extinguishing the light of scripture through misinterpretation. Hobbes sees the main abuse as teaching that the kingdom of God can be found in the church, thus undermining the authority of the civil sovereign. Another general abuse of scripture, in his view, is the turning of consecration into conjuration, or silly ritual.\n\nThe second cause is the demonology of the heathen poets: in Hobbes's opinion, demons are nothing more than constructs of the brain. Hobbes then goes on to criticize what he sees as many of the practices of Catholicism: \"Now for the worship of saints, and images, and relics, and other things at this day practiced in the Church of Rome, I say they are not allowed by the word of God\".\n\nThe third is by mixing with the Scripture diverse relics of the religion, and much of the vain and erroneous philosophy of the Greeks, especially of Aristotle. Hobbes has little time for the various disputing sects of philosophers and objects to what people have taken \"From Aristotle's civil philosophy, they have learned to call all manner of Commonwealths but the popular (such as was at that time the state of Athens), tyranny\". At the end of this comes an interesting section (darkness is suppressing true knowledge as well as introducing falsehoods), which would appear to bear on the discoveries of Galileo Galilei. \"Our own navigations make manifest, and all men learned in human sciences now acknowledge, there are antipodes\" (i.e., the Earth is round) \"...Nevertheless, men... have been punished for it by authority ecclesiastical. But what reason is there for it? Is it because such opinions are contrary to true religion? That cannot be, if they be true.\" However, Hobbes is quite happy for the truth to be suppressed if necessary: if \"they tend to disorder in government, as countenancing rebellion or sedition? Then let them be silenced, and the teachers punished\" – but only by the civil authority.\n\nThe fourth is by mingling with both these, false or uncertain traditions, and feigned or uncertain history.\n\nHobbes finishes by inquiring who benefits from the errors he diagnoses:\n\nHobbes concludes that the beneficiaries are the churches and churchmen.\n\n\n\n\n"}
{"id": "311205", "url": "https://en.wikipedia.org/wiki?curid=311205", "title": "May 68", "text": "May 68\n\nBeginning in May 1968, a period of civil unrest occurred throughout France, lasting some seven weeks and punctuated by demonstrations, general strikes, and the occupation of universities and factories. At the height of events, which have since become known as May 68, the economy of France came to a halt. The protests reached such a point that political leaders feared civil war or revolution; the national government briefly ceased to function after President Charles de Gaulle secretly fled France to Germany at one point. The protests spurred movements worldwide, with songs, imaginative graffiti, posters, and slogans.\n\nThe unrest began with a series of student occupation protests against capitalism, consumerism, American imperialism and traditional institutions. Heavy police repression of the protesters led France's trade union confederations to call for sympathy strikes, which spread far more quickly than expected to involve 11 million workers, more than 22% of the total population of France at the time. The movement was characterized by spontaneous and decentralized wildcat disposition; this created a contrast and at times even conflict internally amongst the trade unions and the parties of the left. It was the largest general strike ever attempted in France, and the first nationwide wildcat general strike.\n\nThe student occupations and general strikes initiated across France were met with forceful confrontation by university administrators and police. The de Gaulle administration's attempts to quell those strikes by police action only inflamed the situation further, leading to street battles with the police in the Latin Quarter, Paris. \n\nHowever, by late May, the flow of events changed. The Grenelle accords concluded on 27 May, between the government, trade unions and employers, won significant wage gains for workers. A counter-demonstration organised by the Gaullist party on 29 May in central Paris gave De Gaulle the confidence to dissolve the National Assembly and call for parliamentary elections for 23 June 1968. Violence evaporated almost as quickly as it arose. Workers went back to their jobs, and when the elections were held in June, the Gaullists emerged stronger than before.\n\nThe events of May 1968 continue to influence French society. The period is considered a cultural, social and moral turning point in the history of the country. As Alain Geismar—one of the leaders of the time—later pointed out, the movement succeeded \"as a social revolution, not as a political one.\"\n\nIn February 1968, the French Communists and French Socialists formed an electoral alliance. Communists had long supported Socialist candidates in elections, but in the \"February Declaration\" the two parties agreed to attempt to form a joint government to replace President Charles de Gaulle and his Gaullist Party.\n\nOn 22 March far-left groups, a small number of prominent poets and musicians, and 150 students occupied an administration building at Paris University at Nanterre and held a meeting in the university council room dealing with class discrimination in French society and the political bureaucracy that controlled the university's funding. The university's administration called the police, who surrounded the university. After the publication of their wishes, the students left the building without any trouble. After this first record some leaders of what was named the \"Movement of 22 March\" were called together by the disciplinary committee of the university.\n\nFollowing months of conflicts between students and authorities at the Nanterre campus of the University of Paris (now Paris Nanterre University), the administration shut down the university on 2 May 1968. Students at the Sorbonne campus of the University of Paris (today Sorbonne University) in Paris met on 3 May to protest against the closure and the threatened expulsion of several students at Nanterre. On Monday, 6 May, the national student union, the Union Nationale des Étudiants de France (UNEF)—still the largest student union in France today—and the union of university teachers called a march to protest against the police invasion of Sorbonne. More than 20,000 students, teachers and supporters marched towards the Sorbonne, still sealed off by the police, who charged, wielding their batons, as soon as the marchers approached. While the crowd dispersed, some began to create barricades out of whatever was at hand, while others threw paving stones, forcing the police to retreat for a time. The police then responded with tear gas and charged the crowd again. Hundreds more students were arrested.\n\nHigh school student unions spoke in support of the riots on 6 May. The next day, they joined the students, teachers and increasing numbers of young workers who gathered at the Arc de Triomphe to demand that:\n\nNegotiations broke down, and students returned to their campuses after a false report that the government had agreed to reopen them, only to discover the police still occupying the schools. This led to a near revolutionary fervor among the students.\n\nOn Friday, 10 May, another huge crowd congregated on the Rive Gauche. When the Compagnies Républicaines de Sécurité again blocked them from crossing the river, the crowd again threw up barricades, which the police then attacked at 2:15 in the morning after negotiations once again floundered. The confrontation, which produced hundreds of arrests and injuries, lasted until dawn of the following day. The events were broadcast on radio as they occurred and the aftermath was shown on television the following day. Allegations were made that the police had participated in the riots, through \"agents provocateurs\", by burning cars and throwing Molotov cocktails.\n\nThe government's heavy-handed reaction brought on a wave of sympathy for the strikers. Many of the nation's more mainstream singers and poets joined after the police brutality came to light. American artists also began voicing support of the strikers. The major left union federations, the Confédération Générale du Travail (CGT) and the Force Ouvrière (CGT-FO), called a one-day general strike and demonstration for Monday, 13 May.\n\nWell over a million people marched through Paris on that day; the police stayed largely out of sight. Prime Minister Georges Pompidou personally announced the release of the prisoners and the reopening of the Sorbonne. However, the surge of strikes did not recede. Instead, the protesters became even more active.\n\nWhen the Sorbonne reopened, students occupied it and declared it an autonomous \"people's university\". Public opinion at first supported the students, but quickly turned against them after their leaders, invited to appear on national television, \"behaved like irresponsible utopianists who wanted to destroy the 'consumer society.'\" Nonetheless, in the weeks that followed, approximately 401 popular action committees were set up in Paris and elsewhere to take up grievances against the government and French society, including the Sorbonne Occupation Committee.\n\nBy the middle of May, demonstrations extended to factories, though its workers' demands significantly varied from that of the students. A union-led general strike on 13 May included 200,000 in a march. The strikes spread to all sectors of the French economy, including state-owned jobs, manufacturing and service industries, management, and administration. Across France, students occupied university structures and up to one-third of the country's workforce was on strike.\n\nThese strikes were not led by the union movement; on the contrary, the CGT tried to contain this spontaneous outbreak of militancy by channeling it into a struggle for higher wages and other economic demands. Workers put forward a broader, more political and more radical agenda, demanding the ousting of the government and President de Gaulle and attempting, in some cases, to run their factories. When the trade union leadership negotiated a 35% increase in the minimum wage, a 7% wage increase for other workers, and half normal pay for the time on strike with the major employers' associations, the workers occupying their factories refused to return to work and jeered their union leaders. In fact, in the May 68 movement there was a lot of \"anti-unionist euphoria,\" against the mainstream unions, the CGT, FO and CFDT, that were more willing to compromise with the powers that be than enact the will of the base.\n\nOn 24 May two people died at the hands of the out of control rioters. In Lyon, Police Inspector Rene Lacroix died when he was crushed by a driverless truck sent careering into police lines by rioters. In Paris, Phillipe Metherion, 26, was stabbed to death during an argument among demonstrators.\n\nAs the upheaval reached its apogee in late May, major trade unions met with employers' organizations and the French government to produce the Grenelle agreements, which would increase the minimum wage 35% and all salaries 10%, and granted employee protections and a shortened working day. The unions were forced to reject the agreement, based on opposition from their members, underscoring a disconnect in organizations that claimed to reflect working class interests.\n\nThe UNEF student union and CFDT trade union held a rally in the Charléty stadium with about 22,000 attendees. Its range of speakers reflected the divide between student and Communist factions. While the rally was held in the stadium partly for security, the insurrectionary messages of the speakers was dissonant with the relative amenities of the sports venue.\n\nThe Socialists saw an opportunity to act as a compromise between de Gaulle and the Communists. On 28 May, François Mitterrand of the Federation of the Democratic and Socialist Left declared that \"there is no more state\" and stated that he was ready to form a new government. He had received a surprisingly high 45% of the vote in the 1965 presidential election. On 29 May, Pierre Mendès France also stated that he was ready to form a new government; unlike Mitterrand he was willing to include the Communists. Although the Socialists did not have the Communists' ability to form large street demonstrations, they had more than 20% of the country's support.\n\nOn the morning of 29 May, de Gaulle postponed the meeting of the Council of Ministers scheduled for that day and secretly removed his personal papers from Élysée Palace. He told his son-in-law Alain de Boissieu, \"I do not want to give them a chance to attack the Élysée. It would be regrettable if blood were shed in my personal defense. I have decided to leave: nobody attacks an empty palace.\" De Gaulle refused Pompidou's request that he dissolve the National Assembly as he believed that their party, the Gaullists, would lose the resulting election. At 11:00 a.m., he told Pompidou, \"I am the past; you are the future; I embrace you.\"\n\nThe government announced that de Gaulle was going to his country home in Colombey-les-Deux-Églises before returning the next day, and rumors spread that he would prepare his resignation speech there. The presidential helicopter did not arrive in Colombey, however, and de Gaulle had told no one in the government where he was going. For more than six hours the world did not know where the French president was. The canceling of the ministerial meeting, and the president's mysterious disappearance, stunned the French, including Pompidou, who shouted, \"He has fled the country!\"\n\nThe national government had effectively ceased to function. Édouard Balladur later wrote that as prime minister, Pompidou \"by himself was the whole government\" as most officials were \"an incoherent group of confabulators\" who believed that revolution would soon occur. A friend of the prime minister offered him a weapon, saying, \"You will need it\"; Pompidou advised him to go home. One official reportedly began burning documents, while another asked an aide how far they could flee by automobile should revolutionaries seize fuel supplies. Withdrawing money from banks became difficult, gasoline for private automobiles was unavailable, and some people tried to obtain private planes or fake national identity cards.\n\nPompidou unsuccessfully requested that military radar be used to follow de Gaulle's two helicopters, but soon learned that he had gone to the headquarters of the French military in Germany, in Baden-Baden, to meet General Jacques Massu. Massu persuaded the discouraged de Gaulle to return to France; now knowing that he had the military's support, de Gaulle rescheduled the meeting of the Council of Ministers for the next day, 30 May, and returned to Colombey by 6:00 p.m. His wife Yvonne gave the family jewels to their son and daughter-in-law—who stayed in Baden for a few more days—for safekeeping, however, indicating that the de Gaulles still considered Germany a possible refuge. Massu kept as a state secret de Gaulle's loss of confidence until others disclosed it in 1982; until then most observers believed that his disappearance was intended to remind the French people of what they might lose. Although the disappearance was real and not intended as motivation, it indeed had such an effect on France.\n\nOn 30 May, 400,000 to 500,000 protesters (many more than the 50,000 the police were expecting) led by the CGT marched through Paris, chanting: \"Adieu, de Gaulle!\" (\"Farewell, de Gaulle!\"). Maurice Grimaud, head of the Paris police, played a key role in avoiding revolution by both speaking to and spying on the revolutionaries, and by carefully avoiding the use of force. While Communist leaders later denied that they had planned an armed uprising, and extreme militants only comprised 2% of the populace, they had overestimated de Gaulle's strength as shown by his escape to Germany. (One scholar, otherwise skeptical of the French Communists' willingness to maintain democracy after forming a government, has claimed that the \"moderate, nonviolent and essentially antirevolutionary\" Communists opposed revolution because they sincerely believed that the party must come to power through legal elections, not armed conflict that might provoke harsh repression from political opponents.)\n\nThe movement was largely centered around the Paris metropolitan area, and not elsewhere. Had the rebellion occupied key public buildings in Paris, the government would have had to use force to retake them. The resulting casualties could have incited a revolution, with the military moving from the provinces to retake Paris as in 1871. Minister of Defence Pierre Messmer and Chief of the Defence Staff Michel Fourquet prepared for such an action, and Pompidou had ordered tanks to Issy-les-Moulineaux. While the military was free of revolutionary sentiment, using an army mostly of conscripts the same age as the revolutionaries would have been very dangerous for the government. A survey taken immediately after the crisis found that 20% of Frenchmen would have supported a revolution, 23% would have opposed it, and 57% would have avoided physical participation in the conflict. 33% would have fought a military intervention, while only 5% would have supported it and a majority of the country would have avoided any action.\n\nAt 2:30 p.m. on 30 May, Pompidou persuaded de Gaulle to dissolve the National Assembly and call a new election by threatening to resign. At 4:30 p.m., de Gaulle broadcast his own refusal to resign. He announced an election, scheduled for 23 June, and ordered workers to return to work, threatening to institute a state of emergency if they did not. The government had leaked to the media that the army was outside Paris. Immediately after the speech, about 800,000 supporters marched through the Champs-Élysées waving the national flag; the Gaullists had planned the rally for several days, which attracted a crowd of diverse ages, occupations, and politics. The Communists agreed to the election, and the threat of revolution was over.\n\nFrom that point, the revolutionary feeling of the students and workers faded away. Workers gradually returned to work or were ousted from their plants by the police. The national student union called off street demonstrations. The government banned a number of leftist organizations. The police retook the Sorbonne on 16 June. Contrary to de Gaulle's fears, his party won the greatest victory in French parliamentary history in the legislative election held in June, taking 353 of 486 seats versus the Communists' 34 and the Socialists' 57. The February Declaration and its promise to include Communists in government likely hurt the Socialists in the election. Their opponents cited the example of the Czechoslovak National Front government of 1945, which led to a Communist takeover of the country in 1948. Socialist voters were divided; in a February 1968 survey a majority had favored allying with the Communists, but 44% believed that Communists would attempt to seize power once in government. (30% of Communist voters agreed.)\n\nOn Bastille Day, there were resurgent street demonstrations in the Latin Quarter, led by socialist students, leftists and communists wearing red arm-bands and anarchists wearing black arm-bands. The Paris police and the Compagnies Républicaines de Sécurité harshly responded starting around 10 pm and continuing through the night, on the streets, in police vans, at police stations, and in hospitals where many wounded were taken. There was, as a result, much bloodshed among students and tourists there for the evening's festivities. No charges were filed against police or demonstrators, but the governments of Britain and West Germany filed formal protests, including for the indecent assault of two English schoolgirls by police in a police station.\n\nDespite the size of de Gaulle's triumph, it was not a personal one. The post-crisis survey showed that a majority of the country saw de Gaulle as too old, too self-centered, too authoritarian, too conservative, and too anti-American. As the April 1969 referendum would show, the country was ready for \"Gaullism without de Gaulle\".\n\nSeveral examples:\n\nMay 1968 is an important reference point in French politics, representing for some the possibility of liberation and for others the dangers of anarchy. For some, May 1968 meant the end of traditional collective action and the beginning of a new era to be dominated mainly by the so-called new social movements.\n\nSomeone who took part in or supported this period of unrest is referred to as (literally a \"68-er\") — a term, derived from the French for \"68\", which has also entered the English language.\n\n\n\n\n\n\n"}
{"id": "1209133", "url": "https://en.wikipedia.org/wiki?curid=1209133", "title": "Orange Revolution", "text": "Orange Revolution\n\nThe Orange Revolution () was a series of protests and political events that took place in Ukraine from late November 2004 to January 2005, in the immediate aftermath of the run-off vote of the 2004 Ukrainian presidential election, which was claimed to be marred by massive corruption, voter intimidation and electoral fraud. Kiev, the Ukrainian capital, was the focal point of the movement's campaign of civil resistance, with thousands of protesters demonstrating daily. Nationwide, the revolution was highlighted by a series of acts of civil disobedience, sit-ins, and general strikes organized by the opposition movement.\n\nThe protests were prompted by reports from several domestic and foreign election monitors as well as the widespread public perception that the results of the run-off vote of 21 November 2004 between leading candidates Viktor Yushchenko and Viktor Yanukovych were rigged by the authorities in favour of the latter. The nationwide protests succeeded when the results of the original run-off were annulled, and a revote was ordered by Ukraine's Supreme Court for 26 December 2004. Under intense scrutiny by domestic and international observers, the second run-off was declared to be \"fair and free\". The final results showed a clear victory for Yushchenko, who received about 52% of the vote, compared to Yanukovych's 44%. Yushchenko was declared the official winner and with his inauguration on 23 January 2005 in Kiev, the Orange Revolution ended.\n\nIn the following years, the Orange Revolution had a negative connotation among pro-government circles in Belarus and Russia.\n\nIn the 2010 presidential election, Yanukovych became Yushchenko's successor as Ukrainian President after the Central Election Commission and international observers declared that the presidential election was conducted fairly. Yanukovych was ousted from power four years later following the February 2014 Euromaidan clashes in Kiev's Independence Square. Unlike the bloodless Orange Revolution, these protests resulted in more than 100 deaths, occurring mostly between 18 and 20 February 2014.\n\nGeorgiy Gongadze, a Ukrainian journalist and the founder of \"Ukrayinska Pravda\" (an Internet newspaper well known for publicising the corruption or unethical conduct of Ukrainian politicians) was kidnapped and murdered in 2000. Though no-one accused Ukrainian President Kuchma of personally murdering him, persistent rumours suggested that the President had ordered the killing. This murder sparked a movement against Kuchma in 2000 that can be seen as the origin of the Orange Revolution in 2004. After two terms of presidency (1994-2005) and the Cassette Scandal of 2000 that ruined his image irreparably, Kuchma decided not to run for a third term in the 2004 elections and instead supported Prime Minister Viktor Yanukovych in the presidential race against Viktor Yushchenko of the Our Ukraine–People's Self-Defense Bloc.\n\nThe state of Ukraine during the 2004 presidential election is considered an \"ideal condition\" for an outburst from the public. During this time Ukrainians were impatient while waiting for the economic and political transformation. The results of the election were thought to be fraudulent and considered \"a nail in the coffin\" of the preceding events.\n\nThe Ukrainian regime that was in power before the Orange Revolution created a path for a democratic society to emerge. It was based on a \"competitive authoritarian regime\" that is considered a \"hybrid regime\", allowing for a democracy and market economy to come to life. The election fraud emphasised the Ukrainian citizens' desire for a more pluralistic type of government.\n\nThe Cassette Scandal sparked the public's desire to create a social reform movement. It not only undermined the peoples' respect for Kuchma as a president, but also for the elite ruling class in general. Because of Kuchma's scandalous behaviour, he lost many of his supporters with high ranking government positions. Many of the government officials who were on his side went on to fully support the election campaign of Yushchenko and well as his ideas in general.\n\nAfter a clear lack of faith in the government had been instilled in the Ukrainian population, Yushchenko's role had never been more important to the revolution. Yushchenko was a charismatic candidate who showed no signs of being corrupt. Yuschenko was on the same level as his constituents and presented his ideas in a \"non-Soviet\" way. Young Ukrainian voters were extremely important to the outcome of the 2004 Presidential election. This new wave of younger people had different views of the main figures in Ukraine. They were exposed to a lot of negativity from the Kuchmagate and therefore had very skewed visions about Kuchma and his ability to lead their country.\n\nThe abundance of younger people who participated showed an increasing sense of nationalism that was developing in the country. The Orange Revolution had enough popular impact that it interested people of all ages. \n\nIn late 2002, Viktor Yushchenko (Our Ukraine), Oleksandr Moroz (Socialist Party of Ukraine), Petro Symonenko (Communist Party of Ukraine) and Yulia Tymoshenko (Yulia Tymoshenko Bloc) issued a joint statement concerning \"the beginning of a state revolution in Ukraine\". The communists left the alliance: Symonenko opposed the idea of a single candidate from the alliance in the Ukrainian presidential election of 2004; but the other three parties remained allies until July 2006. (In the autumn of 2001 both Tymoshenko and Yushchenko had broached the idea of setting up such a coalition.)\n\nOn 2 July 2004 Our Ukraine and the Yulia Tymoshenko Bloc established the \"Force of the People\", a coalition which aimed to stop \"the destructive process that has, as a result of the incumbent authorities, become a characteristic for Ukraine\" - at the time President Leonid Kuchma and Prime Minister Viktor Yanukovych were the \"incumbent authorities\" in Ukraine. The pact included a promise by Viktor Yushchenko to nominate Tymoshenko as Prime Minister if Yushchenko won the October 2004 presidential election.\n\nThe 2004 presidential election in Ukraine eventually featured two main candidates:\n\n\nThe election took place in a highly charged atmosphere, with the Yanukovych team and the outgoing president's administration using their control of the government and state apparatus for intimidation of Yushchenko and his supporters. In September 2004 Yushchenko suffered dioxin poisoning under mysterious circumstances. While he survived and returned to the campaign trail, the poisoning undermined his health and altered his appearance dramatically (his face remains disfigured by the consequences ).\n\nThe two main candidates were neck and neck in the first-round vote held on 31 October 2004, winning 39.32% (Yanukovych) and 39.87% (Yushchenko) of the vote casts. The candidates who came third and fourth collected much less: Oleksandr Moroz of the Socialist Party of Ukraine and Petro Symonenko of the Communist Party of Ukraine received 5.82% and 4.97%, respectively. Since no candidate had won more than 50% of the cast ballots, Ukrainian law mandated a run-off vote between two leading candidates. After the announcement of the run-off, Oleksandr Moroz threw his support behind Viktor Yushchenko. The Progressive Socialist Party's Natalia Vitrenko, who won 1.53% of the vote, endorsed Yanukovych, who hoped for Petro Simonenko's endorsement but did not receive it.\n\nIn the wake of the first round of the election, many complaints emerged regarding voting irregularities in favour of the government-supported Yanukovych. However, as it was clear that neither nominee was close enough to collecting an outright majority in the first round, challenging the initial result would not have affected the final outcome of the round. So the complaints were not actively pursued and both candidates concentrated on the upcoming run-off, scheduled for 21 November.\n\nPora! activists were arrested in October 2004, but the release of many (reportedly on President Kuchma's personal order) gave growing confidence to the opposition.\n\nYushchenko's supporters originally adopted orange as the signifying colour of his election campaign. Later, the colour gave its name to an entire series of political labels, such as \"the Oranges\" (\"Pomaranchevi\" in Ukrainian) for his political camp and its supporters. At the time when the mass protests grew, and especially when they brought about political change in the country, the term \"Orange Revolution\" came to represent the entire series of events.\n\nIn view of the success of using colour as a symbol to mobilise supporters, the Yanukovych camp chose blue for themselves.\n\nProtests began on the eve of the second round of voting, as the official count differed markedly from exit poll results which gave Yushchenko up to an 11% lead, while official results gave the election win to Yanukovych by 3%. While Yanukovych supporters have claimed that Yushchenko's connections to the Ukrainian media explain this disparity, the Yushchenko team publicised evidence of many incidents of electoral fraud in favour of the government-backed Yanukovych, witnessed by many local and foreign observers. These accusations were reinforced by similar allegations, though at a lesser scale, during the first presidential run of 31 October. \n\nThe Yushchenko campaign publicly called for protest on the dawn of election day, 21 November 2004, when allegations of fraud began to spread in the form of leaflets printed and distributed by the 'Democratic Initiatives' foundation, announcing that Yushchenko had won – on the basis of its exit poll. Beginning on 22 November 2004, massive protests started in cities across Ukraine: the largest, in Kiev's Maidan Nezalezhnosti \"(Independence Square)\", attracted an estimated 500,000 participants, who on 23 November 2004, peacefully marched in front of the headquarters of the Verkhovna Rada, the Ukrainian parliament, many wearing orange or carrying orange flags, the colour of Yushchenko's campaign coalition. One of the most prominent activists of that time was Paraska Korolyuk, subsequently bestowed with the Order of Princess Olga. From 22 November Pora! undertook the management of the protests in Kiev until the end of the demonstration.\n\nThe local councils in Kiev, Lviv, and several other cities passed, with the wide popular support of their constituency, a largely symbolic refusal to accept the legitimacy of the official election results, and Yushchenko took a symbolic presidential oath. This \"oath\" taken by Yushchenko in half-empty parliament chambers, lacking the quorum as only the Yushchenko-leaning factions were present, could not have any legal effect. But it was an important symbolic gesture meant to demonstrate the resolve of the Yushchenko campaign not to accept the compromised election results. In response, Yushchenko's opponents denounced him for taking an illegitimate oath, and even some of his moderate supporters were ambivalent about this act, while a more radical side of the Yushchenko camp demanded him to act even more decisively. Some observers argued that this symbolic presidential oath might have been useful to the Yushchenko camp should events have taken a more confrontational route. In such a scenario, this \"presidential oath\" Yushchenko took could be used to lend legitimacy to the claim that he, rather than his rival who tried to gain the presidency through alleged fraud, was a true commander-in-chief authorised to give orders to the military and security agencies.\n\nAt the same time, local officials in Eastern and Southern Ukraine, the stronghold of Viktor Yanukovych, started a series of actions alluding to the possibility of the breakup of Ukraine or an extra-constitutional federalisation of the country, should their candidate's claimed victory not be recognised. Demonstrations of public support for Yanukovych were held throughout Eastern Ukraine and some of his supporters arrived in Kiev. In Kiev the pro-Yanukovych demonstrators were far outnumbered by Yushchenko supporters, whose ranks were continuously swelled by new arrivals from many regions of Ukraine. The scale of the demonstrations in Kiev was unprecedented. By many estimates, on some days they drew up to one million people to the streets, in freezing weather.\n\nIn total 18.4% of Ukrainians have claimed to have taken part in the Orange Revolution (across Ukraine).\n\nAlthough Yushchenko entered into negotiations with outgoing President Leonid Kuchma in an effort to peacefully resolve the situation, the negotiations broke up on 24 November 2004. Yanukovych was officially certified as the victor by the Central Election Commission, which itself was allegedly involved in falsification of electoral results by withholding the information it was receiving from local districts and running a parallel illegal computer server to manipulate the results. The next morning after the certification took place, Yushchenko spoke to supporters in Kiev, urging them to begin a series of mass protests, general strikes and sit-ins with the intent of crippling the government and forcing it to concede defeat.\n\nIn view of the threat of illegitimate government acceding to power, Yushchenko's camp announced the creation of the \"Committee of National Salvation\" which declared a nationwide political strike.\n\nOn 1 December 2004, the Verkhovna Rada passed a resolution that strongly condemned pro-separatist and federalisation actions, and passed a non-confidence vote in the Cabinet of Ministers of Ukraine, a decision Prime Minister Yanukovych refused to recognise. By the Constitution of Ukraine, the non-confidence vote mandated the government's resignation, but the parliament had no means to enforce a resignation without the co-operation of Prime Minister Yanukovych and outgoing President Kuchma.\n\nOn 3 December 2004, Ukraine's Supreme Court finally broke the political deadlock. The court decided that due to the scale of the electoral fraud it became impossible to establish the election results. Therefore, it invalidated the official results that would have given Yanukovych the presidency. As a resolution, the court ordered a revote of the run-off to be held on 26 December 2004. This decision was seen as a victory for the Yushchenko camp while Yanukovych and his supporters favoured a rerun of the entire election rather than just the run-off, as a second-best option if Yanukovych was not awarded the presidency. On 8 December 2004 the parliament amended laws to provide a legal framework for the new round of elections. The parliament also approved the changes to the Constitution, implementing a political reform backed by outgoing President Kuchma as a part of a political compromise between the acting authorities and opposition.\n\nIn November 2009 Yanukovych stated that although his victory in the elections was \"taken away\", he gave up this victory in order to avoid bloodshed. \"I didn't want mothers to lose their children and wives their husbands. I didn't want dead bodies from Kyiv to flow down the Dnipro. I didn't want to assume power through bloodshed.\"\n\nThe 26 December revote was held under intense scrutiny of local and international observers. The preliminary results, announced by the Central Election Commission on 28 December, gave Yushchenko and Yanukovych 51.99% and 44.20% of the total vote which represented a change in the vote by +5.39% to Yushchenko and −5.27% from Yanukovych respectively when compared to the November poll. The Yanukovych team attempted to mount a fierce legal challenge to the election results using both the Ukrainian courts and the Election Commission complaint procedures. However, all their complaints were dismissed as without merit by both the Supreme Court of Ukraine and the Central Election Commission. On 10 January 2005 the Election Commission officially declared Yushchenko as the winner of the presidential election with the final results falling within 0.01% of the preliminary ones. This Election Commission announcement cleared the way for Yushchenko's inauguration as the President of Ukraine. The official ceremony took place in the Verkhovna Rada building on 23 January 2005 and was followed by the \"public inauguration\" of the newly sworn President at Maidan Nezalezhnosti (\"Independence Square\") in front of hundreds of thousands of his supporters. This event brought the Ukrainian Orange Revolution to its peaceful conclusion.\nAccording to one version of events recounted by \"The New York Times\", Ukrainian security agencies played an unusual role in the Orange Revolution, with a KGB successor agency in the former Soviet state providing qualified support to the political opposition. As per the paper report, on 28 November 2004 over 10,000 MVS (Internal Ministry) troops were mobilised to put down the protests in Independence Square in Kiev by the order of their commander, Lt. Gen. Sergei Popkov. The SBU (Security Service of Ukraine, a successor to the KGB in Ukraine) warned opposition leaders of the crackdown. Oleksander Galaka, head of GUR (military intelligence) made calls to \"prevent bloodshed\". Col. Gen. Ihor Smeshko (SBU chief) and Maj. Gen. Vitaly Romanchenko (military counter-intelligence chief) both claimed to have warned Popkov to pull back his troops, which he did, preventing bloodshed.\n\nIn addition to the desire to avoid bloodshed, the \"New York Times\" article suggests that \"siloviki\", as the security officers are often called in the countries of the former Soviet Union, were motivated by personal aversion to the possibility of having to serve President Yanukovych, who was in his youth convicted of robbery and assault and had alleged connection with corrupt businessmen, especially if he were to ascend to the presidency by fraud. The personal feelings of Gen. Smeshko towards Yanukovych may also have played a role. Additional evidence of Yushchenko's popularity and at least partial support among the SBU officers is shown by the fact that several embarrassing proofs of electoral fraud, including incriminating wiretap recordings of conversations among the Yanukovych campaign and government officials discussing how to rig the election, were provided to the Yushchenko camp. These conversations were likely recorded and provided to the opposition by sympathisers in the Ukrainian Security Services.\n\nAccording to Abel Polese, Kuchma was concerned about its reputation in the West; because of lack of natural resources to finance his regime he had to show a commitment to democracy in order to be targeted for Western financial assistance.\n\nThroughout the demonstrations, Ukraine's emerging Internet usage (facilitated by news sites which began to disseminate the Kuchma tapes) was an integral part of the orange revolutionary process. It has even been suggested that the Orange Revolution was the first example of an Internet-organised mass protest. Analysts believe that the Internet and mobile phones allowed an alternative media to flourish that was not subject to self-censorship or overt control by President Kuchma and his allies and pro-democracy activists (such as Pora!) were able to use mobile phones and the Internet to coordinate election monitoring and mass protests.\n\nAs part of the Orange Revolution, the Ukrainian constitution was changed to shift powers from the presidency to the parliament. This was Oleksandr Moroz's price for his decisive role in winning Yushchenko the presidency. The Communists also supported these measures. These came into effect in 2006 during which Yanukovych's Party of Regions won the parliamentary election, creating a coalition government with the Socialists and the Communists under his leadership. As a result, President Viktor Yushchenko had to deal with a powerful Prime Minister Viktor Yanukovych who had control of many important portfolios. His premiership ended in late 2007 after Yushchenko had succeeded in his months-long attempt to dissolve parliament. After the election, Yanukovych's party again was the largest, but Tymoshenko's finished far ahead of Yushchenko's for second place. The Orange parties won a very narrow majority, permitting a new government under Tymoshenko, but Yushchenko's political decline continued to his poor showing in the 2010 presidential election.\n\nOn 1 October 2010, the Constitutional Court of Ukraine overturned the 2004 amendments, considering them unconstitutional.\n\nA Circuit administrative court in Kiev forbade mass actions at Maidan Nezalezhnosti from 9 January 2010 to 5 February 2010. The Mayor's office had requested this in order to avoid \"nonstandard situations\" during the aftermath of the 2010 presidential election. Apparently (in particular) the Party of Regions, All-Ukrainian Union \"Fatherland\" and Svoboda had applied for a permit to demonstrate there. Incumbent President Viktor Yushchenko got 5,5% of votes during the election.\n\"Ukraine is a European democratic country\", said Yushchenko in a sort of political will at the polling station. \"It is a free nation and free people.\" According to him, this is one of the great achievements of the Orange Revolution.\n\nIn the 2010 presidential election Viktor Yanukovych was declared the winner which was labeled by some Yanukovych supporters as \"An end to this Orange nightmare\". Immediately after his election Yanukovych promised to \"clear the debris of misunderstanding and old problems that emerged during the years of the Orange power\". According to influential Party of Regions member Rinat Akhmetov the ideals of the Orange Revolution won at the 2010 election \"We had a fair and democratic independent election. The entire world recognised it, and international observers confirmed its results. That's why the ideals of the Orange Revolution won\". According to Yulia Tymoshenko the 2010 elections was a missed \"chance to become a worthy member of the European family and to put an end to the rule of the oligarchy\".\n\nPresident Viktor Yushchenko decreed in 2005 that 22 November (the starting day of the Orange Revolution) will be a non-public holiday \"Day of Freedom\". This date was moved to 22 January (and merged with Unification Day) by President Viktor Yanukovych late December 2011. President Yanukovych stated he moved \"Day of Freedom\" because of \"numerous appeals from the public\".\n\nOutright vote rigging diminished after the 2004 presidential election. No officials involved in the 2004 elections that preceded the Orange Revolution were convicted for election fraud.\n\nA 2007 research revealed that opinion about the nature of the Orange Revolution had barely shifted since 2004 and that the attitudes about it in the country remained divided along the same largely geographical lines that it had been at the time of the revolution (West and Central Ukraine being more positive about the events and South and Eastern Ukraine more cynical (seniors also)). This research (also) showed that Ukrainians in total had a less positive view on the Orange Revolution in 2007 than they had in 2005. It has been suggested that since the Orange Revolution was impactful enough to interest people of all ages it increased the overall unity of Ukraine.\n\nDuring the elections campaign of the 2012 Ukrainian parliamentary election the Party of Regions' campaign focused heavily on (what they called) \"the coach and ruins of 5 years of orange leadership\".\n\nIn March 2005 Ukrainian Foreign Minister Borys Tarasyuk stated that Ukraine would not be exporting revolution.\n\nDuring Alexander Lukashenko's inauguration (ceremony) as President of Belarus of 22 January 2011 Lukashenko vowed that Belarus would never have its own version of the Orange Revolution and Georgia's 2003 Rose Revolution. In the aftermath of the 2011 South Ossetian presidential election (in December 2011) and during the protests following the 2011 Russian elections (also in December 2011) the Ambassador of South Ossetia to the Russian Federation Dmitry Medoyev and Russian Prime Minister Vladimir Putin and Putin's supporters named the Orange Revolution an infamous foreknowledge for their countries. Putin also claimed that the organisers of the Russian protests in December 2011 were former (Russian) advisors to Yushchenko during his presidency and were transferring the Orange Revolution to Russia. A 4 February 2012 rally in favor of Putin was named the \"anti-Orange protest\". In 2013 a Russian State Duma Oleg Nilov and former fellow Russian politician Sergey Glazyev referred to political adversaries as \"different personalities in some sort of orange or bright shorts\" and \"diplomats and bureaucrats that appeared after the years of the 'orange' hysteria\". In 2016 the Russian newspaper Izvestia claimed\n\"in Central Asia weak regimes are already being attacked by extremists and 'Orange Revolutions'.\"\n\nIn Russian nationalist circles the Orange Revolution has been linked with fascism because, albeit marginal, Ukrainian nationalist extreme right-wing groups and Ukrainian Americans (including Viktor Yushchenko wife, Kateryna Yushchenko, who was born in the United States) were involved in the demonstrations; Russian nationalist groups see both as branches of the same tree of fascism. The involvement of Ukrainian Americans lead them to believe the Orange Revolution was steered by the CIA.\n\n"}
{"id": "25975", "url": "https://en.wikipedia.org/wiki?curid=25975", "title": "Reign of Terror", "text": "Reign of Terror\n\nThe Reign of Terror, or The Terror (), refers to a period during the French Revolution after the First French Republic was established in which multiple massacres and public executions occurred in response to revolutionary fervor, anti-clerical sentiment, and frivolous accusations of treason by Maximilien Robespierre and his Committee of Public Safety.\n\nSeveral historians consider the \"reign of terror\" to have begun in 1793, placing the starting date at either 5 September, June or March (birth of the Revolutionary Tribunal), while some consider it to have begun in September 1792 (September Massacres), or even July 1789 (when the first killing took place), but there is a consensus that it ended with the fall of Maximilien Robespierre in July 1794 as this led to the Thermidorian Reaction. Between June 1793 and the end of July 1794, there were 16,594 official death sentences in France, of which 2,639 were in Paris.\n\nThere was a sense of emergency among leading politicians in France in the summer of 1793 between the widespread civil war and counter-revolution. Bertrand Barère exclaimed on 5 September 1793 in the Convention: \"Let's make terror the order of the day!\" They were determined to avoid street violence such as the September Massacres of 1792 by taking violence into their own hands as an instrument of government.\n\nRobespierre in February 1794 in a speech explained the necessity of terror:\nSome historians argue that such terror was a necessary reaction to the circumstances. Others suggest there were additional causes, including ideological and emotional.\n\nEnlightenment thought emphasized the importance of rational thinking and began challenging legal and moral foundations of society, providing the leaders of the Reign of Terror with new ideas about the role and structure of government. Rousseau's Social Contract argued that each person was born with rights, and they would come together to form a government that would then protect those rights. Under the social contract, the government was required to act for the general will, which represented the interests of everyone rather than a few factions. Drawing from the idea of a general will, Robespierre felt that the French Revolution could result in a Republic built for the general will but only once those who fought this ideal were expelled. Those who resisted the government were deemed \"tyrants\" fighting against the virtue and honor of the general will. The leaders felt their ideal version of government was threatened from the inside and outside of France, and terror was the only way to preserve the dignity of the Republic created from French Revolution.\n\nRobespierre's ideology was not strictly derived from Rousseau. The writings of another Enlightenment thinker of the time, Baron de Montesquieu, greatly influenced Robespierre. One of Montesquieu's writings, \"The Spirit of the Laws\", defines a core principle of a democratic government: virtue. He describes it as \"the love of laws and of our country.\" In Robespierre's speech to the National Convention on 5 February 1794, \"On Political Morality\", he talks about virtue being the \"fundamental principle of popular or democratic government.\" This was, in fact, the same virtue defined by Montesquieu almost 50 years earlier. Robespierre believed that the virtue needed for any democratic government was extremely lacking in the French people. As a result, he decided to weed out those he believed could never possess this virtue. The result was a continual push towards Terror. The Convention used this as justification for the course of action to \"crush the enemies of the revolution, ... let the laws be executed, … and let liberty be saved.\"\n\nThese members of the Enlightenment movement greatly influenced revolutionary leaders; however, cautions from other Enlightenment thinkers were blatantly ignored. Voltaire's warnings were often overlooked, though some of his ideas were used for justification of the Revolution and the start of the Terror. He protested against Catholic Dogmas and the ways of Christianity stating, \"of all religions, the Christian should of course inspire the most toleration, but till now the Christians have been the most intolerant of all men.\" These criticisms were often used by Robespierre and other leaders as justification for their anti-religious reforms. Voltaire also laid down some warnings. In his \"Philosophical Dictionary\", he states, \"we are all steeped in weakness and error; let us forgive each other our follies; that is the first law of nature\" and \"every individual who persecutes a man, his brother, because he is not of his opinion, is a monster.\"\n\nAfter the beginning of the French Revolution, the surrounding monarchies did not show great hostility towards the rebellion. Though mostly ignored, Louis XVI was later able to find support in Leopold II of Austria (Marie Antoinette's brother) and Frederick William II of Prussia. On 27 August 1791, these foreign leaders made the Pillnitz Declaration, saying they would restore the French monarch if other European rulers joined. In response to what they viewed to be the meddling of foreign powers, France declared war on 20 April 1792. However, at this point, the war was only Prussia and Austria against France. France began this war with a large series of defeats, which set a precedent of fear of invasion in the people that would last throughout the war. Massive reforms of military institutions, while very effective in the long run, presented the initial problems of inexperienced forces and leaders of questionable political loyalty. In the time it took for officers of merit to use their new freedoms to climb the chain of command, France suffered. Many of the early battles were definitive losses for the French. There was the constant threat of the Austro-Prussian forces which were advancing easily toward the capital, threatening to destroy Paris if the monarch was harmed. This series of defeats, coupled with militant uprisings and protests within the borders of France, pushed the government to resort to drastic measures to ensure the loyalty of every citizen, not only to France but more importantly to the Revolution.\n\nWhile this series of losses was eventually broken, the reality of what might have happened if they persisted hung over France. The tide would not turn from them until September 1792 when the French won a critical victory at Valmy preventing the Austro-Prussian invasion. While the French military had stabilized and was producing victories by the time the Reign of Terror officially began, the pressure to succeed in this international struggle acted as justification for the government to pursue its tyrannical actions. It was not until after the execution of Louis XVI and the annexation of the Rhineland that the other monarchies began to feel threatened enough to form the First Coalition. The Coalition, consisting of Russia, Austria, Prussia, Spain, Holland, and Sardinia began attacking France from all directions, besieging and capturing ports and retaking ground lost to France. With so many similarities to the first days of the Revolutionary Wars for the French government, with threats on all sides, unification of the country became a top priority. As the war continued and the Reign of Terror began, leaders saw a correlation between using terror and achieving victory. Well phrased by Albert Soboul, \"terror, at first an improvised response to defeat, once organized became an instrument of victory.\" The threat of defeat and foreign invasion may have helped spur the origins of the Terror, but the timely coincidence of the Terror with French victories added justification to its growth.\n\nDuring the Reign of Terror, the sans-culottes and the Hébertists put pressure on the National Convention delegates and contributed to the overall instability of France. The National Convention was bitterly split between the Montagnards and the Girondins. The Girondins were more conservative leaders of the National Convention, while the Montagnards supported radical violence and pressures of the lower classes. Once the Montagnards gained control of the National Convention, they began demanding radical measures. Moreover, the sans-culottes, the urban workers of France, agitated leaders to inflict punishments on those who opposed the interests of the poor. The sans-culottes’ violent demonstrations pushing their demands, created constant pressure for the Montagnards to enact reform. The sans-culottes fed the frenzy of instability and chaos by utilizing popular pressure during the Revolution. For example, the sans-culottes sent letters and petitions to the Committee of Public Safety urging them to protect their interests and rights with measures such as taxation of foodstuffs that favored workers over the rich. They advocated for arrests of those deemed to oppose reforms against those with privilege, and the more militant members would advocate pillage in order to achieve the desired equality. The resulting instability caused problems that made forming the new Republic and achieving full political support even more critical.\n\nThe Reign of Terror was characterized by a dramatic rejection of long-held religious authority, its hierarchical structure, and the corrupt and intolerant influence of the aristocracy and clergy. Religious elements that long stood as symbols of stability for the French people, were replaced by reason and scientific thought. The radical revolutionaries and their supporters desired a cultural revolution that would rid the French state of all Christian influence. This process began with the fall of the monarchy, an event that effectively defrocked the State of its sanctification by the clergy via the doctrine of Divine Right and ushered in an era of reason.\n\nMany long-held rights and powers were stripped from the church and given to the state. In 1789, church lands were expropriated and priests killed or forced to leave France. A Festival of Reason was held in the Notre Dame Cathedral, which was renamed \"The Temple of Reason\", and the old traditional calendar was replaced with a new revolutionary one. The leaders of the Terror tried to address the call for these radical, revolutionary aspirations, while at the same time trying to maintain tight control on the de-Christianization movement that was threatening to the clear majority of the still devoted Catholic population of France. The tension sparked by these conflicting objectives laid a foundation for the \"justified\" use of terror to achieve revolutionary ideals and rid France of the religiosity that revolutionaries believed was standing in the way.\n\nOn 10 March 1793 the National Convention set up the Revolutionary Tribunal. Among those charged by the tribunal, about half were acquitted (though the number dropped to about a quarter after the enactment of the Law of 22 Prairial on 10 June 1794). In March rebellion broke out in the Vendée in response to mass conscription, which developed into a civil war. Discontent in the Vendée lasted - acoording to some accounts - until after the Terror.\n\nOn 6 April 1793 the National Convention established the Committee of Public Safety, which gradually became the \"de facto\" war-time government of France. \nThe Committee oversaw the Reign of Terror. \"During the Reign of Terror, at least 300,000 suspects were arrested; 17,000 were officially executed, and perhaps 10,000 died in prison or without trial.\" \n\nOn 2 June 1793 the Parisian sans-culottes surrounded the National Convention, calling for administrative and political purges, a low fixed-price for bread, and a limitation of the electoral franchise to sans-culottes alone. With the backing of the national guard, they persuaded the Convention to arrest 29 Girondist leaders. In reaction to the imprisonment of the Girondin deputies, some thirteen departments started the Federalist revolts against the National Convention in Paris, which were ultimately crushed.\n\nOn 24 June 1793 the Convention adopted the first republican constitution of France, the French Constitution of 1793. It was ratified by public referendum, but never put into force.\n\nOn 13 July 1793 the assassination of Jean-Paul Marat – a Jacobin leader and journalist – resulted in a further increase in Jacobin political influence. Georges Danton, the leader of the August 1792 uprising against the king, was removed from the Committee of Public Safety on 10 July 1793. On 27 July 1793 Robespierre became part of the Committee of Public Safety.\n\nOn 23 August 1793 the National Convention decreed the \"levée en masse\" - \"The young men shall fight; the married man shall forge arms and transport provisions; the women shall make tents and clothes and shall serve in the hospitals; the children shall turn all lint into linen; the old men shall betake themselves to the public square in order to arouse the courage of the warriors and preach hatred of kings and the unity of the Republic.\"\nOn 9 September the Convention established paramilitary forces, the \"revolutionary armies\", to force farmers to surrender grain demanded by the government. On 17 September, the Law of Suspects was passed, which authorized the imprisonment of vaguely defined \"suspects\". This created a mass overflow in the prison systems. On 29 September, the Convention extended price fixing from grain and bread to other essential goods, and also fixed wages.\n\nOn 10 October the Convention decreed that \"the provisional government shall be revolutionary until peace\". On 24 October the French Republican Calendar was enacted. The trial of the Girondins started on the same day, they were executed on 31 October.\n\nAnti-clerical sentiments increased during 1793 and a campaign of dechristianization occurred. On 10 November (20 Brumaire Year II of the French Republican Calendar), the Hébertists organized a Festival of Reason.\nOn 14 Frimaire (5 December 1793) the National Convention passed the Law of Frimaire, which gave the central government more control over the actions of the representatives on mission.\n\nOn 16 Pluviôse (4 February 1794), the National Convention decreed the abolition of slavery in all of France and in French colonies.\n\nOn 8 and 13 Ventôse (26 February and 3 March 1794), Saint-Just proposed decrees to confiscate the property of exiles and opponents of the revolution, known as the Ventôse Decrees.\n\nBy the end of 1793 two major factions had emerged, both threatening the Revolutionary Government: the Hébertists, who called for an intensification of the Terror and threatened insurrection, and the Dantonists, led by Georges Danton, who demanded moderation and clemency. The Committee of Public Safety took actions against both. The major Hébertists were tried before the Revolutionary Tribunal and executed on 24 March. The Dantonists were arrested on 30 March, tried on 3 to 5 April and executed on 5 April.\n\nOn 20 Prairial (8 June 1794) the Festival of the Supreme Being was celebrated across the country; this was part of the Cult of the Supreme Being, a deist national religion. On 22 Prairial (10 June), the National Convention passed a law proposed by Georges Couthon, known as the Law of 22 Prairial, which simplified the judicial process and greatly accelerated the work of the Revolutionary Tribunal. With the enactment of the law, the number of executions greatly increased, and the period from this time to the Thermidorian Reaction became known as \"The Great Terror\" ().\n\nOn 8 Messidor (26 June 1794), the French army won the Battle of Fleurus, which marked a turning point in France's military campaign and undermined the necessity of wartime measures and the legitimacy of the Revolutionary Government.\n\nThe fall of Robespierre was brought about by a combination of those who wanted more power for the Committee of Public Safety (and a more radical policy than he was willing to allow) and the moderates who completely opposed the revolutionary government. They had, between them, made the Law of 22 Prairial one of the charges against him, so that, after his fall, to advocate terror would be seen as adopting the policy of a convicted enemy of the republic, putting the advocate's own head at risk. Between his arrest and his execution, Robespierre may have tried to commit suicide by shooting himself, although the bullet wound he sustained, whatever its origin, only shattered his jaw. Alternatively, he may have been shot by the gendarme Merda. The great confusion that arose during the storming of the municipal Hall of Paris, where Robespierre and his friends had found refuge, makes it impossible to be sure of the wound's origin.\nIn any case, Robespierre was guillotined the next day.\n\nThe reign of the standing Committee of Public Safety was ended. New members were appointed the day after Robespierre's execution, and limits on terms of office were fixed (a quarter of the committee retired every three months). The Committee's powers were gradually eroded.\n\n\n"}
{"id": "28177", "url": "https://en.wikipedia.org/wiki?curid=28177", "title": "Simony", "text": "Simony\n\nSimony () is the act of selling church offices and roles or sacred things. It is named after Simon Magus, who is described in the Acts of the Apostles as having offered two disciples of Jesus payment in exchange for their empowering him to impart the power of the Holy Spirit to anyone on whom he would place his hands. The term extends to other forms of trafficking for money in \"spiritual things\".\n\nThe appointment of ecclesiastical officials, such as bishops and abbots, by a secular authority came to be considered simoniacal and this became a key issue during the Investiture Controversy.\n\nAlthough an offense against canon law, simony became widespread in the Catholic Church in the 9th and 10th centuries. In the canon law, the word bears a more extended meaning than in English law. \"Simony according to the canonists\", says John Ayliffe in his \"Parergon\",\n\nIn the \"Corpus Juris Canonici\", the \"Decretum\" and the Decretals of Gregory IX dealt with the subject. The offender whether \"simoniacus\" (the perpetrator of a simoniacal transaction) or \"simoniace promotus\" (the beneficiary of a simoniacal transaction), was liable to deprivation of his benefice and deposition from orders if a secular priest, or to confinement in a stricter monastery if a regular. No distinction seems to have been drawn between the sale of an immediate and of a reversionary interest. The innocent \"simoniace promotus\" was, apart from dispensation, liable to the same penalties as though he were guilty.\n\nCertain matters were simoniacal by the canon law but would not be regarded as such in English law. So grave was the crime of simony considered that even infamous persons (deprived of citizens' rights due to conviction) could accuse another of it. English provincial and legatine constitutions continually assailed simony.\n\nIn 1494 a member of the Carmelite order, Adam of Genoa was found murdered in his bed with twenty wounds after preaching against the practice of simony.\n\nIn the 14th century, Dante Alighieri depicted the punishment of many \"clergymen, and popes and cardinals\" in hell for being avaricious or miserly.\n\nHe also criticised certain popes and other simoniacs:\n\nThe Church of England struggled with the practice after its separation from Rome. For the purposes of English law, simony is defined by William Blackstone as \"obtain[ing] orders, or a licence to preach, by money or corrupt practices\" or, more narrowly, \"the corrupt presentation of any one to an ecclesiastical benefice for gift or reward\". While English law recognized simony as an offence, it treated it as merely an ecclesiastical matter, rather than a crime, for which the punishment was forfeiture of the office or any advantage from the offence and severance of any patronage relationship with the person who bestowed the office. Both Edward VI and Elizabeth I promulgated statutes against simony, in the latter case through the Simony Act 1588. The cases of Bishop of St. David's Thomas Watson in 1699 and of Dean of York William Cockburn in 1841 were particularly notable.\n\nBy the Benefices Act 1892, a person guilty of simony is guilty of an offence for which he may be proceeded against under the Clergy Discipline Act 1892. An innocent clerk is under no disability, as he might be by the canon law. Simony may be committed in three ways – in promotion to orders, in presentation to a benefice, and in resignation of a benefice. The common law (with which the canon law is incorporated, as far as it is not contrary to the common or statute law or the prerogative of the Crown) has been considerably modified by statute. Where no statute applies to the case, the doctrines of the canon law may still be of authority.\n\n, simony remains an offence. An unlawfully bestowed office can be declared void by the Crown, and the offender can be disabled from making future appointments and fined up to £1000. Clergy are no longer required to make a declaration as to simony on ordination, but offences are now likely to be dealt with under the Clergy Discipline Measure 2003, r.8.\n\n\n\n"}
{"id": "148283", "url": "https://en.wikipedia.org/wiki?curid=148283", "title": "Velvet Revolution", "text": "Velvet Revolution\n\nThe Velvet Revolution () or Gentle Revolution () was a non-violent transition of power in what was then Czechoslovakia, occurring from 17 November to 29 December 1989. Popular demonstrations against the one-party government of the Communist Party of Czechoslovakia included students and older dissidents. The result was the end of 41 years of one-party rule in Czechoslovakia, and the subsequent dismantling of the command economy and conversion to a parliamentary republic.\n\nOn 17 November 1989 (International Students' Day), riot police suppressed a student demonstration in Prague. The event marked the 50th anniversary of a violently suppressed demonstration against the Nazi storming of Prague University in 1939 where 1,200 students were arrested and 9 killed. (See Origin of International Students' Day for more information.) The 1989 event sparked a series of demonstrations from 17 November to late December and turned into an anti-communist demonstration. On 20 November, the number of protesters assembled in Prague grew from 200,000 the previous day to an estimated 500,000. The entire top leadership of the Communist Party, including General Secretary Miloš Jakeš, resigned on 24 November. On 27 November, a two-hour general strike involving all citizens of Czechoslovakia was held.\n\nIn response to the collapse of other Warsaw Pact governments and the increasing street protests, the Communist Party of Czechoslovakia announced on 28 November that it would relinquish power and end the one-party state. Two days later, the federal parliament formally deleted the sections of the Constitution giving the Communist Party a monopoly of power. Barbed wire and other obstructions were removed from the border with West Germany and Austria in early December. On 10 December, President Gustáv Husák appointed the first largely non-communist government in Czechoslovakia since 1948, and resigned. Alexander Dubček was elected speaker of the federal parliament on 28 December and Václav Havel the President of Czechoslovakia on 29 December 1989.\n\nIn June 1990, Czechoslovakia held its first democratic elections since 1946. On 1 January 1993, Czechoslovakia split into two countries—the Czech Republic and Slovakia.\n\nThe Communist Party seized power on 25 February 1948. No official opposition parties operated thereafter. Dissidents (notably Charter 77 and Civic Forum) created Music Clubs (on a limited basis as only allowed NGOs) and published home-made periodicals (samizdat). Charter 77 was quashed by the government and its signed members were persecuted until the fall of the regime in Czechoslovakia. Later, with the advent of the Civic Forum, independence could truly be seen on the horizon. Until Independence Day on 17 November 1989, the populace faced persecution by the authorities from the secret police. Thus, the general public did not openly support the dissidents for fear of dismissal from work or school. Writers or filmmakers could have their books or films banned for a \"negative attitude towards the socialist regime\". They also didn't allow Czechs and Slovaks to travel to other non-communist countries. Following this they banned music from foreign countries. This blacklisting included children of former entrepreneurs or non-Communist politicians, having family members living in the West, having supported Alexander Dubček during the Prague Spring, opposing Soviet military occupation, promoting religion, boycotting (rigged) parliamentary elections or signing Charter 77 or associating with those who did. These rules were easy to enforce, as all schools, media and businesses belonged to the state. They were under direct supervision and often were used as accusatory weapons against rivals.\n\nThe nature of blacklisting changed gradually after the introduction of Mikhail Gorbachev's policies of Glasnost (openness) and Perestroika (restructuring) in 1985. The Czechoslovak Communist leadership verbally supported Perestroika, but made few changes. Speaking about the Prague Spring of 1968 was taboo. The first anti-government demonstrations occurred in 1988 (the Candle Demonstration, for example) and 1989, but these were dispersed and participants were repressed by the police.\n\nBy the late 1980s, discontent with living standards and economic inadequacy gave way to popular support for economic reform. Citizens began to challenge the system more openly. By 1989, citizens who had been complacent were willing to openly express their discontent with the regime. Numerous important figures as well as ordinary workers signed petitions in support of Václav Havel during his imprisonment in 1989. Reform-minded attitudes were also reflected by the many individuals who signed a petition that circulated in the summer of 1989 calling for the end of censorship and the beginning of fundamental political reform.\n\nThe immediate impetus for the revolution came from developments in neighbouring countries and in the Czechoslovak capital. From August, East German citizens had occupied the West German Embassy in Prague and demanded exile to West Germany. In the days following 3 November, thousands of East Germans left Prague by train to West Germany. On 9 November, the Berlin Wall fell, removing the need for the detour.\n\nBy 16 November, many of Czechoslovakia's neighbours were beginning to shed authoritarian rule. The citizens of Czechoslovakia watched these events on TV through both foreign and domestic channels. The Soviet Union also supported a change in the ruling elite of Czechoslovakia, although it did not anticipate the overthrow of the Communist regime.\n\nOn the eve of International Students Day (the 50th anniversary of \"Sonderaktion Prag\", the 1939 storming of Prague universities by the Nazis), Slovak high school and university students organised a peaceful demonstration in the centre of Bratislava. The Communist Party of Slovakia had expected trouble, and the mere fact that the demonstration was organised was viewed as a problem by the Party. Armed forces were put on alert before the demonstration. In the end, however, the students moved through the city peacefully and sent a delegation to the Slovak Ministry of Education to discuss their demands.\n\nNew movements led by Václav Havel surfaced, invoking the idea of a united society where the state would politically restructure. The Socialist Union of Youth (SSM/SZM, proxy of Communist Party of Czechoslovakia) organised a mass demonstration on 17 November to commemorate International Students Day and the fiftieth anniversary of the murder of student Jan Opletal by the Nazi government.\n\nMost members of SSM were privately opposed to the Communist leadership, but were afraid of speaking up for fear of persecution. This demonstration gave average students an opportunity to join others and express their opinions. By 16:00, about 15,000 people joined the demonstration. They walked (per the strategy of founders of Stuha movement, Jiří Dienstbier and Šimon Pánek) to Karel Hynek Mácha's grave at Vyšehrad Cemetery and — after the official end of the march — continued into the centre of Prague, carrying banners and chanting anti-Communist slogans.\n\nAt about 19:30, the demonstrators were stopped by a cordon of riot police at Národní Street. They blocked all escape routes and attacked the students. Once all the protesters dispersed, one of the participants, secret police agent Ludvík Zifčák, was lying on the street. Zifčák was not physically hurt or pretending to be dead; he was overcome by emotion. Policemen carried his motionless body to an ambulance.\n\nThe atmosphere of fear and hopelessness gave birth to a hoax about a dead student. The story was made up by Dragomíra Dražská as she awaited treatment after she was hurt during the riot. Dražská worked at the college and shared her hoax with several people the next day, including the wife of journalist , a correspondent for Radio Free Europe/Radio Liberty. This incident mobilised the people and triggered the revolution. That same evening, students and theatre actors agreed to go on strike.\n\nTwo students visited Prime Minister Ladislav Adamec at his private residence and described to him what happened on Národní Street. The strike at the Realistic Theatre was declared and other theatres quickly followed. The theaters opened their stages only for public discussions.\n\nAt the initiative of students from the Academy of Performing Arts in Prague, the students in Prague went on strike. This strike was joined by university students throughout Czechoslovakia. Theatre employees and actors in Prague supported the strike. Instead of going on stage, actors read a proclamation by the students and artists to the audience, calling for a general strike on 27 November.\n\nHome-made posters and proclamations were posted. As all media (radio, TV, newspapers) were strictly controlled by the Communist Party (see Mass media in Communist Czechoslovakia), this was the only way to spread the message.\n\nIn the evening, Radio Free Europe reported that a student (named as Martin Šmíd) was killed by the police during the previous day's demonstration. Although the report was false, it heightened the feeling of crisis, and persuaded some hesitant citizens to overcome their fear and join the protests.\n\nTheatres in Bratislava, Brno, Ostrava and other towns went on strike. Members of artistic and literary associations as well as organisations and institutions joined the strike.\n\nMembers of a civic initiative met with the Prime Minister, who told them he was twice prohibited from resigning his post and that change requires mass demonstrations like those in East Germany (some 250,000 students). He asked them to keep the number of \"casualties\" during the expected change to a minimum.\n\nAbout 500 Slovak artists, scientists and leaders met at the Art Forum (Umelecká beseda) in Bratislava at 17:00. They denounced the attack against the students in Prague on 17 November and formed Public Against Violence, which would become the leading force behind the opposition movement in Slovakia. Its founding members included Milan Kňažko, and others.\n\nActors and members of the audience in a Prague theatre, together with Václav Havel and other prominent members of Charter 77 and other dissident organisations, established the Civic Forum (Občanské fórum, an equivalent of the Slovak Public Against Violence for the territory of the Czech Republic) as a mass popular movement for reforms. They called for the dismissal of top officials responsible for the violence, and an independent investigation of the incident and the release of all political prisoners.\n\nCollege students went on strike. On television, government officials called for peace and a return to the city's normal business. An interview with Martin Šmíd was broadcast to persuade the public that nobody had been killed, but the quality of the recording was low and rumours continued. It would take several more days to confirm that nobody was killed, and by then the revolution had gained further momentum.\n\nThe leaders of the Democratic Initiative presented several demands, including the resignation of the government, effective 25 November, and the formation of a temporary government composed of non-compromised members of the current government.\n\nStudents and theatres went on \"permanent\" strike. Police stopped a demonstration from continuing toward Prague Castle, which would have entered the striking theatres.\n\nCivic Forum representatives negotiated unofficially with Adamec without Havel, and Adamec was sympathetic to the students' demands. However, he was outvoted in a special cabinet meeting the same day. The government, in an official statement, made no concessions.\n\nCivic Forum added a demand: the abolition of the \"ruling position\" of the Communist Party from the Constitution. Non-Communist newspapers published information that contradicted the Communist interpretation. The first mass demonstration in Prague (100,000 people) and the first demonstrations in Bratislava occurred.\n\nThe first official meeting of the Civic Forum with the Prime Minister took place. The Prime Minister agreed to personally guarantee that no violence would be used against the people; however he would \"protect socialism, about which no discussion is possible\". An organised mass demonstration took place in Wenceslas Square in central Prague (demonstrations recurred there throughout the following days). Actors and students travelled to factories inside and outside Prague to gain support for their colleagues in other cities.\n\nA mass demonstration erupted in Hviezdoslav Square in downtown Bratislava (in the following days, it moved to the Square of the Slovak National Uprising). The students presented demands and asked the people to participate in the general strike planned for Monday, 27 November. A separate demonstration demanded the release of the political prisoner Ján Čarnogurský (later Prime Minister of Slovakia) in front of the Palace of Justice. Alexander Dubček addressed this demonstration—his first appearance during the Velvet Revolution. As a result, Čarnogurský was released on 23 November. Further demonstrations followed in all major cities of Czechoslovakia.\n\nCardinal František Tomášek, the Roman Catholic primate of the Bohemian lands, declared his support for the students and issued a declaration criticising the current government's policies. For the first time during the Velvet Revolution, the \"radical\" demand to abolish the article of the Constitution establishing the \"leading role\" of the Communist Party was expressed by Ľubomír Feldek at a meeting of Public Against Violence.\n\nIn the evening, Miloš Jakeš, the chairman of the Communist Party of Czechoslovakia, gave a special address on Federal Television. He said that order must be preserved, that socialism was the only alternative for Czechoslovakia, and criticised protest groups. Government officials, especially the Head of the Communist Party Miloš Jakeš, kept their hard-line position. During the night, they had summoned 4,000 members of the \"People's Militias\" (\"Lidové milice\", a paramilitary organisation subordinated directly to the Communist Party) to Prague to crush the protests, but called them off.\n\nCivic Forum announced a two-hour general strike for Monday, 27 November. The first live reports from the demonstration in Wenceslas Square appeared on Federal Television (and were quickly cut off, after one of the participants denounced the present government in favour of Alexander Dubček).\n\nStriking students forced the representatives of the Slovak government and of the Communist Party of Slovakia to participate in a dialogue, in which the official representatives were immediately put on the defensive. Employees of the Slovak section of the Federal Television required the leaders of the Federal Television to provide true information on the events in the country; otherwise they would initiate a strike of TV employees. Uncensored live reports from demonstrations in Bratislava began.\n\nThe evening news showed factory workers heckling Miroslav Štěpán, the Prague Communist Secretary. The military informed the Communist leadership of its readiness to act (ultimately, it was never used against demonstrators). The military and the Ministry of Defense were preparing for actions against the opposition. Immediately after the meeting, however, the Minister of Defence delivered a TV address announcing that the army would never undertake action against the people and called for an end to demonstrations.\n\nThe entire Presidium, including General Secretary Miloš Jakeš, resigned, and Karel Urbánek, a more moderate Communist, was named General Secretary. Federal Television showed pictures from 17 November for the first time and presented the first television address of Václav Havel, dealing mostly with the planned general strike. Czechoslovak TV and Radio announced that they would join the general strike. A discussion with representatives of the opposition was broadcast by the Slovak section of Federal Television. The opposition was represented by Ján Budaj, Fedor Gál and Vladimír Ondruš, while the Communists were represented by Štefan Chudoba (director of Bratislava automotive company), Peter Weiss (secretary of the Institute of Marx-Leninism of the Communist party of Slovakia) and the director of Steelworks Kosice. It was the first free discussion on Czechoslovak television since its inception. As a result, the editorial staff of Slovak newspapers started to join the opposition.\n\nThe new Communist leadership held a press conference, including Miroslav Štěpán while excluding Ladislav Adamec, but did not address the demands of the demonstrators. Later that day, Štěpán resigned as Prague Secretary. The number of participants in the regular anti-government demonstration in Prague-Letná reached an estimated 800,000 people. Demonstrations in Bratislava peaked at around 100,000 participants.\n\nPrime Minister Adamec met with Havel for the first time. The editorial staff of Slovakia's Pravda, the central newspaper of the Communist Party of Slovakia, joined the opposition.\n\nA successful two-hour general strike led by the civic movements strengthened what were at first a set of moderate demands into cries for a new government. The strike took place throughout the country between 12:00 and 14:00, supported by a reported 75% of the population. The Ministry of Culture released anti-Communist literature for public checkouts in libraries, effectively ending decades of censorship. Civic Forum demonstrated its capacity to disrupt the political order and thereby establish itself as the legitimate voice of the nation in negotiations with the state. The civic movements mobilised support for the general strike.\n\nThe Federal Assembly deleted the provision in the constitution referring to the \"leading role\" of the Communist Party, officially ending Communist rule in Czechoslovakia.\n\nPresident Gustáv Husák swore in the first government in 41 years that was not dominated by the Communist Party. He resigned shortly afterward.\n\nThe victory of the revolution was topped off by the election of rebel playwright and human rights activist Václav Havel as President of Czechoslovakia on 29 December 1989. Within weeks, Havel negotiated the removal of all Soviet troops (approx. 73,500) from Czechoslovakia. As per the agreement, the Soviet troops departed within months. Free elections held in June 1990 legitimised this government and set the stage for addressing the remnants of the Communist party's power and the legacy of the Communist period.\n\nThe main threat to political stability and the success of Czechoslovakia's shift to democracy appeared likely to come from ethnic conflicts between the Czechs and the Slovaks, which resurfaced in the post-Communist period. However, there was a general consensus to move toward a market economy, so in early 1990, the President and his top economic advisers decided to liberalise prices, push de-monopolisation and privatise the economy. The end of Communism meant the end of life-long employment, and a subsequent increase in unemployment. To combat this, the government implemented unemployment benefits and a minimum wage. The outcome of the transition to democracy and a market economy would depend on the extent to which developments outside the country facilitated or hindered the process of change.\n\nThe term \"Velvet Revolution\" was coined by Rita Klímová, the dissidents' English translator who later became the ambassador to the United States. The term was used internationally to describe the revolution, although the Czechs also used the term internally. After the dissolution of Czechoslovakia in 1993, Slovakia used the term \"Gentle Revolution\", the term that Slovaks used for the revolution from the beginning. The Czech Republic continues to refer to the event as the \"Velvet Revolution\".\n\nTheorists of revolutions, such as Jaroslav Krejčí, have argued that the \"Velvet Revolution\" was not, in fact, a true revolution because a revolution by definition accomplishes change by means of illegitimate violence. Contending theories of revolution argue that the Velvet Revolution is a legitimate revolution because it is a \"revolutionary situation\" of contested sovereignty that led to a transfer of power (\"revolutionary outcome\").\n\nIn the months leading up to and during the revolution, citizens dispersed ideas using flyers distributed en masse. Hundreds of discrete flyers with varying messages were printed, but most shared the same ideals. In the summer of 1989, one of the most widely circulated documents was \"The Eight Rules of Dialogue,\" which advocated for truth, understanding and empathy, informed and respectful discussion, abstaining from ad hominem attacks, and an open mind. Other documents focused less on communication techniques and more on ideals. Democracy, freedom, nonviolence, fairness, and humanness were prevalent themes, as well as self-organisation, political representation, and improved working conditions.\n\nConspiracy theorists tried to portray the revolution as a plot by the StB, KGB, reformists among party members or Mikhail Gorbachev. According to these theories, the Communist Party only transformed its power into other, less visible forms and still controls society. Belief in such theories has decreased.\n\nThe events of November 1989 confirmed that outside factors were significant catalysts for the downfall of Communism in Czechoslovakia. Therefore, the transformations in Poland and Hungary and the collapse of the regime in East Germany, Karelia both of which could be traced to the new attitude of the Soviets toward East Europe, encouraged Czechs and Slovaks to take to the streets to win their freedom. However, national factors, including the economic and political crisis and the actions of groups and individuals working towards a transformation, destabilised support for the system.\n\nThe State's reaction to the strikes demonstrated that while global isolation produceding pressures for political, social, and economic change, the events that followed could not be predetermined. Hardly anyone thought that the Communist State could collapse so quickly. Striking students and theatres did not seem likely to intimidate a state that was able to suppress any sort of demonstration. This \"popular\" phase of the revolution, was followed by victories made possible by the Civic Forum's successful mobilisation for the general strike on 27 November, which established its legitimacy to speak for the nation in negotiations with the state. The mass demonstrations that followed 17 November led to the resignation of the Party leadership of Milos Jakes, the removal of the Party from its leading role and the creation of the non-Communist government. Supporters of the revolution had to take instant responsibility for running the government, in addition to establishing essential reforms in political organisation and values, economic structure and policies and foreign policy.\n\nOne element of the demonstrations of the Velvet Revolution was the jingling of keys to signify support. The practice had a double meaning—it symbolized the unlocking of doors and was the demonstrators' way of telling the Communists, \"Goodbye, it's time to go home.\"\n\nA commemorative 2 Euro coin was issued by Slovakia on 17 November 2009, to mark the twentieth anniversary. The coin depicts a bell with a key adjoining the clapper. Ursula K. Le Guin wrote a short story, \"Unlocking the Air\", in which the jingling of keys played a central role in the liberation of a fictional country called Orsinia.\n\n\n\n\n\n"}
{"id": "209443", "url": "https://en.wikipedia.org/wiki?curid=209443", "title": "Civil society", "text": "Civil society\n\nCivil society can be understood as the \"third sector\" of society, distinct from government and business, and including the family and the private sphere. By other authors, \"civil society\" is used in the sense of 1) the aggregate of non-governmental organizations and institutions that manifest interests and will of citizens or 2) individuals and organizations in a society which are independent of the government.\n\nSometimes the term \"civil society\" is used in the more general sense of \"the elements such as freedom of speech, an independent judiciary, etc, that make up a democratic society\" (\"Collins English Dictionary\"). Especially in the discussions among thinkers of Eastern and Central Europe, civil society is seen also as a normative concept of civic values.\n\nThe term \"civil society\" goes back to Aristotle's phrase \"koinōnía politikḗ\" (κοινωνία πολιτική), occurring in his \"Politics\", where it refers to a ‘political community’, commensurate with the Greek city-state (\"polis\") characterized by a shared set of norms and ethos, in which free citizens on an equal footing lived under the rule of law. The \"telos\" or end of civil society, thus defined, was eudaimonia (τὸ εὖ ζῆν \"tò eu zēn\") (often translated as human flourishing or common well-being), in as man was defined as a ‘political (social) animal’ (ζῷον πολιτικόν \"zōon politikón\"). The concept was used by Roman writers, such as Cicero, where it referred to the ancient notion of a republic (\"res publica\"). It re-entered into Western political discourse following one of the late medieval translations of Aristotle's \"Politics\" into Latin by Leonardo Bruni who as a first translated \"koinōnía politikḗ\" into \"societas civilis\". With the rise of a distinction between monarchical autonomy and public law, the term then gained currency to denote the corporate estates (\"Ständestaat\") of a feudal elite of land-holders as opposed to the powers exercised by the prince. It had a long history in state theory, and was revived with particular force in recent times, in Eastern Europe, where dissidents such as Václav Havel as late as in the 1990's employed it to denote the sphere of civic associations threatened by the intrusive holistic state-dominated regimes of Communist Eastern Europe. The first post-modern usage of civil society as denoting political opposition stems from writings of Aleksander Smolar in 1978-79. However the term was not in use by Solidarity labor union in 1980-1981 and was popularized on a global scale by anti-communist propaganda only in 1989 as a tool of legitimation of neoliberal transformation.\n\nThe literature on relations between civil society and democratic political society have their roots in classical liberal writings of G.W.F. Hegel from whom they were adapted by Alexis de Tocqueville, Karl Marx, and Ferdinand Tönnies. They were developed in significant ways by 20th century researchers Gabriel Almond and Sidney Verba, who identified the role of political culture in a democratic order as vital.\n\nThey argued that the political element of political organizations facilitates better awareness and a more informed citizenry, who make better voting choices, participate in politics, and hold government more accountable as a result. The statutes of these political organizations have been considered micro-constitutions because they accustom participants to the formalities of democratic decision making.\n\nMore recently, Robert D. Putnam has argued that even non-political organizations in civil society are vital for democracy. This is because they build social capital, trust and shared values, which are transferred into the political sphere and help to hold society together, facilitating an understanding of the interconnectedness of society and interests within it.\n\nOthers, however, have questioned the link between civil society and robust democracy. Some have noted that the civil society actors have now obtained a remarkable amount of political power without anyone directly electing or appointing them. It has been argued that civil society aided the Nazi Party in coming to power in 1930s Germany. It has also been argued that civil society is biased towards the global north. Partha Chatterjee has argued that, in most of the world, \"civil society is demographically limited.\" For Jai Sen civil society is a neo-colonial project driven by global elites in their own interests. Finally, other scholars have argued that, since the concept of civil society is closely related to democracy and representation, it should in turn be linked with ideas of nationality and nationalism. Latest analyses suggest that civil society is a neoliberal ideology legitimizing antidemocratic attacks of economic elites on institutions of the welfare state through the development of the third sector as its substitute.\n\nConstitutional economics is a field of economics and constitutionalism which describes and analyzes the specific interrelationships between constitutional issues and functioning of the economy including budget process. The term \"constitutional economics\" was used by American economist James M. Buchanan as a name for a new budget planning and the latter's transparency to the civil society, are of the primary guiding importance to the implementation of the rule of law. Also, the availability of an effective court system, to be used by the civil society in situations of unfair government spending and executive impoundment of any previously authorized appropriations, becomes a key element for the success of any influential civil society.\n\nCritics and activists currently often apply the term \"civil society\" to the domain of social life which needs to be protected against globalization, and to the sources of resistance thereto, because it is seen as acting beyond boundaries and across different territories. However, as civil society can, under many definitions, include and be funded and directed by those businesses and institutions (especially donors linked to European and Northern states) who support globalization, this is a contested use. Rapid development of civil society on the global scale after the fall of the communist system was a part of neo-liberal strategies linked to the Washington Consensus. Some studies have also been published, which deal with unresolved issues regarding the use of the term in connection with the impact and conceptual power of the international aid system (see for example Tvedt 1998).\n\nOn the other hand, others see globalization as a social phenomenon expanding the sphere of classical liberal values, which inevitably led to a larger role for civil society at the expense of politically derived state institutions.\n\nThe integrated Civil Society Organizations (iCSO) System, developed by the Department of Economic and Social Affairs (DESA), facilitates interactions between civil society organizations and DESA.\n\nCivil Societies also have become involved in the environmental policy making process. These groups impact the environmental policies by setting an agenda on fixing the harms done to the environment. They also get the public informed about environmental issues, which increases the public demand for environmental change.\n\nFrom a historical perspective, the actual meaning of the concept of civil society has changed twice from its original, classical form. The first change occurred after the French Revolution, the second during the fall of communism in Europe.\n\nThe concept of civil society in its pre-modern classical republican understanding is usually connected to the early-modern thought of Age of Enlightenment in the 18th century. However, it has much older history in the realm of political thought. Generally, civil society has been referred to as a political association governing social conflict through the imposition of rules that restrain citizens from harming one another. In the classical period, the concept was used as a synonym for the good society, and seen as indistinguishable from the state. For instance, Socrates taught that conflicts within society should be resolved through public argument using ‘dialectic’, a form of rational dialogue to uncover truth. According to Socrates, public argument through ‘dialectic’ was imperative to ensure ‘civility’ in the polis and ‘good life’ of the people. For Plato, the ideal state was a just society in which people dedicate themselves to the common good, practice civic virtues of wisdom, courage, moderation and justice, and perform the occupational role to which they were best suited. It was the duty of the ‘philosopher king’ to look after people in civility. Aristotle thought the polis was an ‘association of associations’ that enables citizens to share in the virtuous task of ruling and being ruled. His \"koinonia politike\" as political community.\n\nThe concept of \"societas civilis\" is Roman and was introduced by Cicero. The political discourse in the classical period, places importance on the idea of a ‘good society’ in ensuring peace and order among the people. The philosophers in the classical period did not make any distinction between the state and society. Rather they held that the state represented the civil form of society and ‘civility’ represented the requirement of good citizenship. Moreover, they held that human beings are inherently rational so that they can collectively shape the nature of the society they belong to. In addition, human beings have the capacity to voluntarily gather for the common cause and maintain peace in society. By holding this view, we can say that classical political thinkers endorsed the genesis of civil society in its original sense.\n\nThe Middle Ages saw major changes in the topics discussed by political philosophers. Due to the unique political arrangements of feudalism, the concept of classical civil society practically disappeared from mainstream discussion. Instead conversation was dominated by problems of just war, a preoccupation that would last until the end of Renaissance.\n\nThe Thirty Years' War and the subsequent Treaty of Westphalia heralded the birth of the sovereign states system. The Treaty endorsed states as territorially-based political units having sovereignty. As a result, the monarchs were able to exert domestic control by emasculating the feudal lords and to stop relying on the latter for armed troops. Henceforth, monarchs could form national armies and deploy a professional bureaucracy and fiscal departments, which enabled them to maintain direct control and supreme authority over their subjects. In order to meet administrative expenditures, monarchs controlled the economy. This gave birth to absolutism. Until the mid-eighteenth century, absolutism was the hallmark of Europe.\n\nThe absolutist concept of the state was disputed in the Enlightenment period. As a natural consequence of Renaissance, Humanism, and the scientific revolution, the Enlightenment thinkers raised fundamental questions such as \"What legitimacy does heredity confer?\", \"Why are governments instituted?\", \"Why should some human beings have more basic rights than others?\", and so on. These questions led them to make certain assumptions about the nature of the human mind, the sources of political and moral authority, the reasons behind absolutism, and how to move beyond absolutism. The Enlightenment thinkers believed in the inherent goodness of the human mind. They opposed the alliance between the state and the Church as the enemy of human progress and well-being because the coercive apparatus of the state curbed individual liberty and the Church legitimated monarchs by positing the theory of divine origin. Therefore, both were deemed to be against the will of the people.\n\nStrongly influenced by the atrocities of Thirty Years' War, the political philosophers of the time held that social relations should be ordered in a different way from natural law conditions. Some of their attempts led to the emergence of social contract theory that contested social relations existing in accordance with human nature. They held that human nature can be understood by analyzing objective realities and natural law conditions. Thus they endorsed that the nature of human beings should be encompassed by the contours of state and established positive laws. Thomas Hobbes underlined the need of a powerful state to maintain civility in society. For Hobbes, human beings are motivated by self-interests (Graham 1997:23). Moreover, these self-interests are often contradictory in nature. Therefore, in state of nature, there was a condition of a war of all against all. In such a situation, life was \"solitary, poor, nasty, brutish and short\" (Ibid: 25). Upon realizing the danger of anarchy, human beings became aware of the need of a mechanism to protect them. As far as Hobbes was concerned, rationality and self-interests persuaded human beings to combine in agreement, to surrender sovereignty to a common power (Kaviraj 2001:289). Hobbes called this common power, state, Leviathan.\n\nJohn Locke had a similar concept to Hobbes about the political condition in England. It was the period of the Glorious Revolution, marked by the struggle between the divine right of the Crown and the political rights of Parliament. This influenced Locke to forge a social contract theory of a limited state and a powerful society. In Locke's view, human beings led also an unpeaceful life in the state of nature. However, it could be maintained at the sub-optimal level in the absence of a sufficient system (Brown 2001:73). From that major concern, people gathered together to sign a contract and constituted a common public authority. Nevertheless, Locke held that the consolidation of political power can be turned into autocracy, if it is not brought under reliable restrictions (Kaviraj 2001:291). Therefore, Locke set forth two treaties on government with reciprocal obligations. In the first treaty, people submit themselves to the common public authority. This authority has the power to enact and maintain laws. The second treaty contains the limitations of authority, i. e., the state has no power to threaten the basic rights of human beings. As far as Locke was concerned, the basic rights of human beings are the preservation of life, liberty and property. Moreover, he held that the state must operate within the bounds of civil and natural laws.\n\nBoth Hobbes and Locke had set forth a system, in which peaceful coexistence among human beings could be ensured through social pacts or contracts. They considered civil society as a community that maintained civil life, the realm where civic virtues and rights were derived from natural laws. However, they did not hold that civil society was a separate realm from the state. Rather, they underlined the co-existence of the state and civil society. The systematic approaches of Hobbes and Locke (in their analysis of social relations) were largely influenced by the experiences in their period. Their attempts to explain human nature, natural laws, the social contract and the formation of government had challenged the divine right theory. In contrast to divine right, Hobbes and Locke claimed that humans can design their political order. This idea had a great impact on the thinkers in the Enlightenment period.\n\nThe Enlightenment thinkers argued that human beings are rational and can shape their destiny. Hence, no need of an absolute authority to control them. Both Jean-Jacques Rousseau, a critic of civil society, and Immanuel Kant argued that people are peace lovers and that wars are the creation of absolute regimes (Burchill 2001:33). As far as Kant was concerned, this system was effective to guard against the domination of a single interest and check the tyranny of the majority (Alagappa 2004:30).\n\nG. W. F. Hegel completely changed the meaning of civil society, giving rise to a modern liberal understanding of it as a form of non-political society as opposed to institutions of modern nation state. While in classical republicanism civil society where synonymous with political society, Hegel distinguished political state and civil society, what was followed by Tocqueville's distinction between civil and political societies and associations, repeated by Marx and Tönnies.\n\nUnlike his predecessors, Hegel considered civil society () as a separate realm, a \"system of needs\", that is the, \"[stage of] difference which intervenes between the family and the state.\" Civil society is the realm of economic relationships as it exists in the modern industrial capitalist society, for it had emerged at the particular period of capitalism and served its interests: individual rights and private property. Hence, he used the German term \"bürgerliche Gesellschaft\" to denote civil society as \"civilian society\" – a sphere regulated by the civil code. This new way of thinking about civil society was followed by Alexis de Tocqueville and Karl Marx as well. For Hegel, civil society manifested contradictory forces. Being the realm of capitalist interests, there is a possibility of conflicts and inequalities within it (ex: mental and physical aptitude, talents and financial circumstances). He argued that these inequalities influence the choices that members are able to make in relation to the type of work they will do. The diverse positions in Civil Society fall into three estates: the substantial estate (agriculture), the formal estate (trade and industry), and the universal estate (civil society). A man is able to choose his estate, though his choice is limited by the aforementioned inequalities. However, Hegel argues that these inequalities enable all estates in Civil Society to be filled, which leads to a more efficient system on the whole.\n\nKarl Marx followed the Hegelian way of using the concept of civil society. For Marx, the emergence of the modern state created a realm of civil society that reduced society to private interests competing against each other. Political society was autonomised into the state, which was in turn ruled by the bourgeois class (consider also that suffrage only belonged, then, to propertied men). Marx, in his early writings, anticipated the abolition of the separation between state and civil society, and looked forward to the reunification of private and public/political realms (Colletti, 1975). Hence, Marx rejected the positive role of state put forth by Hegel. Marx argued that the state cannot be a neutral problem solver. Rather, he depicted the state as the defender of the interests of the bourgeoisie. He considered the state to be the executive arm of the bourgeoisie, which would wither away once the working class took democratic control of society.\n\nThe above view about civil society was criticised by Antonio Gramsci (Edwards 2004:10). Departing somewhat from Marx, Gramsci did not consider civil society as a realm of private and alienated relationships. Rather, Gramsci viewed civil society as the vehicle for bourgeois hegemony, when it just represents a particular class. He underlined the crucial role of civil society as the contributor of the cultural and ideological capital required for the survival of the hegemony of capitalism. Rather than posing it as a problem, as in earlier Marxist conceptions, Gramsci viewed civil society as the site for problem-solving. Misunderstanding Gramsci, the New Left assigned civil society a key role in defending people against the state and the market and in asserting the democratic will to influence the state. At the same time, neo-liberal thinkers consider civil society as a site for struggle to subvert Communist and authoritarian regimes. Thus, the term civil society occupies an important place in the political discourses of the New Left and neo-liberals.\n\nIt is commonly believed that the post-modern way of understanding civil society was first developed by political opposition in the former Soviet bloc East European countries in the 1980s. However, research shows that communist propaganda had the most important influence on the development and popularization of the idea instead, in an effort to legitimize neoliberal transformation in 1989. According to theory of restructurization of welfare systems, a new way of using the concept of civil society became a neoliberal ideology legitimizing development of the third sector as a substitute for the welfare state. The recent development of the third sector is a result of this welfare systems restructuring, rather than of democratization.\n\nFrom that time stems a political practice of using the idea of civil society instead of political society. Henceforth, postmodern usage of the idea of civil society became divided into two main ones: as political society and as the third sector – apart from plethora of definitions. The Washington Consensus of the 1990s, which involved conditioned loans by the World Bank and IMF to debt-laden developing states, also created pressures for states in poorer countries to shrink. This in turn led to practical changes for civil society that went on to influence the theoretical debate. Initially the new conditionality led to an even greater emphasis on \"civil society\" as a panacea, replacing the state's service provision and social care, Hulme and Edwards suggested that it was now seen as \"the magic bullet.\"\n\nBy the end of the 1990s civil society was seen less as a panacea amid the growth of the anti-globalization movement and the transition of many countries to democracy; instead, civil society was increasingly called on to justify its legitimacy and democratic credentials. This led to the creation by the UN of a high level panel on civil society. However, in the 1990s with the emergence of the nongovernmental organizations and the new social movements (NSMs) on a global scale, civil society as a third sector became treated as a key terrain of strategic action to construct ‘an alternative social and world order.’ Post-modern civil society theory has now largely returned to a more neutral stance, but with marked differences between the study of the phenomena in richer societies and writing on civil society in developing states.\n\nJürgen Habermas said that the public sphere encourages rational will-formation; it is a sphere of rational and democratic social interaction. Habermas analyzes civil society as a sphere of \"commodity exchange and social labor\" and public sphere as a part of political realm. Habermas argues that even though society was representative of capitalist society, there are some institutions that were part of political society. Transformations in economy brought transformations to the public sphere. Though these transformations happen, a civil society develops into political society when it emerges as non-economic and has a populous aspect, and when the state is not represented by just one political party. There needs to be a locus of authority, and this is where society can begin to challenge authority. Jillian Schwedler points out that civil society emerges with the resurrection of the public sphere when individuals and groups begin to challenge boundaries of permissible behaviour – for example, by speaking out against the regime or demanding a government response to social needs – civil society begins to take shape.\n\nCivil society organizations, also known as civic organizations, include among others:\n\n"}
{"id": "294833", "url": "https://en.wikipedia.org/wiki?curid=294833", "title": "Institution", "text": "Institution\n\nInstitutions, according to Samuel P. Huntington, are \"stable, valued, recurring patterns of behavior\". Further, institutions can refer to mechanisms which govern the behavior of a set of individuals within a given community; moreover, institutions are identified with a social purpose, transcending individuals and intentions by mediating the rules that govern living behavior. According to Geoffrey M. Hodgson, it is misleading to say that an institution is a form of behavior. Instead, Hodgson states that institution are \"integrated systems of rules that structure social interactions\".\n\nThe term \"institution\" commonly applies to both informal institutions such as customs, or behavior patterns important to a society, and to particular formal institutions created by entities such as the government and public services. Primary or meta-institutions are institutions such as the family that are broad enough to encompass other institutions.\n\nInstitutions are a principal object of study in social sciences such as political science, anthropology, economics, and sociology (the latter described by Émile Durkheim as the \"science of institutions, their genesis and their functioning\"). Institutions are also a central concern for law, the formal mechanism for political rule-making and enforcement.\n\nPeople may deliberately create individual, formal organizations commonly identified as \"institutions\"—but the development and function of institutions in society in general may be regarded as an instance of emergence. That is, institutions arise, develop and function in a pattern of social self-organization beyond conscious intentions of the individuals involved.\n\nAs mechanisms of social interaction, institutions manifest in both \"formal\" organizations, such as the U.S. Congress, or the Roman Catholic Church, and, also, in \"informal\" social order and organization, reflecting human psychology, culture, habits and customs, and encompassing subjective experience of meaningful enactments. Formal institutions are explicitly set forth by a relevant authority and informal institutions are generally unwritten societal rules, norms, and traditions.\n\nPrimary or meta-institutions are institutions that encompass many other institutions, both formal and informal (e.g. the family, government, the economy, education, and religion. ) Most important institutions, considered abstractly, have both objective and subjective aspects: examples include money and marriage. The institution of money encompasses many formal organizations, including banks and government treasury departments and stock exchanges, which may be termed, \"institutions\", as well as subjective experiences, which guide people in their pursuit of personal well-being. Powerful institutions are able to imbue a paper currency with certain value, and to induce millions into production and trade in pursuit of economic ends abstractly denominated in that currency's units. The subjective experience of money is so pervasive and persuasive that economists talk of the \"money illusion\" and try to disabuse their students of it, in preparation for learning economic analysis.\n\nExamples of institutions include:\n\nIn an extended context:\n\n\nInformal institutions have been largely overlooked in comparative politics, but in many countries it is the informal institutions and rules that govern the political landscape. To understand the political behaviour in a country it is important to look at how that behaviour is enabled or constrained by informal institutions, and how this affects how formal institutions are run. For example, if there are high levels of extrajudicial killings in a country, it might be that while it is prohibited by the state the police are actually enabled to carry out such killings and informally encouraged to prop up an inefficient formal state police institution. An informal institution tends to have socially shared rules, which are unwritten and yet are often known by all inhabitants of a certain country, as such they are often referred to as being an inherent part of the culture of a given country. Informal practices are often referred to as \"cultural\", for example clientelism or corruption is sometimes stated as a part of the political culture in a certain place, but an informal institution itself is not cultural, it may be shaped by culture or behaviour of a given political landscape, but they should be looked at in the same way as formal institutions to understand their role in a given country. Informal institutions might be particularly used to pursue a political agenda, or a course of action that might not be publicly popular, or even legal, and can be seen as an effective way of making up for lack of efficiency in a formal institution. For example, in countries where formal institutions are particularly inefficient, an informal institution may be the most cost effective way or actually carrying out a given task, and this ensures that there is little pressure on the formal institutions to become more efficient. The relationship between formal and informal institutions is often closely aligned and informal institutions step in to prop up inefficient institutions. However, because they do not have a centre, which directs and coordinates their actions, changing informal institutions is a slow and lengthy process. It is as such important to look at any given country and note the presence of informal institutions when looking at the political landscape, and note that they are not necessarily a rejection of the state, but an integral part of it and broadening the scope of the role of the state in a given country.\n\nWhile institutions tend to appear to people in society as part of the natural, unchanging landscape of their lives, study of institutions by the social sciences tends to reveal the nature of institutions as social constructions, artifacts of a particular time, culture and society, produced by collective human choice, though not directly by individual intention. Sociology traditionally analyzed social institutions in terms of interlocking social roles and expectations. Social institutions created and were composed of groups of roles, or expected behaviors. The social function of the institution was executed by the fulfillment of roles. Basic biological requirements, for reproduction and care of the young, are served by the institutions of marriage and family, for example, by creating, elaborating and prescribing the behaviors expected for husband/father, wife/mother, child, etc.\n\nThe relationship of institutions to human nature is a foundational question for the social sciences. Institutions can be seen as \"naturally\" arising from, and conforming to, human nature—a fundamentally conservative view—or institutions can be seen as artificial, almost accidental, and in need of architectural redesign, informed by expert social analysis, to better serve human needs—a fundamentally progressive view. Adam Smith anchored his economics in the supposed human \"propensity to truck, barter and exchange\". Modern feminists have criticized traditional marriage and other institutions as element of an oppressive and obsolete patriarchy. The Marxist view—which sees human nature as historically 'evolving' towards voluntary social cooperation, shared by some anarchists—is that supra-individual institutions such as the market and the state are incompatible with the individual liberty of a truly free society.\n\nEconomics, in recent years, has used game theory to study institutions from two perspectives. Firstly, how do institutions survive and evolve? In this perspective, institutions arise from Nash equilibria of games. For example, whenever people pass each other in a corridor or thoroughfare, there is a need for customs, which avoid collisions. Such a custom might call for each party to keep to their own right (or left—such a choice is arbitrary, it is only necessary that the choice be uniform and consistent). Such customs may be supposed to be the origin of rules, such as the rule, adopted in many countries, which requires driving automobiles on the right side of the road.\n\nSecondly, how do institutions affect behaviour? In this perspective, the focus is on behaviour arising from a given set of institutional rules. In these models, institutions determine the rules (i.e. strategy sets and utility functions) of games, rather than arise as equilibria out of games. Douglass North argues, the very emergence of an institution reflects behavioral adaptations through his application of increasing returns. Over time institutions develop rules that incentivize certain behaviors over others because they present less risk or induce lower cost, and establish path dependent outcomes. For example, the Cournot duopoly model is based on an institution involving an auctioneer who sells all goods at the market-clearing price. While it is always possible to analyze behaviour with the institutions-as-equilibria approach instead, it is much more complicated.\n\nIn political science, the effect of institutions on behavior has also been considered from a meme perspective, like game theory borrowed from biology. A \"memetic institutionalism\" has been proposed, suggesting that institutions provide selection environments for political action, whereby differentiated retention arises and thereby a Darwinian evolution of institutions over time. Public choice theory, another branch of economics with a close relationship to political science, considers how government policy choices are made, and seeks to determine what the policy outputs are likely to be, given a particular political decision-making process and context. Credibility thesis purports that institutions emerge from intentional institution-building but never in the originally intended form. Instead, institutional development is endogenous and spontaneously ordered and institutional persistence can be explained by their credibility, which is provided by the function that particular institutions serve.\n\nIn history, a distinction between eras or periods, implies a major and fundamental change in the system of institutions governing a society. Political and military events are judged to be of historical significance to the extent that they are associated with changes in institutions. In European history, particular significance is attached to the long transition from the feudal institutions of the Middle Ages to the modern institutions, which govern contemporary life.\n\nIn order to understand why some institutions persist and other institutions only appear in certain contexts, it is important to understand what drives institutional change.\nAcemoglu, Johnson and Robinson assert that institutional change is endogenous. They posit a framework for institutional change that is rooted in the distribution of resources across society and preexisting political institutions. These two factors determine de jure and de facto political power, respectively, which in turn defines this period's economic institutions and next period's political institutions. Finally, the current economic institutions determine next period's distribution of resources and the cycle repeats. Douglass North attributes institutional change to the work of \"political entrepreneurs\", who see personal opportunities to be derived from a change institutional framework. These entrepreneurs weigh the expected costs of altering the institutional framework against the benefits they can derive from the change. North describes institutional change as a process that is extremely incremental, and that works through both formal and informal institutions. Lipscy argues that patterns of institutional change vary according to underlying characteristics of issue areas, such as network effects. Carmine Guerriero provides a theory of the design of formal and informal institutions---like market design, legal traditions, property rights, a culture of cooperation and inclusive political institutions---based on the unifying idea that institutions help solve inefficiencies in the exchange of assets and input and in the investment in innovation. In the case of a culture of cooperation and inclusive political institutions, Guerriero builds on the institutional revolution that shook Europe during the Middle Ages to develop a model highlighting two ideas. First, a profitable enough joint investment pushes the elite to introduce more inclusive political institutions to convince the citizens that a sufficient part of its return will be shared via public spending and so, they should cooperate. Second, the citizens accumulate culture to share consumption risk and to commit to cooperating in investment at its intermediate values threatening democracy. Consistent with these predictions, in a panel of 90 European regions spanning the period 1000–1600, reforms towards tighter constraints on the elite's power were driven by factors curbing the observability of farming investments and raising the value of long-distance trade, i.e., terrain ruggedness and direct access to the coast. Moreover, the activity of the Cistercians and Franciscans, who dictated charity-based norms of cooperation in exchange for guidance on how to share consumption risk, intensified with the climate volatility and with the severity of shocks to the investment value, i.e., changes in the trading routes.\n\nNorth argues that because of the preexisting influence that existing organizations have over the existing framework, change that is brought about is often in the interests of these organizations. This produces a phenomenon called path dependence, which states that institutional patterns are persistent and endure over time. These paths are determined at critical junctures, analogous to a fork in the road, whose outcome leads to a narrowing of possible future outcomes. Once a choice is made during a critical juncture, it becomes progressively difficult to return to the initial point where the choice was made. James Mahoney studies path dependence in the context of national regime change in Central America and finds that liberal policy choices of Central American leaders in the 19th century was the critical juncture that led to the divergent levels of development that we see in these countries today. The policy choices that leaders made in the context of liberal reform policy led to a variety of self-reinforcing institutions that created divergent development outcomes for the Central American countries.\n\nThough institutions are persistent, North states that paths can change course when external forces weaken the power of an existing organization. This allows other entrepreneurs to affect change in the institutional framework. This change can also occur as a result of gridlock between political actors produced by a lack of mediating institutions and an inability to reach a bargain. Artificial implementation of institutional change has been tested in political development but can have unintended consequences. North, Wallis, and Weingast divide societies into different social orders: open access orders, which about a dozen developed countries fall into today, and limited access orders, which accounts for the rest of the countries. Open access orders and limited access orders differ fundamentally in the way power and influence is distributed. As a result, open access institutions placed in limited access orders face limited success and are often coopted by the powerful elite for self-enrichment. Transition to more democratic institutions is not created simply by transplanting these institutions into new contexts, but happens when it is in the interest of the dominant coalition to widen access.\n\nIan Lustick suggests that the social sciences, particularly those with the institution as a central concept, can benefit by applying the concept of natural selection to the study of how institutions change over time. By viewing institutions as existing within a fitness landscape, Lustick argues that the gradual improvements typical of many institutions can be seen as analogous to hill-climbing within one of these fitness landscapes. This can eventually lead to institutions becoming stuck on local maxima, such that for the institution to improve any further, it would first need to decrease its overall fitness score (e.g., adopt policies that may cause short-term harm to the institution's members). The tendency to get stuck on local maxima can explain why certain types of institutions may continue to have policies that are harmful to its members or to the institution itself, even when members and leadership are all aware of the faults of these policies.\n\nAs an example, Lustick cites Amyx's analysis of the gradual rise of the Japanese economy and its seemingly sudden reversal in the so-called \"Lost Decade\". According to Amyx, Japanese experts were not unaware of the possible causes of Japan's economic decline. Rather, to return Japan's economy back to the path to economic prosperity, policymakers would have had to adopt policies that would first cause short-term harm to the Japanese people and government. Under this analysis, says Ian Lustick, Japan was stuck on a \"local maxima\", which it arrived at through gradual increases in its fitness level, set by the economic landscape of the 1970s and 80s. Without an accompanying change in institutional flexibility, Japan was unable to adapt to changing conditions, and even though experts may have known which changes the country needed, they would have been virtually powerless to enact those changes without instituting unpopular policies that would have been harmful in the short-term.\n\nThe lessons from Lustick's analysis applied to Sweden's economic situation can similarly apply to the political gridlock that often characterizes politics in the United States. For example, Lustick observes that any politician who hopes to run for elected office stands very little to no chance if they enact policies that show no short-term results. Unfortunately, there is a mismatch between policies that bring about short-term benefits with minimal sacrifice, and those that bring about long-lasting change by encouraging institution-level adaptations.\n\nThere are some criticisms to Lustick's application of natural selection theory to institutional change. Lustick himself notes that identifying the inability of institutions to adapt as a symptom of being stuck on a local maxima within a fitness landscape does nothing to solve the problem. At the very least, however, it might add credibility to the idea that truly beneficial change might require short-term harm to institutions and their members. David Sloan Wilson notes that Lustick needs to more carefully distinguish between two concepts: multilevel selection theory and evolution on multi-peaked landscapes. Bradley Thayer points out that the concept of a fitness landscape and local maxima only makes sense if one institution can be said to be \"better\" than another, and this in turn only makes sense insofar as there exists some objective measure of an institution's quality. This may be relatively simple in evaluating the economic prosperity of a society, for example, but it is difficult to see how objectively a measure can be applied to the amount of freedom of a society, or the quality of life of the individuals within.\n\nThe term \"institutionalization\" is widely used in social theory to refer to the process of embedding something (for example a concept, a social role, a particular value or mode of behavior) within an organization, social system, or society as a whole. The term may also be used to refer to committing a particular individual to an institution, such as a mental institution. To this extent, \"institutionalization\" may carry negative connotations regarding the treatment of, and damage caused to, vulnerable human beings by the oppressive or corrupt application of inflexible systems of social, medical, or legal controls by publicly owned, private or not-for-profit organizations.\n\nThe term \"institutionalization\" may also be used in a political sense to apply to the creation or organization of governmental institutions or particular bodies responsible for overseeing or implementing policy, for example in welfare or development.\n\n\n\n"}
{"id": "286537", "url": "https://en.wikipedia.org/wiki?curid=286537", "title": "Consortium", "text": "Consortium\n\nA consortium is an association of two or more individuals, companies, organizations or governments (or any combination of these entities) with the objective of participating in a common activity or pooling their resources for achieving a common goal.\n\n\"Consortium\" is a Latin word meaning \"partnership\", \"association\" or \"society\", and derives from \"consors\" 'partner', itself from \"con-\" 'together' and \"sors\" 'fate', meaning \"owner of means\" or \"comrade\".\n\nThe Big Ten Academic Alliance, Claremont Colleges consortium in Southern California, Five College Consortium in Massachusetts, and Consórcio Nacional Honda are among the oldest and most successful higher education consortia in the World. The Big Ten Academic Alliance, formerly known as the Committee on Institutional Cooperation, includes the members of the Big Ten athletic conference. The participants in Five Colleges, Inc. are: Amherst College, Hampshire College, Mount Holyoke College, Smith College, and the University of Massachusetts Amherst. Another example of a successful consortium is the Five Colleges of Ohio of Ohio: Oberlin College, Ohio Wesleyan University, Kenyon College, College of Wooster and Denison University. The aforementioned Claremont Consortium (known as the Claremont Colleges) consists of Pomona College, Claremont Graduate University, Scripps College, Claremont McKenna College, Harvey Mudd College, Pitzer College, and Keck Graduate Institute. These consortia have pooled the resources of their member colleges and the universities to share human and material assets as well as to link academic and administrative resources.\n\nAn example of a non-profit consortium is the Appalachian College Association located in Richmond, Kentucky. The association consists of 35 private liberal arts colleges and universities spread across the central Appalachian mountains in Kentucky, North Carolina, Tennessee, Virginia, and West Virginia. Collectively these higher education institutions serve approximately 42,500 students. Six research universities in the region (University of Kentucky, University of North Carolina, University of Tennessee, West Virginia University, University of Virginia, and Virginia Tech) are affiliated with the ACA. These institutions assist the ACA in reviewing grant and fellowship applications, conducting workshops, and providing technical assistance. The ACA works to serve higher education in the rural regions of these five states.\n\nAn example of a for-profit consortium is a group of banks that collaborate to make a loan—also known as a syndicate. This type of loan is more commonly known as a syndicated loan. In England it is common for a consortium to buy out financially struggling football clubs in order to keep them out of liquidation.\n\nAlyeska Pipeline Service Company, the company that built the Trans-Alaska Pipeline System in the 1970s, initially was a consortium of BP, ARCO, ConocoPhillips, Exxon, Mobil, Unocal, and Koch Alaska Pipeline Company.\n\nAirbus Industries was formed in 1970 as a consortium of aerospace manufacturers. The retention of production and engineering assets by the partner companies in effect made Airbus Industries a sales and marketing company. This arrangement led to inefficiencies due to the inherent conflicts of interest that the four partner companies faced; they were both shareholders of, and subcontractors to, the consortium. The companies collaborated on development of the Airbus range, but guarded the financial details of their own production activities and sought to maximize the transfer prices of their sub-assemblies.\n\nIn 2001, EADS (created by the merger of French, German and Spanish Airbus partner companies) and BAE Systems (the British partner company) transferred their Airbus production assets to a new company, Airbus SAS. In return, they got 80% and 20% shares respectively. BAE would later sell its share to EADS.\n\nThe Tornado was developed and built by Panavia Aircraft GmbH, a tri-national consortium consisting of British Aerospace (previously British Aircraft Corporation), MBB of West Germany, and Aeritalia of Italy. It first flew on 14 August 1974 and was introduced into service in 1979–1980. Due to its multirole design, it was able to replace several different fleets of aircraft in the adopting air forces. The Royal Saudi Air Force (RSAF) became the only export operator of the Tornado in addition to the three original partner nations. Including all variants, 992 aircraft were built.\n\n\"Coopetition\" is a word coined from cooperation and competition. It is used when companies otherwise competitors collaborate in a consortium to cooperate on areas non-strategic for their core businesses. They prefer to reduce their costs on these non-strategic areas and compete on other areas where they can differentiate better.\n\nFor example, the GENIVI Alliance is a not-for-profit consortium between different car makers in order to ease building an In-Vehicle Infotainment system.\n\nAnother example is the World Wide Web Consortium (W3C), which is a consortium that standardizes web technologies like HTML, XML and CSS.\n\nThe Institute for Food Safety and Health is a consortium consisting of the Illinois Institute of Technology, the Food and Drug Administration's Center for Food Safety and Applied Nutrition, and members of the food industry. Some of the work done at the institute includes, \"assessment and validation of new and novel food safety and preservation technologies, processing and packaging systems, microbiological and chemical methods, health promoting food components, and risk management strategies\".\n\n"}
{"id": "105070", "url": "https://en.wikipedia.org/wiki?curid=105070", "title": "Organization", "text": "Organization\n\nAn organization or organisation is an entity comprising multiple people, such as an institution or an association, that has a particular purpose.\n\nThe word is derived from the Greek word \"organon\", which means tool or instrument, musical instrument, and organ.\n\nThere are a variety of legal types of organisations, including corporations, governments, non-governmental organisations, political organisations, international organisations, armed forces, charities, not-for-profit corporations, partnerships, cooperatives, and educational institutions.\n\nA hybrid organisation is a body that operates in both the public sector and the private sector simultaneously, fulfilling public duties and developing commercial market activities.\n\nA voluntary association is an organisation consisting of volunteers. Such organisations may be able to operate without legal formalities, depending on jurisdiction, including informal clubs or coordinating bodies with a goal in mind which they may express in the form of an Manifesto, Mission statement,or in an informal manner reflected in what they do because remember every action done by an organization both legal and illegal reflects a goal in mind.\n\nOrganisations may also operate secretly or illegally in the case of secret societies, criminal organisations and resistance movements. And in some cases may have obstacles from other organizations (ex: MLK's organization) but what makes an organization an organization is not the paperwork that makes it official but to be an organization there must be four things:\n\n\nBut what makes an organization recognized by the government is either filling out Incorporation (business) or recognition in the form of either societal pressure (ex: Advocacy group), causing concerns (ex: Resistance movement) or being considered the spokesperson of a group of people subject to negotiation (ex: the Polisario Front being recognized as the sole representative of the Sahawri people and forming a partially recognized state.)\n\nCompare the concept of social groups, which may include non-organizations.\n\nThe study of organisations includes a focus on optimising organisational structure. According to management science, most human organisations fall roughly into four types:\n\nThese consist of a group of peers who decide as a group, perhaps by voting. The difference between a jury and a committee is that the members of the committee are usually assigned to perform or lead further actions after the group comes to a decision, whereas members of a jury come to a decision. In common law countries, legal juries render decisions of guilt, liability and quantify damages; juries are also used in athletic contests, book awards and similar activities. Sometimes a selection committee functions like a jury. In the Middle Ages, juries in continental Europe were used to determine the law according to consensus among local notables.\n\nCommittees are often the most reliable way to make decisions. Condorcet's jury theorem proved that if the average member votes better than a roll of dice, then adding more members increases the number of majorities that can come to a correct vote (however correctness is defined). The problem is that if the average member is subsequently \"worse\" than a roll of dice, the committee's decisions grow worse, not better; therefore, staffing is crucial.\n\nParliamentary procedure, such as Robert's Rules of Order, helps prevent committees from engaging in lengthy discussions without reaching decisions.\n\nThis organisational structure promotes internal competition. Inefficient components of the organisation starve, while effective ones get more work. Everybody is paid for what they actually do, and so runs a tiny business that has to show a profit, or they are fired.\n\nCompanies who utilise this organisation type reflect a rather one-sided view of what goes on in ecology. It is also the case that a natural ecosystem has a natural border - ecoregions do not, in general, compete with one another in any way, but are very autonomous.\n\nThe pharmaceutical company GlaxoSmithKline talks about functioning as this type of organisation in this external article from \"The Guardian\".\nBy:Bastian Batac De Leon.\n\nThis organisational type assigns each worker two bosses in two different hierarchies. One hierarchy is \"functional\" and assures that each type of expert in the organisation is well-trained, and measured by a boss who is super-expert in the same field. The other direction is \"executive\" and tries to get projects completed using the experts. Projects might be organised by products, regions, customer types, or some other schemes.\n\nAs an example, a company might have an individual with overall responsibility for products X and Y, and another individual with overall responsibility for engineering, quality control, etc. Therefore, subordinates responsible for quality control of project X will have two reporting lines.\n\nA hierarchy exemplifies an arrangement with a leader who leads other individual members of the organisation. This arrangement is often associated with basis that there are enough imagine a real pyramid, if there are not enough stone blocks to hold up the higher ones, gravity would irrevocably bring down the monumental structure. So one can imagine that if the leader does not have the support of his subordinates, the entire structure will collapse. Hierarchies were satirised in \"The Peter Principle\" (1969), a book that introduced \"hierarchiology\" and the saying that \"in a hierarchy every employee tends to rise to his level of incompetence.\"\n\nIn the social sciences, organisations are the object of analysis for a number of disciplines, such as sociology, economics, political science, psychology, management, and organisational communication. The broader analysis of organisations is commonly referred to as organisational structure, organisational studies, organisational behaviour, or organisation analysis. A number of different perspectives exist, some of which are compatible:\n\nSociology can be defined as the science of the institutions of modernity; specific institutions serve a function, akin to the individual organs of a coherent body. In the social and political sciences in general, an \"organisation\" may be more loosely understood as the planned, coordinated and purposeful action of human beings working through collective action to reach a common goal or construct a tangible product. This action is usually framed by formal membership and form (institutional rules). Sociology distinguishes the term organisation into planned formal and unplanned informal (i.e. spontaneously formed) organisations. Sociology analyses organisations in the first line from an institutional perspective. In this sense, organisation is an enduring arrangement of elements. These elements and their actions are determined by rules so that a certain task can be fulfilled through a system of coordinated division of labour.\n\nEconomic approaches to organisations also take the division of labour as a starting point. The division of labour allows for (economies of) specialisation. Increasing specialisation necessitates coordination. From an economic point of view, markets and organisations are alternative coordination mechanisms for the execution of transactions.\n\nAn organisation is defined by the elements that are part of it (who belongs to the organisation and who does not?), its communication (which elements communicate and how do they communicate?), its autonomy (which changes are executed autonomously by the organisation or its elements?), and its rules of action compared to outside events (what causes an organisation to act as a collective actor?).\n\nBy coordinated and planned cooperation of the elements, the organisation is able to solve tasks that lie beyond the abilities of the single elements. The price paid by the elements is the limitation of the degrees of freedom of the elements. Advantages of organisations are enhancement (more of the same), addition (combination of different features) and extension. Disadvantages can be inertness (through co-ordination) and loss of interaction.\n\nAmong the theories that are or have been influential are:\n\n\nA leader in a formal, hierarchical organisation, is appointed to a managerial position and has the right to command and enforce obedience by virtue of the authority of his position. However, he must possess adequate personal attributes to match his authority, because authority is only potentially available to him. In the absence of sufficient personal competence, a manager may be confronted by an emergent leader who can challenge his role in the organisation and reduce it to that of a figurehead. However, only authority of position has the backing of formal sanctions. It follows that whoever wields personal influence and power can legitimise this only by gaining a formal position in the hierarchy, with commensurate authority.\n\nAn organisation that is established as a means for achieving defined objectives has been referred to as a formal organisation. Its design specifies how goals are subdivided and reflected in subdivisions of the organisation. Divisions, departments, sections, positions, jobs, and tasks make up this work structure. Thus, the formal organisation is expected to behave impersonally in regard to relationships with clients or with its members. According to Weber's definition, entry and subsequent advancement is by merit or seniority. Each employee receives a salary and enjoys a degree of tenure that safeguards him from the arbitrary influence of superiors or of powerful clients. The higher his position in the hierarchy, the greater his presumed expertise in adjudicating problems that may arise in the course of the work carried out at lower levels of the organisation. It is this bureaucratic structure that forms the basis for the appointment of heads or chiefs of administrative subdivisions in the organisation and endows them with the authority attached to their position.\n\nIn contrast to the appointed head or chief of an administrative unit, a leader emerges within the context of the informal organisation that underlies the formal structure. The informal organisation expresses the personal objectives and goals of the individual membership. Their objectives and goals may or may not coincide with those of the formal organisation. The informal organisation represents an extension of the social structures that generally characterise human life – the spontaneous emergence of groups and organisations as ends in themselves.\n\nIn prehistoric times, man was preoccupied with his personal security, maintenance, protection, and survival. Now man spends a major portion of his waking hours working for organisations. His need to identify with a community that provides security, protection, maintenance, and a feeling of belonging continues unchanged from prehistoric times. This need is met by the informal organisation and its emergent, or unofficial, leaders.\n\nLeaders emerge from within the structure of the informal organisation. Their personal qualities, the demands of the situation, or a combination of these and other factors attract followers who accept their leadership within one or several overlay structures. Instead of the authority of position held by an appointed head or chief, the emergent leader wields influence or power. Influence is the ability of a person to gain cooperation from others by means of persuasion or control over rewards. Power is a stronger form of influence because it reflects a person's ability to enforce action through the control of a means of punishment.\n\n\n\n"}
{"id": "72487", "url": "https://en.wikipedia.org/wiki?curid=72487", "title": "Nonprofit organization", "text": "Nonprofit organization\n\nA nonprofit organization (NPO), also known as a non-business entity, not-for-profit organization, or nonprofit institution, is an organization dedicated to furthering a particular social cause or advocating for a shared point of view. In economic terms, it is an organization using its surplus of the revenues to further achieve its ultimate objective, rather than distributing its income to the organization's shareholders, leaders, or members. Nonprofits are tax-exempt or charitable, meaning they do not pay income tax on the money that they receive for their organization. They can operate in religious, scientific, research, or educational settings.\n\nThe key aspects of nonprofits are accountability, trustworthiness, honesty, and openness to every person who has invested time, money, and faith into the organization. Nonprofit organizations are accountable to the donors, founders, volunteers, program recipients, and the public community. Public confidence is a factor in the amount of money that a nonprofit organization is able to raise. The more nonprofits focus on their mission, the more public confidence they will have, and as a result, more money for the organization. The activities a nonprofit is partaking in can help build the public's confidence in nonprofits, as well as how ethical the standards and practices are.\n\nAccording to the National Center for Charitable Statistics (NCCS), there are more than 1.5 million nonprofit organizations registered in the United States, including public charities, private foundations, and other nonprofit organizations. Contributions to different charities reached $358.38 billion in 2014, which was an increase of 7.1% from the 2013 estimates. Out of these contributions, religious organizations received 32%, educational institutions received 15%, and human service organizations received 12%. Between September 2010 and September 2014, approximately 25.3% of Americans over the age of 16 volunteered for a nonprofit.\n\nNonprofits are not driven by generating profit, but they must bring in enough income to pursue their social goals. Nonprofits are able to raise money in different ways. This includes income from donations from individual donors or foundations; sponsorship from corporations; government funding; programs, services or merchandise sales; and investments. Each NPO is unique in which source of income works best for them. With an increase in NPO's within the last decade, organizations have adopted competitive advantages to create revenue for themselves to remain financially stable. Donations from private individuals or organizations can change each year and government grants have diminished. With changes in funding from year to year, many nonprofit organizations have been moving toward increasing the diversity of their funding sources. For example, many nonprofits that have relied on government grants have started fundraising efforts to appeal to individual donors.\n\nNPO's challenges primarily stem from lack of funding. Funding can either come from within the organization, fundraising, donations, or from the federal government. When cutbacks are made from the federal government, the organization suffers from devolution. This term describes when there is a shift of responsibility from a central government to a local, sub-national authority. The shift is due to the loss of funds; therefore, resulting in changes of responsibilities in running programs. Because of this frequent challenge, management must be innovative and effective in the pursuit of success.\n\nNonprofit and not-for-profit are terms that are used similarly, but do not mean the same thing. Both are organizations that do not make a profit, but may receive an income to sustain their missions. The income that nonprofit and not-for-profit organizations generate is used differently. Nonprofit organizations return any extra income to the organization. Not-for-profits use their excess money to pay their members who do work for them. Another difference between nonprofit organizations and not-for-profit organizations is their membership. Nonprofits have volunteers or employees who do not receive any money from the organization's fundraising efforts. They may earn a salary for their work that is independent from the money the organization has fundraised. Not-for-profit members have the opportunity to benefit from the organization's fundraising efforts.\n\nIn the United States, both nonprofits and not-for-profits are tax-exempt under IRS publication 557. Although they are both tax-exempt, each organization faces different tax code requirements. A nonprofit is tax-exempt under 501(c)(3) requirements if it is either a religious, charitable, or educational based organizations that do not influence state and federal legislation. Not-for-profits are tax-exempt under 501(c)(7) requirements if they are an organization for pleasure, recreation or another nonprofit purpose.\n\nNonprofits are either member-serving or community-serving. Member-serving nonprofit organizations create a benefit for the members of their organization and can include but are not limited to credit unions, sports clubs, and advocacy groups. Community-serving nonprofit organizations focus on providing services to the community either globally or locally. Community-serving nonprofits include organizations that deliver aid and development programs, medical research, education, and health services. It is possible for a nonprofit to be both member-serving and community-serving.\n\nA common misconception about nonprofits is that they are run completely by volunteers. Most nonprofits have staff that work for the company, possibly using volunteers to perform the nonprofit's services under the direction of the paid staff. Nonprofits must be careful to balance the salaries paid to staff against the money paid to provide services to the nonprofit's beneficiaries. Organizations whose salary expenses are too high relative to their program expenses may face regulatory scrutiny.\n\nA second misconception is that nonprofit organizations may not make a profit. Although the goal of nonprofits isn't specifically to maximize profits, they still have to operate as a fiscally responsible business. They must manage their income (both grants and donations and income from services) and expenses so as to remain a fiscally viable entity. Nonprofits have the responsibility of focusing on being professional, financially responsible, replacing self-interest and profit motive with mission motive.\n\nThough nonprofits are managed differently from for-profit businesses, they have felt pressure to be more businesslike. To combat private and public business growth in the public service industry, nonprofits have modeled their business management and mission, shifting their raison d’être to establish sustainability and growth.\n\nSetting effective missions is a key for the successful management of nonprofit organizations. There are three important conditions for effective mission: opportunity, competence, and commitment.\n\nOne way of managing the sustainability of nonprofit organizations is to establish strong relations with donor groups. This requires a donor marketing strategy, something many nonprofits lack.\n\nNPOs have a wide diversity of structures and purposes. For legal classification, there are, nevertheless, some elements of importance:\n\nSome of the above must be (in most jurisdictions in the USA at least) expressed in the organization's charter of establishment or constitution. Others may be provided by the supervising authority at each particular jurisdiction.\n\nWhile affiliations will not affect a legal status, they may be taken into consideration by legal proceedings as an indication of purpose. Most countries have laws that regulate the establishment and management of NPOs and that require compliance with corporate governance regimes. Most larger organizations are required to publish their financial reports detailing their income and expenditure publicly.\n\nIn many aspects, they are similar to corporate business entities though there are often significant differences. Both not-for-profit and for-profit corporate entities must have board members, steering-committee members, or trustees who owe the organization a fiduciary duty of loyalty and trust. A notable exception to this involves churches, which are often not required to disclose finances to anyone, including church members.\n\nIn the United States, nonprofit organizations are formed by filing bylaws or articles of incorporation or both in the state in which they expect to operate. The act of incorporation creates a legal entity enabling the organization to be treated as a distinct body (corporation) by law and to enter into business dealings, form contracts, and own property as individuals or for-profit corporations can.\n\nNonprofits can have members, but many do not. The nonprofit may also be a trust or association of members. The organization may be controlled by its members who elect the board of directors, board of governors or board of trustees. A nonprofit may have a delegate structure to allow for the representation of groups or corporations as members. Alternatively, it may be a non-membership organization and the board of directors may elect its own successors.\n\nThe two major types of nonprofit organization are membership and board-only. A membership organization elects the board and has regular meetings and the power to amend the bylaws. A board-only organization typically has a self-selected board and a membership whose powers are limited to those delegated to it by the board. A board-only organization's bylaws may even state that the organization does not have any membership, although the organization's literature may refer to its donors or service recipients as 'members'; examples of such organizations are FairVote and the National Organization for the Reform of Marijuana Laws. The Model Nonprofit Corporation Act imposes many complexities and requirements on membership decision-making. Accordingly, many organizations, such as the Wikimedia Foundation, have formed board-only structures. The National Association of Parliamentarians has generated concerns about the implications of this trend for the future of openness, accountability, and understanding of public concerns in nonprofit organizations. Specifically, they note that nonprofit organizations, unlike business corporations, are not subject to market discipline for products and shareholder discipline of their capital; therefore, without membership control of major decisions such as the election of the board, there are few inherent safeguards against abuse. A rebuttal to this might be that as nonprofit organizations grow and seek larger donations, the degree of scrutiny increases, including expectations of audited financial statements. A further rebuttal might be that NPOs are constrained, by their choice of legal structure, from financial benefit as far as distribution of profit to members and directors is concerned.\n\nIn many countries, nonprofits may apply for tax-exempt status, so that the organization itself may be exempt from income tax and other taxes. In the United States, to be exempt from federal income taxes, the organization must meet the requirements set forth in the Internal Revenue Code. Granting nonprofit status is done by the state, while granting tax-exempt designation (such as 501(c)(3)) is granted by the federal government via the IRS. This means that not all nonprofits are eligible to be tax-exempt. NPOs use the model of a double bottom line in that furthering their cause is more important than making a profit, though both are needed to ensure the organization's sustainability.\n\nIn Australia, nonprofit organizations include trade unions, charitable entities, co-operatives, universities and hospitals, mutual societies, grass-root and support groups, political parties, religious groups, incorporated associations, not-for-profit companies, trusts and more. Furthermore, they operate across a multitude of domains and industries, from health, employment, disability and other human services to local sporting clubs, credit unions, and research institutes. A nonprofit organization in Australia can choose from a number of legal forms depending on the needs and activities of the organization: co-operative, company limited by guarantee, unincorporated association, incorporated association (by the Associations Incorporation Act 1985) or incorporated association or council (by the Commonwealth Aboriginal Councils and Associations Act 1976). From an academic perspective, social enterprise is, for the most part, considered a sub-set of the nonprofit sector as typically they too are concerned with a purpose relating to a public good. However, these are not bound to adhere to a nonprofit legal structure, and many incorporate and operate as for-profit entities.\n\nIn Australia, nonprofit organizations are primarily established in one of three ways: companies limited by guarantee, trusts, and incorporated associations. However, the incorporated association form is typically used by organizations intending to operate only within one Australian state jurisdiction. Nonprofit organizations seeking to establish a presence across Australia typically consider incorporating as a company or as a trust.\n\nBy Belgian law, there are several kinds of nonprofit organization:\nThese three kinds of nonprofit organization are in contrast to a fourth:\n\nCanada allows nonprofit organizations to be incorporated or unincorporated. They may incorporate either federally, under Part II of the Canada Business Corporations Act, or under provincial legislation. Many of the governing Acts for Canadian nonprofits date to the early 1900s, meaning that nonprofit legislation has not kept pace with legislation that governs for-profit corporations, particularly with regards to corporate governance. Federal, and in some provinces (including Ontario), incorporation is by way of Letters Patent, and any change to the Letters Patent (even a simple name change) requires formal approval by the appropriate government, as do bylaw changes. Other provinces (including Alberta) permit incorporation \"as of right\", by the filing of Articles of Incorporation or Articles of Association.\n\nDuring 2009, the federal government enacted new legislation repealing the Canada Corporations Act, Part II – the Canada Not-for-Profit Corporations Act. This Act was last amended on 10 October 2011, and the act was current until 4 March 2013. It allows for incorporation \"as of right\", by Articles of Incorporation; does away with the \"ultra vires\" doctrine for nonprofits; establishes them as legal persons; and substantially updates the governance provisions for nonprofits. Ontario also overhauled its legislation, adopting the Ontario Not-for-Profit Corporations Act during 2010; the new Act is expected to be in effect as of 1 July 2013.\n\nCanada also permits a variety of charities (including public and private foundations). Charitable status is granted by the Canada Revenue Agency (CRA) upon application by a nonprofit; charities are allowed to issue income tax receipts to donors, must spend a certain percentage of their assets (including cash, investments, and fixed assets) and file annual reports in order to maintain their charitable status. In determining whether an organization can become a charity, CRA applies a common law test to its stated objects and activities. These must be:\n\nCharities are not permitted to engage in partisan political activity; doing so may result in the revocation of charitable status. However, a charity can carry out a small number of political activities that are non-partisan, help further the charities' purposes, and subordinate to the charity's charitable purposes.\n\nIn France, nonprofits are called \"associations\". They are based on a law enacted 1 July 1901. As a consequence, the nonprofits are also called \"association loi 1901\".\n\nA nonprofit can be created by two people to accomplish a common goal. The \"association\" can have industrial or commercial activities or both, but the members cannot make any profit from the activities. Thereby, worker's unions and political parties can be organized from this law.\n\nIn 2008, the National Institute of Statistics and Economic Studies (INSEE) counted more than a million of these \"associations\" in the country, and about 16 million people older than 16 are members of a nonprofit in France (a third of the population over 16 years old). The nonprofits employ 1.6 million people, and 8 million are volunteers for them.\n\nThis law is also relevant in many former French colonies, particularly in Africa.\n\nThe Hong Kong Company Registry provides a memorandum of procedure for applying to Registrar of Companies for a Licence under Section 21 of the Companies Ordinance (Cap.32) for a limited company for the purpose of promoting commerce, art, science, religion, charity, or any other useful object.\n\nIn India, non-governmental organizations are the most common type of societal institutions that do not have commercial interests. However, they are not the only category of non-commercial organizations that can gain official recognition. For example, memorial trusts, which honor renowned individuals through social work, may not be considered as NGOs.\n\nThey can be registered in four ways:\n\nRegistration can be with either the Registrar of Companies (RoC) or the Registrar of Societies (RoS).\n\nThe following laws or Constitutional Articles of the Republic of India are relevant to the NGOs:\n\nThe Irish Nonprofits Database was created by Irish Nonprofits Knowledge Exchange (INKEx) to act as a repository for regulatory and voluntarily disclosed information about Irish public-benefit nonprofits. The database lists more than 10,000 nonprofit organizations in Ireland. In 2012 INKEx ceased to operate due to lack of funding.\n\nIn Israel nonprofit organizations (NPOs) and non-governmental organizations (NGOs) are usually established as registered nonprofit associations (Hebrew \"amutah,\" plural \"amutot\") or public benefit companies (Hebrew \"Chevrah LeTo’elet Hatzibur,\" not to be confused with public benefit corporations). The structure of financial statements of nonprofit organizations is regulated Israel's Accounting Standard No. 5, and must include a balance sheet, a report on activities, the income and expenditure for the particular period, a report on changes in assets, a statement of cash flows, and notes to the financial statements. A report showing the level of restriction imposed on the assets and liabilities can be given, though this is not required.\n\n‘'Amutot'’ are regulated by the Associations Law, 1980. An \"amutah\" is a body corporate, though not a company. The \"amutah\" is successor to the Ottoman Society which predated the State of Israel, and was established by the now-superseded Ottoman Societies Law of 1909, based on the French law of 1901. Public benefit companies are governed solely by company law; if their regulations and objectives meet the two conditions specified in Section 345A of the Companies Act, they will in effect be \"amutot\" in all but name.\n\nAn \"amutah\" must register with the \"Rasham Ha’amutot\" ('Registrar of Amutot'); a public benefit company must register with the \"Rasham HaChavarot\" [Registrar of Companies]. Both are under the purview of the \"Rashot Hata’agidim\" ('Corporations Authority') of the Ministry of Justice.\n\nIn Japan, an NPO is any citizen's group that serves the public interest and does not produce a profit for its members. NPOs are given corporate status to assist them in conducting business transactions. As at February 2011, there were 41,600 NPOs in Japan. Two hundred NPOs were given tax-deductible status by the government, which meant that only contributions to those organizations were tax deductible for the contributors.\n\nIn New Zealand, nonprofit organizations usually are established as incorporated societies or charitable trusts. An incorporated society requires a membership of at least 15people.\n\nRussian law contains many legal forms of non-commercial organization (NCO), resulting in a complex, often contradictory, and limiting regulatory framework. The primary requirements are that NCOs, whatever their type, do not have the generation of profit as their main objective and do not distribute any such profit among their participants (Article 50(1), Civil Code). Most commonly there are five forms of NCO:\n\nIn South Africa, certain types of charity may issue a tax certificate when requested, which donors can use to apply for a tax deduction. Charities/NGOs may be established as voluntary associations, trusts or nonprofit companies (NPCs). Voluntary associations are established by agreement under the common law, and trusts are registered by the Master of the High Court.\n\nNonprofit companies (NPCs) are registered by the Companies and Intellectual Property Commission. All of these may voluntarily register with The Directorate for Nonprofit Organisations and may apply for tax-exempt status to the South African Revenue Service (SARS).\n\nIn Ukraine, nonprofit organizations include non-governmental organizations, cooperatives (inc. housing cooperatives), charitable organizations, religious organizations, political parties, commodities exchanges (in Ukraine, commodities exchanges can't be organized for profit) and more. Nonprofit organizations obtain their non-profit status from tax authorities. The state fiscal service is the main registration authority for nonprofit status.\n\nIn the UK a nonprofit organization may take the form of an unincorporated association, a charitable trust, a charitable incorporated organisation (CIO), a company limited by guarantee (which may or may not be charitable), a charter organization (which may or may not be charitable), a charitable company, a community interest company (CIC) (which may or may not be charitable), a community benefit society (which may or may not be charitable), or a cooperative society (which may or may not be charitable). Thus a nonprofit may be charitable (see under Charitable Organisation) or not, and may be required to be registered or not.\n\nAfter a nonprofit organization has been formed at the state level, the organization may seek recognition of tax-exempt status with respect to U.S. federal income tax. That is done typically by applying to the Internal Revenue Service (IRS), although statutory exemptions exist for limited types of nonprofit organization. The IRS, after reviewing the application to ensure the organization meets the conditions to be recognized as a tax-exempt organization (such as the purpose, limitations on spending, and internal safeguards for a charity), may issue an authorization letter to the nonprofit granting it tax-exempt status for income-tax payment, filing, and deductibility purposes. The exemption does not apply to other federal taxes such as employment taxes. Additionally, a tax-exempt organization must pay federal tax on income that is unrelated to their exempt purpose. Failure to maintain operations in conformity to the laws may result in the loss of tax-exempt status.\n\nIndividual states and localities offer nonprofits exemptions from other taxes such as sales tax or property tax. Federal tax-exempt status does not guarantee exemption from state and local taxes and vice versa. These exemptions generally have separate applications, and their requirements may differ from the IRS requirements. Furthermore, even a tax-exempt organization may be required to file annual financial reports (IRS Form 990) at the state and federal levels. A tax-exempt organization's 990 forms are required to be available for public scrutiny.\n\nThe board of directors has ultimate control over the organization, but typically an executive director is hired. In some cases, the board is elected by a membership, but commonly, the board of directors is self-perpetuating. In these 'board-only' organizations, board members nominate new members and vote on their fellow directors' nominations. Part VI Governance, Management, and Disclosure, section A, question 7a of the Form 990 asks 'Did the organization have members, stockholders, or other persons who had the power to elect or appoint one or more members of the governing body?'; the IRS instructions added '(other than the organization's governing body itself, acting in such capacity)'.\n\nFounder's syndrome is an issue organizations experience as they expand. Dynamic founders, who have a strong vision of how to operate the project, try to retain control of the organization, even as new employees or volunteers want to expand the project's scope or change policy.\n\nResource mismanagement is a particular problem with NPOs because the employees are not accountable to anybody who has a direct stake in the organization. For example, an employee may start a new program without disclosing its complete liabilities. The employee may be rewarded for improving the NPO's reputation, making other employees happy, and attracting new donors. Liabilities promised on the full faith and credit of the organization but not recorded anywhere constitute accounting fraud. But even indirect liabilities negatively affect the financial sustainability of the NPO, and the NPO will have financial problems unless strict controls are instated. Some commenters have argued that the receipt of significant funding from large for-profit corporations can ultimately alter the NPO's functions. A frequent measure of an NPO's efficiency is its expense ratio (i.e. expenditures on things other than its programs, divided by its total expenditures).\n\nCompetition for employees with the public and private sector is another problem that nonprofit organizations inevitably face, particularly for management positions. There are reports of major talent shortages in the nonprofit sector today regarding newly graduated workers, and NPOs have for too long relegated hiring to a secondary priority, which could be why they find themselves in the position many do. While many established NPOs are well-funded and comparative to their public sector competitors, many more are independent and must be creative with which incentives they use to attract and maintain vibrant personalities. The initial interest for many is the remuneration package, though many who have been questioned after leaving an NPO have reported that it was stressful work environments and implacable work that drove them away.\n\nPublic- and private-sector employment have, for the most part, been able to offer more to their employees than most nonprofit agencies throughout history. Either in the form of higher wages, more comprehensive benefit packages, or less tedious work, the public and private sectors have enjoyed an advantage over NPOs in attracting employees. Traditionally, the NPO has attracted mission-driven individuals who want to assist their chosen cause. Compounding the issue is that some NPOs do not operate in a manner similar to most businesses, or only seasonally. This leads many young and driven employees to forego NPOs in favor of more stable employment. Today, however, nonprofit organizations are adopting methods used by their competitors and finding new means to retain their employees and attract the best of the newly minted workforce.\n\nIt has been mentioned that most nonprofits will never be able to match the pay of the private sector and therefore should focus their attention on benefits packages, incentives and implementing pleasurable work environments. A good environment is ranked higher than salary and pressure of work. NPOs are encouraged to pay as much as they are able and offer a low-stress work environment that the employee can associate him or herself positively with. Other incentives that should be implemented are generous vacation allowances or flexible work hours.\n\nMany NPOs often use the .org or .us (or the country code top-level domain of their respective country) or .edu top-level domain (TLD) when selecting a domain name to differentiate themselves from more commercial entities, which typically use the .com space.\n\nIn the traditional domain noted in , .org is for 'organizations that didn't fit anywhere else' in the naming system, which implies that it is the proper category for non-commercial organizations if they are not governmental, educational, or one of the other types with a specific TLD. It is not designated specifically for charitable organizations or any specific organizational or tax-law status; however, it encompasses anything that is not classifiable as another category. Currently, no restrictions are enforced on registration of .com or .org, so one can find organizations of all sorts in either of these domains, as well as other top-level domains including newer, more specific ones which may apply to particular sorts of organization including .museum for museums and .coop for cooperatives. Organizations might also register by the appropriate country code top-level domain for their country.\n\nInstead of being defined by 'non' words, some organizations are suggesting new, positive-sounding terminology to describe the sector. The term 'civil society organization' (CSO) has been used by a growing number of organizations, including the Center for the Study of Global Governance. The term 'citizen sector organization' (CSO) has also been advocated to describe the sector – as one of citizens, for citizens – by organizations including . Advocates argue that these terms describe the sector in its own terms, without relying on terminology used for the government or business sectors. However, use of terminology by a nonprofit of self-descriptive language that is not legally compliant risks confusing the public about nonprofit abilities, capabilities, and limitations.\n\nIn some Spanish-language jurisdictions, nonprofit organizations are called \"civil associations\".\n\n\n"}
{"id": "222783", "url": "https://en.wikipedia.org/wiki?curid=222783", "title": "Partnership", "text": "Partnership\n\nA partnership is an arrangement where parties, known as business partners, agree to cooperate to advance their mutual interests. The partners in a partnership may be individuals, businesses, interest-based organizations, schools, governments or combinations. Organizations may partner to increase the likelihood of each achieving their mission and to amplify their reach. A partnership may result in issuing and holding equity or may be only governed by a contract.\n\nPartnerships have a long history; they were already in use in Medieval times in Europe and in the Middle East. \nIn Europe, the partnerships contributed to the Commercial Revolution which started in the 13th century. In the 15th century the cities of the Hanseatic League, would mutually strengthen each other; a ship from Hamburg to Danzig would not only carry its own cargo but was also commissioned to transport freight for other members of the league. This practice not only saved time and money, but also constituted a first step toward partnership. This capacity to join forces in reciprocal services became a distinctive feature, and a long lasting success factor, of the Hanseatic team spirit.\n\nA close examination of Medieval trade in Europe shows that numerous significant credit based trades were not bearing interest. Hence, pragmatism and common sense called for a fair compensation for the risk of lending money, and a compensation for the opportunity cost of lending money without using it for other fruitful purposes. In order to circumvent the usury laws edicted by the Church, other forms of reward were created, in particular through the widespread form of partnership called \"commenda\", very popular with Italian merchant bankers. Florentine merchant banks were almost sure to make a positive return on their loans, but this would be before taking into account solvency risks.\n\nIn the Middle East, the \"Qirad\" and \"Mudarabas\" institutions developed when trade with the Levant, namely the Ottoman Empire and the Muslim Near East, flourished and when early trading companies, contracts, bills of exchange and long-distance international trade were established. After the fall of the Roman Empire, the Levant trade revived in the tenth to eleventh centuries in Byzantine Italy. The eastern and western Mediterranean formed part of a single commercial civilization in the Middle Ages, and the two regions were economically interdependent through trade (in varying degrees). \n\nThe Mongols adopted and developed the concepts of liability in relation to investments and loans in Mongol–ortoq partnerships, promoting trade and investment to facilitate the commercial integration of the Mongol Empire. The contractual features of a Mongol-\"ortoq\" partnership closely resembled that of qirad and commenda arrangements, however, Mongol investors used metal coins, paper money, gold and silver ingots and tradable goods for partnership investments and primarily financed money-lending and trade activities. Moreover, Mongol elites formed trade partnerships with merchants from Central and Western Asia and Europe, including Marco Polo’s family.\n\nAlthough not required by law, partners may benefit from a partnership agreement that defines the important terms of the relationship between them. Partnership agreements can be formed in the following areas: \n\nPartnerships present the involved parties with complex negotiation and special challenges that must be navigated unto agreement. Overarching goals, levels of give-and-take, areas of responsibility, lines of authority and succession, how success is evaluated and distributed, and often a variety of other factors must all be negotiated. Once agreement is reached, the partnership is typically enforceable by civil law, especially if well documented. Partners who wish to make their agreement affirmatively explicit and enforceable typically draw up Articles of Partnership. Trust and pragmatism are also essential as it cannot be expected that everything can be written in the initial partnership agreement, therefore quality governance and clear communication are critical success factors in the long run. It is common for information about formally partnered entities to be made public, such as through a press release, a newspaper ad, or public records laws.\n\nWhile industrial partnerships stand to amplify mutual interests and accelerate success, some forms of collaboration may be considered ethically problematic. When a politician, for example, partners with a corporation to advance the latter's interest in exchange for some benefit, a conflict of interest results; consequentially, the public good may suffer. While technically lawful in some jurisdictions, such practice is broadly viewed negatively or as corruption.\n\nPartner compensation will often be defined by the terms of a partnership agreement. Partners who work for the partnership may receive compensation for their labor before any division of profits between partners.\n\nIn certain partnerships of individuals, particularly law firms and accountancy firms, equity partners are distinguished from salaried partners (or contract or income partners). The degree of control which each type of partner exerts over the partnership depends on the relevant partnership agreement.\n\nAlthough individuals in both categories are described as partners, equity partners and salaried partners have little in common other than joint and several liability. In many legal systems, salaried partners are not technically \"partners\" at all in the eyes of the law. However, if their firm holds them out as partners, they are nonetheless subject to joint and several liability.\n\nIn their most basic form, equity partners enjoy a fixed share of the partnership (usually, but not always an equal share with the other partners) and, upon distribution of profits, receive a portion of the partnership's profits proportionate to that share. In more sophisticated partnerships, different models exist for determining either ownership interest, profit distribution, or both. Two common alternate approaches to distribution of profit are \"lockstep\" and \"source of origination\" compensation (sometimes referred to, more graphically, as \"eat what you kill\").\n\nSource of origination compensation is rarely seen outside of law firms. The principle is simply that each partner receives a share of the partnership profits up to a certain amount, with any additional profits being distributed to the partner who was responsible for the \"origination\" of the work that generated the profits.\n\nBritish law firms tend to use the lockstep principle, whereas American firms are more accustomed to source of origination. When British firm Clifford Chance merged with American firm Rogers & Wells, many of the difficulties associated with that merger were blamed on the difficulties of merging a lockstep culture with a source of origination culture.\n\nPartnerships recognized by a government body may enjoy special benefits from taxation policy. Among developed countries, for example, business partnerships are often favored over corporations in taxation policy, since dividend taxes only occur on profit before they are distributed to the partners. However, depending on the partnership structure and the jurisdiction in which it operates, owners of a partnership may be exposed to greater personal liability than they would as shareholders of a corporation. In such countries, partnerships are often regulated via antitrust laws, so as to inhibit monopolistic practices and foster free market competition. Enforcement of the laws, however, varies considerably. Domestic partnerships recognized by governments typically enjoy tax benefits, as well.\n\nAt common law, members of a business partnership are personally liable for the debts and obligations of the partnership. Forms of partnership have evolved that may limit a partner's liability.\n\nAs common law there are two basic forms of partnership:\n\nMore recently, additional forms of partnership have been recognized:\n\nA silent partner or sleeping partner is one who still shares in the profits and losses of the business, but who is not involved in its management. Sometimes the silent partner's interest in the business will not be publicly known. A silent partner is often an investor in the partnership, who is entitled to a share of the partnership's profits. Silent partners may prefer to invest in limited partnerships in order to insulate their personal assets from the debts or liabilities of the partnership.\n\nSummarising s. 5 of the \"Partnership Act 1958\" (Vic), for a partnership in Australia to exist, four main criteria must be satisfied. They are:\nPartners share profits and losses. A partnership is basically a settlement between two or more groups or firms in which profit and loss are equally divided\n\nIn Bangladesh, the relevant law for regulating partnership is the Partnership Act 1932. A partnership is defined as the relation between persons who have agreed to share the profits of a business carried on by all or any of them acting for all. The law does not require written partnership agreement between the partners to form a partnership. A partnership does not also required to be registered, however an unregistered partnership has a number of limitation regarding enforcing its rights in any court. A partnership is considered as a separate legal identity (i.e. separate from its owners) in Bangladesh only if the partnership is registered. There must be a minimum of 2 partners and maximum of 20 partners.\n\nStatutory regulation of partnerships in Canada fall under provincial jurisdiction. A partnership is not a separate legal entity and partnership income is taxed at the rate of the partner receiving the income. It can be deemed to exist regardless of the intention of the partners. Common elements considered by courts in determining the existence of a partnership are that two or more legal persons:\n\nA partnership in Hong Kong is a business entity formed by the Hong Kong Partnerships Ordinance, which defines a partnership as \"the relation between persons carrying on a business in common with a view of profit\" and is not a joint stock company or an incorporated company. If the business entity registers with the Registrar of Companies it takes the form of a limited partnership defined in the Limited Partnerships Ordinance. However, if this business entity fails to register with the Registrar of Companies, then it becomes a general partnership as a default.\n\nAccording to section 4 of the Partnership Act of 1932,\"Partnership is defined as the relation between two or more persons who have agreed to share the profits of a business run by all or any one of them acting for all\". This definition superseded the previous definition given in section 239 of Indian Contract Act 1872 as – “Partnership is the relation which subsists between persons who have agreed to combine their property, labor, skill in some business, and to share the profits thereof between them”. The 1932 definition added the concept of mutual agency. The Indian Partnerships have the following common characteristics:\n\n1) A partnership firm is not a legal entity apart from the partners constituting it. It has limited identity for the purpose of tax law as per section 4 of the Partnership Act of 1932.\n\n2) Partnership is a concurrent subject. Contracts of partnerships are included in the Entry no.7 of List III of The Constitution of India (the list constitutes the subjects on which both the State government and Central (National) Government can legislate i.e. pass laws on).\n\n3) Unlimited Liability. The major disadvantage of partnership is the unlimited liability of partners for the debts and liabilities of the firm. Any partner can bind the firm and the firm is liable for all liabilities incurred by any firm on behalf of the firm. If property of partnership firm is insufficient to meet liabilities, personal property of any partner can be attached to pay the debts of the firm.\n\n4) Partners are Mutual Agents.The business of firm can be carried on by all or any of them acting for all. Any partner has authority to bind the firm. Act of any one partner is binding on all the partners. Thus, each partner is ‘agent’ of all the remaining partners. Hence, partners are ‘mutual agents’. Section 18 of the Partnership Act, 1932 says \"Subject to the provisions of this Act, a partner is the agent of the firm for the purpose of the business of the firm\"\n\n5) Oral or Written Agreements. The Partnership Act, 1932 nowhere mentions that the Partnership Agreement is to be in written or oral format. Thus the general rule of the Contract Act applies that the contract can be 'oral' or 'written' as long as it satisfies the basic conditions of being a contract i.e. the agreement between partners is legally enforceable. A written agreement is advisable to establish existence of partnership and to prove rights and liabilities of each partner, as it is difficult to prove an oral agreement.\n\n6) Number of Partners is minimum 2 and maximum 50 in any kind of business activities.Since partnership is ‘agreement’ there must be minimum two partners. The Partnership Act does not put any restrictions on maximum number of partners. However, section 464 of Companies Act 2013, and Rule 10 of Companies (Miscellaneous) Rules, 2014 prohibits partnership consisting of more than 50 for any businesses, unless it is registered as a company under Companies Act, 2013 or formed in pursuance of some other law. Some other law means companies and corporations formed via some other law passed by Parliament of India.\n\n7) Mutual agency is the real test. The real test of ‘partnership firm’ is ‘mutual agency’ set by the Courts of India, i.e. whether a partner can bind the firm by his act, i.e. whether he can act as agent of all other partners.\n\nA limited partnership in the United Kingdom consists of:\n\n\nLimited partners may not:\n\n\nIf they do, they become liable for all the debts and obligations of the firm up to the amount drawn out or received back or incurred while taking part in the management, as the case may be.\n\nUnder U.S. law a partnership is a business association of two or more individuals, through which partners share the profits and responsibility for the liabilities of their venture. U.S. states recognize forms of limited partnership that may allow a partner who does not participate in the business venture to avoid liability for the partnership's debts and obligations. Partnerships typically pay less taxes than corporations in fields like fund management.\n\nThe federal government of the United States does not have specific statutory law governing the establishment of partnerships. Instead, every U.S. state and the District of Columbia has its own statutes and common law that govern partnerships. The National Conference of Commissioners on Uniform State Laws has issued non-binding model laws (called uniform act) in which to encourage the adoption of uniformity of partnership law into the states by their respective legislatures. Model laws include the Uniform Partnership Act and the Uniform Limited Partnership Act. Most U.S. states have adopted a form of the Uniform Partnership Act, which includes provisions regulating general partnerships, limited partnerships and limited liability partnerships.\n\nAlthough the federal government does not have specific statutory law for establishing partnerships, it has an extensive statutory and regulatory scheme for the taxation of partnerships, set forth in the Internal Revenue Code (IRC) and Code of Federal Regulations. The IRC defines federal tax obligations for partnership operations that effectively serve as federal regulation of some aspects of partnerships.\n"}
{"id": "164143", "url": "https://en.wikipedia.org/wiki?curid=164143", "title": "Secret society", "text": "Secret society\n\nA secret society is a club or an organization whose activities, events, inner functioning, or membership are concealed from non-members. The society may or may not attempt to conceal its existence. The term usually excludes covert groups, such as intelligence agencies or guerrilla warfare insurgencies, that hide their activities and memberships but maintain a public presence. \n\nThe exact qualifications for labeling a group a secret society are disputed, but definitions generally rely on the degree to which the organization insists on secrecy, and might involve the retention and transmission of secret knowledge, the denial about membership or knowledge of the group, the creation of personal bonds between members of the organization, and the use of secret rites or rituals which solidify members of the group.\n\nAnthropologically and historically, secret societies have been deeply interlinked with the concept of the Männerbund, the all-male \"warrior-band\" or \"warrior-society\" of pre-modern cultures (see H. Schurtz, \"Alterklassen und Männerbünde\", Berlin, 1902; A. Van Gennep, \"The Rites of Passage\", Chicago, 1960).\n\nA purported \"family tree of secret societies\" has been proposed, although it may not be comprehensive.\n\nAlan Axelrod, author of the \"International Encyclopedia of Secret Societies and Fraternal Orders\", defines a secret society as an organization that:\n\nHistorian Richard B. Spence of the University of Idaho offered a similar three-pronged definition: \n\nDavid V. Barrett, author of \"Secret Societies: From the Ancient and Arcane to the Modern and Clandestine\", has used alternative terms to define what qualifies a secret society. He defined it as any group that possesses the following characteristics:\n\nBarrett goes on to say that \"a further characteristic common to most of them is the practice of rituals which non-members are not permitted to observe, or even to know the existence of.\" Barrett's definition would rule out many organizations called secret societies; graded teaching is usually not part of the American college fraternities, the Carbonari, or the 19th-century Know Nothings.\n\nHistorian Jasper Ridley argues that Freemasons is, \"the world's most powerful secret Society.\"\n\nBecause some secret societies have political aims, they are illegal in several countries. Italy (Constitution of Italy, Section 2, Articles 13–28) and Poland, for example, ban secret political parties and political organizations in their constitutions.\n\nMany student societies established on university campuses in the United States have been considered secret societies. Perhaps one of the most famous secret collegiate societies is Skull and Bones at Yale University. The influence of undergraduate secret societies at colleges such as Harvard College, Cornell University, Dartmouth College, the University of Chicago, the University of Virginia, Georgetown University, New York University, and Wellesley College has been publicly acknowledged, if anonymously and circumspectly, since the 19th century.\n\nBritish Universities, too, have a long history of secret societies or quasi-secret societies, such as The Pitt Club at Cambridge University, Bullingdon Club at Oxford University, and the 16' Club at St David's College. Another British secret society is the Cambridge Apostles, founded as an essay and debating society in 1820. Not all British Universities host solely academic secret societies, for both The Night Climbers of Cambridge and The Night Climbers of Oxford require both brains and brawn. \n\nIn France, Vandermonde is the secret society of the Conservatoire National des Arts et Métiers.\n\nNotable examples in Canada include Episkopon at the University of Toronto's Trinity College, and the Society of Thoth at the University of British Columbia.\n\nSecret societies are disallowed in a few colleges. The Virginia Military Institute has rules that no cadet may join a secret society, and secret societies have been banned at Oberlin College from 1847 to the present, and at Princeton University since the beginning of the 20th century.\n\nConfraternities in Nigeria are secret-society like student groups within higher education. The exact death toll of confraternity activities is unclear. One estimate in 2002 was that 250 people had been killed in campus cult-related murders in the previous decade, while the Exam Ethics Project lobby group estimated that 115 students and teachers had been killed between 1993 and 2003.\n\nThe Mandatory Monday Association is thought to operate out of a variety of Australian universities including the Australian Defence Force Academy. The Association has numerous chapters that meet only on Mondays to discuss business and carry out rituals.\n\nThe only secret society abolished and then legalized is that of the philomaths; it is now a legitimate academic association founded on a strict selection of its members.\n\nWhile their existence had been speculated for years, internet-based secret societies first became known to the public in 2012 when the secret society known as Cicada 3301 began recruiting from the public via internet-based puzzles. The goals of the society remain unknown, but it is believed that they are involved in cryptography and cryptocurrency.\n\nChina\n\nIndia\n\nJapan\n\nSingapore\nNigeria\n\nWest African\n\nZimbabwe\n\nGermany\n\nIreland\n\nSerbia\n\nUnited Kingdom\n\nPan-European\n\n\nUnited States\n\nMany Christian Churches forbid their members from joining secret societies. For example, ¶41 of the General Rules contained in \"Discipline\" of the Allegheny Wesleyan Methodist Connection teaches:\n\n\n"}
{"id": "34327569", "url": "https://en.wikipedia.org/wiki?curid=34327569", "title": "Social network", "text": "Social network\n\nA social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\n\nSocial networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and \"web of group affiliations\". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.\n\nThe social network is a theoretical construct useful in the social sciences to study relationships between individuals, groups, organizations, or even entire societies (social units, see differentiation). The term is used to describe a social structure determined by such interactions. The ties through which any given social unit connects represent the convergence of the various social contacts of that unit. This theoretical approach is, necessarily, relational. An axiom of the social network approach to understanding social interaction is that social phenomena should be primarily conceived and investigated through the properties of relations between and within units, instead of the properties of these units themselves. Thus, one common criticism of social network theory is that individual agency is often ignored although this may not be the case in practice (see agent-based modeling). Precisely because many different types of relations, singular or in combination, form these network configurations, network analytics are useful to a broad range of research enterprises. In social science, these fields of study include, but are not limited to anthropology, biology, communication studies, economics, geography, information science, organizational studies, social psychology, sociology, and sociolinguistics.\n\nIn the late 1890s, both Émile Durkheim and Ferdinand Tönnies foreshadowed the idea of social networks in their theories and research of social groups. Tönnies argued that social groups can exist as personal and direct social ties that either link individuals who share values and belief (\"Gemeinschaft\", German, commonly translated as \"community\") or impersonal, formal, and instrumental social links (\"Gesellschaft\", German, commonly translated as \"society\"). Durkheim gave a non-individualistic explanation of social facts, arguing that social phenomena arise when interacting individuals constitute a reality that can no longer be accounted for in terms of the properties of individual actors. Georg Simmel, writing at the turn of the twentieth century, pointed to the nature of networks and the effect of network size on interaction and examined the likelihood of interaction in loosely knit networks rather than groups.\nMajor developments in the field can be seen in the 1930s by several groups in psychology, anthropology, and mathematics working independently. In psychology, in the 1930s, Jacob L. Moreno began systematic recording and analysis of social interaction in small groups, especially classrooms and work groups (see sociometry). In anthropology, the foundation for social network theory is the theoretical and ethnographic work of Bronislaw Malinowski, Alfred Radcliffe-Brown, and Claude Lévi-Strauss. A group of social anthropologists associated with Max Gluckman and the Manchester School, including John A. Barnes, J. Clyde Mitchell and Elizabeth Bott Spillius, often are credited with performing some of the first fieldwork from which network analyses were performed, investigating community networks in southern Africa, India and the United Kingdom. Concomitantly, British anthropologist S. F. Nadel codified a theory of social structure that was influential in later network analysis. In sociology, the early (1930s) work of Talcott Parsons set the stage for taking a relational approach to understanding social structure. Later, drawing upon Parsons' theory, the work of sociologist Peter Blau provides a strong impetus for analyzing the relational ties of social units with his work on social exchange theory.\n\nBy the 1970s, a growing number of scholars worked to combine the different tracks and traditions. One group consisted of sociologist Harrison White and his students at the Harvard University Department of Social Relations. Also independently active in the Harvard Social Relations department at the time were Charles Tilly, who focused on networks in political and community sociology and social movements, and Stanley Milgram, who developed the \"six degrees of separation\" thesis. Mark Granovetter and Barry Wellman are among the former students of White who elaborated and championed the analysis of social networks.\n\nBeginning in the late 1990s, social network analysis experienced work by sociologists, political scientists, and physicists such as Duncan J. Watts, Albert-László Barabási, Peter Bearman, Nicholas A. Christakis, James H. Fowler, and others, developing and applying new models and methods to emerging data available about online social networks, as well as \"digital traces\" regarding face-to-face networks.\n\nIn general, social networks are self-organizing, emergent, and complex, such that a globally coherent pattern appears from the local interaction of the elements that make up the system. These patterns become more apparent as network size increases. However, a global network analysis of, for example, all interpersonal relationships in the world is not feasible and is likely to contain so much information as to be uninformative. Practical limitations of computing power, ethics and participant recruitment and payment also limit the scope of a social network analysis. The nuances of a local system may be lost in a large network analysis, hence the quality of information may be more important than its scale for understanding network properties. Thus, social networks are analyzed at the scale relevant to the researcher's theoretical question. Although levels of analysis are not necessarily mutually exclusive, there are three general levels into which networks may fall: micro-level, meso-level, and macro-level.\n\nAt the micro-level, social network research typically begins with an individual, snowballing as social relationships are traced, or may begin with a small group of individuals in a particular social context.\n\nDyadic level: A dyad is a social relationship between two individuals. Network research on dyads may concentrate on structure of the relationship (e.g. multiplexity, strength), social equality, and tendencies toward reciprocity/mutuality.\n\nTriadic level: Add one individual to a dyad, and you have a triad. Research at this level may concentrate on factors such as balance and transitivity, as well as social equality and tendencies toward reciprocity/mutuality. In the balance theory of Fritz Heider the triad is the key to social dynamics. The discord in a rivalrous love triangle is an example of an unbalanced triad, likely to change to a balanced triad by a change in one of the relations. The dynamics of social friendships in society has been modeled by balancing triads. The study is carried forward with the theory of signed graphs.\n\nActor level: The smallest unit of analysis in a social network is an individual in their social setting, i.e., an \"actor\" or \"ego\". Egonetwork analysis focuses on network characteristics such as size, relationship strength, density, centrality, prestige and roles such as isolates, liaisons, and bridges. Such analyses, are most commonly used in the fields of psychology or social psychology, ethnographic kinship analysis or other genealogical studies of relationships between individuals.\n\nSubset level: Subset levels of network research problems begin at the micro-level, but may cross over into the meso-level of analysis. Subset level research may focus on distance and reachability, cliques, cohesive subgroups, or other group actions or behavior.\n\nIn general, meso-level theories begin with a population size that falls between the micro- and macro-levels. However, meso-level may also refer to analyses that are specifically designed to reveal connections between micro- and macro-levels. Meso-level networks are low density and may exhibit causal processes distinct from interpersonal micro-level networks.\n\nOrganizations: Formal organizations are social groups that distribute tasks for a collective goal. Network research on organizations may focus on either intra-organizational or inter-organizational ties in terms of formal or informal relationships. Intra-organizational networks themselves often contain multiple levels of analysis, especially in larger organizations with multiple branches, franchises or semi-autonomous departments. In these cases, research is often conducted at a work group level and organization level, focusing on the interplay between the two structures. Experiments with networked groups online have documented ways to optimize group-level coordination through diverse interventions, including the addition of autonomous agents to the groups.\n\nRandomly distributed networks: Exponential random graph models of social networks became state-of-the-art methods of social network analysis in the 1980s. This framework has the capacity to represent social-structural effects commonly observed in many human social networks, including general degree-based structural effects commonly observed in many human social networks as well as reciprocity and transitivity, and at the node-level, homophily and attribute-based activity and popularity effects, as derived from explicit hypotheses about dependencies among network ties. Parameters are given in terms of the prevalence of small subgraph configurations in the network and can be interpreted as describing the combinations of local social processes from which a given network emerges. These probability models for networks on a given set of actors allow generalization beyond the restrictive dyadic independence assumption of micro-networks, allowing models to be built from theoretical structural foundations of social behavior.\n\nScale-free networks: A scale-free network is a network whose degree distribution follows a power law, at least asymptotically. In network theory a scale-free ideal network is a random network with a degree distribution that unravels the size distribution of social groups. Specific characteristics of scale-free networks vary with the theories and analytical tools used to create them, however, in general, scale-free networks have some common characteristics. One notable characteristic in a scale-free network is the relative commonness of vertices with a degree that greatly exceeds the average. The highest-degree nodes are often called \"hubs\", and may serve specific purposes in their networks, although this depends greatly on the social context. Another general characteristic of scale-free networks is the clustering coefficient distribution, which decreases as the node degree increases. This distribution also follows a power law. The Barabási model of network evolution shown above is an example of a scale-free network.\n\nRather than tracing interpersonal interactions, macro-level analyses generally trace the outcomes of interactions, such as economic or other resource transfer interactions over a large population.\n\nLarge-scale networks: Large-scale network is a term somewhat synonymous with \"macro-level\" as used, primarily, in social and behavioral sciences, in economics. Originally, the term was used extensively in the computer sciences (see large-scale network mapping).\n\nComplex networks: Most larger social networks display features of social complexity, which involves substantial non-trivial features of network topology, with patterns of complex connections between elements that are neither purely regular nor purely random (see, complexity science, dynamical system and chaos theory), as do biological, and technological networks. Such complex network features include a heavy tail in the degree distribution, a high clustering coefficient, assortativity or disassortativity among vertices, community structure (see stochastic block model), and hierarchical structure. In the case of agency-directed networks these features also include reciprocity, triad significance profile (TSP, see network motif), and other features. In contrast, many of the mathematical models of networks that have been studied in the past, such as lattices and random graphs, do not show these features.\n\nVarious theoretical frameworks have been imported for the use of social network analysis. The most prominent of these are Graph theory, Balance theory, Social comparison theory, and more recently, the Social identity approach.\n\nFew complete theories have been produced from social network analysis. Two that have are structural role theory and heterophily theory.\n\nThe basis of Heterophily Theory was the finding in one study that more numerous weak ties can be important in seeking information and innovation, as cliques have a tendency to have more homogeneous opinions as well as share many common traits. This homophilic tendency was the reason for the members of the cliques to be attracted together in the first place. However, being similar, each member of the clique would also know more or less what the other members knew. To find new information or insights, members of the clique will have to look beyond the clique to its other friends and acquaintances. This is what Granovetter called \"the strength of weak ties\".\n\nIn the context of networks, social capital exists where people have an advantage because of their location in a network. Contacts in a network provide information, opportunities and perspectives that can be beneficial to the central player in the network. Most social structures tend to be characterized by dense clusters of strong connections. Information within these clusters tends to be rather homogeneous and redundant. Non-redundant information is most often obtained through contacts in different clusters. When two separate clusters possess non-redundant information, there is said to be a structural hole between them. Thus, a network that bridges structural holes will provide network benefits that are in some degree additive, rather than overlapping. An ideal network structure has a vine and cluster structure, providing access to many different clusters and structural holes.\n\nNetworks rich in structural holes are a form of social capital in that they offer information benefits. The main player in a network that bridges structural holes is able to access information from diverse sources and clusters. For example, in business networks, this is beneficial to an individual's career because he is more likely to hear of job openings and opportunities if his network spans a wide range of contacts in different industries/sectors. This concept is similar to Mark Granovetter's theory of weak ties, which rests on the basis that having a broad range of contacts is most effective for job attainment.\n\nCommunication Studies are often considered a part of both the social sciences and the humanities, drawing heavily on fields such as sociology, psychology, anthropology, information science, biology, political science, and economics as well as rhetoric, literary studies, and semiotics. Many communication concepts describe the transfer of information from one source to another, and can thus be conceived of in terms of a network.\n\nIn J.A. Barnes' day, a \"community\" referred to a specific geographic location and studies of community ties had to do with who talked, associated, traded, and attended church with whom. Today, however, there are extended \"online\" communities developed through telecommunications devices and social network services. Such devices and services require extensive and ongoing maintenance and analysis, often using network science methods. Community development studies, today, also make extensive use of such methods.\n\nComplex networks require methods specific to modelling and interpreting social complexity and complex adaptive systems, including techniques of dynamic network analysis.\nMechanisms such as Dual-phase evolution explain how temporal changes in connectivity contribute to the formation of structure in social networks.\n\nIn criminology and urban sociology, much attention has been paid to the social networks among criminal actors. For example, Andrew Papachristos has studied gang murders as a series of exchanges between gangs. Murders can be seen to diffuse outwards from a single source, because weaker gangs cannot afford to kill members of stronger gangs in retaliation, but must commit other violent acts to maintain their reputation for strength.\n\nDiffusion of ideas and innovations studies focus on the spread and use of ideas from one actor to another or one culture and another. This line of research seeks to explain why some become \"early adopters\" of ideas and innovations, and links social network structure with facilitating or impeding the spread of an innovation.\n\nIn demography, the study of social networks has led to new sampling methods for estimating and reaching populations that are hard to enumerate (for example, homeless people or intravenous drug users.) For example, respondent driven sampling is a network-based sampling technique that relies on respondents to a survey recommending further respondents.\n\nThe field of sociology focuses almost entirely on networks of outcomes of social interactions. More narrowly, economic sociology considers behavioral interactions of individuals and groups through social capital and social \"markets\". Sociologists, such as Mark Granovetter, have developed core principles about the interactions of social structure, information, ability to punish or reward, and trust that frequently recur in their analyses of political, economic and other institutions. Granovetter examines how social structures and social networks can affect economic outcomes like hiring, price, productivity and innovation and describes sociologists' contributions to analyzing the impact of social structure and networks on the economy.\n\nAnalysis of social networks is increasingly incorporated into health care analytics, not only in epidemiological studies but also in models of patient communication and education, disease prevention, mental health diagnosis and treatment, and in the study of health care organizations and systems.\n\nHuman ecology is an interdisciplinary and transdisciplinary study of the relationship between humans and their natural, social, and built environments. The scientific philosophy of human ecology has a diffuse history with connections to geography, sociology, psychology, anthropology, zoology, and natural ecology.\n\nStudies of language and linguistics, particularly evolutionary linguistics, focus on the development of linguistic forms and transfer of changes, sounds or words, from one language system to another through networks of social interaction. Social networks are also important in language shift, as groups of people add and/or abandon languages to their repertoire.\n\nIn the study of literary systems, network analysis has been applied by Anheier, Gerhards and Romo, De Nooy, and Senekal, to study various aspects of how literature functions. The basic premise is that polysystem theory, which has been around since the writings of Even-Zohar, can be integrated with network theory and the relationships between different actors in the literary network, e.g. writers, critics, publishers, literary histories, etc., can be mapped using visualization from SNA.\n\nResearch studies of formal or informal organization relationships, organizational communication, economics, economic sociology, and other resource transfers. Social networks have also been used to examine how organizations interact with each other, characterizing the many informal connections that link executives together, as well as associations and connections between individual employees at different organizations. Intra-organizational networks have been found to affect organizational commitment, organizational identification, interpersonal citizenship behaviour.\n\nSocial capital is a form of economic and cultural capital in which social networks are central, transactions are marked by reciprocity, trust, and cooperation, and market agents produce goods and services not mainly for themselves, but for a common good. Social capital is split into three dimensions: the structural, the relational and the cognitive dimension. The structural dimension describes how partners interact with each other and which specific partners meet in a social network. Also The structural dimension of social capital indicates the level of ties among organizations.. This dimension is highly connected to the relational dimension which refers to trustworthiness, norms, expectations and idenfications of the bonds between partners.The relational dimension explains the nature of these ties which is mainly illustrated by the level of trust accorded to the network of organizations. The cognitive dimension analyses the extent to which organizations share common goals and objectives as a result of their ties and interactions. \n\nSocial capital is a sociological concept about the value of social relations and the role of cooperation and confidence to achieve positive outcomes. The term refers to the value one can get from their social ties. For example, newly arrived immigrants can make use of their social ties to established migrants to acquire jobs they may otherwise have trouble getting (e.g., because of unfamiliarity with the local language). A positive relationship exists between social capital and the intensity of social network use. In a dynamic framework, higher activity in a network feeds into higher social capital which itself encourages more activity.\n\nIn many organizations, members tend to focus their activities inside their own groups, which stifles creativity and restricts opportunities. A player whose network bridges structural holes has an advantage in detecting and developing rewarding opportunities. Such a player can mobilize social capital by acting as a \"broker\" of information between two clusters that otherwise would not have been in contact, thus providing access to new ideas, opinions and opportunities. British philosopher and political economist John Stuart Mill, writes, \"it is hardly possible to overrate the value ... of placing human beings in contact with persons dissimilar to themselves... Such communication [is] one of the primary sources of progress.\" Thus, a player with a network rich in structural holes can add value to an organization through new ideas and opportunities. This in turn, helps an individual's career development and advancement.\n\nA social capital broker also reaps control benefits of being the facilitator of information flow between contacts. In the case of consulting firm Eden McCallum, the founders were able to advance their careers by bridging their connections with former big three consulting firm consultants and mid-size industry firms. By bridging structural holes and mobilizing social capital, players can advance their careers by executing new opportunities between contacts.\n\nThere has been research that both substantiates and refutes the benefits of information brokerage. A study of high tech Chinese firms by Zhixing Xiao found that the control benefits of structural holes are \"dissonant to the dominant firm-wide spirit of cooperation and the information benefits cannot materialize due to the communal sharing values\" of such organizations. However, this study only analyzed Chinese firms, which tend to have strong communal sharing values. Information and control benefits of structural holes are still valuable in firms that are not quite as inclusive and cooperative on the firm-wide level. In 2004, Ronald Burt studied 673 managers who ran the supply chain for one of America's largest electronics companies. He found that managers who often discussed issues with other groups were better paid, received more positive job evaluations and were more likely to be promoted. Thus, bridging structural holes can be beneficial to an organization, and in turn, to an individual's career.\n\nComputer networks combined with social networking software produces a new medium for social interaction. A relationship over a computerized social networking service can be characterized by context, direction, and strength. The content of a relation refers to the resource that is exchanged. In a computer mediated communication context, social pairs exchange different kinds of information, including sending a data file or a computer program as well as providing emotional support or arranging a meeting. With the rise of electronic commerce, information exchanged may also correspond to exchanges of money, goods or services in the \"real\" world. Social network analysis methods have become essential to examining these types of computer mediated communication.\n\nIn addition, the sheer size and the volatile nature of social media has given rise to new network metrics. A key concern with networks extracted from social media is the lack of robustness of network metrics given missing data.\n\n\n\n\n\n"}
{"id": "45413", "url": "https://en.wikipedia.org/wiki?curid=45413", "title": "Tribe", "text": "Tribe\n\nThe term tribe is used in many different contexts to refer to a category of human social group. The predominant usage of the term is in the discipline of anthropology. The definition is contested, in part due to conflicting theoretical understandings of social and kinship structures, and also reflecting the problematic application of this concept to extremely diverse human societies. The concept is often contrasted by anthropologists with other social and kinship groups, being hierarchically larger than a lineage or clan, but smaller than a chiefdom, nation or state. These terms are equally disputed. In some cases tribes have legal recognition and some degree of political autonomy from national or federal government, but this legalistic usage of the term may conflict with anthropological definitions.\n\nThe word \"tribe\" first occurred in English in 12th-century Middle English-literature, in reference to the twelve tribes of Israel. The Middle English word is derived from Old French \"tribu\" and, in turn, from Latin \"tribus\" (plural \"tribūs\"), in reference to a supposed tripartite division of the original Roman state along ethnic lines, into \"tribūs\" known as the \"Ramnes\" (or \"Ramnenses\"), \"Tities\" (or \"Titienses\"), and \"Luceres\", corresponding, according to Marcus Terentius Varro, to the Latins, Sabines and Etruscans respectively. The \"Ramnes\" were named after Romulus, leader of the Latins, \"Tities\" after Titus Tatius, leader of the Sabines, and \"Luceres\" after Lucumo, leader of an Etruscan army that had assisted the Latins. In 242–240 BC, the Tribal Assembly (\"comitia tributa\") in the Roman Republic included 35 tribes (four \"urban tribes\" and 31 \"rural tribes\"). According to Livy, the three \"tribes\" were squadrons of cavalry, rather than ethnic divisions.\n\nThe ultimate etymology of the term \"tribe\" is uncertain, perhaps from the Proto-Indo-European roots \"tri-\" (\"three\") and \"bhew\" (\"to be\"). The classicist Gregory Nagy says, citing the linguist Émile Benveniste, that the Umbrian \"trifu\" (equivalent of the Latin \"tribus\") is apparently derived from a combination of \"*tri-\" and \"*bhu-\", where the second element is cognate with the Greek root \"phúō\" φύω “to bring forth” and the Greek \"phulē\" φυλή \"clan, race, people\" (plural \"phylai\" φυλαί). The Greek \"polis\" (\"state\" or \"city\") was, like the Roman state, often divided into \"phylai\". In Europe during the late medieval era, the Bible was written mostly in New Latin and instead of \"tribus\" the word \"phyle\" was used, derived from the Greek \"phulē\". \n\nConsiderable debate has accompanied efforts to define and characterize tribes. In the popular imagination, tribes reflect a primordial social structure from which all subsequent civilizations and states developed. Anthropologist Elman Service presented a system of classification for societies in all human cultures, based on the evolution of social inequality and the role of the state. This system of classification contains four categories:\n\n\nTribes are therefore considered to be a political unit formed from an organisation of families (including clans and lineages) based on social or ideological solidarity. Membership of a tribe may be understood simplistically as being an identity based on factors such as kinship (\"clan\"), ethnicity (\"race\"), language, dwelling place, political group, religious beliefs, oral tradition and/or cultural practices.\n\nArchaeologists continue to explore the development of pre-state tribes. Current research suggests that tribal structures constituted one type of adaptation to situations providing plentiful yet unpredictable resources. Such structures proved flexible enough to coordinate production and distribution of food in times of scarcity, without limiting or constraining people during times of surplus.\n\nThe term \"tribe\"\"\" was in common use in the field of anthropology until the late 1950s and 1960s. The continued use of the term has attracted controversy among anthropologists and other academics active in the social sciences, with scholars of anthropological and ethnohistorical research challenging the utility of the concept. In 1970, anthropologist J. Clyde Mitchell wrote:\n\nDespite the membership boundaries for a tribe being conceptually simple, in reality they are often vague and subject to change over time. In his 1975 study, \"The Notion of the Tribe\", anthropologist Morton H. Fried provided numerous examples of tribes that encompassed members who spoke different languages and practiced different rituals, or who shared languages and rituals with members of other tribes. Similarly, he provided examples of tribes in which people followed different political leaders, or followed the same leaders as members of other tribes. He concluded that tribes in general are characterized by fluid boundaries, heterogeneity and dynamism, and are not parochial.\n\nPart of the difficulty with the term is that it seeks to construct and apply a common conceptual framework across diverse cultures and peoples. Different anthropologists studying different peoples therefore draw conflicting conclusions about the nature, structure and practices of tribes. Writing on the Kurdish peoples, anthropologist Martin van Bruinessen argued, \"the terms of standard anthropological usage, 'tribe', 'clan' and 'lineage' appear to be a straitjacket that ill fits the social reality of Kurdistan\".\n\nThere are further negative connotations of the term \"tribe\" that have reduced its use. Writing in 2013, scholar Matthew Ortoleva noted that \"like the word \"Indian\", \"[t]ribe\" is a word that has connotations of colonialism.\" Survival International says \"It is important to make the distinction between \"tribal\" and \"indigenous\" because tribal peoples have a special status acknowledged in international law as well as problems in addition to those faced by the wider category of indigenous peoples.\"\n\nFew tribes today remain isolated from the development of the modern state system. Tribes have lost their legitimacy to conduct traditional functions, such as tithing, delivering justice and defending territory, with these being replaced by states functions and institutions, such as taxation, law courts and the military. Most have suffered decline and loss of cultural identity. Some have adapted to the new political context and transformed their culture and practices in order to survive, whilst others have secured legal rights and protections.\n\nAnthropologist Morton Fried proposed that most surviving tribes do not have their origin in pre-state tribes, but rather in pre-state bands. Such \"secondary\" tribes, he suggested, developed as modern products of state expansion. Bands comprise small, mobile, and fluid social formations with weak leadership. They do not generate surpluses, pay no taxes, and support no standing army. Fried argued that secondary tribes develop in one of two ways. First, states could set them up as means to extend administrative and economic influence in their hinterland, where direct political control costs too much. States would encourage (or require) people on their frontiers to form more clearly bounded and centralized polities, because such polities could begin producing surpluses and taxes, and would have a leadership responsive to the needs of neighboring states (the so-called \"scheduled\" tribes of the United States or of British India provide good examples of this). Second, bands could form \"secondary\" tribes as a means to defend against state expansion. Members of bands would form more clearly bounded and centralized polities, because such polities could begin producing surpluses that could support a standing army that could fight against states, and they would have a leadership that could co-ordinate economic production and military activities.\n\nIn the Scheduled Castes and Scheduled Tribes of India and Native American tribes of North America, tribes are considered polities, or sovereign nations, that have retained or been granted legal recognition and some degree of autonomy by a national or federal government.\n\n"}
{"id": "88380", "url": "https://en.wikipedia.org/wiki?curid=88380", "title": "Boarding school", "text": "Boarding school\n\nA boarding school provides education for pupils who live on the premises, as opposed to a day school. The word \"boarding\" is used in the sense of \"room and board\", i.e. lodging and meals. As they have existed for many centuries, and now extend across many countries, their function and ethos varies greatly. Traditionally, pupils stayed at the school for the length of the term; some schools facilitate returning home every weekend, and some welcome day pupils. Some are for either boys or girls while others are co-educational.\n\nIn the United Kingdom, which has a more extensive history of such schools, many independent (private) schools offer boarding, but likewise so do a few dozen state schools, many of which serve children from remote areas. In the United States, most boarding schools cover grades seven or nine through grade twelve—the high school years. Some American boarding schools offer a post-graduate year of study to help students prepare for college entrance.\n\nIn some times and places boarding schools are the most elite educational option (such as Eton and Harrow, which have produced several prime ministers), whereas in other contexts, they serve as places to segregate children deemed a problem to their parents or wider society. Canada and the United States tried to assimilate indigenous children in the Canadian Indian residential school system and American Indian boarding schools respectively. Some function essentially as orphanages, e.g. the G.I. Rossolimo Boarding School Number 49 in Russia. Tens of millions of rural children are now educated at boarding schools in China. Therapeutic boarding schools offer treatment for psychological difficulties. Military academies provide strict discipline. Education for children with special needs has a long association with boarding; see, for example, Deaf education and Council of Schools and Services for the Blind. Some boarding schools offer an immersion into democratic education, such as Summerhill School. Others are determinedly international, such as the United World Colleges.\n\nThe term \"boarding school\" often refers to classic British boarding schools and many boarding schools around the world are modeled on these.\n\nA typical boarding school has several separate residential houses, either within the school grounds or in the surrounding area.\n\nA number of senior teaching staff are appointed as housemasters, housemistresses, dorm parents, prefects, or residential advisors, each of whom takes quasi-parental responsibility (\"in loco parentis\") for anywhere from 5 to 50 students resident in their \"house\" or dormitory at all times but particularly outside school hours. Each may be assisted in the domestic management of the house by a housekeeper often known in U.K. or Commonwealth countries as \"matron\", and by a \"house tutor\" for academic matters, often providing staff of each gender. In the U.S., boarding schools often have a resident family that lives in the dorm, known as dorm parents. They often have janitorial staff for maintenance and housekeeping, but typically do not have tutors associated with an individual dorm. Nevertheless, older students are often less supervised by staff, and a system of monitors or prefects gives limited authority to senior students. Houses readily develop distinctive characters, and a healthy rivalry between houses is often encouraged in sport.\n\nHouses or dorms usually include study-bedrooms or dormitories, a dining room or refectory where students take meals at fixed times, a library and possibly study carrels where students can do their homework. Houses may also have common rooms for television and relaxation and kitchens for snacks, and occasionally storage facilities for bicycles or other sports equipment. Some facilities may be shared between several houses or dorms.\n\nIn some schools, each house has students of all ages, in which case there is usually a prefect system, which gives older students some privileges and some responsibility for the welfare of the younger ones. In others, separate houses accommodate needs of different years or classes. In some schools, day students are assigned to a dorm or house for social activities and sports purposes.\n\nMost school dormitories have an \"in your room by\" and a \"lights out\" time, depending on their age, when the students are required to prepare for bed, after which no talking is permitted. Such rules may be difficult to enforce; students may often try to break them, for example by using their laptop computers or going to another student's room to talk or play computer games. International students may take advantage of the time difference between countries (e.g. 7 hours between UK and China) to contact friends or family. Students sharing study rooms are less likely to disturb others and may be given more latitude.\n\nAs well as the usual academic facilities such as classrooms, halls, libraries and laboratories, boarding schools often provide a wide variety of facilities for extracurricular activities such as music rooms, gymnasiums, sports fields and school grounds, boats, squash courts, swimming pools, cinemas and theaters. A school chapel is often found on site. Day students often stay on after school to use these facilities. Many North American boarding schools are located in beautiful rural environments, and have a combination of architectural styles that vary from modern to hundreds of years old.\n\nFood quality can vary from school to school, but most boarding schools offer diverse menu choices for many kinds of dietary restrictions and preferences. Some boarding schools have a dress code for specific meals like dinner or for specific days of the week. Students are generally free to eat with friends, teammates, as well as with faculty and coaches. Extra curricular activities groups, e.g. the French Club, may have meetings and meals together. The Dining Hall often serves a central place where lessons and learning can continue between students and teachers or other faculty mentors or coaches. Some schools welcome day students to attend breakfast and dinner, in additional to the standard lunch, while others charge a fee.\n\nMany boarding schools have an on-campus school store or snack hall where additional food and school supplies can be purchased; and may also have a student recreational center where food can be purchased during specified hours.\n\nStudents generally need permission to go outside defined school bounds; they may be allowed to travel off-campus at certain times.\n\nDepending on country and context, boarding schools generally offer one or more options: full (students stay at the school full-time), weekly (students stay in the school from Monday through Friday, then return home for the weekend), or on a flexible schedule (students choose when to board, e.g. during exam week).\n\nEach student has an individual timetable, which in the early years allows little discretion. Boarders and day students are taught together in school hours and in most cases continue beyond the school day to include sports, clubs and societies, or excursions.\n\nBritish boarding schools have three terms a year, approximately twelve weeks each, with a few days' half-term holiday during which students are expected to go home or at least away from school. There may be several exeats, or weekends, in each half of the term when students may go home or away (e.g. international students may stay with their appointed guardians, or with a host family). Boarding students nowadays often go to school within easy traveling distance of their homes, and so may see their families frequently; e.g. families are encouraged to come and support school sports teams playing at home against other schools, or for school performances in music, drama or theatre.\n\nSome boarding schools allow only boarding students, while others have both boarding students and day students who go home at the end of the school day. Day students are sometimes known as day boys or day girls. Some schools welcome day students to attend breakfast and dinner, while others charge a fee. For schools that have designated study hours or quiet hours in the evenings, students on campus (including day students) are usually required to observe the same \"quiet\" rules (such as no television, students must stay in their rooms, library or study hall, etc.). Schools that have both boarding and day students sometimes describe themselves as semi-boarding schools or day boarding schools. Some schools also have students who board during the week but go home on weekends: these are known as weekly boarders, quasi-boarders, or five-day boarders.\n\nBoarding schools are residential schools; however, not all residential schools are \"classic\" boarding schools. Other forms of residential schools include:\n\nIn the UK, almost all boarding schools are independent schools, which are not subject to the national curriculum or other educational regulations applicable to state schools. Nevertheless, there are some regulations, primarily for health and safety purposes, as well as the general law. The Department for Children, Schools and Families, in conjunction with the Department of Health of the United Kingdom, has prescribed guidelines for boarding schools, called the National Boarding Standards.\n\nOne example of regulations covered within the National Boarding Standards are those for the minimum floor area or living space required for each student and other aspects of basic facilities. The minimum floor area of a dormitory accommodating two or more students is defined as the number of students sleeping in the dormitory multiplied by 4.2 m², plus 1.2 m². A minimum distance of 0.9 m should also be maintained between any two beds in a dormitory, bedroom or cubicle. In case students are provided with a cubicle, then each student must be provided with a window and a floor area of 5.0 m² at the least. A bedroom for a single student should be at least of floor area of 6.0 m². Boarding schools must provide a total floor area of at least 2.3 m² living accommodation for every boarder. This should also be incorporated with at least one bathtub or shower for every ten students.\n\nThese are some of the few guidelines set by the department among many others. It could probably be observed that not all boarding schools around the world meet these minimum basic standards, despite their apparent appeal.\n\nBoarding schools manifest themselves in different ways in different societies. For example, in some societies children enter at an earlier age than in others. In some societies, a tradition has developed in which families send their children to the same boarding school for generations. One observation that appears to apply globally is that a significantly larger number of boys than girls attend boarding school and for a longer span of time. The practice of sending children, particularly boys, to other families or to schools so that they could learn together is of very long standing, recorded in classical literature and in UK records going back over 1,000 years.\n\nIn Europe, a practice developed by early medieval times of sending boys to be taught by literate clergymen, either in monasteries or as pages in great households. The King's School, Canterbury, arguably the world's oldest boarding school, dates its foundation from the development of the monastery school in around 597 AD. The author of the \"Croyland Chronicle\" recalls being tested on his grammar by Edward the Confessor's wife Queen Editha in the abbey cloisters as a Westminster schoolboy, in around the 1050s. Monastic schools as such were generally dissolved with the monasteries themselves under Henry VIII, although Westminster School was specifically preserved by the King's letters patent, and it seems likely that most schools were immediately replaced. Winchester College founded by Bishop William of Wykeham in 1382 and Oswestry School founded by David Holbache in 1407 are the oldest boarding schools in continuous operation.\n\nBoarding schools in Britain started in medieval times, when boys were sent to be educated at a monastery or noble household, where a lone literate cleric could be found. In the 12th century, the Pope ordered all Benedictine monasteries such as Westminster to provide charity schools, and many public schools started when such schools attracted paying students. These public schools reflected the collegiate universities of Oxford and Cambridge, as in many ways they still do, and were accordingly staffed almost entirely by clergymen until the 19th century. Private tuition at home remained the norm for aristocratic families, and for girls in particular, but after the 16th century it was increasingly accepted that adolescents of any rank might best be educated collectively. The institution has thus adapted itself to changing social circumstances over 1,000 years.\n\nBoarding preparatory schools tend to reflect the public schools they feed. They often have a more or less official tie to particular schools.\n\nThe classic British boarding school became highly popular during the colonial expansion of the British Empire. British colonial administrators abroad could ensure that their children were brought up in British culture at public schools at home in the UK, and local rulers were offered the same education for their sons. More junior expatriates would send their children to local British-run schools, which would also admit selected local children who might travel from considerable distances. The boarding schools, which inculcated their own values, became an effective way to encourage local people to share British ideals, and so help the British achieve their imperial goals.\n\nOne of the reasons sometimes stated for sending children to boarding schools is to develop wider horizons than their family can provide. A boarding school a family has attended for generations may define the culture parents aspire to for their children. Equally, by choosing a fashionable boarding school, parents may aspire to better their children by enabling them to mix on equal terms with children of the upper classes. However, such stated reasons may conceal other reasons for sending a child away from home. These might apply to children who are considered too disobedient or underachieving, children from families with divorced spouses, and children to whom the parents do not much relate. These reasons are rarely explicitly stated, though the child might be aware of them.\n\nIn 1998, there were 772 private-sector boarding schools in the United Kingdom with over 100,000 children attending them all across the country. In Britain, they are an important factor in the class system. About one percent of British children are sent to boarding schools. Also in Britain children as young as 5 to 9 years of age are sent to boarding schools.\n\nIn the United States, boarding schools for students below the age of 13 are called \"junior boarding schools\", and are relatively uncommon. The oldest junior boarding school is the Fay School in Southborough, Massachusetts, established in 1866. Other boarding schools are intended for high school age students, generally of ages 14–18. Some of the oldest of these boarding schools include West Nottingham Academy (est. 1744), Linden Hall (school) (est. 1756), The Governor's Academy (est. 1763), and Phillips Academy Andover (est. 1778). Boarding schools for this age group are often referred to as prep schools. About half of one percent or (.5%) of school children attend boarding schools, about half the percentage of British children.\n\nIn the late 19th century, the United States government undertook a policy of educating Native American youth in the ways of the dominant Western culture so that Native Americans might then be able to assimilate into Western society. At these boarding schools, managed and regulated by the government, Native American students were subjected to a number of tactics to prepare them for life outside their reservation homes.\n\nIn accordance with the assimilation methods used at the boarding schools, the education that the Native American children received at these institutions centered on the dominant society's construction of gender norms and ideals. Thus boys and girls were separated in almost every activity and their interactions were strictly regulated along the lines of Victorian ideals. In addition, the instruction that the children received reflected the roles and duties that they were to assume once outside the reservation. Thus girls were taught skills that could be used in the home, such as \"sewing, cooking, canning, ironing, child care, and cleaning\" (Adams 150). Native American boys in the boarding schools were taught the importance of an agricultural lifestyle, with an emphasis on raising livestock and agricultural skills like \"plowing and planting, field irrigation, the care of stock, and the maintenance of fruit orchards\" (Adams 149). These ideas of domesticity were in stark contrast to those existing in native communities and on reservations: many indigenous societies were based on a matrilineal system where the women's lineage was honored and the women's place in society respected. For example, women in native society held powerful roles in their own communities, undertaking tasks that Western society deemed only appropriate for men: indigenous women could be leaders, healers, and farmers.\n\nWhile the Native American children were exposed to and were likely to adopt some of the ideals set out by the whites operating these boarding schools, many resisted and rejected the gender norms that were being imposed upon them. See also: Carlisle Indian Industrial School in Carlisle, Pennsylvania.\n\nMost societies around the world decline to make boarding schools the preferred option for the upbringing of their children. However, boarding schools are one of the preferred modes of education in former British colonies or Commonwealth countries like India, Pakistan, Nigeria, and other former African colonies of Great Britain. For instance in Ghana the majority of the secondary schools are boarding. In China some children are sent to boarding schools at 2 years of age. In some countries, such as New Zealand and Sri Lanka, a number of state schools have boarding facilities. These state boarding schools are frequently the traditional single-sex state schools, whose ethos is much like that of their independent counterparts. Furthermore, the proportion of boarders at these schools is often much lower than at independent boarding schools, typically around 10%.\n\nIn Canada, the largest independent boarding school is Columbia International College, with an enrollment of 1,700 students from all over the world. Robert Land Academy in Wellandport, Ontario is Canada's only private military style boarding school for boys in Grades 6 through 12.\n\nIn the former Soviet Union these schools were sometimes known as Internat-schools (Russian: \"Школа-интернат\") (from Latin: \"internus\"). They varied in their organization. Some schools were a type of specialized school with a specific focus in a particular field or fields such as mathematics, physics, language, science, sports, etc. Other schools were associated with orphanages after which all children enrolled in Internat-school automatically. Also, separate boarding schools were established for children with special needs (schools for blind, for deaf and other). General schools offered \"extended stay\" programs (Russian: Группа продленного дня) featuring cheap meals for children and preventing them from coming home too early before parents were back from work (education in the Soviet Union was free). In post-Soviet countries, the concept of boarding school differs from country to country.\n\nThe Swiss government developed a strategy of fostering private boarding schools for foreign students as a business integral to the country's economy. Their boarding schools offer instruction in several major languages and have a large number of quality facilities organized through the Swiss Federation of Private Schools. In 2015, a Swiss boarding school named A+ World Academy was established on the Norwegian Tall Ship Fullriggeren Sørlandet. The top four most expensive boarding schools in the world are the Swiss schools Institut Le Rosey, Beau Soleil, Collège du Léman and Collège Champittet.\n\n there were about 100,000 boarding schools in rural areas of Mainland China, with about 33 million children living in them. The majority of these boarding schools are in western China, which generally is not as wealthy as eastern and central China. Many migrant workers and farmers send their children to boarding schools.\n\nSome elite university-preparatory boarding schools for students from age 13 to 18 are seen by sociologists as centers of socialization for the next generation of the political upper class and reproduces an elitist class system. This attracts families who value power and hierarchy for the socialization of their family members. These families share a sense of entitlement to social class or hierarchy and power.\n\nBoarding schools are seen by certain families as centres of socialization where students mingle with others of similar social hierarchy to form what is called an old boy network. Elite boarding school students are brought up with the assumption that they are meant to control society. Significant numbers of them enter the political upper class of society or join the financial elite in fields such as international banking and venture capital. Elite boarding school socialization causes students to internalize a strong sense of entitlement and social control or hierarchy. This form of socialization is called \"deep structure socialization\" by Peter Cookson & Caroline Hodges (1985). This refers to the way in which boarding schools not only manage to control the students' physical lives but also their emotional lives.\n\nBoarding school establishment involves control of behaviour regarding several aspects of life including what is appropriate and/or acceptable which adolescents would consider as intrusive. This boarding school socialization is carried over well after leaving school and into their dealings with the social world. Thus it causes boarding school students to adhere to the values of the elite social class which they come from or which they aspire to be part of. Nick Duffell, author of \"Wounded Leaders: British elitism and the Entitlement Illusion – A Psychohistory\", states that the education of the elite in the British boarding school system leaves the nation with \"a cadre of leaders who perpetuate a culture of elitism, bullying and misogyny affecting the whole of society\". According to Peter W Cookson Jr (2009) the elitist tradition of preparatory boarding schools has declined due to the development of modern economy and the political rise of the liberal west coast of the United States of America. Further, , there are over twenty boarding schools on the west coast of the United States.\n\nThe boarding school socialization of control and hierarchy develops deep rooted and strong adherence to social roles and rigid gender stratification. In one studied school the social pressure for conformity was so severe that several students abused performance drugs like Adderall and Ritalin for both academic performance and to lose weight. The distinct and hierarchical nature of socialization in boarding school culture becomes very obvious in the manner students sit together and form cliques, especially in the refectory, or dining hall. This leads to pervasive form of explicit and implicit bullying, and excessive competition between cliques and between individuals. The rigid gender stratification and role control is displayed in the boys forming cliques on the basis of wealth and social background, and the girls overtly accepting that they would marry only for money, while choosing only rich or affluent males as boyfriends. Students are not able to display much sensitivity and emotional response and are unable to have closer relationships except on a superficial and politically correct level, engaging in social behaviour that would make them seem appropriate and rank high in social hierarchy. This affects their perceptions of gender and social roles later in life.\n\nOne alumnus of a military boarding school also questions whether leadership is truly being learned by the school's students.\n\nThe aspect of boarding school life with its round the clock habitation of students with each other in the same environment, involved in studying, sleeping and socializing can lead to pressures and stress in boarding school life. This is manifested in the form of hypercompetitiveness, use of recreational or illegal drugs and psychological depression that at times may manifest in suicide or its attempt. Studies show that about 90% of boarding school students acknowledge that living in a total institution like boarding school has significant impact and changed their perception and interaction with social relationships.\n\nIt is claimed that children may be sent to boarding schools to give more opportunities than their family can provide. However, that involves spending significant parts of one's early life in what may be seen as a total institution and possibly experiencing social detachment, as suggested by social-psychologist Erving Goffman. This may involve long-term separation from one's parents and culture, leading to the experience of homesickness and emotional abandonment and may give rise to a phenomenon known as the 'TCK' or third culture kid.\n\nThe celebrated British classicist and poet, Robert Graves (1895–1985), who attended six different preparatory schools at a young age during the early 20th century, wrote:\n\nSome modern philosophies of education, such as constructivism and new methods of music training for children including Orff Schulwerk and the Suzuki method, make the everyday interaction of the child and parent an integral part of training and education. In children, separation involves maternal deprivation. The European Union-Canada project \"Child Welfare Across Borders\" (2003), an important international venture on child development, considers boarding schools as one form of permanent displacement of the child. This view reflects a new outlook towards education and child growth in the wake of more scientific understanding of the human brain and cognitive development.\n\nData have not yet been tabulated regarding the statistical ratio of boys to girls that matriculate boarding schools, the total number of children in a given population in boarding schools by country, the average age across populations when children are sent to boarding schools, and the average length of education (in years) for boarding school students. There is also little evidence or research about the complete circumstances or complete set of reasons about sending kids to boarding schools.\n\nThe term \"boarding school syndrome\" was coined by psychotherapist Joy Schaverien in 2011. It is used to identify a set of lasting psychological problems that are observable in adults who, as children, were sent away to boarding schools at an early age.\n\nScharverien's observations are echoed by boarding school boy, George Monbiot, who goes so far as to attribute some dysfunctionalities of the UK government to boarding schools.\n\nBoarding schools and their surrounding settings and situations became in the late Victorian period a genre in British literature with its own identifiable conventions. (Typically, protagonists find themselves occasionally having to break school rules for honourable reasons the reader can identify with, and might get severely punished when caught – but usually they do not embark on a total rebellion against the school as a system.)\n\nNotable examples of the school story include:\n\nThe setting has also been featured in notable North American fiction:\n\nThere is also a huge boarding-school genre literature, mostly uncollected, in British comics and serials from the 1900s to the 1980s.\n\nThe subgenre of books and films set in a military or naval academy has many similarities with the above.\n\n\n\n"}
{"id": "387703", "url": "https://en.wikipedia.org/wiki?curid=387703", "title": "Orphanage", "text": "Orphanage\n\nHistorically, an orphanage is a residential institution, or group home, devoted to the care of orphans and other children who were separated from their biological families. Examples of what would cause a child to be placed in orphanages are when the parents were deceased, the biological family was abusive to the child, there was substance abuse or mental illness in the biological home that was detrimental to the child, or the parents had to leave to work elsewhere and were unable or unwilling to take the child. The role of legal responsibility for the support of children whose parent(s) have died or are otherwise unable to provide care differs internationally.\n\nThe use of government-run orphanages has been phased out in the United States, Canada, the United Kingdom, and in the European Union member-states during the latter half of the 20th century but continue to operate in many other regions internationally. While the term \"orphanage\" is no longer typically used in the United States, nearly every US state continues to operate residential group homes for children in need of a safe place to live and in which to be supported in their educational and life-skills pursuits. Homes like the Milton Hershey School in Pennsylvania, Mooseheart in Illinois and the Crossnore School and Children's Home in North Carolina continue to provide care and support for children in need. While a place like the Milton Hershey School houses nearly 2,000 children, each child lives in a small group-home environment with \"house parents\" who often live many years in that home. Children who grow up in these residential homes have higher rates of high school and college graduation than those who spend equivalent numbers of years in the US Foster Care system, wherein only 44 to 66 percent of children graduate from high school.\n\nResearch from the Bucharest Early Intervention Project (BEIP) is often cited as demonstrating that residential institutions negatively impact the wellbeing of children. The BEIP selected orphanages in Bucharest, Romania that raised abandoned children in socially and emotionally deprived environments in order to study the changes in development of infants and children after they had been placed with specially trained foster families in the local community. This powerful study demonstrated how the lack of loving attention typically provided to children by their parents or caregivers is pivotal for optimal human development, specifically of the brain; adequate nutrition is not enough. Further research of children who were adopted from institutions in Eastern European countries to the US demonstrated that for every 3.5 months that an infant spent in the institution, they lagged behind their peers in growth by 1 month. Further, a meta-analysis of research on the IQs of children in orphanages found lower IQs among the children in many institutions, but this result was not found in the low-income country setting.\n\nWorldwide, residential institutions like orphanages can often be detrimental to the psychological development of affected children. In countries where orphanages are no longer in use, the long-term care of unwarded children by the state has been transitioned to a domestic environment, with an emphasis on replicating a family home. Many of these countries, such as the United States, utilize a system of monetary stipends paid to foster parents to incentivize and subsidize the care of state wards in private homes. A distinction must be made between foster care and adoption, as adoption would remove the child from the care of the state and transfer the legal responsibility for that child's care to the adoptive parent completely and irrevocably, whereas in the case of foster care, the child would remain a ward of the state with the foster parent acting only as caregiver.\n\nMost children who live in orphanages are not orphans; four out of five children in orphanages have at least one living parent and most having some extended family. Developing countries and their governments rely on kinship care to aid in the orphan crisis, because it is cheaper to financially help extended families in taking in an orphaned child than it is to institutionalize them. Additionally, developing nations are lacking in child welfare and their well-being because of lack of resources. Research that is being collected in the developing world shows that these countries focus purely on survival indicators instead of a combination of their survival and other positive indicators like a developed nation would do. This speaks to the way that many developed countries treat an orphan crisis, as the only focus is to obtain a way to insure their survival. In the developed nations orphans can expect to find not only a home but also these countries will try an ensure a secure future as well. Furthermore, orphans in developing nations are seen as a problem that needs to be solved, this also makes them vulnerable to exploitation or neglect. In Pakistan, alternative care for orphans often falls on to extended families and Pakistan society as the government feels puts the burden of caring for orphans on them. Although it is very common for Pakistan citizens to take in orphans because of their culture and religion, only orphans whose parents have died are taken in. This neglects a population of children who need alternative care, either due to abuse, or parents who are unable to care for their child because of poverty, mental, or physical issues.\n\nA few large international charities continue to fund orphanages, but most are still commonly founded by smaller charities and religious groups. Especially in developing countries, orphanages may prey on vulnerable families at risk of breakdown and actively recruit children to ensure continued funding. Orphanages in developing countries are rarely run by the state. However, not all orphanages that are state-run are less corrupted; the Romanian orphanages, like those in Bucharest, were founded due to the soaring population numbers catalyzed by dictator Nicolae Ceaușescu, who banned abortion and birth control and incentivized procreation in order to increase the Romanian workforce.\n\nToday's residential institutions for children, also described as congregate care, include group homes, residential child care communities, children's homes, refuges, rehabilitation centers, night shelters, and youth treatment centers.\n\nThe Romans formed their first orphanages around 400 AD. Jewish law prescribed care for the widow and the orphan, and Athenian law supported all orphans of those killed in military service until the age of eighteen. Plato (\"Laws\", 927) says: \"Orphans should be placed under the care of public guardians. Men should have a fear of the loneliness of orphans and of the souls of their departed parents. A man should love the unfortunate orphan of whom he is guardian as if he were his own child. He should be as careful and as diligent in the management of the orphan's property as of his own or even more careful still.\" The care of orphans was referred to bishops and, during the Middle Ages, to monasteries. As soon as they were old enough, children were often given as apprentices to households to ensure their support and to learn an occupation.\n\nIn medieval Europe, care for orphans tended to reside with the Church. The Elizabethan Poor Laws were enacted at the time of the Reformation, and placed public responsibility on individual parishes to care for the indigent poor.\n\nThe growth of sentimental philanthropy in the 18th century led to the establishment of the first charitable institutions that would cater to orphans.\nThe Foundling Hospital was founded in 1741 by the philanthropic sea captain Thomas Coram in London, England, as a children's home for the \"education and maintenance of exposed and deserted young children.\" The first children were admitted into a temporary house located in Hatton Garden. At first, no questions were asked about child or parent, but a distinguishing token was put on each child by the parent.\n\nOn reception, children were sent to wet nurses in the countryside, where they stayed until they were about four or five years old. At sixteen, girls were generally apprenticed as servants for four years; at fourteen, boys were apprenticed into variety of occupations, typically for seven years. There was a small benevolent fund for adults.\n\nIn 1756, the House of Commons resolved that all children offered should be received, that local receiving places should be appointed all over the country, and that the funds should be publicly guaranteed. A basket was accordingly hung outside the hospital; the maximum age for admission was raised from two months to twelve, and a flood of children poured in from country workhouses. Parliament soon came to the conclusion that the indiscriminate admission should be discontinued. The hospital adopted a system of receiving children only with considerable sums. This practice was finally stopped in 1801; and it henceforth became a fundamental rule that no money was to be received.\n\nBy the early nineteenth century, the problem of abandoned children in urban areas, especially London, began to reach alarming proportions. The workhouse system, instituted in 1834, although often brutal, was an attempt at the time to house orphans as well as other vulnerable people in society who could not support themselves in exchange for work.\nConditions, especially for the women and children, were so bad as to cause an outcry among the social reform-minded middle-class; some of Charles Dickens' most famous novels, including \"Oliver Twist\", highlighted the plight of the vulnerable and the often abusive conditions that were prevalent in the London orphanages.\n\nClamour for change led to the birth of the orphanage movement. In England, the movement really took off in the mid-19th century although orphanages such as the Orphan Working Home in 1758 and the Bristol Asylum for Poor Orphan Girls in 1795, had been set up earlier. Private orphanages were founded by private benefactors; these often received royal patronage and government oversight. \nRagged schools, founded by John Pounds and the Lord Shaftesbury were also set up to provide pauper children with basic education.\n\nOrphanages were also set up in the United States from the early 19th century; for example, in 1806, the first private orphanage in New York (the Orphan Asylum Society, now Graham Windham) was co-founded by Elizabeth Schuyler Hamilton, widow of Alexander Hamilton, one of the founding fathers of the United States. Under the influence of Charles Loring Brace, foster care became a popular alternative from the mid-19th century. Later, the Social Security Act of 1935 improved conditions by authorizing Aid to Families with Dependent Children as a form of social security.\nA very influential philanthropist of the era was Thomas John Barnardo, the founder of the charity Barnardos. Becoming aware of the great numbers of homeless and destitute children adrift in the cities of England and encouraged by the 7th Earl of Shaftesbury and the 1st Earl Cairns, he opened the first of the \"Dr Barnardo’s Homes\" in 1870. By his death in 1905, he had established 112 district homes, which searched for and received waifs and strays, to feed, clothe and educate them. The system under which the institution was carried on is broadly as follows: the infants and younger girls and boys were chiefly \"boarded out\" in rural districts; girls above fourteen years of age were sent to the industrial training homes, to be taught useful domestic occupations; boys above seventeen years of age were first tested in labour homes and then placed in employment at home, sent to sea, or emigrated; boys of between thirteen and seventeen years of age were trained for the various trades for which they might be mentally or physically fitted\n\nEvidence from a variety of studies support the vital importance of attachment security and later development of children. Deinstitutionalization of orphanages and children's homes program in the United States began in the 1950s, after a series of scandals involving the coercion of birth parents and abuse of orphans (notably at Georgia Tann's Tennessee Children's Home Society). In Romania, a decree was established that aggressively promoted population growth, banning contraception and abortions for women with fewer than four children, despite the wretched poverty of most families. After Ceausescu was overthrown, he left a society unable and unwilling to take care of its children. Researchers conducted a study to see what the implications of this early childhood neglect were on development. Typically reared Romanian children showed high rates of secure attachment. Whereas the institutionally raised children showed huge rates of disorganized attachment. Many countries accepted the need to de-institutionalize the care of vulnerable children—that is, close down orphanages in favor of foster care and accelerated adoption.\n\nFoster care operates by taking in children from their homes due to the lack of care or abuse of their parents, where orphanages take in children with no parents or children whose parents have dropped them off for a better life, typically due to income.<ref>\n"}
{"id": "19008450", "url": "https://en.wikipedia.org/wiki?curid=19008450", "title": "Prison", "text": "Prison\n\nA prison, also known as a correctional facility, jail, gaol (dated, British and Australian English), penitentiary (American English), detention center (or centre if outside the US), correctional center, (American English) or remand center, is a facility in which inmates (or prisoners) are forcibly confined and denied a variety of freedoms under the authority of the state. Prisons are most commonly used within a criminal justice system: people charged with crimes may be imprisoned until their trial; those pleading or being found guilty of crimes at trial may be sentenced to a specified period of imprisonment. In simplest terms, a prison can also be described as a building in which people are legally held as a punishment for a crime they have committed.\n\nPrisons can also be used as a tool of political repression by authoritarian regimes. Their perceived opponents may be imprisoned for political crimes, often without trial or other legal due process; this use is illegal under most forms of international law governing fair administration of justice. In times of war, prisoners of war or detainees may be detained in military prisons or prisoner of war camps, and large groups of civilians might be imprisoned in internment camps.\n\nIn American English, the terms \"prison\" and \"jail\" have separate definitions, though this is not always followed in casual speech. A \"prison\" or \"penitentiary\" holds people for longer periods of time, such as many years, and is operated by a state or federal government. A \"jail\" holds people for shorter periods of time (e.g. for shorter sentences or pre-trial detention) and is usually operated by a local government. Outside of North America, \"prison\" and \"jail\" have the same meaning.\n\nSlang terms for a prison include: \"the pokey\", \"the slammer\", \"the cage\", \"the can\", \"the clink\", \"the joint\", \"the calaboose\", \"the hoosegow\", \"crowbar hotel\" and \"the big house\". Slang terms for imprisonment include: \"behind bars\", \"in stir\" and \"up the river\" (a possible reference to Sing Sing).\n\nThe use of prisons can be traced back to the rise of the state as a form of social organization. Corresponding with the advent of the state was the development of written language, which enabled the creation of formalized legal codes as official guidelines for society. The best known of these early legal codes is the Code of Hammurabi, written in Babylon around 1750 BC. The penalties for violations of the laws in Hammurabi's Code were almost exclusively centered on the concept of \"lex talionis\" (\"the law of retaliation\"), whereby people were punished as a form of vengeance, often by the victims themselves. This notion of punishment as vengeance or retaliation can also be found in many other legal codes from early civilizations, including the ancient Sumerian codes, the Indian \"Manusmriti\" (Manava Dharma Sastra), the \"Hermes Trismegistus\" of Egypt, and the Israelite Mosaic Law.\nSome Ancient Greek philosophers, such as Plato, began to develop ideas of using punishment to reform offenders instead of simply using it as retribution. Imprisonment as a penalty was used initially for those who could not afford to pay their fines. Eventually, since impoverished Athenians could not pay their fines, leading to indefinite periods of imprisonment, time limits were set instead. The prison in Ancient Athens was known as the \"desmoterion\" (\"place of chains\").\n\nThe Romans were among the first to use prisons as a form of punishment, rather than simply for detention. A variety of existing structures were used to house prisoners, such as metal cages, basements of public buildings, and quarries. One of the most notable Roman prisons was the Mamertine Prison, established around 640 B.C. by Ancus Marcius. The Mamertine Prison was located within a sewer system beneath ancient Rome and contained a large network of dungeons where prisoners were held in squalid conditions, contaminated with human waste. Forced labor on public works projects was also a common form of punishment. In many cases, citizens were sentenced to slavery, often in ergastula (a primitive form of prison where unruly slaves were chained to workbenches and performed hard labor).\n\nDuring the Middle Ages in Europe, castles, fortresses, and the basements of public buildings were often used as makeshift prisons. The possession of the right and the capability to imprison citizens, however, granted an air of legitimacy to officials at all levels of government, from kings to regional courts to city councils; and the ability to have someone imprisoned or killed served as a signifier of who in society possessed power or authority over others. Another common punishment was sentencing people to galley slavery, which involved chaining prisoners together in the bottoms of ships and forcing them to row on naval or merchant vessels.\n\nThe influence of French philosopher Michel Foucault; especially his book \"\" (1975) has energized the historical study of prisons and their role in the overall social system. \"Discipline and Punish: The Birth of the Prison\" is an analysis of the social and theoretical mechanisms behind the changes that occurred in Western penal systems during the modern age based on historical documents from France. Foucault argues that prison did not become the principal form of punishment just because of the humanitarian concerns of reformists. He traces the cultural shifts that led to the predominance of prison via the body and power. Prison used by the \"disciplines\" – new technological powers that can also be found, according to Foucault, in places such as schools, hospitals, and military barracks.\n\nFrom the late 17th century and during the 18th century, popular resistance to public execution and torture became more widespread both in Europe and in the United States. Particularly under the Bloody Code, with few sentencing alternatives, imposition of the death penalty for petty crimes, such as theft, was proving increasingly unpopular with the public; many jurors were refusing to convict defendants of petty crimes when they knew the defendants would be sentenced to death. Rulers began looking for means to punish and control their subjects in a way that did not cause people to associate them with spectacles of tyrannical and sadistic violence. They developed systems of mass incarceration, often with hard labor, as a solution. The prison reform movement that arose at this time was heavily influenced by two somewhat contradictory philosophies. The first was based in Enlightenment ideas of utilitarianism and rationalism, and suggested that prisons should simply be used as a more effective substitute for public corporal punishments such as whipping, hanging, etc. This theory, referred to as \"deterrence\", claims that the primary purpose of prisons is to be so harsh and terrifying that they deter people from committing crimes out of fear of going to prison. The second theory, which saw prisons as a form of \"rehabilitation\" or \"moral reform\", was based on religious ideas that equated crime with sin, and saw prisons as a place to instruct prisoners in Christian morality, obedience and proper behavior. These later reformers believed that prisons could be constructed as humane institutions of moral instruction, and that prisoners' behavior could be \"corrected\" so that when they were released, they would be model members of society.\n\nThe concept of the modern prison was invented in the early 19th-century. Punishment usually consisted of physical forms of punishment, including capital punishment, mutilation, flagellation (whipping), branding, and non-physical punishments, such as public shaming rituals (like the stocks). From the Middle Ages up to the 16th and 17th centuries in Europe, imprisonment was rarely used as a punishment in its own right, and prisons were mainly to hold those awaiting trial and convicts awaiting punishment.\n\nHowever, an important innovation at the time was the Bridewell House of Corrections, located at Bridewell Palace in London, which resulted in the building of other houses of correction. These houses held mostly petty offenders, vagrants, and the disorderly local poor. In these facilities the inmates were given \"prison labour\" jobs that were anticipated to shape them into hardworking individuals and prepare them for the real world. By the end of the 17th century, houses of correction were absorbed into local prison facilities under the control of the local justice of the peace.\n\nEngland used penal transportation of convicted criminals (and others generally young and poor) for a term of indentured servitude within the general population of British America between the 1610s and 1776. The Transportation Act 1717 made this option available for lesser crimes, or offered it by discretion as a longer-term alternative to the death penalty, which could theoretically be imposed for the growing number of offenses. The substantial expansion of transportation was the first major innovation in eighteenth-century British penal practice. Transportation to America was abruptly suspended by the Criminal Law Act 1776 (16 Geo. 3 c.43) with the start of the American Rebellion. While sentencing to transportation continued, the act instituted a punishment policy of hard labour instead. The suspension of transport also prompted the use of prisons for punishment and the initial start of a prison building program. Britain would resume transportation to specifically planned penal colonies in Australia between 1788 and 1868.\n\nGaols at the time were run as business ventures, and contained both felons and debtors; the latter were often housed with their wives and younger children. The gaolers made their money by charging the inmates for food, drink, and other services, and the system was generally corruptible. One reform of the seventeenth century was the establishment of the \"London Bridewell\" as a house of correction for women and children. It was the first facility to make any medical services available to prisoners.\n\nWith the widely used alternative of penal transportation halted in the 1770s, the immediate need for additional penal accommodations emerged. Given the undeveloped institutional facilities, old sailing vessels, termed \"hulks\", were the most readily available and expandable choice to be used as places of temporary confinement. While conditions on these ships were generally appalling, their use and the labor thus provided set a precedent which persuaded many people that mass incarceration and labour were viable methods of crime prevention and punishment. The turn of the 19th century would see the first movement toward prison reform, and by the 1810s, the first state prisons and correctional facilities were built, thereby inaugurating the modern prison facilities available today.\n\nFrance also sent criminals to overseas penal colonies, including Louisiana, in the early 18th century. Penal colonies in French Guiana operated until 1952, such as the notable Devil's Island (\"Île du Diable\"). Katorga prisons were harsh work camps established in the 17th century in Russia, in remote underpopulated areas of Siberia and the Russian Far East, that had few towns or food sources. Siberia quickly gained its fearful connotation of punishment.\nJohn Howard was one of the most notable early prison reformers. After having visited several hundred prisons across England and Europe, in his capacity as high sheriff of Bedfordshire, he published \"The State of the Prisons\" in 1777. He was particularly appalled to discover prisoners who had been acquitted but were still confined because they couldn't pay the gaoler's fees. He proposed wide-ranging reforms to the system, including the housing of each prisoner in a separate cell; the requirements that staff should be professional and paid by the government, that outside inspection of prisons should be imposed, and that prisoners should be provided with a healthy diet and reasonable living conditions. The prison reform charity, the Howard League for Penal Reform, was established in 1866 by his admirers.\n\nFollowing Howard's agitation, the Penitentiary Act was passed in 1779. This introduced solitary confinement, religious instruction, a labor regime, and proposed two state penitentiaries (one for men and one for women). However, these were never built due to disagreements in the committee and pressures from wars with France, and gaols remained a local responsibility. But other measures passed in the next few years provided magistrates with the powers to implement many of these reforms, and eventually, in 1815, gaol fees were abolished.\n\nQuakers were prominent in campaigning against and publicizing the dire state of the prisons at the time. Elizabeth Fry documented the conditions that prevailed at Newgate prison, where the ladies' section was overcrowded with women and children, some of whom had not even received a trial. The inmates did their own cooking and washing in the small cells in which they slept on straw. In 1816, Fry was able to found a prison school for the children who were imprisoned with their parents. She also began a system of supervision and required the women to sew and to read the Bible. In 1817, she helped found the Association for the Reformation of the Female Prisoners in Newgate.\n\nThe theory of the modern prison system was born in London, influenced by the utilitarianism of Jeremy Bentham. Bentham's panopticon introduced the principle of observation and control that underpins the design of the modern prison. The notion of prisoners being incarcerated as part of their punishment and not simply as a holding state until trial or hanging, was at the time revolutionary. His views influenced the establishment of the first prisons used as criminal rehabilitation centers. At a time when the implementation of capital punishment for a variety of relatively trivial offences was on the decline, the notion of incarceration as a form of punishment and correction held great appeal to reform-minded thinkers and politicians.\n\nIn the first half of the 19th century, capital punishment came to be regarded as inappropriate for many crimes that it had previously been carried out for, and by the mid-19th century, imprisonment had replaced the death penalty for the most serious offenses except for murder.\n\nThe first state prison in England was the Millbank Prison, established in 1816 with a capacity for just under 1000 inmates. By 1824, 54 prisons had adopted the disciplinary system advocated by the SIPD. By the 1840s, penal transportation to Australia and the use of hulks was on the decline, and the Surveyor-General of convict prisons, Joshua Jebb, set an ambitious program of prison building in the country, with one large prison opening per year. Pentonville prison opened in 1842, beginning a trend of ever increasing incarceration rates and the use of prison as the primary form of crime punishment. Robert Peel's Gaols Act of 1823 introduced regular visits to prisoners by chaplains, provided for the payment of gaolers and prohibited the use of irons and manacles.\n\nIn 1786, the state of Pennsylvania passed a law which mandated that all convicts who had not been sentenced to death would be placed in penal servitude to do public works projects such as building roads, forts, and mines. Besides the economic benefits of providing a free source of hard labor, the proponents of the new penal code also thought that this would deter criminal activity by making a conspicuous public example of consequences of breaking the law. However, what actually ended up happening was frequent spectacles of disorderly conduct by the convict work crews, and the generation of sympathetic feelings from the citizens who witnessed the mistreatment of the convicts. The laws quickly drew criticism from a humanitarian perspective (as cruel, exploitative and degrading) and from a utilitarian perspective (as failing to deter crime and delegitimizing the state in the eyes of the public). Reformers such as Benjamin Rush came up with a solution that would enable the continued used of forced labor, while keeping disorderly conduct and abuse out of the eyes of the public. They suggested that prisoners be sent to secluded \"houses of repentance\" where they would be subjected (out of the view of the public) to \"bodily pain, labour, watchfulness, solitude, and silence ... joined with cleanliness and a simple diet\".\n\nPennsylvania soon put this theory into practice, and turned its old jail at Walnut Street in Philadelphia into a state prison, in 1790. This prison was modeled on what became known as the \"Pennsylvania system\" (or \"separate system\"), and placed all prisoners into solitary cells with nothing other than religious literature, and forced them to be completely silent to reflect on their wrongs. New York soon built the Newgate state prison in Greenwich Village, which was modeled on the Pennsylvania system, and other states followed.\nBut by 1820 faith in the efficacy of legal reform had declined as statutory changes had no discernible effect on the level of crime, and the prisons, where prisoners shared large rooms and booty including alcohol, had become riotous and prone to escapes. In response, New York developed the Auburn system in which prisoners were confined in separate cells and prohibited from talking when eating and working together, implementing it at Auburn State Prison and Sing Sing at Ossining. The aim of this was rehabilitative: the reformers talked about the penitentiary serving as a model for the family and the school and almost all the states adopted the plan (though Pennsylvania went even further in separating prisoners). The system's fame spread and visitors to the U.S. to see the prisons included de Tocqueville who wrote \"Democracy in America\" as a result of his visit.\n\nThe use of prisons in Continental Europe was never as popular as it became in the English-speaking world, although state prison systems were largely in place by the end of the 19th century in most European countries. After the unification of Italy in 1861, the government reformed the repressive and arbitrary prison system they inherited, and modernized and secularized criminal punishment by emphasizing discipline and deterrence. Italy developed an advanced penology under the leadership of Cesare Lombroso (1835–1909).\n\nAnother prominent prison reformer who made important contributions was Alexander Paterson who advocated for the necessity of humanising and socialising methods within the prison system in Great Britain and America.\n\nPrisons are normally surrounded by fencing, walls, earthworks, geographical features, or other barriers to prevent escape. Multiple barriers, concertina wire, electrified fencing, secured and defensible main gates, armed guard towers, security lighting, motion sensors, dogs and roving patrols may all also be present depending on the level of security.\n\nRemotely controlled doors, CCTV monitoring, alarms, cages, restraints, nonlethal and lethal weapons, riot-control gear and physical segregation of units and prisoners may all also be present within a prison to monitor and control the movement and activity of prisoners within the facility.\nModern prison designs have increasingly sought to restrict and control the movement of prisoners throughout the facility and also to allow a smaller prison staff to monitor prisoners directly; often using a decentralized \"podular\" layout. (In comparison, 19th-century prisons had large landings and cell blocks which permitted only intermittent observation of prisoners.) Smaller, separate and self-contained housing units known as \"pods\" or \"modules\" are designed to hold 16 to 50 prisoners and are arranged around exercise yards or support facilities in a decentralized \"campus\" pattern. A small number of prison officers, sometimes a single officer, supervise each pod. The pods contain tiers of cells arranged around a central control station or desk from which a single officer can monitor all the cells and the entire pod, control cell doors and communicate with the rest of the prison.\n\nPods may be designed for high-security \"indirect supervision\", in which officers in segregated and sealed control booths monitor smaller numbers of prisoners confined to their cells. An alternative is \"direct supervision\", in which officers work within the pod and directly interact with and supervise prisoners, who may spend the day outside their cells in a central \"dayroom\" on the floor of the pod. Movement in or out of the pod to and from exercise yards, work assignments or medical appointments can be restricted to individual pods at designated times and is generally centrally controlled. Goods and services, such as meals, laundry, commissary, educational materials, religious services and medical care can increasingly be brought to individual pods or cells as well. Some modern prisons may exclude certain inmates from the general population, usually for safety reasons, such as those within solitary confinement, celebrities, political figures and former law enforcement officers, those convicted of sexual crimes and/or crimes against children, or those on the medical wing or protective custody.\n\nGenerally, when an inmate arrives at a prison, they go through a security classification screening and risk assessment that determines where they will be placed within the prison system. Classifications are assigned by assessing the prisoner's personal history and criminal record, and through subjective determinations made by intake personnel (which include mental health workers, counselors, prison unit managers, and others). This process will have a major impact on the prisoner's experience, determining their security level, educational and work programs, mental health status (e.g. will they be placed in a mental health unit), and many other factors. This sorting of prisoners is one of the fundamental techniques through which the prison administration maintains control over the inmate population. Along with this it creates an orderly and secure prison environment. At some prisons, prisoners are made to wear a prison uniform.\n\nThe levels of security within a prison system are categorized differently around the world, but tend to follow a distinct pattern. At one end of the spectrum are the most secure facilities (\"maximum security\"), which typically hold prisoners that are considered dangerous, disruptive or likely to try to escape. Furthermore, in recent times, supermax prisons have been created where the custody level goes beyond maximum security for people such as terrorists or political prisoners deemed a threat to national security, and inmates from other prisons who have a history of violent or other disruptive behavior in prison or are suspected of gang affiliation. These inmates have individual cells and are kept in lockdown, often for more than 23 hours per day. Meals are served through \"chuck-holes\" in the cell door, and each inmate is allotted one hour of outdoor exercise per day, alone. They are normally permitted no contact with other inmates and are under constant surveillance via closed-circuit television cameras.\nOn the other end are \"minimum security\" prisons which are most often used to house those for whom more stringent security is deemed unnecessary. For example, while white-collar crime rarely results in incarceration—when it does, offenders are almost always sent to minimum-security prisons due to them having committed nonviolent crimes. Lower-security prisons are often designed with less restrictive features, confining prisoners at night in smaller locked dormitories or even cottage or cabin-like housing while permitting them free movement around the grounds to work or activities during the day. Some countries (such as Britain) also have \"open\" prisons where prisoners are allowed home-leave or part-time employment outside of the prison. Suomenlinna Island facility in Finland is an example of one such \"open\" correctional facility. The prison has been open since 1971 and, as of September 2013, the facility's 95 male prisoners leave the prison grounds on a daily basis to work in the corresponding township or commute to the mainland for either work or study. Prisoners can rent flat-screen televisions, sound systems, and mini-refrigerators with the prison-labor wages that they can earn—wages range between 4.10 and €7.30 per hour. With electronic monitoring, prisoners are also allowed to visit their families in Helsinki and eat together with the prison staff. Prisoners in Scandinavian facilities are permitted to wear their own clothes.\n\nModern prisons often hold hundreds or thousands of inmates, and must have facilities onsite to meet most of their needs, including dietary, health, fitness, education, religious practices, entertainment, and many others. Conditions in prisons vary widely around the world, and the types of facilities within prisons depend on many intersecting factors including funding, legal requirements, and cultural beliefs/practices. Nevertheless, in addition to the cell blocks that contain the prisoners, also there are certain auxiliary facilities that are common in prisons throughout the world.\n\nPrisons generally have to provide food for a large number of individuals, and thus are generally equipped with a large institutional kitchen. There are many security considerations, however, that are unique to the prison dining environment. For instance, cutlery equipment must be very carefully monitored and accounted for at all times, and the layout of prison kitchens must be designed in a way that allows staff to observe activity of the kitchen staff (who are usually prisoners). The quality of kitchen equipment varies from prison to prison, depending on when the prison was constructed, and the level of funding available to procure new equipment. Prisoners are often served food in a large cafeteria with rows of tables and benches that are securely attached to the floor. However, inmates that are locked in control units, or prisons that are on \"lockdown\" (where prisoners are made to remain in their cells all day) have trays of food brought to their cells and served through \"chuck-holes\" in the cell door. Prison food in many developed countries is nutritionally adequate for most inmates.\n\nPrisons in wealthy, industrialized nations provide medical care for most of their inmates. Additionally, prison medical staff play a major role in monitoring, organizing, and controlling the prison population through the use of psychiatric evaluations and interventions (psychiatric drugs, isolation in mental health units, etc.). Prison populations are largely from poor minority communities that experience greater rates of chronic illness, substance abuse, and mental illness than the general population. This leads to a high demand for medical services, and in countries such as the US that don't provide tax-payer funded healthcare, prison is often the first place that people are able to receive medical treatment (which they couldn't afford outside).\n\nPrison medical facilities include primary care, mental health services, dental care, substance abuse treatment, and other forms of specialized care, depending on the needs of the inmate population. Health care services in many prisons have long been criticized as inadequate, underfunded, and understaffed, and many prisoners have experienced abuse and mistreatment at the hands of prison medical staff who are entrusted with their care.\n\nIn the United States, a million people who are incarcerated suffer from mental illness without any assistance or treatment for their condition and the tendency of a convicted criminal to reoffend, known as the rate of recidivism, is unusually high for those with the most serious disorders. Analysis of data in 2000 from several forensic hospitals in California, New York and Oregon found that with treatment the rate of recidivism was \"much lower\" than untreated mentally ill offenders.\n\nSome prisons provide educational programs for inmates that can include basic literacy, secondary education, or even college education. Prisoners seek education for a variety of reasons, including the development of skills for after release, personal enrichment and curiosity, finding something to fill their time, or trying to please prison staff (which can often secure early release for good behavior). However, the educational needs of prisoners often come into conflict with the security concerns of prison staff and with a public that wants to be \"tough on crime\" (and thus supports denying prisoners access to education). Whatever their reasons for participating in educational programs, prison populations tend to have very low literacy rates and lack of basic mathematical skills, and many have not completed secondary education. This lack of basic education severely limits their employment opportunities outside of prison, leading to high rates of recidivism, and research has shown that prison education can play a significant role in helping prisoners reorient their lives and become successful after reentry.\n\nMany prisons also provide a library where prisoners can check out books, or do legal research for their cases. Often these libraries are very small, consisting of a few shelves of books. In some countries, such as the United States, drastic budget cuts have resulted in many prison libraries being shut down. Meanwhile, many nations that have historically lacked prison libraries are starting to develop them. Prison libraries can dramatically improve the quality of life for prisoners, who have large amounts of empty time on their hands that can be occupied with reading. This time spent reading has a variety of benefits including improved literacy, ability to understand rules and regulations (leading to improved behavior), ability to read books that encourage self-reflection and analysis of one's emotional state, consciousness of important real-world events, and education that can lead to successful re-entry into society after release.\n\nMany prisons provide limited recreational and fitness facilities for prisoners. The provision of these services is controversial, with certain elements of society claiming that prisons are being \"soft\" on inmates, and others claiming that it is cruel and dehumanizing to confine people for years without any recreational opportunities. The tension between these two opinions, coupled with lack of funding, leads to a large variety of different recreational procedures at different prisons. Prison administrators, however, generally find the provision of recreational opportunities to be useful at maintaining order in the prisons, because it keeps prisoners occupied and provides leverage to gain compliance (by depriving prisoners of recreation as punishment). Examples of common facilities/programs that are available in some prisons are: gyms and weightlifting rooms, arts and crafts, games (such as cards, chess, or bingo), television sets, and sports teams. Additionally, many prisons have an outdoor recreation area, commonly referred to as an \"exercise yard\".\n\nMost prisoners are part of the \"general population\" of the prison, members of which are generally able to socialize with each other in common areas of the prison. A ' or ' (also called a \"block\" or \"isolation cell\") is a highly secure area of the prison, where inmates are placed in solitary confinement to isolate them from the general population. Other prisoners that are often segregated from the general population include those who are in protective custody, or who are on suicide watch, and those whose behavior presents a threat to other prisoners.\n\nIn addition to the above facilities, others that are common include prison factories and workshops, visiting areas, mail rooms, telephone and computer rooms, a prison store (often called a \"canteen\") where prisoners can purchase goods. Some prisons have a death row where prisoners who have been sentenced to death await execution and an execution room, where the death sentence is carried out. In places like Singapore and Malaysia, there is place for corporal punishment]] ( carried out by caning). \n\nPrisons for juveniles are known by a variety of names, including \"youth detention facilities\", \"juvenile detention centers\", and \"reformatories\". The purpose of youth detention facilities is to keep young offenders away from the public, while working towards rehabilitation. The idea of separately treating youthful and adult offenders is a relatively modern idea. The earliest known use of the term \"juvenile delinquency\" was in London in 1816, from where it quickly spread to the United States. The first juvenile correctional institution in the United States opened in 1825 in New York City. By 1917, juvenile courts had been established in all but 3 states. It was estimated that in 2011 more than 95,000 juveniles were locked up in prisons and jails in the United States (the largest youth prisoner population in the world). Besides prisons, many other types of residential placement exist within juvenile justice systems, including youth homes, community-based programs, training schools and boot camps.\n\nLike adult facilities, youth detention centers in some countries are experiencing overcrowding due to large increases in incarceration rates of young offenders. Crowding can create extremely dangerous environments in juvenile detention centers and juvenile correctional facilities. Overcrowding may also lead to the decrease in availability to provide the youth with much needed and promised programs and services while they are in the facility. Many times the administration is not prepared to handle the large number of residents and therefore the facilities can become unstable and create instability in simple logistics.\n\nIn addition to overcrowding, juvenile prisons are questioned for their overall effectiveness in rehabilitating youth. Many critics note high juvenile recidivism rates, and the fact that the most of the youths that are incarcerated are those from lower socio-economic classes (who often suffer from broken families, lack of educational/job opportunities, and violence in their communities).\n\nIn the 19th century, a growing awareness that female prisoners had different needs to male prisoners led to the establishment of dedicated prisons for women. In modern times, it is the norm for female inmates to be housed in either a separate prison or a separate wing of a unisex prison. The aim is to protect them from physical and sexual abuse that would otherwise occur.\n\nIn the Western world, the guards of women's prisons are usually female, though not always. For example, in federal women's correction facilities of the United States, 70% of guards are male. Rape and sexual offences remain commonplace in many women's prisons, and are usually underreported. Two studies in the late 2000s noted that because a high proportion of female inmates have experienced sexual abuse in the past, they are particularly vulnerable to further abuse.\n\nThe needs of mothers during pregnancy and childbirth often conflict with the demands of the prison system. The Rebecca Project, a non-profit organization that campaigns for women's rights issues, reports that \"In 2007, the Bureau of Justice Statistics stated that, on average, 5% of women who enter into state prisons are pregnant and in jails [local prisons] 6% of women are pregnant\". The standard of care that female prisoners receive before and after giving birth is often far worse than the standard expected by the general population, and sometimes almost none is given. In some countries, female prisoners may be restrained while giving birth. In many countries including the United States, mothers will frequently be separated from their baby after giving birth.\n\nPrisons have formed parts of military systems since the French Revolution. France set up its system in 1796. They were modernized in 1852 and since their existence, are used variously to house prisoners of war, unlawful combatants, those whose freedom is deemed a national security risk by military or civilian authorities, and members of the military found guilty of a serious crime. Military prisons in the United States have also been converted to civilian prisons, to include Alcatraz Island. Alcatraz was formerly a military prison for soldiers during the American Civil War.\n\nIn the American Revolution, British prisoners held by the U.S. were assigned to local farmers as laborers. The British kept American sailors in broken down ship hulks with high death rates.\n\nIn the Napoleonic wars, the broken down hulks were still in use for naval prisoners. One French surgeon recalled his captivity in Spain, where scurvy, diarrhea, dysentery, and typhus abounded, and prisoners died by the thousands:\n\nIn the American Civil War, at first prisoners of war were released, after they promised not to fight again unless formally exchanged. When the Confederacy refused to exchange black prisoners the system broke down, and each side built large-scale POW camps. Conditions in terms of housing, food, and medical care were bad in the Confederacy, and the Union retaliated by imposing harsh conditions.\n\nBy 1900 the legal framework of the Geneva and Hague Convention provided considerable protection. In the First World War, millions of prisoners were held on both sides, with no major atrocities. Officers received privileged treatment. There was an increase in the use of forced labor throughout Europe. Food and medical treatment were generally comparable to what active duty soldiers received, and housing was much better than front-line conditions.\n\nPolitical prisoners are people who have been imprisoned because of their political beliefs, activities and affiliations. There is much debate about who qualifies as a \"political prisoner\". The category of \"political prisoner\" is often contested, and many regimes that incarcerate political prisoners often claim that they are merely \"criminals\". Others who are sometimes classified as \"political prisoners\" include prisoners who were politicized in prison, and are subsequently punished for their involvement with political causes.\n\nMany countries maintain or have in the past had a system of prisons specifically intended for political prisoners. In some countries, dissidents can be detained, tortured, executed, and/or \"disappeared\" without trial. This can happen either legally, or extralegally (sometimes by falsely accusing people and fabricating evidence against them).\n\n\"Administrative detention\" is a classification of prisons or detention centers where people are held without trial.\n\nSome psychiatric facilities have characteristics of prisons, particularly when confining patients who have committed a crime and are considered dangerous. In addition, many prisons have psychiatric units dedicated to housing offenders diagnosed with a wide variety of mental disorders. The United States government refers to psychiatric prisons as \"Federal Medical Centers (FMC)\".\n\nSome jurisdictions refer to the prison population (total or per-prison) as the prison muster.\n\nIn 2010, the International Centre for Prison Studies that at least 10.1 million people were imprisoned worldwide.\n\nNot all countries have experienced a rise in prison population; Sweden closed four prisons in 2013 due to a significant drop in the number of inmates. The head of Sweden's prison and probation services characterised the decrease in the number of Swedish prisoners as \"out-of-the-ordinary\", with prison numbers in Sweden falling by around 1% a year since 2004.\n\nIn the United States alone, more than $74 billion per year is spent on prisons, with over 800,000 people employed in the prison industry. As the prison population grows, revenues increase for a variety of small and large businesses that construct facilities, and provide equipment (security systems, furniture, clothing), and services (transportation, communications, healthcare, food) for prisons. These parties have a strong interest in the expansion of the prison system since their development and prosperity directly depends on the number of inmates.<ref name=\"http://www.nij.gov/journals/259/prison-privatization.htm\"></ref>\n\nThe prison industry also includes private businesses that benefit from the exploitation of the prison labor. Some scholars, using the term prison-industrial complex, have argued that the trend of \"hiring out prisoners\" is a continuation of the slavery tradition, pointing out that the Thirteenth Amendment to the United States Constitution freed slaves but allowed forced labor for people convicted of crimes. Prisons are very attractive to employers, because prisoners can be made to perform a great array of jobs, under conditions that most free laborers wouldn't accept (and would be illegal outside of prisons): sub-minimum wage payments, no insurance, no collective bargaining, lack of alternative options, etc. Prison labor can soon deprive the free labor of jobs in a number of sectors, since the organized labor turns out to be uncompetitive compared to the prison counterpart.\n\nPrisons can be difficult places to live and work in, even in developed countries in the present day. By their very definition, prisons house individuals who may be prone to violence and rule-breaking. It is also typical that a high proportion of inmates have mental health concerns. A 2014 US report found that this included 64% of local jail inmates, 54% of state prisoners and 45% of federal prisoners. The environment may be worsened by overcrowding; poor sanitation and maintenance; violence by prisoners against other prisoners or staff; staff misconduct; prison gangs; self-harm; and the widespread smuggling of illegal drugs and other contraband. The social system within the prison commonly develops an \"inmate code\", an informal set of internal values and rules that govern prison life and relationships, but that may be at odds with the interests of prison management or external society, compromising future rehabilitation. In some cases, disorder can escalate into a full-scale prison riot. Academic research has found that poor conditions tend to increase the likelihood of violence within prisons.\n\nPrisoners can face difficulty re-integrating back into society upon their release. They often have difficulty finding work, earn less money when they do find work, and experience a wide range of medical and psychological issues. Many countries have a high recidivism rate. According to the Bureau of Justice Statistics, 67.8% of released prisoners in the United States are rearrested within three years and 76.6% are rearrested within five years. If the prisoner has a family, they are likely to suffer socially and economically from their absence.\n\nIf a society has a very high imprisonment rate, these effects become noticeable not just on family units, but also on entire poor communities. The expensive cost of maintaining a high imprisonment rate also costs money that must come at the expense of either the taxpayer or other government agencies.\n\nA variety of justifications and explanations are put forth for why people are imprisoned by the state. The most common of these are:\n\n\nAcademic studies have been inconclusive as to whether high imprisonment rates reduce crime rates in comparison to low imprisonment rates; only a minority suggest it creates a significant reduction, and others suggest it increases crime.\n\nPrisoners are at risk of being drawn further into crime, as they may become acquainted with other criminals, trained in further criminal activity, exposed to further abuse (both from staff and other prisoners) and left with criminal records that make it difficult to find legal employment after release. All of these things can result in a higher likelihood of reoffending upon release.\n\nThis has resulted in a series of studies that are skeptical towards the idea that prison can rehabilitate offenders. As Morris and Rothman (1995) point out, \"It's hard to train for freedom in a cage.\" A few countries have been able to operate prison systems with a low recidivism rate, including Norway and Sweden. On the other hand, in many countries including the United States, the vast majority of prisoners are rearrested within 3 years of their release. Prison reform organizations such as the Howard League for Penal Reform are not entirely opposed to attempting to rehabilitate offenders, but instead argue that most prisoners would be more likely to be rehabilitated if they received a punishment other than prison.\n\nThe National Institute of Justice argues that offenders can be deterred by the fear of being caught but are unlikely to be deterred by the fear or experience of the punishment. Like Lawrence W. Sherman, they argue that better policing is a more effective way to reduce crime rates.\n\nThe argument that prisons can reduce crime through incapacitation is more widely accepted, even among academics who doubt that prisons can rehabilitate or deter offenders. A dissenting argument from Arrigo and Milovanovic, who argue that prisoners will simply continue to victimize people inside of the prison and that this harm has impacts on the society outside.\n\nModern prison reform movements generally seek to reduce prison populations. A key goal is to improve conditions by reducing overcrowding. Prison reformers also argue that alternative methods are often better at rehabilitating offenders and preventing crime in the long term. Among the countries that have sought to actively reduce prison populations include Sweden, Germany and the Netherlands.\n\nAlternatives to prison sentences include:\nWhen these alternatives are used, actual imprisonment may be used as a punishment for noncompliance.\n\nThe prison abolition movement seeks to eliminate prisons altogether. It is distinct from prison reform, although abolitionists often support reform campaigns, regarding them as incremental steps towards abolishing prisons. The abolition movement is motivated by a belief that prisons are inherently ineffective and discriminatory. The movement is associated with libertarian socialism, anarchism and anti-authoritarianism, with some prison abolitionists arguing that imprisoning people for actions the state designates as crimes is not only inexpedient but also immoral.\n\n\n"}
{"id": "146717", "url": "https://en.wikipedia.org/wiki?curid=146717", "title": "Social work", "text": "Social work\n\nSocial work is an academic discipline and profession that concerns itself with individuals, families, groups and communities in an effort to enhance social functioning and overall well-being. Social functioning is the way in which people perform their social roles, and the structural institutions that are provided to sustain them. Social work applies social sciences, such as sociology, psychology, political science, public health, community development, law, and economics, to engage with client systems, conduct assessments, and develop interventions to solve social and personal problems; and to bring about social change. Social work practice is often divided into micro-work, which involves working directly with individuals or small groups; and macro-work, which involves working with communities, and - within social policy - fostering change on a larger scale.\n\nThe social work industry\ndeveloped in the 19th century, with some of its roots in voluntary philanthropy and in grassroots organizing. However, responses to social needs had existed long before then, primarily from private charities and from religious organizations. The effects of the Industrial Revolution and of the Great Depression of the 1930s placed pressure on social work to become a more defined discipline.\n\nSocial work is a broad profession that intersects with several disciplines. Social work organizations offer the following definitions: “Social work is a practice-based profession and an academic discipline that promotes social change and development, social cohesion, and the empowerment and liberation of people. Principles of social justice, human rights, collective responsibility and respect for diversities are central to social work. Underpinned by theories of social work, social sciences, humanities, and indigenous knowledge, social work\" \"engages people and structures to address life challenges and enhance well-being.\" –International Federation of Social Workers \"Social work is a profession concerned with helping individuals, families, groups and communities to enhance their individual and collective well-being. It aims to help people develop their skills and their ability to use their resources and those of the community to resolve problems. Social work is concerned with individual and personal problems but also with broader social issues such as poverty, unemployment, and domestic violence.\" –Canadian Association of Social Workers Social work practice consists of the professional application of social work values, principles, and techniques to one or more of the following ends: helping people obtain tangible services; counseling and psychotherapy with individuals, families, and groups; helping communities or groups provide or improve social and health services, and participating in legislative processes. The practice of social work requires knowledge of human development and behavior; of social and economic, and cultural institutions; and the interaction of all these factors.\" –National Association of Social Workers\"Social workers work with individuals and families to help improve outcomes in their lives. This may be helping to protect vulnerable people from harm or abuse or supporting people to live independently. Social workers support people, act as advocates and direct people to the services they may require. Social workers often work in multi-disciplinary teams alongside health and education professionals.\" –British Association of Social Workers\n\nThe practice and profession of social work has a relatively modern and scientific origin, and is generally considered to have developed out of three strands. The first was individual casework, a strategy pioneered by the Charity Organization Society in the mid-19th century, which was founded by Helen Bosanquet and Octavia Hill in London, England. Most historians identify COS as the pioneering organization of the social theory that led to the emergence of social work as a professional occupation. COS had its main focus on individual casework. The second was social administration, which included various forms of poverty relief – 'relief of paupers'. Statewide poverty relief could be said to have its roots in the English Poor Laws of the 17th century but was first systematized through the efforts of the Charity Organization Society. The third consisted of social action – rather than engaging in the resolution of immediate individual requirements, the emphasis was placed on political action working through the community and the group to improve their social conditions and thereby alleviate poverty. This approach was developed originally by the Settlement House Movement.\n\nThis was accompanied by a less easily defined movement; the development of institutions to deal with the entire range of social problems. All had their most rapid growth during the nineteenth century, and laid the foundation basis for modern social work, both in theory and in practice.\n\nProfessional social work originated in 19th century England, and had its roots in the social and economic upheaval wrought by the Industrial Revolution, in particular, the societal struggle to deal with the resultant mass urban-based poverty and its related problems. Because poverty was the main focus of early social work, it was intricately linked with the idea of charity work.\n\nOther important historical figures that shaped the growth of the social work profession are Jane Addams, who founded the Hull House in Chicago and won the Nobel Peace Prize in 1931; Mary Ellen Richmond, who wrote Social Diagnosis, one of the first social workbooks to incorporate law, medicine, psychiatry, psychology, and history; and William Beveridge, who created the social welfare state, framing the debate on social work within the context of social welfare provision.\n\nSocial work is an interdisciplinary profession, meaning it draws from a number of areas, such as (but not limited to) psychology, sociology, politics, criminology, economics, ecology, education, health, law, philosophy, anthropology, and counseling, including psychotherapy. Field work is a distinctive attribution to social work pedagogy. This equips the trainee in understanding the theories and models within the field of work. Professional practitioners from multicultural aspects have their roots in this social work immersion engagements from the early 19th century in the western countries. As an example, here are some of the models and theories used within social work practice:\nAbraham Flexner in a 1915 lecture, \"Is Social Work a Profession?\", delivered at the National Conference on Charities and Corrections, examined the characteristics of a profession concerning social work. It is not a 'single model', such as that of health, followed by medical professions such as nurses and doctors, but an integrated profession, and the likeness with medical profession is that social work requires a continued study for professional development to retain knowledge and skills that are evidence-based by practice standards. A social work professional's services lead toward the aim of providing beneficial services to individuals, dyads, families, groups, organizations, and communities to achieve optimum psychosocial functioning.\n\nIts seven core functions are described by Popple and Leighninger as:\n\nSix other core values identified by the National Association of Social Workers' (NASW) Code of Ethics are:\n\n\nA historic and defining feature of social work is the profession's focus on individual well-being in a social context and the well-being of society. Social workers promote social justice and social change with and on behalf of clients. A \"client\" can be an individual, family, group, organization, or community. In the broadening scope of the modern social worker's role, some practitioners have in recent years traveled to war-torn countries to provide psychosocial assistance to families and survivors.\n\nNewer areas of social work practice involve management science. The growth of \"social work administration\" for transforming social policies into services and directing activities of an organization toward achievement of goals is a related field. Helping clients with accessing benefits such as unemployment insurance and disability benefits, to assist individuals and families in building savings and acquiring assets to improve their financial security over the long-term, to manage large operations, etc. requires social workers to know financial management skills to help clients and organization's to be financially self-sufficient.Financial social work also helps clients with low-income or low to middle-income, people who are either unbanked (do not have a banking account) or underbanked (individuals who have a bank account but tend to rely on high cost non-bank providers for their financial transactions), with better mediation with financial institutions and induction of money management skills. Another area that social workers are focusing is risk management, risk in social work is taken as Knight in 1921 defined \"If you don't even know for sure what will happen, but you know the odds, that is risk and If you don't even know the odds, that is uncertainty.\" Risk management in social work means minimizing the risks while increasing potential benefits for clients by analyzing the risks and benefits in the duty of care or decisions.\n\nIn the United States, according to the Substance Abuse and Mental Health Services Administration (SAMHSA), a branch of the U.S. Department of Health and Human Services, professional social workers are the largest group of mental health services providers. There are more clinically trained social workers—over 200,000—than psychiatrists, psychologists, and psychiatric nurses combined. Federal law and the National Institutes of Health recognize social work as one of five core mental health professions.\n\nExamples of fields a social worker may be employed in are poverty relief, life skills education, community organizing, community organization, community development, rural development, forensics and corrections, legislation, industrial relations, project management, child protection, elder protection, women's rights, human rights, systems optimization, finance, addictions rehabilitation, child development, cross-cultural mediation, occupational safety and health, disaster management, mental health, psychosocial therapy, disabilities, etc.\n\nThe education of social workers begins with a bachelor's degree (BA, BSc, BSSW, BSW, etc.) or diploma in social work or a Bachelor of Social Services. Some countries offer postgraduate degrees in social work, such as a master's degree (MSW, MSSW, MSS, MSSA, MA, MSc, MRes, MPhil.) or doctoral studies (Ph.D. and DSW (Doctor of Social Work)). Increasingly, graduates of social work programs pursue post-masters and post-doctoral studies, including training in psychotherapy.\n\nIn the United States, social work undergraduate and master's programs are accredited by the Council on Social Work Education. A CSWE-accredited degree is required for one to become a state-licensed social worker. The CSWE even accredits online master's in social work programs in traditional and advanced standing options. In 1898, the New York Charity Organization Society, which was the Columbia University School of Social Work's earliest entity, began offering formal \"social philanthropy\" courses, marking both the beginning date for social work education in the United States, as well as the launching of professional social work.\n\nSeveral countries and jurisdictions require registration or licensure of people working as social workers, and there are mandated qualifications. In other places, a professional association sets academic requirements for admission to the profession. The success of these professional bodies' efforts is demonstrated in that these same requirements are recognized by employers as necessary for employment.\n\nSocial workers have several professional associations that provide ethical guidance and other forms of support for their members and social work in general. These associations may be international, continental, semi-continental, national, or regional. The main international associations are the International Federation of Social Workers (IFSW) and the International Association of Schools of Social Work (IASSW).\n\nThe largest professional social work association in the United States is the National Association of Social Workers. There also exist organizations that represent clinical social workers such as The American Association of Psychoanalysis in Clinical Social Work. AAPCSW is a national organization representing social workers who practice psychoanalytic social work and psychoanalysis. There are also several states with Clinical Social Work Societies which represent all social workers who conduct psychotherapy from a variety of theoretical frameworks with families, groups, and individuals. The Association for Community Organization and Social Administration (ACOSA) is a professional organization for social workers who practice within the community organizing, policy, and political spheres.\n\nIn the UK, the professional association is the British Association of Social Workers (BASW) with just over 18,000 members (as of August 2015).\n\nThe Code of Ethics of the US-based National Association of Social Workers provides a code for daily conduct and a set of principles rooted in 6 core values: service, social justice, dignity and worth of the person, importance of human relationships, integrity, and competence.\n\nIn the United Kingdom, just over half of social workers are employed by local authorities, and many of these are represented by UNISON, the public sector employee union. Smaller numbers are members of the Unite the Union and the GMB (trade union). The British Union of Social Work Employees (BUSWE) has been a section of the Community (trade union) since 2008.\n\nWhile at that stage, not a union, the British Association of Social Workers operated a professional advice and representation service from the early 1990s. Social Work qualified staff who are also experienced in employment law and industrial relations provide the kind of representation you would expect from a trade union in the event of a grievance, discipline or conduct matters specifically in respect of professional conduct or practice. However, this service depended on the goodwill of employers to allow the representatives to be present at these meetings, as only trade unions have the legal right and entitlement of representation in the workplace.\n\nBy 2011 several councils had realized that they did not have to permit BASW access, and those that were challenged by the skilled professional representation of their staff were withdrawing permission. For this reason BASW once again took up trade union status by forming its arms-length trade union section, SWU (Social Workers Union). This gives the legal right to represent its members whether the employer or Trades Union Congress (TUC) recognizes SWU or not. In 2015 the TUC was still resisting SWU application for admission to congress membership and while most employers are not making formal statements of recognition until the TUC may change its policy, they are all legally required to permit SWU (BASW) representation at internal discipline hearings, etc.\n\nInformation technology is vital in social work, it transforms the documentation part of the work into electronic media. This makes the process transparent, accessible and provides data for analytics. Observation is a tool used in social work for developing solutions. Anabel Quan-Haase in Technology and Society defines the term surveillance as “watching over” (Quan-Haase. 2016. P 213), she continues to explain that the observation of others socially and behaviorally is natural, but it becomes more like surveillance when the purpose of the observation is to keep guard over someone (Quan-Haase. 2016. P 213). Often, at the surface level, the use of surveillance and surveillance technologies within the social work profession is seemingly an unethical invasion of privacy. When engaging with the social work code of ethics a little more deeply, it becomes obvious that the line between ethical and unethical becomes blurred. Within the social work code of ethics, there are multiple mentions of the use of technology within social work practice. The one that seems the most applicable to surveillance or artificial intelligence is 5.02 article f, “When using electronic technology to facilitate evaluation or research” and it goes on to explain that clients should be informed when technology is being used within the practice (Workers. 2008. Article 5.02).\n\nIn 2011, a critic stated that \"novels about social work are rare,\" and as recently as 2004, another critic claimed to have difficulty finding novels featuring a main character holding a Master of Social Work degree.\n\nHowever, social workers have been the subject of many novels, including:\n\n\n"}
{"id": "72156", "url": "https://en.wikipedia.org/wiki?curid=72156", "title": "Human population planning", "text": "Human population planning\n\nHuman reproduction planning is the practice of intentionally controlling the rate of growth of a human population. Historically, human population planning has been implemented with the goal of increasing the rate of human population growth. However, in the period from the 1950s to the 1980s, concerns about global population growth and its effects on poverty, environmental degradation and political stability led to efforts to reduce human population growth rates. More recently, some countries, such as China, Iran, and Spain, have begun efforts to increase their birth rates once again.\nWhile population planning can involve measures that improve people's lives by giving them greater control of their reproduction, a few programs, most notably the Chinese government's \"one-child policy and two-child policy\", have resorted to coercive measures.\n\nFour types of population planning goals pursued by governments can be identified:\n\nWhile a specific population planning practice may be legal/mandated in one country, it may be illegal or restricted in another, indicative of the controversy surrounding this topic.\n\nPopulation planning that is intended to reduce a population or sub-population's growth rates may promote or enforce one or more of the following practices, although there are other methods:\n\n\nThe method(s) chosen can be strongly influenced by the religious and cultural beliefs of community members. The failure of other methods of population planning can lead to the use of abortion or infanticide as solutions.\n\nPopulation policies that are intended to increase a population or subpopulation growth rates may use practices such as:\n\n\nA number of ancient writers have reflected on the issue of population. At about 300 BC, the Indian political philosopher Chanakya (c. 350-283 BC) considered population a source of political, economic, and military strength. Though a given region can house too many or too few people, he considered the latter possibility to be the greater evil. Chanakya favored the remarriage of widows (which at the time was forbidden in India), opposed taxes encouraging emigration, and believed in restricting asceticism to the aged.\n\nIn ancient Greece, Plato (427-347 BC) and Aristotle (384-322 BC) discussed the best population size for Greek city-states such as Sparta, and concluded that cities should be small enough for efficient administration and direct citizen participation in public affairs, but at the same time needed to be large enough to defend themselves against hostile neighbors. In order to maintain a desired population size, the philosophers advised that procreation, and if necessary, immigration, should be encouraged if the population size was too small. Emigration to colonies would be encouraged should the population become too large. Aristotle concluded that a large increase in population would bring, \"certain poverty on the citizenry and poverty is the cause of sedition and evil.\" To halt rapid population increase, Aristotle advocated the use of abortion and the exposure of newborns (that is, infanticide).\n\nConfucius (551-478 BC) and other Chinese writers cautioned that, \"excessive growth may reduce output per worker, repress levels of living for the masses and engender strife.\" Confucius also observed that, \"mortality increases when food supply is insufficient; that premature marriage makes for high infantile mortality rates, that war checks population growth.\"\n\nAncient Rome, especially in the time of Augustus (63 BC-AD 14), needed manpower to acquire and administer the vast Roman Empire. A series of laws were instituted to encourage early marriage and frequent childbirth. Lex Julia (18 BC) and the Lex Papia Poppaea (AD 9) are two well-known examples of such laws, which among others, provided tax breaks and preferential treatment when applying for public office for those that complied with the laws. Severe limitations were imposed on those who did not. For example, the surviving spouse of a childless couple could only inherit one-tenth of the deceased fortune, while the rest was taken by the state. These laws encountered resistance from the population which led to the disregard of their provisions and to their eventual abolition.\n\nTertullian, an early Christian author (ca. AD 160-220), was one of the first to describe famine and war as factors that can prevent overpopulation. He wrote: \"The strongest witness is the vast population of the earth to which we are a burden and she scarcely can provide for our needs; as our demands grow greater, our complaints against Nature's inadequacy are heard by all. The scourges of pestilence, famine, wars, and earthquakes have come to be regarded as a blessing to overcrowded nations since they serve to prune away the luxuriant growth of the human race.\"\n\nIbn Khaldun, a North African Arab polymath (1332–1406), considered population changes to be connected to economic development, linking high birth rates and low death rates to times of economic upswing, and low birth rates and high death rates to economic downswing. Khaldoun concluded that high population density rather than high absolute population numbers were desirable to achieve more efficient division of labour and cheap administration.\n\nDuring the Middle Ages in Christian Europe, population issues were rarely discussed in isolation. Attitudes were generally pro-natalist in line with the Biblical command, \"Be ye fruitful and multiply.\"\n\nEuropean cities grew more rapidly than before, and throughout the 16th century and early 17th century discussions on the advantages and disadvantages of population growth were frequent. Niccolò Machiavelli, an Italian Renaissance political philosopher, wrote, \"When every province of the world so teems with inhabitants that they can neither subsist where they are nor remove themselves elsewhere... the world will purge itself in one or another of these three ways,\" listing floods, plague and famine. Martin Luther concluded, \"God makes children. He is also going to feed them.\"\n\nJean Bodin, a French jurist and political philosopher (1530–1596), argued that larger populations meant more production and more exports, increasing the wealth of a country. Giovanni Botero, an Italian priest and diplomat (1540–1617), emphasized that, \"the greatness of a city rests on the multitude of its inhabitants and their power,\" but pointed out that a population cannot increase beyond its food supply. If this limit was approached, late marriage, emigration, and the war would serve to restore the balance.\n\nRichard Hakluyt, an English writer (1527–1616), observed that, \"Through our longe peace and seldom sickness... we are grown more populous than ever heretofore;... many thousands of idle persons are within this realme, which, having no way to be sett on work, be either mutinous and seek alteration in the state, or at least very burdensome to the commonwealth.\" Hakluyt believed that this led to crime and full jails and in \"A Discourse on Western Planting\" (1584), Hakluyt advocated for the emigration of the surplus population. With the onset of the Thirty Years' War (1618–48), characterized by widespread devastation and deaths brought on by hunger and disease in Europe, concerns about depopulation returned.\n\nIn the 20th century, population planning proponents have drawn from the insights of Thomas Malthus, a British clergyman and economist who published \"An Essay on the Principle of Population\" in 1798. Malthus argued that, \"Population, when unchecked, increases in a geometrical ratio. Subsistence only increases in an arithmetical ratio.\" He also outlined the idea of \"positive checks\" and \"preventative checks.\" \"Positive checks\", such as diseases, wars, disasters, famines, and genocides are factors which Malthus believed could increase the death rate.\n\"Preventative checks\" were factors which Malthus believed could affect the birth rate such as moral restraint, abstinence and birth control. He predicted that \"positive checks\" on exponential population growth would ultimately save humanity from itself and he also believed that human misery was an \"absolute necessary consequence.\" Malthus went on to explain why he believed that this misery affected the poor in a disproportionate manner.\n\nFinally, Malthus advocated for the education of the lower class about the use of \"moral restraint\" or voluntary abstinence, which he believed would slow the growth rate.\n\nPaul R. Ehrlich, a US biologist and environmentalist, published \"The Population Bomb\" in 1968, advocating stringent population planning policies. His central argument on population is as follows:\n\nIn his concluding chapter, Ehrlich offered a partial solution to the \"population problem,\"\n\"[We need] compulsory birth regulation... [through] the addition of temporary sterilants to water supplies or staple food. Doses of the antidote would be carefully rationed by the government to produce the desired family size\".\n\nEhrlich's views came to be accepted by many population planning advocates in the United States and Europe in the 1960s and 1970s. Since Ehrlich introduced his idea of the \"population bomb,\" overpopulation has been blamed for a variety of issues, including increasing poverty, high unemployment rates, environmental degradation, famine and genocide. In a 2004 interview, Ehrlich reviewed the predictions in his book and found that while the specific dates within his predictions may have been wrong, his predictions about climate change and disease were valid. Ehrlich continued to advocate for population planning and co-authored the book \"The Population Explosion\", released in 1990 with his wife Anne Ehrlich.\n\nHowever, it is controversial as to whether human population stabilization will avert environmental risks.\n\nPaige Whaley Eager argues that the shift in perception that occurred in the 1960s must be understood in the context of the demographic changes that took place at the time. It was only in the first decade of the 19th century that the world's population reached one billion. The second billion was added in the 1930s, and the next billion in the 1960s. 90 percent of this net increase occurred in developing countries. Eager also argues that, at the time, the United States recognised that these demographic changes could significantly affect global geopolitics. Large increases occurred in China, Mexico and Nigeria, and demographers warned of a \"population explosion,\" particularly in developing countries from the mid-1950s onwards.\n\nIn the 1980s, tension grew between population planning advocates and women's health activists who advanced women's reproductive rights as part of a human rights-based approach. Growing opposition to the narrow population planning focus led to a significant change in population planning policies in the early 1990s.\n\nOpinions vary among economists about the effects of population change on a nation's economic health. US scientific research in 2009 concluded that the raising of a child cost about $16,000 yearly ($291,570 total for raising the child to its 18th birthday). In the US, the multiplication of this number with the yearly population growth will yield the overall cost of the population growth. Costs for other developed countries are usually of a similar order of magnitude.\n\nSome economists, such as Thomas Sowell and Walter E. Williams, have argued that poverty and famine are caused by bad government and bad economic policies, not by overpopulation.\n\nIn his book \"The Ultimate Resource\", economist Julian Simon argued that higher population density leads to more specialization and technological innovation, which in turn leads to a higher standard of living. He claimed that human beings are the ultimate resource since we possess \"productive and inventive minds that help find creative solutions to man’s problems, thus leaving us better off over the long run\". He also claimed that, \"Our species is better off in just about every measurable material way.\" \n\nSimon also claimed that when considering a list of countries ranked in order by population density, there is no correlation between population density and poverty and starvation. Instead, if a list of countries is considered according to corruption within their respective governments, there is a significant correlation between government corruption, poverty and famine.\n\nAs early as 1798, Thomas Malthus argued in his Essay on the Principle of Population for implementation of population planning. Around the year 1900, Sir Francis Galton said in his publication \"Hereditary Improvement\": \"The unfit could become enemies to the State if they continue to propagate.\" In 1968, Paul Ehrlich noted in \"The Population Bomb\", \"We must cut the cancer of population growth\", and \"if this was not done, there would be only one other solution, namely the 'death rate solution' in which we raise the death rate through war-famine-pestilence, etc.”\n\nIn the same year, another prominent modern advocate for mandatory population planning was Garrett Hardin, who proposed in his landmark 1968 essay \"Tragedy of the commons\", society must relinquish the \"freedom to breed\" through \"mutual coercion, mutually agreed upon.\" Later on, in 1972, he reaffirmed his support in his new essay \"Exploring New Ethics for Survival\", by stating, \" We are breeding ourselves into oblivion.\" Many prominent personalities, such as Bertrand Russell, Margaret Sanger (1939), John D. Rockefeller, Frederick Osborn (1952), Isaac Asimov, Arne Næss and Jacques Cousteau have also advocated for population planning.\nToday, a number of influential people advocate population planning such as these:\n\n\nThe head of the UN Millennium Project Jeffrey Sachs is also a strong proponent of decreasing the effects of overpopulation. In 2007, Jeffrey Sachs gave a number of lectures (2007 Reith Lectures) about population planning and overpopulation. In his lectures, called \"Bursting at the Seams\", he featured an integrated approach that would deal with a number of problems associated with overpopulation and poverty reduction. For example, when criticized for advocating mosquito nets he argued that child survival was, \"by far one of the most powerful ways,\" to achieve fertility reduction, as this would assure poor families that the smaller number of children they had would survive.\n\nThe Roman Catholic Church has opposed abortion, sterilization, and artificial contraception as a general practice but especially in regard to population planning policies. Pope Benedict XVI has stated, \"The extermination of millions of unborn children, in the name of the fight against poverty, actually constitutes the destruction of the poorest of all human beings.\" The reformed Theology pastor Dr. Stephen Tong also opposes the planning of human population.\n\nThe Nation has criticised some white Quiverfull families for having large families motivated by demographic change and worries about \"race suicide\".\n\nIn 1946, Poland introduced a , discontinued in the 1970s, as part of natalist policies in the Communist government. From 1941 to the 1990s, the Soviet Union had a similar tax to replenish the population losses incurred during the Second World War.\n\nThe Socialist Republic of Romania under Nicolae Ceaușescu severely repressed abortion, (the most common birth control method at the time) in 1966, and forced gynecological revisions and penalties for unmarried women and childless couples.\nThe surge of the birth rate taxed the public services received by the \"decreţei 770\" (\"Scions of the Decree 770\") generation. A consequence of Ceaușescu's natalist policy is that large numbers of children ended up living in orphanages, because their parents could not cope. The vast majority of children who lived in the communist orphanages were not actually orphans, but were simply children whose parents could not afford to raise them. The Romanian Revolution of 1989 preceded a fall in population growth.\n\nNativity in the Western world dropped during the interwar period. Swedish sociologists Alva and Gunnar Myrdal published Crisis in the Population Question in 1934, suggesting an extensive welfare state with universal healthcare and childcare, to increase overall Swedish birth rates, and level the number of children at a reproductive level for all social classes in Sweden. Swedish fertility rose throughout World War II (as Sweden was largely unharmed by the war) and peaked in 1946.\n\nAustralia currently offers fortnightly Family Tax Benefit payments plus a free immunization scheme, and recently proposed to pay all child care costs for women who want to work.\n\nThe most significant population planning system in the world was China's one-child policy, in which, with various exceptions, having more than one child was discouraged. Unauthorized births were punished by fines, although there were also allegations of illegal forced abortions and forced sterilization. As part of China's planned birth policy, (work) unit supervisors monitored the fertility of married women and may decide whose turn it is to have a baby.\n\nThe Chinese government introduced the policy in 1978 to alleviate the social and environmental problems of China. According to government officials, the policy has helped prevent 400 million births. The success of the policy has been questioned, and reduction in fertility has also been attributed to the modernization of China. The policy is controversial both within and outside of China because of its manner of implementation and because of concerns about negative economic and social consequences e.g. female infanticide. In oriental cultures, the oldest male child has responsibility of caring for the parents in their old age. Therefore, it is common for oriental families to invest most heavily in the oldest male child, such as providing college, steering them into the most lucrative careers, and so on. To these families, having an oldest male child is paramount, so in a one-child policy, a daughter has no economic benefit, so daughters, especially as a first child, is often targeted for abortion or infanticide. China introduced several government reforms to increase retirement payments to coincide with the one-child policy. During that time, couples could request permission to have more than one child.\n\nAccording to Tibetologist Melvyn Goldstein, natalist feelings run high in China's Tibet Autonomous Region, among both ordinary people and government officials. Seeing population control \"as a matter of power and ethnic survival\" rather than in terms of ecological sustainability, Tibetans successfully argued for an exemption of Tibetan people from the usual family planning policies in China such as the one-child policy.\n\nIn November 2014, the Chinese government allowed its people to conceive a second child under the supervision of government regulation.\n\nOn October 29, 2015, the ruling Chinese Communist Party announced that all one-child policies would be scrapped, allowing all couples to have two children. The change was needed to allow a better balance of male and female children, and to grow the young population to ease the problem of paying for the aging population. Two-child policy begin from January 1, 2016 and one-child policy abolished follow the beginning of two-child policy.\n\nThe Second Orbán Government made saving the nation from the demographic abyss a key aspect and therefore has introduced generous breaks for large families and greatly increased social benefits for all families. Those with three or more children pay virtually no taxes. In just a couple years, Hungary went from being one of the countries that spend the least on families in the OECD to being one of those that do so the most. In 2015, it was almost 4% of GDP.\n\nOnly those with two or fewer children are eligible for election to a gram panchayat, or local government.\n\n\"Us two, our two\" (\"Hum do, hamare do\" in Hindi) is a slogan meaning \"one family, two children\" and is intended to reinforce the message of family planning thereby aiding population planning.\n\nFacilities offered by government to its employees are limited to two children. The government offers incentives for families accepted for sterilization. Moreover, India was the first country to take measures for family planning back in 1952.\n\nAfter the Iran–Iraq War, Iran encouraged married couples to produce as many children as possible to replace population lost to the war.\n\nIran succeeded in sharply reducing its birth rate from the late 1980s to 2010. Mandatory contraceptive courses are required for both males and females before a marriage license can be obtained, and the government emphasized the benefits of smaller families and the use of contraception. This changed in 2012, when a major policy shift back towards increasing birth rates and against population planning was announced. In 2014, permanent contraception and advertising of birth control were to be outlawed.\n\nIn Israel, Haredi families with many children receive economic support through generous governmental child allowances, government assistance in housing young religious couples, as well as specific funds by their own community institutions. Haredi women have an average of 6.7 children while the average Jewish Israeli woman has 3 children.\n\nJapan has experienced a shrinking population for many years. The government is trying to encourage women to have children or to have more children – many Japanese women do not have children, or even remain single. The population is culturally opposed to immigration.\n\nSome Japanese localities, facing significant population loss, are offering economic incentives. Yamatsuri, a town of 7 000 just north of Tokyo, offers parents $4 600 for the birth of a child and $460 a year for 10 years.\n\nIn Myanmar, the Population planning Health Care Bill requires some parents to space each child three years apart. The measure is expected to be used against the persecuted Muslim Rohingyas minority.\n\nRussian President Vladimir Putin directed Parliament in 2006 to adopt a 10-year program to stop the sharp decline in Russia's population, principally by offering financial incentives and subsidies to encourage women to have children.\n\nSingapore has undergone two major phases in its population planning: first to slow and reverse the baby boom in the Post-World War II era; then from the 1980s onwards to encourage couples to have more children as the birth rate had fallen below the replacement-level fertility. In addition, during the interim period, eugenics policies were adopted.\n\nThe anti-natalist policies flourished in the 1960s and 1970s: initiatives advocating small families were launched and developed into the \"Stop at Two\" programme, pushing for two-children families and promoting sterilisation. In 1984, the government announced the \"Graduate Mothers' Scheme\", which favoured children of more well-educated mothers; the policy was however soon abandoned due to the outcry in the general election of the same year.\nEventually, the government became pro-natalist in the late 1980s, marked by its \"Have Three or More\" plan in 1987. Singapore pays $3,000 for the first child, $9,000 in cash and savings for the second; and up to $18,000 each for the third and fourth.\n\nIn 2017, the government of Spain appointed Edelmira Barreira, as \"minister for sex\", in a pro-natalist attempt to reverse a \"negative\" population growth rate.\n\nIn May 2012, Turkey's Prime Minister Recep Tayyip Erdogan argued that abortion is murder and announced that legislative preparations to severely limit the practice are underway. Erdogan also argued that abortion and C-section deliveries are plots to stall Turkey's economic growth. Prior to this move, Erdogan had repeatedly demanded that each couple have at least three children.\n\nEnacted in 1970, Title X of the Public Health Service Act provides access to contraceptive services, supplies and information to those in need. Priority for services is given to people with low incomes. The Title X Family Planning program is administered through the Office of Population Affairs under the Office of Public Health and Science.\nIt is directed by the Office of Family Planning. In 2007, Congress appropriated roughly $283 million for family planning under Title X, at least 90 percent of which was used for services in family planning clinics. Title X is a vital source of funding for family planning clinics throughout the nation, which provide reproductive health care, including abortion.\n\nThe education and services supplied by the Title X-funded clinics support young individuals and low-income families. The goals of developing healthy families are accomplished by helping individuals and couples decide whether to have children and when the appropriate time to do so would be.\n\nTitle X has made the prevention of unintended pregnancies possible. It has allowed millions of American women to receive necessary reproductive health care, plan their pregnancies and prevent abortions. Title X is dedicated exclusively to funding family planning and reproductive health care services.\n\nTitle X as a percentage of total public funding to family planning client services has steadily declined from 44% of total expenditures in 1980 to 12% in 2006. Medicaid has increased from 20% to 71% in the same time. In 2006, Medicaid contributed $1.3 billion to public family planning.\n\nIn a 2004 editorial in \"The New York Times\", David Brooks expressed the opinion that the relatively high birthrate of the United States in comparison to Europe could be attributed to social groups with \"natalist\" attitudes. The article is referred to in an analysis of the Quiverfull movement. However, the figures identified for the demographic are extremely low.\n\nFormer US Senator Rick Santorum made natalism part of his platform for his 2012 presidential campaign. This is not an isolated case. Many of those categorized in the General Social Survey as \"Fundamentalist Protestant\" are more or less natalist, and have a higher birth rate than \"Moderate\" and \"Liberal\" Protestants. However, Rick Santorum is not a Protestant but a practicing Catholic.\n\nIt is reported that Uzbekistan has been pursuing a policy of forced sterilizations, hysterectomies and IUD insertions since the late 1990s in order to impose population planning.\n\n\n\n\n"}
{"id": "765", "url": "https://en.wikipedia.org/wiki?curid=765", "title": "Abortion", "text": "Abortion\n\nAbortion is the ending of a pregnancy by removal or expulsion of an embryo or fetus before it can survive outside the uterus. An abortion that occurs without intervention is known as a miscarriage or spontaneous abortion. When deliberate steps are taken to end a pregnancy, it is called an induced abortion, or less frequently \"induced miscarriage\". The unmodified word \"abortion\" generally refers to an induced abortion. A similar procedure after the fetus has potential to survive outside the womb is known as a \"late termination of pregnancy\" or less accurately as a \"late term abortion\".\nWhen properly done, abortion is one of the safest procedures in medicine, but unsafe abortion is a major cause of maternal death, especially in the developing world. Making safe abortion legal and accessible reduces maternal deaths. It is safer than childbirth, which has a 14 times higher risk of death in the United States. Modern methods use medication or surgery for abortions. The drug mifepristone in combination with prostaglandin appears to be as safe and effective as surgery during the first and second trimester of pregnancy. The most common surgical technique involves dilating the cervix and using a suction device. Birth control, such as the pill or intrauterine devices, can be used immediately following abortion. When performed legally and safely on a woman who desires it, induced abortions do not increase the risk of long-term mental or physical problems. In contrast, unsafe abortions (those performed by unskilled individuals, with hazardous equipment, or in unsanitary facilities) cause 47,000 deaths and 5 million hospital admissions each year. The World Health Organization recommends safe and legal abortions be available to all women.\nAround 56 million abortions are performed each year in the world, with about 45% done unsafely. Abortion rates changed little between 2003 and 2008, before which they decreased for at least two decades as access to family planning and birth control increased. , 40% of the world's women had access to legal abortions without limits as to reason. Countries that permit abortions have different limits on how late in pregnancy abortion is allowed.\nHistorically, abortions have been attempted using herbal medicines, sharp tools, forceful massage, or through other traditional methods. Abortion laws and cultural or religious views of abortions are different around the world. In some areas abortion is legal only in specific cases such as rape, problems with the fetus, poverty, risk to a woman's health, or incest. There is debate over the moral, ethical, and legal issues of abortion. Those who oppose abortion often argue that an embryo or fetus is a human with a right to life, and they may compare abortion to murder. Those who support the legality of abortion often hold that it is part of a woman's right to make decisions about her own body. Others favor legal and accessible abortion as a public health measure.\n\nAn induced abortion may be classified as \"therapeutic\" (done in response to a health condition of the women or fetus) or \"elective\" (chosen for other reasons).\n\nApproximately 205 million pregnancies occur each year worldwide. Over a third are unintended and about a fifth end in induced abortion. Most abortions result from unintended pregnancies. In the United Kingdom, 1 to 2% of abortions are done due to genetic problems in the fetus. A pregnancy can be intentionally aborted in several ways. The manner selected often depends upon the gestational age of the embryo or fetus, which increases in size as the pregnancy progresses. Specific procedures may also be selected due to legality, regional availability, and doctor or a woman's personal preference.\n\nReasons for procuring induced abortions are typically characterized as either therapeutic or elective. An abortion is medically referred to as a therapeutic abortion when it is performed to save the life of the pregnant woman; to prevent harm to the woman's physical or mental health; to terminate a pregnancy where indications are that the child will have a significantly increased chance of mortality or morbidity; or to selectively reduce the number of fetuses to lessen health risks associated with multiple pregnancy. An abortion is referred to as an elective or voluntary abortion when it is performed at the request of the woman for non-medical reasons. Confusion sometimes arises over the term \"elective\" because \"elective surgery\" generally refers to all scheduled surgery, whether medically necessary or not.\n\nMiscarriage, also known as spontaneous abortion, is the unintentional expulsion of an embryo or fetus before the 24th week of gestation. A pregnancy that ends before 37 weeks of gestation resulting in a live-born infant is a \"premature birth\" or a \"preterm birth\". When a fetus dies in utero after viability, or during delivery, it is usually termed \"stillborn\". Premature births and stillbirths are generally not considered to be miscarriages although usage of these terms can sometimes overlap.\n\nOnly 30% to 50% of conceptions progress past the first trimester. The vast majority of those that do not progress are lost before the woman is aware of the conception, and many pregnancies are lost before medical practitioners can detect an embryo. Between 15% and 30% of known pregnancies end in clinically apparent miscarriage, depending upon the age and health of the pregnant woman. 80% of these spontaneous abortions happen in the first trimester.\n\nThe most common cause of spontaneous abortion during the first trimester is chromosomal abnormalities of the embryo or fetus, accounting for at least 50% of sampled early pregnancy losses. Other causes include vascular disease (such as lupus), diabetes, other hormonal problems, infection, and abnormalities of the uterus. Advancing maternal age and a woman's history of previous spontaneous abortions are the two leading factors associated with a greater risk of spontaneous abortion. A spontaneous abortion can also be caused by accidental trauma; intentional trauma or stress to cause miscarriage is considered induced abortion or feticide.\n\nMedical abortions are those induced by abortifacient pharmaceuticals. Medical abortion became an alternative method of abortion with the availability of prostaglandin analogs in the 1970s and the antiprogestogen mifepristone (also known as RU-486) in the 1980s.\n\nThe most common early first-trimester medical abortion regimens use mifepristone in combination with misoprostol (or sometimes another prostaglandin analog, gemeprost) up to 10 weeks (70 days) gestational age, methotrexate in combination with a prostaglandin analog up to 7 weeks gestation, or a prostaglandin analog alone. Mifepristone–misoprostol combination regimens work faster and are more effective at later gestational ages than methotrexate–misoprostol combination regimens, and combination regimens are more effective than misoprostol alone. This regime is effective in the second trimester. Medical abortion regiments involving mifepristone followed by misoprostol in the cheek between 24 and 48 hours later are effective when performed before 70 days' gestation.\n\nIn very early abortions, up to 7 weeks gestation, medical abortion using a mifepristone–misoprostol combination regimen is considered to be more effective than surgical abortion (vacuum aspiration), especially when clinical practice does not include detailed inspection of aspirated tissue. Early medical abortion regimens using mifepristone, followed 24–48 hours later by buccal or vaginal misoprostol are 98% effective up to 9 weeks gestational age; from 9 to 10 weeks efficacy decreases modestly to 94%. If medical abortion fails, surgical abortion must be used to complete the procedure.\n\nEarly medical abortions account for the majority of abortions before 9 weeks gestation in Britain, France, Switzerland, and the Nordic countries. In the United States, the percentage of early medical abortions performed in non-hospital facilities is 31% .\n\nMedical abortion regimens using mifepristone in combination with a prostaglandin analog are the most common methods used for second-trimester abortions in Canada, most of Europe, China and India, in contrast to the United States where 96% of second-trimester abortions are performed surgically by dilation and evacuation.\n\nUp to 15 weeks' gestation, suction-aspiration or vacuum aspiration are the most common surgical methods of induced abortion. \"Manual vacuum aspiration\" (MVA) consists of removing the fetus or embryo, placenta, and membranes by suction using a manual syringe, while \"electric vacuum aspiration\" (EVA) uses an electric pump. These techniques can both be used very early in pregnancy. MVA can be used up to 14 weeks but is more often used earlier in the U.S. EVA can be used later.\n\nMVA, also known as \"mini-suction\" and \"menstrual extraction\" or EVA can be used in very early pregnancy when cervical dilation may not be required. Dilation and curettage (D&C) refers to opening the cervix (dilation) and removing tissue (curettage) via suction or sharp instruments. D&C is a standard gynecological procedure performed for a variety of reasons, including examination of the uterine lining for possible malignancy, investigation of abnormal bleeding, and abortion. The World Health Organization recommends \"sharp curettage\" only when suction aspiration is unavailable.\n\nDilation and evacuation (D&E), used after 12 to 16 weeks, consists of opening the cervix and emptying the uterus using surgical instruments and suction. D&E is performed vaginally and does not require an incision. Intact dilation and extraction(D&X) refers to a variant of D&E sometimes used after 18 to 20 weeks when removal of an intact fetus improves surgical safety or for other reasons.\n\nAbortion may also be performed surgically by hysterotomy or gravid hysterectomy. Hysterotomy abortion is a procedure similar to a caesarean section and is performed under general anesthesia. It requires a smaller incision than a caesarean section and can be used during later stages of pregnancy. Gravid hysterectomy refers to removal of the whole uterus while still containing the pregnancy. Hysterotomy and hysterectomy are associated with much higher rates of maternal morbidity and mortality than D&E or induction abortion.\n\nFirst-trimester procedures can generally be performed using local anesthesia, while second-trimester methods may require deep sedation or general anesthesia.\n\nIn places lacking the necessary medical skill for dilation and extraction, or where preferred by practitioners, an abortion can be induced by first inducing labor and then inducing fetal demise if necessary. This is sometimes called \"induced miscarriage\". This procedure may be performed from 13 weeks gestation to the third trimester. Although it is very uncommon in the United States, more than 80% of induced abortions throughout the second trimester are labor-induced abortions in Sweden and other nearby countries.\n\nOnly limited data are available comparing this method with dilation and extraction. Unlike D&E, labor-induced abortions after 18 weeks may be complicated by the occurrence of brief fetal survival, which may be legally characterized as live birth. For this reason, labor-induced abortion is legally risky in the United States.\n\nHistorically, a number of herbs reputed to possess abortifacient properties have been used in folk medicine. Among these are: tansy, pennyroyal, black cohosh, and the now-extinct silphium.\n\nIn 1978 one woman in Colorado died and another was seriously injured when they attempted to procure an abortion by taking pennyroyal oil.\nBecause the indiscriminant use of herbs as abortifacients can cause serious—even lethal—side effects, such as multiple organ failure, such use is not recommended by physicians.\n\nAbortion is sometimes attempted by causing trauma to the abdomen. The degree of force, if severe, can cause serious internal injuries without necessarily succeeding in inducing miscarriage. In Southeast Asia, there is an ancient tradition of attempting abortion through forceful abdominal massage. One of the bas reliefs decorating the temple of Angkor Wat in Cambodia depicts a demon performing such an abortion upon a woman who has been sent to the underworld.\n\nReported methods of unsafe, self-induced abortion include misuse of misoprostol and insertion of non-surgical implements such as knitting needles and clothes hangers into the uterus. These and other methods to terminate pregnancy may be called \"induced miscarriage\". Such methods are rarely used in countries where surgical abortion is legal and available.\nThe health risks of abortion depend principally upon whether the procedure is performed safely or unsafely. The World Health Organization defines unsafe abortions as those performed by unskilled individuals, with hazardous equipment, or in unsanitary facilities. Legal abortions performed in the developed world are among the safest procedures in medicine. In the United States as of 2012, abortion was estimated to be about 14 times safer for women than childbirth. CDC estimated in 2019 that US pregnancy-related mortality was 17.2 maternal deaths per 100,000 live births, while the US abortion mortality rate is 0.7 maternal deaths per 100,000 procedures. In the UK, guidelines of the Royal College of Obstetricians and Gynaecologists state that \"Women should be advised that abortion is generally safer than continuing a pregnancy to term.\" Worldwide, on average, abortion is safer than carrying a pregnancy to term. A 2007 study reported that \"26% of all pregnancies worldwide are terminated by induced abortion,\" whereas \"deaths from improperly performed [abortion] procedures constitute 13% of maternal mortality globally.\" In Indonesia in 2000 it was estimated that 2 million pregnancies ended in abortion, 4.5 million pregnancies were carried to term, and 14-16 percent of maternal deaths resulted from abortion.\n\nIn the US from 2000 to 2009, abortion had a lower mortality rate than plastic surgery, and a similar or lower mortality rate than running a marathon. Five years after seeking abortion services, women who gave birth after being denied an abortion reported worse health than women who had either first or second trimester abortions. The risk of abortion-related mortality increases with gestational age, but remains lower than that of childbirth. Outpatient abortion is as safe from 64 to 70 days' gestation as it before 63 days.\n\nThere is little difference in terms of safety and efficacy between medical abortion using a combined regimen of mifepristone and misoprostol and surgical abortion (vacuum aspiration) in early first trimester abortions up to 10 weeks gestation. Medical abortion using the prostaglandin analog misoprostol alone is less effective and more painful than medical abortion using a combined regimen of mifepristone and misoprostol or surgical abortion.\n\nVacuum aspiration in the first trimester is the safest method of surgical abortion, and can be performed in a primary care office, abortion clinic, or hospital. Complications, which are rare, can include uterine perforation, pelvic infection, and retained products of conception requiring a second procedure to evacuate. Infections account for one-third of abortion-related deaths in the United States. The rate of complications of vacuum aspiration abortion in the first trimester is similar regardless of whether the procedure is performed in a hospital, surgical center, or office. Preventive antibiotics (such as doxycycline or metronidazole) are typically given before abortion procedures, as they are believed to substantially reduce the risk of postoperative uterine infection; however, antibiotics are not routinely given with abortion pills. The rate of failed procedures does not appear to vary significantly depending on whether the abortion is performed by a doctor or a mid-level practitioner. Complications after second-trimester abortion are similar to those after first-trimester abortion, and depend somewhat on the method chosen. Second-trimester abortions are generally well-tolerated.\n\nSome purported risks of abortion are promoted primarily by anti-abortion groups,\nbut lack scientific support. For example, the question of a link between induced abortion and breast cancer has been investigated extensively. Major medical and scientific bodies (including the World Health Organization, National Cancer Institute, American Cancer Society, Royal College of OBGYN and American Congress of OBGYN) have concluded that abortion does not cause breast cancer.\n\nIn the past even illegality has not automatically meant that the abortions were unsafe. Referring to the U.S., historian Linda Gordon states: \"In fact, illegal abortions in this country have an impressive safety record.\" According to Rickie Solinger,\n\nAuthors Jerome Bates and Edward Zawadzki describe the case of an illegal abortionist in the eastern U.S. in the early 20th century who was proud of having successfully completed 13,844 abortions without any fatality.\nIn 1870s New York City the famous abortionist/midwife Madame Restell (Anna Trow Lohman) appears to have lost very few women among her more than 100,000 patients—a lower mortality rate than the childbirth mortality rate at the time. In 1936 the prominent professor of obstetrics and gynecology Frederick J. Taussig wrote that a cause of increasing mortality during the years of illegality in the U.S. was that\nCurrent evidence finds no relationship between most induced abortions and mental-health problems other than those expected for any unwanted pregnancy. A report by the American Psychological Association concluded that a woman's first abortion is not a threat to mental health when carried out in the first trimester, with such women no more likely to have mental-health problems than those carrying an unwanted pregnancy to term; the mental-health outcome of a woman's second or greater abortion is less certain. Some older reviews concluded that abortion was associated with an increased risk of psychological problems; however, they did not use an appropriate control group.\n\nAlthough some studies show negative mental-health outcomes in women who choose abortions after the first trimester because of fetal abnormalities, more rigorous research would be needed to show this conclusively. Some proposed negative psychological effects of abortion have been referred to by anti-abortion advocates as a separate condition called \"post-abortion syndrome\", but this is not recognized by medical or psychological professionals in the United States.\n\nA long term-study among US women found that about 99% of women felt that they made the right decision five years after they had an abortion. Relief was the primary emotion with few women feeling sadness or guilt. Social stigma was a main factor predicting negative emotions and regret years later.\n\nWomen seeking an abortion may use unsafe methods, especially when abortion is legally restricted. They may attempt self-induced abortion or seek the help of a person without proper medical training or facilities. This can lead to severe complications, such as incomplete abortion, sepsis, hemorrhage, and damage to internal organs.\n\nUnsafe abortions are a major cause of injury and death among women worldwide. Although data are imprecise, it is estimated that approximately 20 million unsafe abortions are performed annually, with 97% taking place in developing countries. Unsafe abortions are believed to result in millions of injuries. Estimates of deaths vary according to methodology, and have ranged from 37,000 to 70,000 in the past decade; deaths from unsafe abortion account for around 13% of all maternal deaths. The World Health Organization believes that mortality has fallen since the 1990s. To reduce the number of unsafe abortions, public health organizations have generally advocated emphasizing the legalization of abortion, training of medical personnel, and ensuring access to reproductive-health services. In response, opponents of abortion point out that abortion bans in no way affect prenatal care for women who choose to carry their fetus to term. The Dublin Declaration on Maternal Health, signed in 2012, notes, \"the prohibition of abortion does not affect, in any way, the availability of optimal care to pregnant women.\"\n\nA major factor in whether abortions are performed safely or not is the legal standing of abortion. Countries with restrictive abortion laws have higher rates of unsafe abortion and similar overall abortion rates compared to those where abortion is legal and available. For example, the 1996 legalization of abortion in South Africa had an immediate positive impact on the frequency of abortion-related complications, with abortion-related deaths dropping by more than 90%. Similar reductions in maternal mortality have been observed after other countries have liberalized their abortion laws, such as Romania and Nepal. A 2011 study concluded that in the United States, some state-level anti-abortion laws are correlated with lower rates of abortion in that state. The analysis, however, did not take into account travel to other states without such laws to obtain an abortion. In addition, a lack of access to effective contraception contributes to unsafe abortion. It has been estimated that the incidence of unsafe abortion could be reduced by up to 75% (from 20 million to 5 million annually) if modern family planning and maternal health services were readily available globally. Rates of such abortions may be difficult to measure because they can be reported variously as miscarriage, \"induced miscarriage\", \"menstrual regulation\", \"mini-abortion\", and \"regulation of a delayed/suspended menstruation\".\n\nForty percent of the world's women are able to access therapeutic and elective abortions within gestational limits, while an additional 35 percent have access to legal abortion if they meet certain physical, mental, or socioeconomic criteria. While maternal mortality seldom results from safe abortions, unsafe abortions result in 70,000 deaths and 5 million disabilities per year. Complications of unsafe abortion account for approximately an eighth of maternal mortalities worldwide, though this varies by region. Secondary infertility caused by an unsafe abortion affects an estimated 24 million women. The rate of unsafe abortions has increased from 44% to 49% between 1995 and 2008. Health education, access to family planning, and improvements in health care during and after abortion have been proposed to address this phenomenon.\n\nIn 2019, a US Senate Bill entitled the \"Born-Alive Abortion Survivors Protection Act\" raised the issue of live birth after abortion. The bill would mandate that medical providers resuscitate neonates delivered showing signs of life during an abortion process. During the debate around this issue, US Republicans falsely alleged that medical providers \"execute\" live-born babies. Existing US laws would punish execution as homicide. Furthermore, US abortion experts refute the claim that a \"born-alive\" fetus is a common event and reject laws that would mandate resuscitation against the wishes of the parents.\n\nOnly 1.3% of abortions occur after 21 weeks of pregnancy in the US. Although it is very uncommon, women undergoing surgical abortion after this gestational age sometimes give birth to a fetus that may survive briefly. The periviable period is considered to be between 20 and 25 weeks gestation. Long-term survival is possible after 22 weeks. However, odds of long-term survival between 22 and 23 weeks are 2–3 percent and odds of survival between 23 and 24 weeks are 20 percent. \"Intact survival\", which means survival of a neonate without subsequent damage to organs such as the brain or bowel is 1% at 22 weeks and 13% at 23 weeks. Survival odds increase with increasing gestational age.\n\nIf medical staff observe signs of life, they may be required to provide care: emergency medical care if the child has a good chance of survival and palliative care if not. Induced fetal demise before termination of pregnancy after 20–21 weeks gestation is recommended by some sources to avoid this and to comply with the US Partial Birth Abortion Ban. Induced fetal demise does not improve the safety of an abortion procedure and may incur risks to the health of the woman having the abortion.\n\nThere are two commonly used methods of measuring the incidence of abortion:\n\nIn many places, where abortion is illegal or carries a heavy social stigma, medical reporting of abortion is not reliable. For this reason, estimates of the incidence of abortion must be made without determining certainty related to standard error.\n\nThe number of abortions performed worldwide seems to have remained stable in recent years, with 41.6 million having been performed in 2003 and 43.8 million having been performed in 2008. The abortion rate worldwide was 28 per 1000 women, though it was 24 per 1000 women for developed countries and 29 per 1000 women for developing countries. The same 2012 study indicated that in 2008, the estimated abortion percentage of known pregnancies was at 21% worldwide, with 26% in developed countries and 20% in developing countries.\n\nOn average, the incidence of abortion is similar in countries with restrictive abortion laws and those with more liberal access to abortion. However, restrictive abortion laws are associated with increases in the percentage of abortions performed unsafely. The unsafe abortion rate in developing countries is partly attributable to lack of access to modern contraceptives; according to the Guttmacher Institute, providing access to contraceptives would result in about 14.5 million fewer unsafe abortions and 38,000 fewer deaths from unsafe abortion annually worldwide.\n\nThe rate of legal, induced abortion varies extensively worldwide. According to the report of employees of Guttmacher Institute it ranged from 7 per 1000 women (Germany and Switzerland) to 30 per 1000 women (Estonia) in countries with complete statistics in 2008. The proportion of pregnancies that ended in induced abortion ranged from about 10% (Israel, the Netherlands and Switzerland) to 30% (Estonia) in the same group, though it might be as high as 36% in Hungary and Romania, whose statistics were deemed incomplete.\n\nAn American study in 2002 concluded that about half of women having abortions were using a form of contraception at the time of becoming pregnant. Inconsistent use was reported by half of those using condoms and three-quarters of those using the birth control pill; 42% of those using condoms reported failure through slipping or breakage. The Guttmacher Institute estimated that \"most abortions in the United States are obtained by minority women\" because minority women \"have much higher rates of unintended pregnancy\".\n\nThe abortion rate may also be expressed as the average number of abortions a woman has during her reproductive years; this is referred to as \"total abortion rate\" (TAR).\n\nAbortion rates also vary depending on the stage of pregnancy and the method practiced. In 2003, the Centers for Disease Control and Prevention (CDC) reported that 26% of reported legal induced abortions in the United States were known to have been obtained at less than 6 weeks' gestation, 18% at 7 weeks, 15% at 8 weeks, 18% at 9 through 10 weeks, 10% at 11 through 12 weeks, 6% at 13 through 15 weeks, 4% at 16 through 20 weeks and 1% at more than 21 weeks. 91% of these were classified as having been done by \"curettage\" (suction-aspiration, dilation and curettage, dilation and evacuation), 8% by \"medical\" means (mifepristone), >1% by \"intrauterine instillation\" (saline or prostaglandin), and 1% by \"other\" (including hysterotomy and hysterectomy). According to the CDC, due to data collection difficulties the data must be viewed as tentative and some fetal deaths reported beyond 20 weeks may be natural deaths erroneously classified as abortions if the removal of the dead fetus is accomplished by the same procedure as an induced abortion.\n\nThe Guttmacher Institute estimated there were 2,200 intact dilation and extraction procedures in the US during 2000; this accounts for <0.2% of the total number of abortions performed that year. Similarly, in England and Wales in 2006, 89% of terminations occurred at or under 12 weeks, 9% between 13 and 19 weeks, and 2% at or over 20 weeks. 64% of those reported were by vacuum aspiration, 6% by D&E, and 30% were medical. There are more second trimester abortions in developing countries such as China, India and Vietnam than in developed countries.\n\nThe reasons why women have abortions are diverse and vary across the world. Some of the reasons may include an inability to afford a child, domestic violence, lack of support, feeling they are too young, and the wish to complete education or advance a career. Additional reasons include not being willing to raise a child conceived as a result of rape or incest.\n\nSome abortions are undergone as the result of societal pressures. These might include the preference for children of a specific sex or race, disapproval of single or early motherhood, stigmatization of people with disabilities, insufficient economic support for families, lack of access to or rejection of contraceptive methods, or efforts toward population control (such as China's one-child policy). These factors can sometimes result in compulsory abortion or sex-selective abortion.\n\nAn additional factor is maternal health which was listed as the main reason by about a third of women in 3 of 27 countries and about 7% of women in a further 7 of these 27 countries.\n\nIn the U.S., the Supreme Court decisions in \"Roe v. Wade\" and \"Doe v. Bolton\": \"ruled that the state's interest in the life of the fetus became compelling only at the point of viability, defined as the point at which the fetus can survive independently of its mother. Even after the point of viability, the state cannot favor the life of the fetus over the life or health of the pregnant woman. Under the right of privacy, physicians must be free to use their \"medical judgment for the preservation of the life or health of the mother.\" On the same day that the Court decided Roe, it also decided Doe v. Bolton, in which the Court defined health very broadly: \"The medical judgment may be exercised in the light of all factors—physical, emotional, psychological, familial, and the woman's age—relevant to the well-being of the patient. All these factors may relate to health. This allows the attending physician the room he needs to make his best medical judgment.\"\n\nPublic opinion shifted in America following television personality Sherri Finkbine's discovery during her fifth month of pregnancy that she had been exposed to thalidomide. Unable to obtain a legal abortion in the United States, she traveled to Sweden. From 1962 to 1965, an outbreak of German measles left 15,000 babies with severe birth defects. In 1967, the American Medical Association publicly supported liberalization of abortion laws. A National Opinion Research Center poll in 1965 showed 73% supported abortion when the mother's life was at risk, 57% when birth defects were present and 59% for pregnancies resulting from rape or incest.\n\nThe rate of cancer during pregnancy is 0.02–1%, and in many cases, cancer of the mother leads to consideration of abortion to protect the life of the mother, or in response to the potential damage that may occur to the fetus during treatment. This is particularly true for cervical cancer, the most common type of which occurs in 1 of every 2,000–13,000 pregnancies, for which initiation of treatment \"cannot co-exist with preservation of fetal life (unless neoadjuvant chemotherapy is chosen)\". Very early stage cervical cancers (I and IIa) may be treated by radical hysterectomy and pelvic lymph node dissection, radiation therapy, or both, while later stages are treated by radiotherapy. Chemotherapy may be used simultaneously. Treatment of breast cancer during pregnancy also involves fetal considerations, because lumpectomy is discouraged in favor of modified radical mastectomy unless late-term pregnancy allows follow-up radiation therapy to be administered after the birth.\n\nExposure to a single chemotherapy drug is estimated to cause a 7.5–17% risk of teratogenic effects on the fetus, with higher risks for multiple drug treatments. Treatment with more than 40 Gy of radiation usually causes spontaneous abortion. Exposure to much lower doses during the first trimester, especially 8 to 15 weeks of development, can cause intellectual disability or microcephaly, and exposure at this or subsequent stages can cause reduced intrauterine growth and birth weight. Exposures above 0.005–0.025 Gy cause a dose-dependent reduction in IQ. It is possible to greatly reduce exposure to radiation with abdominal shielding, depending on how far the area to be irradiated is from the fetus.\n\nThe process of birth itself may also put the mother at risk. \"Vaginal delivery may result in dissemination of neoplastic cells into lymphovascular channels, haemorrhage, cervical laceration and implantation of malignant cells in the episiotomy site, while abdominal delivery may delay the initiation of non-surgical treatment.\"\n\nSince ancient times abortions have been done using a number of methods, including herbal medicines, sharp tools, with force, or through other traditional methods. Induced abortion has long history and can be traced back to civilizations as varied as China under Shennong (c. 2700 BCE), Ancient Egypt with its Ebers Papyrus (c. 1550 BCE), and the Roman Empire in the time of Juvenal (c. 200 CE). One of the earliest known artistic representations of abortion is in a bas relief at Angkor Wat (c. 1150). Found in a series of friezes that represent judgment after death in Hindu and Buddhist culture, it depicts the technique of abdominal abortion.\n\nSome medical scholars and abortion opponents have suggested that the Hippocratic Oath forbade Ancient Greek physicians from performing abortions; other scholars disagree with this interpretation, and state that the medical texts of Hippocratic Corpus contain descriptions of abortive techniques right alongside the Oath. The physician Scribonius Largus wrote in 43 CE that the Hippocratic Oath prohibits abortion, as did Soranus, although apparently not all doctors adhered to it strictly at the time. According to Soranus' 1st or 2nd century CE work \"Gynaecology\", one party of medical practitioners banished all abortives as required by the Hippocratic Oath; the other party—to which he belonged—was willing to prescribe abortions, but only for the sake of the mother's health. Aristotle, in his treatise on government \"Politics\" (350 BCE), condemns infanticide as a means of population control. He preferred abortion in such cases, with the restriction \"<nowiki>[that it]</nowiki> must be practised on it before it has developed sensation and life; for the line between lawful and unlawful abortion will be marked by the fact of having sensation and being alive\". \n\nIn Christianity, Pope Sixtus V (1585–90) was the first Pope before 1869 to declare that abortion is homicide regardless of the stage of pregnancy; and his pronouncement of 1588 was reversed three years later by his successor. Through most of its history the Catholic Church was divided on whether it believed that early abortion was murder, and it did not begin vigorously opposing abortion until the 19th century. Several historians have written that prior to the 19th century most Catholic authors did not regard termination of pregnancy before \"quickening\" or \"ensoulment\" as an abortion. From 1750, excommunication became the punishment for abortions. Statements made in 1992 in the Catechism of the Catholic Church, the codified summary of the Church's teachings, opposed abortion.\n\nA 1995 survey reported that Catholic women are as likely as the general population to terminate a pregnancy, Protestants are less likely to do so, and Evangelical Christians are the least likely to do so. Islamic tradition has traditionally permitted abortion until a point in time when Muslims believe the soul enters the fetus, considered by various theologians to be at conception, 40 days after conception, 120 days after conception, or quickening.<ref name=\"BBC and Islam / Abortion\"></ref> However, abortion is largely heavily restricted or forbidden in areas of high Islamic faith such as the Middle East and North Africa.\n\nIn Europe and North America, abortion techniques advanced starting in the 17th century. However, conservatism by most physicians with regards to sexual matters prevented the wide expansion of safe abortion techniques. Other medical practitioners in addition to some physicians advertised their services, and they were not widely regulated until the 19th century, when the practice (sometimes called \"restellism\") was banned in both the United States and the United Kingdom. Church groups as well as physicians were highly influential in anti-abortion movements. In the US, according to some sources, abortion was more dangerous than childbirth until about 1930 when incremental improvements in abortion procedures relative to childbirth made abortion safer. However, other sources maintain that in the 19th century early abortions under the hygienic conditions in which midwives usually worked were relatively safe.\nIn addition, some commentators have written that, despite improved medical procedures, the period from the 1930s until legalization also saw more zealous enforcement of anti-abortion laws, and concomitantly an increasing control of abortion providers by organized crime.\n\nSoviet Russia (1919), Iceland (1935) and Sweden (1938) were among the first countries to legalize certain or all forms of abortion. In 1935 Nazi Germany, a law was passed permitting abortions for those deemed \"hereditarily ill\", while women considered of German stock were specifically prohibited from having abortions. Beginning in the second half of the twentieth century, abortion was legalized in a greater number of countries.\n\nInduced abortion has long been the source of considerable debate. Ethical, moral, philosophical, biological, religious and legal issues surrounding abortion are related to value systems. Opinions of abortion may be about fetal rights, governmental authority, and women's rights.\n\nIn both public and private debate, arguments presented in favor of or against abortion access focus on either the moral permissibility of an induced abortion, or justification of laws permitting or restricting abortion. The World Medical Association Declaration on Therapeutic Abortion notes, \"circumstances bringing the interests of a mother into conflict with the interests of her unborn child create a dilemma and raise the question as to whether or not the pregnancy should be deliberately terminated.\" Abortion debates, especially pertaining to abortion laws, are often spearheaded by groups advocating one of these two positions. Anti-abortion groups who favor greater legal restrictions on abortion, including complete prohibition, most often describe themselves as \"pro-life\" while abortion rights groups who are against such legal restrictions describe themselves as \"pro-choice\". Generally, the former position argues that a human fetus is a human person with a right to live, making abortion morally the same as murder. The latter position argues that a woman has certain reproductive rights, especially the right to decide whether or not to carry a pregnancy to term.\n\nCurrent laws pertaining to abortion are diverse. Religious, moral, and cultural factors continue to influence abortion laws throughout the world. The right to life, the right to liberty, the right to security of person, and the right to reproductive health are major issues of human rights that sometimes constitute the basis for the existence or absence of abortion laws.\n\nIn jurisdictions where abortion is legal, certain requirements must often be met before a woman may obtain a safe, legal abortion (an abortion performed without the woman's consent is considered feticide). These requirements usually depend on the age of the fetus, often using a trimester-based system to regulate the window of legality, or as in the U.S., on a doctor's evaluation of the fetus' viability. Some jurisdictions require a waiting period before the procedure, prescribe the distribution of information on fetal development, or require that parents be contacted if their minor daughter requests an abortion. Other jurisdictions may require that a woman obtain the consent of the fetus' father before aborting the fetus, that abortion providers inform women of health risks of the procedure—sometimes including \"risks\" not supported by the medical literature—and that multiple medical authorities certify that the abortion is either medically or socially necessary. Many restrictions are waived in emergency situations. China, which has ended their one-child policy, and now has a two child policy, has at times incorporated mandatory abortions as part of their population control strategy.\n\nOther jurisdictions ban abortion almost entirely. Many, but not all, of these allow legal abortions in a variety of circumstances. These circumstances vary based on jurisdiction, but may include whether the pregnancy is a result of rape or incest, the fetus' development is impaired, the woman's physical or mental well-being is endangered, or socioeconomic considerations make childbirth a hardship. In countries where abortion is banned entirely, such as Nicaragua, medical authorities have recorded rises in maternal death directly and indirectly due to pregnancy as well as deaths due to doctors' fears of prosecution if they treat other gynecological emergencies. Some countries, such as Bangladesh, that nominally ban abortion, may also support clinics that perform abortions under the guise of menstrual hygiene. This is also a terminology in traditional medicine. In places where abortion is illegal or carries heavy social stigma, pregnant women may engage in medical tourism and travel to countries where they can terminate their pregnancies. Women without the means to travel can resort to providers of illegal abortions or attempt to perform an abortion by themselves.\n\nThe organization Women on Waves, has been providing education about medical abortions since 1999. The NGO created a mobile medical clinic inside a shipping container, which then travels on rented ships to countries with restrictive abortion laws. Because the ships are registered in the Netherlands, Dutch law prevails when the ship is in international waters. While in port, the organization provides free workshops and education; while in international waters, medical personnel are legally able to prescribe medical abortion drugs and counseling.\n\nSonography and amniocentesis allow parents to determine sex before childbirth. The development of this technology has led to sex-selective abortion, or the termination of a fetus based on sex. The selective termination of a female fetus is most common.\n\nSex-selective abortion is partially responsible for the noticeable disparities between the birth rates of male and female children in some countries. The preference for male children is reported in many areas of Asia, and abortion used to limit female births has been reported in Taiwan, South Korea, India, and China. This deviation from the standard birth rates of males and females occurs despite the fact that the country in question may have officially banned sex-selective abortion or even sex-screening. In China, a historical preference for a male child has been exacerbated by the one-child policy, which was enacted in 1979.\n\nMany countries have taken legislative steps to reduce the incidence of sex-selective abortion. At the International Conference on Population and Development in 1994 over 180 states agreed to eliminate \"all forms of discrimination against the girl child and the root causes of son preference\", conditions also condemned by a PACE resolution in 2011. The World Health Organization and UNICEF, along with other United Nations agencies, have found that measures to reduce access to abortion are much less effective at reducing sex-selective abortions than measures to reduce gender inequality.\n\nIn a number of cases, abortion providers and these facilities have been subjected to various forms of violence, including murder, attempted murder, kidnapping, stalking, assault, arson, and bombing. Anti-abortion violence is classified by both governmental and scholarly sources as terrorism. In the U.S. and Canada, over 8,000 incidents of violence, trespassing, and death threats have been recorded by providers since 1977, including over 200 bombings/arsons and hundreds of assaults. The majority of abortion opponents have not been involved in violent acts.\n\nIn the United States, four physicians who performed abortions have been murdered: David Gunn (1993), John Britton (1994), Barnett Slepian (1998), and George Tiller (2009). Also murdered, in the U.S. and Australia, have been other personnel at abortion clinics, including receptionists and security guards such as James Barrett, Shannon Lowney, Lee Ann Nichols, and Robert Sanderson. Woundings (e.g., Garson Romalis) and attempted murders have also taken place in the United States and Canada. Hundreds of bombings, arsons, acid attacks, invasions, and incidents of vandalism against abortion providers have occurred. Notable perpetrators of anti-abortion violence include Eric Robert Rudolph, Scott Roeder, Shelley Shannon, and Paul Jennings Hill, the first person to be executed in the United States for murdering an abortion provider.\n\nLegal protection of access to abortion has been brought into some countries where abortion is legal. These laws typically seek to protect abortion clinics from obstruction, vandalism, picketing, and other actions, or to protect women and employees of such facilities from threats and harassment.\n\nFar more common than physical violence is psychological pressure. In 2003, Chris Danze organized anti-abortion organizations throughout Texas to prevent the construction of a Planned Parenthood facility in Austin. The organizations released the personal information online, of those involved with construction, sending them up to 1200 phone calls a day and contacting their churches. Some protestors record women entering clinics on camera.\n\nSpontaneous abortion occurs in various animals. For example, in sheep it may be caused by stress or physical exertion, such as crowding through doors or being chased by dogs. In cows, abortion may be caused by contagious disease, such as brucellosis or \"Campylobacter\", but can often be controlled by vaccination. Eating pine needles can also induce abortions in cows.\nSeveral plants, including broomweed, skunk cabbage, poison hemlock, and tree tobacco, are known to cause fetal deformities and abortion in cattle and in sheep and goats. In horses, a fetus may be aborted or resorbed if it has lethal white syndrome (congenital intestinal aganglionosis). Foal embryos that are homozygous for the dominant white gene (WW) are theorized to also be aborted or resorbed before birth. In many species of sharks and rays, stress-induced abortions occur frequently on capture.\n\nViral infection can cause abortion in dogs. Cats can experience spontaneous abortion for many reasons, including hormonal imbalance. A combined abortion and spaying is performed on pregnant cats, especially in Trap-Neuter-Return programs, to prevent unwanted kittens from being born.\nFemale rodents may terminate a pregnancy when exposed to the smell of a male not responsible for the pregnancy, known as the Bruce effect.\n\nAbortion may also be induced in animals, in the context of animal husbandry. For example, abortion may be induced in mares that have been mated improperly, or that have been purchased by owners who did not realize the mares were pregnant, or that are pregnant with twin foals. Feticide can occur in horses and zebras due to male harassment of pregnant mares or forced copulation, although the frequency in the wild has been questioned. Male gray langur monkeys may attack females following male takeover, causing miscarriage.\n\n\n"}
{"id": "3608404", "url": "https://en.wikipedia.org/wiki?curid=3608404", "title": "Corruption", "text": "Corruption\n\nCorruption is a form of dishonesty or criminal offense undertaken by a person or organization entrusted with a position of authority, to acquire illicit benefit or abuse power for one's private gain. Corruption may include many activities including bribery and embezzlement, though it may also involve practices that are legal in many countries. Political corruption occurs when an office-holder or other governmental employee acts in an official capacity for personal gain. Corruption is most commonplace in kleptocracies, oligarchies, narco-states and mafia states.\n\nCorruption can occur on different scales. Corruption ranges from small favors between a small number of people (petty corruption), to corruption that affects the government on a large scale (grand corruption), and corruption that is so prevalent that it is part of the everyday structure of society, including corruption as one of the symptoms of organized crime. Corruption and crime are endemic sociological occurrences which appear with regular frequency in virtually all countries on a global scale in varying degree and proportion. Individual nations each allocate domestic resources for the control and regulation of corruption and crime. Strategies to counter corruption are often summarized under the umbrella term anti-corruption.\n\nStephen D. Morris, a professor of politics, wrote that political corruption is the illegitimate use of public power to benefit a private interest. Economist Ian Senior defined corruption as an action to (a) secretly provide (b) a good or a service to a third party (c) so that he or she can influence certain actions which (d) benefit the corrupt, a third party, or both (e) in which the corrupt agent has authority. \nWorld Bank economist Daniel Kaufmann, extended the concept to include \"legal corruption\" in which power is abused within the confines of the law—as those with power often have the ability to make laws for their protection. The effect of corruption in infrastructure is to increase costs and construction time, lower the quality and decrease the benefit.\n\nCorruption can occur on different scales. Corruption ranges from small favors between a small number of people (petty corruption), to corruption that affects the government on a large scale (grand corruption), and corruption that is so prevalent that it is part of the everyday structure of society, including corruption as one of the symptoms of organized crime.\n\nA number of indicators and tools have been developed which can measure different forms of corruption with increasing accuracy.\n\nPetty corruption occurs at a smaller scale and takes place at the implementation end of public services when public officials meet the public. For example, in many small places such as registration offices, police stations, state licensing boards, and many other private and government sectors.\n\nGrand corruption is defined as corruption occurring at the highest levels of government in a way that requires significant subversion of the political, legal and economic systems. Such corruption is commonly found in countries with authoritarian or dictatorial governments but also in those without adequate policing of corruption.\n\nThe government system in many countries is divided into the legislative, executive and judicial branches in an attempt to provide independent services that are less subject to grand corruption due to their independence from one another.\n\nSystemic corruption (or endemic corruption) is corruption which is primarily due to the weaknesses of an organization or process. It can be contrasted with individual officials or agents who act corruptly within the system.\n\nFactors which encourage systemic corruption include conflicting incentives, discretionary powers; monopolistic powers; lack of transparency; low pay; and a culture of impunity. Specific acts of corruption include \"bribery, extortion, and embezzlement\" in a system where \"corruption becomes the rule rather than the exception.\" Scholars distinguish between centralized and decentralized systemic corruption, depending on which level of state or government corruption takes place; in countries such as the Post-Soviet states both types occur.\nSome scholars argue that there is a negative duty of western governments to protect against systematic corruption of underdeveloped governments.\n\nCorruption has been a major issue in China, where society depends heavily on personal relationships. By the late 20th century that combined with the new lust for wealth, produced escalating corruption. Historian Keith Schoppa says that bribery was only one of the tools of Chinese corruption, which also included, \"embezzlement, nepotism, smuggling, extortion, cronyism, kickbacks, deception, fraud, squandering public monies, illegal business transactions, stock manipulation and real estate fraud.\" Given the repeated anti-corruption campaigns it was a prudent precaution to move as much of the fraudulent money as possible overseas.\n\nPer R. Klitgaard corruption will occur if the corrupt gain is greater than the penalty multiplied by the likelihood of being caught and prosecuted:\n\nCorrupt gain > Penalty × Likelihood of being caught and prosecuted\n\nKlitgaard has also coined a metaphorical formula to illustrate how the amount of corruption depends on three variables: \nmonopoly (M) on the supply of a good or service, the discretion (D) enjoyed by suppliers, and the supplier's accountability and transparency (A) to others. The amount of corruption (C) could be expressed as:\n\nC = M  +  D  –  A.\n\nSince a high degree of monopoly and discretion accompanied by a low degree of transparency does not automatically lead to corruption, a fourth variable of \"morality\" or \"integrity\" has been introduced by others. The moral dimension has an intrinsic component and refers to a \"mentality problem\", and an extrinsic component referring to circumstances like poverty, inadequate remuneration, inappropriate work conditions and inoperable or over-complicated procedures which demoralize people and let them search for \"alternative\" solutions. Hence the amended Klitgaard equation is\n\nDegree of corruption = Monopoly + Discretion – Transparency – Morality\n\nAccording to a 2017 survey study, the following factors have been attributed as causes of corruption:\n\nIt has been noted that in a comparison of the most corrupt with the least corrupt countries, the former group contains nations with huge socio-economic inequalities, and the latter contains nations with a high degree of social and economic justice.\n\nCorruption can occur in many sectors, whether they be public or private industry or even NGOs (especially in public sector). However, only in democratically controlled institutions is there an interest of the public (owner) to develop internal mechanisms to fight active or passive corruption, whereas in private industry as well as in NGOs there is no public control. Therefore, the owners' investors' or sponsors' profits are largely decisive.\n\nPublic corruption includes corruption of the political process and of government agencies such as the police as well as corruption in processes of allocating public funds for contracts, grants, and hiring. Recent research by the World Bank suggests that who makes policy decisions (elected officials or bureaucrats) can be critical in determining the level of corruption because of the incentives different policy-makers face.\n\nPolitical corruption is the abuse of public power, office, or resources by elected government officials for personal gain, by extortion, soliciting or offering bribes. It can also take the form of office holders maintaining themselves in office by purchasing votes by enacting laws which use taxpayers' money. Evidence suggests that corruption can have political consequences- with citizens being asked for bribes becoming less likely to identify with their country or region.\n\nThe political act of graft (American English), is a well known and now global form of political corruption, being the unscrupulous and illegal use of a politician's authority for personal gain, when funds intended for public projects are intentionally misdirected in order to maximize the benefits to illegally private interests of the corrupted individual(s) and their cronies.\n\nThe Kaunas golden toilet case was a major Lithuanian scandal. In 2009, the municipality of Kaunas (led by mayor Andrius Kupčinskas) ordered that a shipping container was to be converted into an outdoor toilet at a cost of 500,000 litai (around 150,000 euros). It was to also require 5,000 litai (1,500 euros) in monthly maintenance costs. At the same time when Kaunas's \"Golden Toilet\" was built, Kėdainiai tennis club acquired a very similar, but more advanced solution for 4,500 euros. Because of the inflated cost of the outdoor toilet, it was nicknamed the \"Golden Toilet\". Despite the investment, the \"Golden Toilet\" remained closed for years due to the dysfunctionality and was a subject of a lengthy anti-corruption investigation into those who had created it and the local municipality even considered demolishing the building at one point. The group of public servants involved in the toilet's procurement received various prison sentences for recklessness, malfeasance, misuse of power and document falsifications in a 2012 court case, but were cleared of their corruption charges and received compensation, which pushed the total construction cost and subsequent related financial losses to 352,000 euros.\n\nVarious sources acclaim the Spanish People's Party – Partido Popular -, to be Europe's most corrupt party, with about yearly 45 billion euro worth of corruption.\n\nPolice corruption is a specific form of police misconduct designed to obtain financial benefits, personal gain, career advancement for a police officer or officers in exchange for not pursuing or selectively pursuing an investigation or arrest or aspects of the \"thin blue line\" itself where force members collude in lies to protect other members from accountability. One common form of police corruption is soliciting or accepting bribes in exchange for not reporting organized drug or prostitution rings or other illegal activities.\n\nAnother example is police officers flouting the police code of conduct in order to secure convictions of suspects—for example, through the use of falsified evidence. More rarely, police officers may deliberately and systematically participate in organized crime themselves. In most major cities, there are internal affairs sections to investigate suspected police corruption or misconduct. Similar entities include the British Independent Police Complaints Commission.\n\nJudicial corruption refers to corruption-related misconduct of judges, through receiving or giving bribes, improper sentencing of convicted criminals, bias in the hearing and judgement of arguments and other such misconduct.\n\nGovernmental corruption of judiciary is broadly known in many transitional and developing countries because the budget is almost completely controlled by the executive. The latter undermines the separation of powers, as it creates a critical financial dependence of the judiciary. The proper national wealth distribution including the government spending on the judiciary is subject to the constitutional economics.\n\nIt is important to distinguish between the two methods of corruption of the judiciary: the government (through budget planning and various privileges), and the private. Judicial corruption can be difficult to completely eradicate, even in developed countries.\nCorruption in judiciary also involves the government in power using the judicial arm of government to oppress the opposition parties in the detriments of the state.\n\nCorruption, the abuse of entrusted power for private gain, as defined by Transparency International is systemic in the health sector. \nThe characteristics of health systems with their concentrated supply of a service, high discretionary power of its members controlling the supply, and low accountability to others are the exact constellation of the variables described by Klitgaard, on which corruption depends.\n\nCorruption in health care is more dangerous than in any other sector, because it affects health outcomes and is literally deadly. It is widespread and yet, little has been published in medical journals about this topic and as of 2019 there is no evidence on what might reduce corruption in the health sector. Corruption occurs within the private and public health sectors and may appear as theft, embezzlement, nepotism, bribery up til extortion, or as undue influence. and occurs anywhere within the sector, be it in service provision, purchasing, construction and hiring. In 2019, Transparency International has described teh 6 most common ways of service corruption as follows: absenteeism, informal payments from patients, embezzlement, inflating services and the costs of services, favouritism and manipulation of data (billing for goods and services that were never sent or done).\n\nCorruption in education is a worldwide phenomenon. Corruption in admissions to universities is traditionally considered as one of the most corrupt areas of the education sector. Recent attempts in some countries, such as Russia and Ukraine, to curb corruption in admissions through the abolition of university entrance examinations and introduction of standardized computer-graded tests have largely failed. Vouchers for university entrants have never materialized. The cost of corruption is in that it impedes sustainable economic growth.\n\nEndemic corruption in educational institutions leads to the formation of sustainable corrupt hierarchies. While higher education in Russia is distinct with widespread bribery, corruption in the US and the UK features a significant amount of fraud. The US is distinct with grey areas and institutional corruption in the higher education sector. Authoritarian regimes, including those in the former Soviet republics, encourage educational corruption and control universities, especially during the election campaigns. This is typical for Russia, Ukraine, and Central Asian regimes, among others. The general public is well aware of the high level of corruption in colleges and universities, including thanks to the media. Doctoral education is no exception, with dissertations and doctoral degrees available for sale, including for politicians. Russian Parliament is notorious for \"highly educated\" MPs High levels of corruption are a result of universities not being able to break away from their Stalinist past, over bureaucratization, and a clear lack of university autonomy. Both quantitative and qualitative methodologies are employed to study education corruption, but the topic remains largely unattended by the scholars. In many societies and international organizations, education corruption remains a taboo.\nIn some countries, such as certain eastern European countries, some Balkan countries and certain Asian countries, corruption occurs frequently in universities. This can include bribes to bypass bureaucratic procedures and bribing faculty for a grade. The willingness to engage in corruption such as accepting bribe money in exchange for grades decreases if individuals perceive such behavior as very objectionable, i.e. a violation of social norms and if they fear sanctions regarding the severity and probability of sanctions.\n\nThe Teamsters (International Brotherhood of Teamsters) is an example of how the civil RICO process can be used. For decades, the Teamsters had been substantially controlled by La Cosa Nostra. Since 1957, four of eight Teamster presidents were indicted, yet the union continued to be controlled by organized crime elements. The federal government has been successful at removing the criminal influence from this 1.4 million-member union by using the civil process.\n\nThe history of religion includes numerous examples of religious leaders calling attention to corruption in the religious practices and institutions of their time. Jewish prophets Isaiah and Amos berate the rabbinical establishment of Ancient Judea for failing to live up to the ideals of the Torah. In the New Testament, Jesus accuses the rabbinical establishment of his time of hypocritically following only the ceremonial parts of the Torah and neglecting the more important elements of justice, mercy and faithfulness. Corruption was one of the important issues during the Investiture Controversy. In 1517, Martin Luther accuses the Catholic Church of widespread corruption, including selling of indulgences.\n\nIn 2015, Princeton University professor Kevin M. Kruse advances the thesis that business leaders in the 1930s and 1940s collaborated with clergymen, including James W. Fifield Jr., to develop and promote a new hermeneutical approach to Scripture that would de-emphasize the social Gospel and emphasize themes, such as individual salvation, more congenial to free enterprise.\n\n19th century German philosopher Arthur Schopenhauer acknowledged that academics, including philosophers, are subject to the same sources of corruption as the society they inhabit. He distinguished the corrupt \"university\" philosophers, whose \"real concern is to earn with credit an honest livelihood for themselves and ... to enjoy a certain prestige in the eyes of the public\" from the genuine philosopher, whose sole motive is to discover and bear witness to the truth.\n\nIn criminology, corporate crime refers to crimes committed either by a corporation (i.e., a business entity having a separate legal personality from the natural persons that manage its activities), or by individuals acting on behalf of a corporation or other business entity (see vicarious liability and corporate liability). Some negative behaviours by corporations may not be criminal; laws vary between jurisdictions. For example, some jurisdictions allow insider trading.\nPetróleo Brasileiro S.A. — Petrobras, more commonly known as simply Petrobras (), is a semi-public Brazilian multinational corporation in the petroleum industry headquartered in Rio de Janeiro, Brazil. The company's name translates to Brazilian Petroleum Corporation – Petrobras. The company was ranked No. 58 in the 2016 Fortune Global 500 list. It is being investigated over corporate and political collusion and corruption.\n\nOdebrecht is a privately held Brazilian conglomerate consisting of businesses in the fields of engineering, real estate, construction, chemicals and petrochemicals. The company was founded in 1944 in Salvador da Bahia by Norberto Odebrecht, and the firm is now present in South America, Central America, North America, the Caribbean, Africa, Europe and the Middle East. Its leading company is . Odebrecht is one of the 25 largest international construction companies and led by Odebrecht family.\n\nIn 2016, the firm's executives were examined during Operation Car Wash part of an investigation over Odebrecht Organization bribes to executives of Petrobras, in exchange for contracts and influence. Operation Car Wash is an ongoing criminal money laundering and bribes related corporate crime investigation being carried out by the Federal Police of Brazil, Curitiba Branch, and judicially commanded by Judge Sérgio Moro since 17 March 2014.\n\n\"Arms for cash\" can be done by either a state-sanctioned arms dealer, firm or state itself to another party it just in regards regards as only a good business partner and not political kindred or allies, thus making them no better than regular gun runners. Arms smugglers, who are already into arms trafficking may work for them on the ground or with shipment. The money is often laundered and records are often destroyed.\n\nIt often breaks UN, national or international law.\nPayment can also be in strange or indirect ways like arms paid for in post-war oil contracts, post-war hotel ownership, conflict diamonds, corporate shares or the long term post-war promises of superfus future contracts between the parties involved in it, etc...\nIn 2006 Transparency International ranked Angola a lowly 142 out of 163 countries in the Corruption Perception Index just after Venezuela and before the Republic of the Congo with a 2.2 rating. Angola was at 168th place (out of 178 countries) on Transparency International's Corruption Perceptions Index (CPI), receiving a 1.9 on a scale from 0 to 10. On the World Bank's 2009 Worldwide Governance Index, Angola had done very poorly on all six aspects of governance assessed. While its score for political stability improved to 35.8 in 2009 (on a 100-point scale) from 19.2 in 2004, Angola earned especially low scores for accountability, regulatory standards, and rule of law. The score for corruption declined from an extremely low 6.3 in 2004 to 5.2 in 2009.\n\nThe country is regarded poorly and that corruption is wounding the economy badly despite the emerging oil industries wealth.\n\nThe Mitterrand–Pasqua affair, also known informally as Angolagate, was an international political scandal over the secret and illegal sale and shipment of arms from the nations of Central Europe to the government of Angola by the Government of France in the 1990s. It led to arrests and judiciary actions in the 2000s, involved an illegal arms sale to Angola despite a UN embargo, with business interests in France and elsewhere improperly obtaining a share of Angolan oil revenues. The scandal has subsequently been tied to several prominent figures in French politics.\n\n42 individuals, including: 42 people, including Jean-Christophe Mitterrand, Jacques Attali, Charles Pasqua and Jean-Charles Marchiani, Pierre Falcone. Arcadi Gaydamak, Paul-Loup Sulitzer, Union for a Popular Movement deputy Georges Fenech, the son of François Mitterrand and a former French Minister of the Interior, were charged, accused, indicted or convicted with illegal arms trading, tax fraud, embezzlement, money laundering and other crimes. \"\n\nIn systemic corruption and grand corruption, multiple methods of corruption are used concurrently with similar aims.\n\nBribery involves the improper use of gifts and favours in exchange for personal gain. This is also known as kickbacks or, in the Middle East, as baksheesh. It is a common form of corruption. The types of favours given are diverse and may include money, gifts, sexual favours, company shares, entertainment, employment and political benefits. The personal gain that is given can be anything from actively giving preferential treatment to having an indiscretion or crime overlooked.\n\nBribery can sometimes form a part of the systemic use of corruption for other ends, for example to perpetrate further corruption. Bribery can make officials more susceptible to blackmail or to extortion.\n\nEmbezzlement and theft involve someone with access to funds or assets illegally taking control of them. Fraud involves using deception to convince the owner of funds or assets to give them up to an unauthorized party.\n\nExamples include the misdirection of company funds into \"shadow companies\" (and then into the pockets of corrupt employees), the skimming of foreign aid money, scams and other corrupt activity.\n\nThe political act of graft is when funds intended for public projects are intentionally misdirected to maximize the benefits to private interests of the corrupt individuals.\n\nWhile bribery is the use of positive inducements for corrupt aims, extortion and blackmail centre around the use of threats. This can be the threat of violence or false imprisonment as well as exposure of an individual's secrets or prior crimes.\n\nThis includes such behavior as an influential person threatening to go to the media if they do not receive speedy medical treatment (at the expense of other patients), threatening a public official with exposure of their secrets if they do not vote in a particular manner, or demanding money in exchange for continued secrecy.\n\nInfluence peddling is the illegal practice of using one's influence in government or connections with persons in authority to obtain favors or preferential treatment, usually in return for payment.\n\nNetworking can be an effective way for job-seekers to gain a competitive edge over others in the job-market. The idea is to cultivate personal relationships with prospective employers, selection panelists, and others, in the hope that these personal affections will influence future hiring decisions. This form of networking has been described as an attempt to corrupt formal hiring processes, where all candidates are given an equal opportunity to demonstrate their merits to selectors. The networker is accused of seeking non-meritocratic advantage over other candidates; advantage that is based on personal fondness rather than on any objective appraisal of which candidate is most qualified for the position.\n\nAbuse of discretion refers to the misuse of one's powers and decision-making facilities. Examples include a judge improperly dismissing a criminal case or a customs official using their discretion to allow a banned substance through a port.\n\nFavouritism, nepotism and clientelism involve the favouring of not the perpetrator of corruption but someone related to them, such as a friend, family member or member of an association. Examples would include hiring or promoting a family member or staff member to a role they are not qualified for, who belongs to the same political party as you, regardless of merit.\n\nSome states do not forbid these forms of corruption.\n\nCorruption is strongly negatively associated with the share of private investment and, hence, it lowers the rate of economic growth.\n\nCorruption reduces the returns of productive activities. If the returns to production fall faster than the returns to corruption and rent-seeking activities, resources will flow from productive activities to corruption activities over time. This will result in a lower stock of producible inputs like human capital in corrupted countries.\n\nCorruption creates the opportunity for increased inequality, reduces the return of productive activities, and, hence, makes rentseeking and corruption activities more attractive. This opportunity for increased inequality not only generates psychological frustration to the underprivileged but also reduces productivity growth, investment, and job opportunities.\n\nAccording to the amended Klitgaard equation, limitation of monopoly and regulator discretion of individuals and a high degree of transparency through independent oversight by non-governmental organisations (NGOs) and the media plus public access to reliable information could reduce the problem. Djankov and other researchers have independently addressed the role information plays in fighting corruption with evidence from both developing and developed countries. Disclosing financial information of government officials to the public is associated with improving institutional accountability and eliminating misbehavior such as vote buying. The effect is specifically remarkable when the disclosures concern politicians' income sources, liabilities and asset level instead of just income level. Any extrinsic aspects that might reduce morality should be eliminated. Additionally, a country should establish a culture of ethical conduct in society with the government setting the good example in order to enhance the intrinsic morality.\n\nIn 1969, Christian anarchist Dorothy Day argued that God will resolve economic abuses such as corruption. She wrote,\n\nCreating bottom-up mechanisms, promoting citizens participation and encouraging the values of integrity, accountability, and transparency are crucial components of fighting corruption. As of 2012, the implementation of the \"Advocacy and Legal Advice Centres (ALACs)” in Europe had led to a significant increase in the number of citizen complaints against acts of corruption received and documented and also to the development of strategies for good governance by involving citizens willing to fight against corruption.\n\nThe Foreign Corrupt Practices Act (FCPA, USA 1977) was an early paradigmatic law for many western countries i.e. industrial countries of the OECD. There, for the first time the old principal-agent approach was moved back where mainly the victim (a society, private or public) and a passive corrupt member (an individual) were considered, whereas the active corrupt part was not in the focus of legal prosecution. Unprecedented, the law of an industrial country directly condemned active corruption, particularly in international business transactions, which was at that time in contradiction to anti-bribery activities of the World Bank and its spin-off organization Transparency International.\n\nAs early as 1989 the OECD had established an ad hoc Working Group in order to explore \"...the concepts fundamental to the offense of corruption, and the exercise of national jurisdiction over offenses committed wholly or partially abroad.\"\nBased on the FCPA concept, the Working Group presented in 1994 the then \"OECD Anti-Bribery Recommendation\" as precursor for the OECD Convention on Combating Bribery of Foreign Public Officials in International Business Transactions which was signed in 1997 by all member countries and came finally into force in 1999. However, because of ongoing concealed corruption in international transactions several instruments of Country Monitoring have been developed since then by the OECD in order to foster and evaluate related national activities in combating foreign corrupt practices. One survey shows that after the implementation of heightened review of multinational firms under the convention in 2010 firms from countries that had signed the convention were less likely to use bribery.\n\nIn 2013, a document produced by the economic and private sector professional evidence and applied knowledge services help-desk discusses some of the existing practices on anti-corruption. They found:\n\nIn some countries people travel to corruption hot spots or a specialist tour company takes them on corruption city tours, as it is the case in Prague. Corruption tours have also occurred in Chicago, and Mexico City\n\nThough corruption is often viewed as illegal, a concept of legal corruption has been described by Daniel Kaufmann and Pedro Vicente. It might be termed as processes which are corrupt, but are protected by a legal (that is, specifically permitted, or at least not proscribed by law) framework.\n\nIn 1994, the German Parliamentary Financial Commission in Bonn presented a comparative study on \"legal corruption\" in industrialized OECD countries They reported that in most industrial countries foreign corruption was legal, and that their foreign corrupt practices ranged from simple legalization, through governmental subsidization (tax deduction), up to extreme cases as in Germany, where foreign corruption was fostered, whereas domestic was legally prosecuted.\nThe German Parliamentary Financial Commission rejected a Parliamentary Proposal by the opposition, which had been aiming to limit German foreign corruption on the basis of the US \"Foreign Corrupt Practices Act\" (FCPA from 1977), thus fostering national export corporations. In 1997 a corresponding OECD Anti-Bribery Convention was signed by its members.\nIt took until 1999, after the OECD Anti-Bribery Convention came into force, that Germany withdrew the legalization of foreign corruption.\n\nThe Foreign corrupt practices of industrialized OECD countries 1994 (Parliamentary Financial Commission study, Bonn).\n\nBelgium: bribe payments are generally tax deductible as business expenses if the name and address of the beneficiary is disclosed. Under the following conditions kickbacks in connection with exports abroad are permitted for deduction even without proof of the receiver:\nIn the absence of the required conditions, for corporate taxable companies paying bribes without proof of the receiver, a special tax of 200% is charged. This special tax may, however, be abated along with the bribe amount as an operating expense.\n\nDenmark: bribe payments are deductible when a clear operational context exists and its adequacy is maintained.\n\nFrance: basically all operating expenses can be deducted. However, staff costs must correspond to an actual work done and must not be excessive compared to the operational significance. This also applies to payments to foreign parties. Here, the receiver shall specify the name and address, unless the total amount in payments per beneficiary does not exceed 500 FF. If the receiver is not disclosed the payments are considered \"rémunérations occult\" and are associated with the following disadvantages:\n\nJapan: in Japan, bribes are deductible as business expenses that are justified by the operation (of the company) if the name and address of the recipient is specified. This also applies to payments to foreigners. If the indication of the name is refused, the expenses claimed are not recognized as operating expenses.\n\nCanada: there is no general rule on the deductibility or non-deductibility of kickbacks and bribes. Hence the rule is that necessary expenses for obtaining the income (contract) are deductible. Payments to members of the public service and domestic administration of justice, to officers and employees and those charged with the collection of fees, entrance fees etc. for the purpose to entice the recipient to the violation of his official duties, can not be abated as business expenses as well as illegal payments according to the Criminal Code.\n\nLuxembourg: bribes, justified by the operation (of a company) are deductible as business expenses. However, the tax authorities may require that the payer is to designate the receiver by name. If not, the expenses are not recognized as operating expenses.\n\nNetherlands: all expenses that are directly or closely related to the business are deductible. This also applies to expenditure outside the actual business operations if they are considered beneficial as to the operation for good reasons by the management. What counts is the good merchant custom. Neither the law nor the administration is authorized to determine which expenses are not operationally justified and therefore not deductible. For the business expense deduction it is not a requirement that the recipient is specified. It is sufficient to elucidate to the satisfaction of the tax authorities that the payments are in the interest of the operation.\n\nAustria: bribes justified by the operation (of a company) are deductible as business expenses. However, the tax authority may require that the payer names the recipient of the deducted payments exactly. If the indication of the name is denied e.g. because of business comity, the expenses claimed are not recognized as operating expenses. This principle also applies to payments to foreigners.\n\nSwitzerland: bribe payments are tax deductible if it is clearly operation initiated and the consignee is indicated.\n\nUS: (rough résumé: \"generally operational expenses are deductible if they are not illegal according to the FCPA\")\n\nUK: kickbacks and bribes are deductible if they have been paid for operating purposes. The tax authority may request the name and address of the recipient.\"\n\nReferring to the recommendation of the above-mentioned Parliamentary Financial Commission's study, the then Kohl administration (1991–1994) decided to maintain the legality of corruption against officials exclusively in foreign transactions and confirmed the full deductibility of bribe money, co-financing thus a specific nationalistic corruption practice (§4 Abs. 5 Nr. 10 EStG, valid until 19 March 1999) in contradiction to the 1994 OECD recommendation. The respective law was not changed before the OECD Convention also in Germany came into force (1999).\nAccording to the Parliamentary Financial Commission's study, however, in 1994 most countries' corruption practices were not nationalistic and much more limited by the respective laws compared to Germany.\nParticularly, the non-disclosure of the bribe money recipients' name in tax declarations had been a powerful instrument for Legal Corruption during the 1990s for German corporations, enabling them to block foreign legal jurisdictions which intended to fight corruption in their countries. Hence, they uncontrolled established a strong network of clientelism around Europe (e.g. SIEMENS) along with the formation of the European Single Market in the upcoming European Union and the Eurozone.\nMoreover, in order to further strengthen active corruption the prosecution of tax evasion during that decade had been severely limited. German tax authorities were instructed to refuse any disclosure of bribe recipients' names from tax declarations to the German criminal prosecution. As a result, German corporations have been systematically increasing their informal economy from 1980 until today up to 350 bn € per annum (see diagram on the right), thus continuously feeding their black money reserves.\n\nIn 2007, Siemens was convicted in the District Court of Darmstadt of criminal corruption against the Italian corporation Enel Power SpA. Siemens had paid almost €3.5 million in bribes to be selected for a €200 million project from the Italian corporation, partially owned by the government. The deal was handled through black money accounts in Switzerland and Liechtenstein that were established specifically for such purposes.\nBecause the crime was committed in 1999, after the OECD convention had come into force, this foreign corrupt practice could be prosecuted. It was the first time a German court of law convicted foreign corrupt practices like a national practice, although the corresponding law did not yet protect foreign competitors in business.\n\nDuring the judicial proceedings it was disclosed that numerous such black accounts had been established in the past decades.\n\nPhilosophers and religious thinkers have responded to the inescapable reality of corruption in different ways. Plato, in \"The Republic\", acknowledges the corrupt nature of political institutions, and recommends that philosophers \"shelter behind a wall\" to avoid senselessly martyring themselves.\n\n\"Disciples of philosophy ... have tasted how sweet and blessed a possession philosophy is, and have also seen and been satisfied of the madness of the multitude, and known that there is no one who ever acts honestly in the administration of States, nor any helper who will save any one who maintains the cause of the just. Such a savior would be like a man who has fallen among wild beasts—unable to join in the wickedness of his fellows, neither would he be able alone to resist all their fierce natures, and therefore he would be of no use to the State or to his friends, and would have to throw away his life before he had done any good to himself or others. And he reflects upon all this, and holds his peace, and does his own business. He is like one who retires under the shelter of a wall in the storm of dust and sleet which the driving wind hurries along; and when he sees the rest of mankind full of wickedness, he is content if only he can live his own life and be pure from evil or unrighteousness, and depart in peace and good will, with bright hopes.\"\n\nThe New Testament, in keeping with the tradition of Ancient Greek thought, also frankly acknowledges the corruption of the world (ὁ κόσμος) and claims to offer a way of keeping the spirit \"unspotted from the world.\" Paul of Tarsus acknowledges his readers must inevitably \"deal with the world,\" and recommends they adopt an attitude of \"as if not\" in all their dealings. When they buy a thing, for example, they should relate to it \"as if it were not theirs to keep.\" New Testament readers are advised to refuse to \"conform to the present age\" and not to be ashamed to be peculiar or singular. They are advised not be friends of the corrupt world, because \"friendship with the world is enmity with God.\" They are advised not to love the corrupt world or the things of the world. The rulers of this world, Paul explains, \"are coming to nothing\" While readers must obey corrupt rulers in order to live in the world, the spirit is subject to no law but to love God and love our neighbors as ourselves. New Testament readers are advised to adopt a disposition in which they are \"in the world, but not of the world.\" This disposition, Paul claims, shows us a way to escape \"slavery to corruption\" and experience the freedom and glory of being innocent \"children of God\".\n\n"}
{"id": "8103", "url": "https://en.wikipedia.org/wiki?curid=8103", "title": "Deforestation", "text": "Deforestation\n\nDeforestation, clearance, clearcutting or clearing is the removal of a forest or stand of trees from land which is then converted to a non-forest use. Deforestation can involve conversion of forest land to farms, ranches, or urban use. The most concentrated deforestation occurs in tropical rainforests. About 31% of Earth's land surface is covered by forests.\n\nDeforestation has many causes: trees can be cut down to be used for building or sold as fuel (sometimes in the form of charcoal or timber), while cleared land can be used as pasture for livestock and plantation. Disregard of ascribed value, lax forest management, and deficient environmental laws are some of the factors that lead to large-scale deforestation. In many countries, deforestation—both naturally occurring and human-induced—is an ongoing issue. Between 2000 and 2012, of forests around the world were cut down. As of 2005, net deforestation rates had ceased to increase in countries with a per capita GDP of at least US$4,600.\n\nThe removal of trees without sufficient reforestation has resulted in habitat damage, biodiversity loss, and aridity. Deforestation causes extinction, changes to climatic conditions, desertification, and displacement of populations, as observed by current conditions and in the past through the fossil record. Deforestation also has adverse impacts on biosequestration of atmospheric carbon dioxide, increasing negative feedback cycles contributing to global warming. Global warming also puts increased pressure on communities who seek food security by clearing forests for agricultural use and reducing arable land more generally. Deforested regions typically incur significant other environmental effects such as adverse soil erosion and degradation into wasteland.\n\nDeforestation is more extreme in tropical and subtropical forests in emerging economies. More than half of all plant and land animal species in the world live in tropical forests. As a result of deforestation, only remain of the original of tropical rainforest that formerly covered the Earth. An area the size of a football pitch is cleared from the Amazon rainforest every minute, with of rainforest cleared for animal agriculture overall. More than 3.6 million hectares of virgin tropical forest was lost in 2018.\n\nAccording to the United Nations Framework Convention on Climate Change (UNFCCC) secretariat, the overwhelming direct cause of deforestation is agriculture. Subsistence farming is responsible for 48% of deforestation; commercial agriculture is responsible for 32%; logging is responsible for 14%, and fuel wood removals make up 5%.\n\nExperts do not agree on whether industrial logging is an important contributor to global deforestation. Some argue that poor people are more likely to clear forest because they have no alternatives, others that the poor lack the ability to pay for the materials and labour needed to clear forest. One study found that population increases due to high fertility rates were a primary driver of tropical deforestation in only 8% of cases.\n\nOther causes of contemporary deforestation may include corruption of government institutions, the inequitable distribution of wealth and power, population growth and overpopulation, and urbanization. Globalization is often viewed as another root cause of deforestation, though there are cases in which the impacts of globalization (new flows of labor, capital, commodities, and ideas) have promoted localized forest recovery.\n\nAnother cause of deforestation is climate change. 23% of tree cover losses result from wildfires and climate change increase their frequency and power. The rising temperatures cause massive wildfires especially in the Boreal forests. One possible effect is the change of the forest composition.\nIn 2000 the United Nations Food and Agriculture Organization (FAO) found that \"the role of population dynamics in a local setting may vary from decisive to negligible\", and that deforestation can result from \"a combination of population pressure and stagnating economic, social and technological conditions\".\n\nThe degradation of forest ecosystems has also been traced to economic incentives that make forest conversion appear more profitable than forest conservation. Many important forest functions have no markets, and hence, no economic value that is readily apparent to the forests' owners or the communities that rely on forests for their well-being. From the perspective of the developing world, the benefits of forest as carbon sinks or biodiversity reserves go primarily to richer developed nations and there is insufficient compensation for these services. Developing countries feel that some countries in the developed world, such as the United States of America, cut down their forests centuries ago and benefited economically from this deforestation, and that it is hypocritical to deny developing countries the same opportunities, i.e. that the poor should not have to bear the cost of preservation when the rich created the problem.\n\nSome commentators have noted a shift in the drivers of deforestation over the past 30 years. Whereas deforestation was primarily driven by subsistence activities and government-sponsored development projects like transmigration in countries like Indonesia and colonization in Latin America, India, Java, and so on, during the late 19th century and the earlier half of the 20th century, by the 1990s the majority of deforestation was caused by industrial factors, including extractive industries, large-scale cattle ranching, and extensive agriculture. Since 2001, commodity-driven deforestation, which is more likely to be permanent, has accounted for about a quarter of all forest disturbance, and this loss has been concentrated in South America and Southeast Asia.\n\nDeforestation is ongoing and is shaping climate and geography.\n\nDeforestation is a contributor to global warming, and is often cited as one of the major causes of the enhanced greenhouse effect. Tropical deforestation is responsible for approximately 20% of world greenhouse gas emissions. According to the Intergovernmental Panel on Climate Change deforestation, mainly in tropical areas, could account for up to one-third of total anthropogenic carbon dioxide emissions. But recent calculations suggest that carbon dioxide emissions from deforestation and forest degradation (excluding peatland emissions) contribute about 12% of total anthropogenic carbon dioxide emissions with a range from 6% to 17%. Deforestation causes carbon dioxide to linger in the atmosphere. As carbon dioxide accrues, it produces a layer in the atmosphere that traps radiation from the sun. The radiation converts to heat which causes global warming, which is better known as the greenhouse effect. Plants remove carbon in the form of carbon dioxide from the atmosphere during the process of photosynthesis, but release some carbon dioxide back into the atmosphere during normal respiration. Only when actively growing can a tree or forest remove carbon, by storing it in plant tissues. Both the decay and the burning of wood release much of this stored carbon back into the atmosphere. Although an accumulation of wood is generally necessary for carbon sequestration, in some forests the network of symbiotic fungi that surround the trees' roots can store a significant amount of carbon, storing it underground even if the tree which supplied it dies and decays, or is harvested and burned. Another way carbon can be sequestered by forests is for the wood to be harvested and turned into long-lived products, with new young trees replacing them. Deforestation may also cause carbon stores held in soil to be released. Forests can be either sinks or sources depending upon environmental circumstances. Mature forests alternate between being net sinks and net sources of carbon dioxide (see carbon dioxide sink and carbon cycle).\n\nIn deforested areas, the land heats up faster and reaches a higher temperature, leading to localized upward motions that enhance the formation of clouds and ultimately produce more rainfall. However, according to the Geophysical Fluid Dynamics Laboratory, the models used to investigate remote responses to tropical deforestation showed a broad but mild temperature increase all through the tropical atmosphere. The model predicted <0.2 °C warming for upper air at 700 mb and 500 mb. However, the model shows no significant changes in other areas besides the Tropics. Though the model showed no significant changes to the climate in areas other than the Tropics, this may not be the case since the model has possible errors and the results are never absolutely definite. Deforestation affects wind flows,\nwater vapour flows and absorption of solar energy thus clearly influencing local and global climate. \n\nReducing emissions from deforestation and forest degradation (REDD) in developing countries has emerged as a new potential to complement ongoing climate policies. The idea consists in providing financial compensations for the reduction of greenhouse gas (GHG) emissions from deforestation and forest degradation\". REDD can be seen as an alternative to the emissions trading system as in the latter, polluters must pay for permits for the right to emit certain pollutants (i.e. CO2).\n\nRainforests are widely believed by laymen to contribute a significant amount of the world's oxygen, although it is now accepted by scientists that rainforests contribute little net oxygen to the atmosphere and deforestation has only a minor effect on atmospheric oxygen levels. However, the incineration and burning of forest plants to clear land releases large amounts of CO, which contributes to global warming. Scientists also state that tropical deforestation releases 1.5 billion tons of carbon each year into the atmosphere.\n\nThe water cycle is also affected by deforestation. Trees extract groundwater through their roots and release it into the atmosphere. When part of a forest is removed, the trees no longer transpire this water, resulting in a much drier climate. Deforestation reduces the content of water in the soil and groundwater as well as atmospheric moisture. The dry soil leads to lower water intake for the trees to extract. Deforestation reduces soil cohesion, so that erosion, flooding and landslides ensue.\n\nShrinking forest cover lessens the landscape's capacity to intercept, retain and transpire precipitation. Instead of trapping precipitation, which then percolates to groundwater systems, deforested areas become sources of surface water runoff, which moves much faster than subsurface flows. Forests return most of the water that falls as precipitation to the atmosphere by transpiration. In contrast, when an area is deforested, almost all precipitation is lost as run-off. That quicker transport of surface water can translate into flash flooding and more localized floods than would occur with the forest cover. Deforestation also contributes to decreased evapotranspiration, which lessens atmospheric moisture which in some cases affects precipitation levels downwind from the deforested area, as water is not recycled to downwind forests, but is lost in runoff and returns directly to the oceans. According to one study, in deforested north and northwest China, the average annual precipitation decreased by one third between the 1950s and the 1980s.\nTrees, and plants in general, affect the water cycle significantly:\nAs a result, the presence or absence of trees can change the quantity of water on the surface, in the soil or groundwater, or in the atmosphere. This in turn changes erosion rates and the availability of water for either ecosystem functions or human services. Deforestation on lowland plains moves cloud formation and rainfall to higher elevations.\n\nThe forest may have little impact on flooding in the case of large rainfall events, which overwhelm the storage capacity of forest soil if the soils are at or close to saturation.\n\nTropical rainforests produce about 30% of our planet's fresh water.\n\nDeforestation disrupts normal weather patterns creating hotter and drier weather thus increasing drought, desertification, crop failures, melting of the polar ice caps, coastal flooding and displacement of major vegetation regimes.\n\nDue to surface plant litter, forests that are undisturbed have a minimal rate of erosion. The rate of erosion occurs from deforestation, because it decreases the amount of litter cover, which provides protection from surface runoff. The rate of erosion is around 2 metric tons per square kilometre. This can be an advantage in excessively leached tropical rain forest soils. Forestry operations themselves also increase erosion through the development of (forest) roads and the use of mechanized equipment.\n\nDeforestation in China's Loess Plateau many years ago has led to soil erosion; this erosion has led to valleys opening up. The increase of soil in the runoff causes the Yellow River to flood and makes it yellow colored.\n\nGreater erosion is not always a consequence of deforestation, as observed in the southwestern regions of the US. In these areas, the loss of grass due to the presence of trees and other shrubbery leads to more erosion than when trees are removed.\n\nSoils are reinforced by the presence of trees, which secure the soil by binding their roots to soil bedrock. Due to deforestation, the removal of trees causes sloped lands to be more susceptible to landslides.\n\nDeforestation on a human scale results in decline in biodiversity, and on a natural global scale is known to cause the extinction of many species. The removal or destruction of areas of forest cover has resulted in a degraded environment with reduced biodiversity. Forests support biodiversity, providing habitat for wildlife; moreover, forests foster medicinal conservation. With forest biotopes being irreplaceable source of new drugs (such as taxol), deforestation can destroy genetic variations (such as crop resistance) irretrievably.\n\nSince the tropical rainforests are the most diverse ecosystems on Earth and about 80% of the world's known biodiversity could be found in tropical rainforests, removal or destruction of significant areas of forest cover has resulted in a degraded environment with reduced biodiversity. A study in Rondônia, Brazil, has shown that deforestation also removes the microbial community which is involved in the recycling of nutrients, the production of clean water and the removal of pollutants.\n\nIt has been estimated that we are losing 137 plant, animal and insect species every single day due to rainforest deforestation, which equates to 50,000 species a year. Others state that tropical rainforest deforestation is contributing to the ongoing Holocene mass extinction. The known extinction rates from deforestation rates are very low, approximately 1 species per year from mammals and birds which extrapolates to approximately 23,000 species per year for all species. Predictions have been made that more than 40% of the animal and plant species in Southeast Asia could be wiped out in the 21st century. Such predictions were called into question by 1995 data that show that within regions of Southeast Asia much of the original forest has been converted to monospecific plantations, but that potentially endangered species are few and tree flora remains widespread and stable.\n\nScientific understanding of the process of extinction is insufficient to accurately make predictions about the impact of deforestation on biodiversity. Most predictions of forestry related biodiversity loss are based on species-area models, with an underlying assumption that as the forest declines species diversity will decline similarly. However, many such models have been proven to be wrong and loss of habitat does not necessarily lead to large scale loss of species. Species-area models are known to overpredict the number of species known to be threatened in areas where actual deforestation is ongoing, and greatly overpredict the number of threatened species that are widespread.\n\nA recent study of the Brazilian Amazon predicts that despite a lack of extinctions thus far, up to 90 percent of predicted extinctions will finally occur in the next 40 years.\n\nDamage to forests and other aspects of nature could halve living standards for the world's poor and reduce global GDP by about 7% by 2050, a report concluded at the Convention on Biological Diversity (CBD) meeting in Bonn in 2008. Historically, utilization of forest products, including timber and fuel wood, has played a key role in human societies, comparable to the roles of water and cultivable land. Today, developed countries continue to utilize timber for building houses, and wood pulp for paper. In developing countries, almost three billion people rely on wood for heating and cooking.\n\nThe forest products industry is a large part of the economy in both developed and developing countries. Short-term economic gains made by conversion of forest to agriculture, or over-exploitation of wood products, typically leads to a loss of long-term income and long-term biological productivity. West Africa, Madagascar, Southeast Asia and many other regions have experienced lower revenue because of declining timber harvests. Illegal logging causes billions of dollars of losses to national economies annually.\n\nThe new procedures to get amounts of wood are causing more harm to the economy and overpower the amount of money spent by people employed in logging. According to a study, \"in most areas studied, the various ventures that prompted deforestation rarely generated more than US$5 for every ton of carbon they released and frequently returned far less than US$1\". The price on the European market for an offset tied to a one-ton reduction in carbon is 23 euro (about US$35).\n\nRapidly growing economies also have an effect on deforestation. Most pressure will come from the world's developing countries, which have the fastest-growing populations and most rapid economic (industrial) growth. In 1995, economic growth in developing countries reached nearly 6%, compared with the 2% growth rate for developed countries. As our human population grows, new homes, communities, and expansions of cities will occur. Connecting all of the new expansions will be roads, a very important part in our daily life. Rural roads promote economic development but also facilitate deforestation. About 90% of the deforestation has occurred within 100 km of roads in most parts of the Amazon.\n\nThe European Union is one of the largest importer of products made from illegal deforestation.\n\nThe forest area change may follow a pattern suggested by the forest transition (FT) theory, whereby at early stages in its development a country is characterized by high forest cover and low deforestation rates (HFLD countries).\n\nThen deforestation rates accelerate (HFHD, high forest cover – high deforestation rate), and forest cover is reduced (LFHD, low forest cover – high deforestation rate), before the deforestation rate slows (LFLD, low forest cover – low deforestation rate), after which forest cover stabilizes and eventually starts recovering. FT is not a \"law of nature\", and the pattern is influenced by national context (for example, human population density, stage of development, structure of the economy), global economic forces, and government policies. A country may reach very low levels of forest cover before it stabilizes, or it might through good policies be able to \"bridge\" the forest transition.\n\nFT depicts a broad trend, and an extrapolation of historical rates therefore tends to underestimate future BAU deforestation for counties at the early stages in the transition (HFLD), while it tends to overestimate BAU deforestation for countries at the later stages (LFHD and LFLD).\n\nCountries with high forest cover can be expected to be at early stages of the FT. GDP per capita captures the stage in a country's economic development, which is linked to the pattern of natural resource use, including forests. The choice of forest cover and GDP per capita also fits well with the two key scenarios in the FT:\n\n(i) a forest scarcity path, where forest scarcity triggers forces (for example, higher prices of forest products) that lead to forest cover stabilization; and\n\n(ii) an economic development path, where new and better off-farm employment opportunities associated with economic growth (= increasing GDP per capita) reduce the profitability of frontier agriculture and slows deforestation.\n\nThe Carboniferous Rainforest Collapse was an event that occurred 300 million years ago. Climate change devastated tropical rainforests causing the extinction of many plant and animal species. The change was abrupt, specifically, at this time climate became cooler and drier, conditions that are not favorable to the growth of rainforests and much of the biodiversity within them. Rainforests were fragmented forming shrinking 'islands' further and further apart. Populations such as the sub class Lissamphibia were devastated, whereas Reptilia survived the collapse. The surviving organisms were better adapted to the drier environment left behind and served as legacies in succession after the collapse.\nRainforests once covered 14% of the earth's land surface; now they cover a mere 6% and experts estimate that the last remaining rainforests could be consumed in less than 40 years.\nSmall scale deforestation was practiced by some societies for tens of thousands of years before the beginnings of civilization. The first evidence of deforestation appears in the Mesolithic period. It was probably used to convert closed forests into more open ecosystems favourable to game animals. With the advent of agriculture, larger areas began to be deforested, and fire became the prime tool to clear land for crops. In Europe there is little solid evidence before 7000 BC. Mesolithic foragers used fire to create openings for red deer and wild boar. In Great Britain, shade-tolerant species such as oak and ash are replaced in the pollen record by hazels, brambles, grasses and nettles. Removal of the forests led to decreased transpiration, resulting in the formation of upland peat bogs. Widespread decrease in elm pollen across Europe between 8400–8300 BC and 7200–7000 BC, starting in southern Europe and gradually moving north to Great Britain, may represent land clearing by fire at the onset of Neolithic agriculture.\n\nThe Neolithic period saw extensive deforestation for farming land. Stone axes were being made from about 3000 BC not just from flint, but from a wide variety of hard rocks from across Britain and North America as well. They include the noted Langdale axe industry in the English Lake District, quarries developed at Penmaenmawr in North Wales and numerous other locations. Rough-outs were made locally near the quarries, and some were polished locally to give a fine finish. This step not only increased the mechanical strength of the axe, but also made penetration of wood easier. Flint was still used from sources such as Grimes Graves but from many other mines across Europe.\n\nEvidence of deforestation has been found in Minoan Crete; for example the environs of the Palace of Knossos were severely deforested in the Bronze Age.\n\nThroughout prehistory, humans were hunter gatherers who hunted within forests. In most areas, such as the Amazon, the tropics, Central America, and the Caribbean, only after shortages of wood and other forest products occur are policies implemented to ensure forest resources are used in a sustainable manner.\n\nThree regional studies of historic erosion and alluviation in ancient Greece found that, wherever adequate evidence exists, a major phase of erosion follows the introduction of farming in the various regions of Greece by about 500-1,000 years, ranging from the later Neolithic to the Early Bronze Age. The thousand years following the mid-first millennium BC saw serious, intermittent pulses of soil erosion in numerous places. The historic silting of ports along the southern coasts of Asia Minor (\"e.g.\" Clarus, and the examples of Ephesus, Priene and Miletus, where harbors had to be abandoned because of the silt deposited by the Meander) and in coastal Syria during the last centuries BC.\n\nEaster Island has suffered from heavy soil erosion in recent centuries, aggravated by agriculture and deforestation. Jared Diamond gives an extensive look into the collapse of the ancient Easter Islanders in his book \"Collapse\". The disappearance of the island's trees seems to coincide with a decline of its civilization around the 17th and 18th century. He attributed the collapse to deforestation and over-exploitation of all resources.\n\nThe famous silting up of the harbor for Bruges, which moved port commerce to Antwerp, also followed a period of increased settlement growth (and apparently of deforestation) in the upper river basins. In early medieval Riez in upper Provence, alluvial silt from two small rivers raised the riverbeds and widened the floodplain, which slowly buried the Roman settlement in alluvium and gradually moved new construction to higher ground; concurrently the headwater valleys above Riez were being opened to pasturage.\n\nA typical progress trap was that cities were often built in a forested area, which would provide wood for some industry (for example, construction, shipbuilding, pottery). When deforestation occurs without proper replanting, however; local wood supplies become difficult to obtain near enough to remain competitive, leading to the city's abandonment, as happened repeatedly in Ancient Asia Minor. Because of fuel needs, mining and metallurgy often led to deforestation and city abandonment.\n\nWith most of the population remaining active in (or indirectly dependent on) the agricultural sector, the main pressure in most areas remained land clearing for crop and cattle farming. Enough wild green was usually left standing (and partially used, for example, to collect firewood, timber and fruits, or to graze pigs) for wildlife to remain viable. The elite's (nobility and higher clergy) protection of their own hunting privileges and game often protected significant woodland.\n\nMajor parts in the spread (and thus more durable growth) of the population were played by monastical 'pioneering' (especially by the Benedictine and Commercial orders) and some feudal lords' recruiting farmers to settle (and become tax payers) by offering relatively good legal and fiscal conditions. Even when speculators sought to encourage towns, settlers needed an agricultural belt around or sometimes within defensive walls. When populations were quickly decreased by causes such as the Black Death, the colonization of the Americas, or devastating warfare (for example, Genghis Khan's Mongol hordes in eastern and central Europe, Thirty Years' War in Germany), this could lead to settlements being abandoned. The land was reclaimed by nature, but the secondary forests usually lacked the original biodiversity. The Mongol invasions and conquests alone resulted in the reduction of 700 million tons of carbon from the atmosphere by enabling the re-growth of carbon-absorbing forests on depopulated lands over a significant period of time.\n\nFrom 1100 to 1500 AD, significant deforestation took place in Western Europe as a result of the expanding human population. The large-scale building of wooden sailing ships by European (coastal) naval owners since the 15th century for exploration, colonisation, slave trade–and other trade on the high seas consumed many forest resources. Piracy also contributed to the over harvesting of forests, as in Spain. This led to a weakening of the domestic economy after Columbus' discovery of America, as the economy became dependent on colonial activities (plundering, mining, cattle, plantations, trade, etc.)\n\nIn \"Changes In the Land\" (1983), William Cronon analyzed and documented 17th-century English colonists' reports of increased seasonal flooding in New England during the period when new settlers initially cleared the forests for agriculture. They believed flooding was linked to widespread forest clearing upstream.\n\nThe massive use of charcoal on an industrial scale in Early Modern Europe was a new type of consumption of western forests; even in Stuart England, the relatively primitive production of charcoal has already reached an impressive level. Stuart England was so widely deforested that it depended on the Baltic trade for ship timbers, and looked to the untapped forests of New England to supply the need. Each of Nelson's Royal Navy war ships at Trafalgar (1805) required 6,000 mature oaks for its construction. In France, Colbert planted oak forests to supply the French navy in the future. When the oak plantations matured in the mid-19th century, the masts were no longer required because shipping had changed.\n\nNorman F. Cantor's summary of the effects of late medieval deforestation applies equally well to Early Modern Europe:\nIn the 19th century, introduction of steamboats in the United States was the cause of deforestation of banks of major rivers, such as the Mississippi River, with increased and more severe flooding one of the environmental results. The steamboat crews cut wood every day from the riverbanks to fuel the steam engines. Between St. Louis and the confluence with the Ohio River to the south, the Mississippi became more wide and shallow, and changed its channel laterally. Attempts to improve navigation by the use of snag pullers often resulted in crews' clearing large trees 100 to back from the banks. Several French colonial towns of the Illinois Country, such as Kaskaskia, Cahokia and St. Philippe, Illinois, were flooded and abandoned in the late 19th century, with a loss to the cultural record of their archeology.\n\nThe wholescale clearance of woodland to create agricultural land can be seen in many parts of the world, such as the Central forest-grasslands transition and other areas of the Great Plains of the United States. Specific parallels are seen in the 20th-century deforestation occurring in many developing nations.\n\nGlobal deforestation sharply accelerated around 1852. It has been estimated that about half of the Earth's mature tropical forests—between 7.5 million and 8 million km (2.9 million to 3 million sq mi) of the original 15 million to 16 million km (5.8 million to 6.2 million sq mi) that until 1947 covered the planet—have now been destroyed. Some scientists have predicted that unless significant measures (such as seeking out and protecting old growth forests that have not been disturbed) are taken on a worldwide basis, by 2030 there will only be 10% remaining, with another 10% in a degraded condition. 80% will have been lost, and with them hundreds of thousands of irreplaceable species. Some cartographers have attempted to illustrate the sheer scale of deforestation by country using a cartogram.\n\nEstimates vary widely as to the extent of tropical deforestation. Over a 50-year period, percentage of land cover by tropical rainforests has decreased by 50%. Where total land coverage by tropical rainforests decreased from 14% to 6%. A large contribution to this loss can be identified between 1960 and 1990, when 20% of all tropical rainforests were destroyed. At this rate, extinction of such forests is projected to occur by the mid 21st century.\n\nA 2002 analysis of satellite imagery suggested that the rate of deforestation in the humid tropics (approximately 5.8 million hectares per year) was roughly 23% lower than the most commonly quoted rates. Conversely, a newer analysis of satellite images reveals that deforestation of the Amazon rainforest is twice as fast as scientists previously estimated. Deforestation in Brazil increased by 88% for the month of June 2019, as compared with the previous year. Some 80% of the deforestation of the Amazon can be attributed to cattle ranching, as Brazil is the largest exporter of beef in the world. The Amazon region has become one of the largest cattle ranching territories in the world.\n\nSome have argued that deforestation trends may follow a Kuznets curve, which if true would nonetheless fail to eliminate the risk of irreversible loss of non-economic forest values (for example, the extinction of species).\n\nA 2005 report by the United Nations Food and Agriculture Organization (FAO) estimated that although the Earth's total forest area continued to decrease at about 13 million hectares per year, the global rate of deforestation has recently been slowing. The 2016 report by the FAO reports from 2010 to 2015 there was a worldwide decrease in forest area of 3.3 million ha per year. During this five-year period, the biggest forest area loss occurred in the tropics, particularly in South America and Africa. Per capita forest area decline was also greatest in the tropics and subtropics but is occurring in every climatic domain (except in the temperate) as populations increase.\n\nOthers claim that rainforests are being destroyed at an ever-quickening pace. The London-based Rainforest Foundation notes that \"the UN figure is based on a definition of forest as being an area with as little as 10% actual tree cover, which would therefore include areas that are actually savannah-like ecosystems and badly damaged forests\". Other critics of the FAO data point out that they do not distinguish between forest types, and that they are based largely on reporting from forestry departments of individual countries, which do not take into account unofficial activities like illegal logging.\n\nDespite these uncertainties, there is agreement that destruction of rainforests remains a significant environmental problem. Up to 90% of West Africa's coastal rainforests have disappeared since 1900.\nIn South Asia, about 88% of the rainforests have been lost. Much of what remains of the world's rainforests is in the Amazon basin, where the Amazon Rainforest covers approximately 4 million square kilometres. The regions with the highest tropical deforestation rate between 2000 and 2005 were Central America—which lost 1.3% of its forests each year—and tropical Asia. In Central America, two-thirds of lowland tropical forests have been turned into pasture since 1950 and 40% of all the rainforests have been lost in the last 40 years. Brazil has lost 90–95% of its Mata Atlântica forest. \nParaguay was losing its natural semi humid forests in the country's western regions at a rate of 15.000 hectares at a randomly studied 2-month period in 2010, Paraguay's parliament refused in 2009 to pass a law that would have stopped cutting of natural forests altogether.\nMadagascar has lost 90% of its eastern rainforests. As of 2007, less than 50% of Haiti's forests remained. Mexico, India, the Philippines, Indonesia, Thailand, Burma, Malaysia, Bangladesh, China, Sri Lanka, Laos, Nigeria, the Democratic Republic of the Congo, Liberia, Guinea, Ghana and the Ivory Coast, have lost large areas of their rainforest. Several countries, notably Brazil, have declared their deforestation a national emergency. The World Wildlife Fund's ecoregion project catalogues habitat types throughout the world, including habitat loss such as deforestation, showing for example that even in the rich forests of parts of Canada such as the Mid-Continental Canadian forests of the prairie provinces half of the forest cover has been lost or altered.\n\nRates of deforestation vary around the world.\n\nIn 2011 Conservation International listed the top 10 most endangered forests, characterized by having all lost 90% or more of their original habitat, and each harboring at least 1500 endemic plant species (species found nowhere else in the world).\n\nMain international organizations including the United Nations and the World Bank, have begun to develop programs aimed at curbing deforestation. The blanket term Reducing Emissions from Deforestation and Forest Degradation (REDD) describes these sorts of programs, which use direct monetary or other incentives to encourage developing countries to limit and/or roll back deforestation. Funding has been an issue, but at the UN Framework Convention on Climate Change (UNFCCC) Conference of the Parties-15 (COP-15) in Copenhagen in December 2009, an accord was reached with a collective commitment by developed countries for new and additional resources, including forestry and investments through international institutions, that will approach US$30 billion for the period 2010–2012. Significant work is underway on tools for use in monitoring developing country adherence to their agreed REDD targets. These tools, which rely on remote forest monitoring using satellite imagery and other data sources, include the Center for Global Development's FORMA (Forest Monitoring for Action) initiative and the Group on Earth Observations' Forest Carbon Tracking Portal. Methodological guidance for forest monitoring was also emphasized at COP-15. The environmental organization Avoided Deforestation Partners leads the campaign for development of REDD through funding from the U.S. government. In 2014, the Food and Agriculture Organization of the United Nations and partners launched Open Foris – a set of open-source software tools that assist countries in gathering, producing and disseminating information on the state of forest resources. The tools support the inventory lifecycle, from needs assessment, design, planning, field data collection and management, estimation analysis, and dissemination. Remote sensing image processing tools are included, as well as tools for international reporting for Reducing emissions from deforestation and forest degradation (REDD) and MRV (Measurement, Reporting and Verification) and FAO's Global Forest Resource Assessments.\n\nIn evaluating implications of overall emissions reductions, countries of greatest concern are those categorized as High Forest Cover with High Rates of Deforestation (HFHD) and Low Forest Cover with High Rates of Deforestation (LFHD). Afghanistan, Benin, Botswana, Burma, Burundi, Cameroon, Chad, Ecuador, El Salvador, Ethiopia, Ghana, Guatemala, Guinea, Haiti, Honduras, Indonesia, Liberia, Malawi, Mali, Mauritania, Mongolia, Namibia, Nepal, Nicaragua, Niger, Nigeria, Pakistan, Paraguay, Philippines, Senegal, Sierra Leone, Sri Lanka, Sudan, Togo, Uganda, United Republic of Tanzania, Zimbabwe are listed as having Low Forest Cover with High Rates of Deforestation (LFHD). Brazil, Cambodia, Democratic People's Republic of Korea, Equatorial Guinea, Malaysia, Solomon Islands, Timor-Leste, Venezuela, Zambia are listed as High Forest Cover with High Rates of Deforestation (HFHD).\n\nControl can be made by the companies. In 2018 the biggest palm oil traider, Wilmar, decided to control his suppliers for avoid deforestation. This is an important precedent\n\nIn Bolivia, deforestation in upper river basins has caused environmental problems, including soil erosion and declining water quality. An innovative project to try and remedy this situation involves landholders in upstream areas being paid by downstream water users to conserve forests. The landholders receive US$20 to conserve the trees, avoid polluting livestock practices, and enhance the biodiversity and forest carbon on their land. They also receive US$30, which purchases a beehive, to compensate for conservation for two hectares of water-sustaining forest for five years. Honey revenue per hectare of forest is US$5 per year, so within five years, the landholder has sold US$50 of honey. The project is being conducted by Fundación Natura Bolivia and Rare Conservation, with support from the Climate & Development Knowledge Network.\n\nTransferring rights over land from public domain to its indigenous inhabitants is argued to be a cost effective strategy to conserve forests. This includes the protection of such rights entitled in existing laws, such as India's Forest Rights Act. The transferring of such rights in China, perhaps the largest land reform in modern times, has been argued to have increased forest cover. In Brazil, forested areas given tenure to indigenous groups have even lower rates of clearing than national parks.\n\nNew methods are being developed to farm more intensively, such as high-yield hybrid crops, greenhouse, autonomous building gardens, and hydroponics. These methods are often dependent on chemical inputs to maintain necessary yields. In cyclic agriculture, cattle are grazed on farm land that is resting and rejuvenating. Cyclic agriculture actually increases the fertility of the soil. Intensive farming can also decrease soil nutrients by consuming at an accelerated rate the trace minerals needed for crop growth. The most promising approach, however, is the concept of food forests in permaculture, which consists of agroforestal systems carefully designed to mimic natural forests, with an emphasis on plant and animal species of interest for food, timber and other uses. These systems have low dependence on fossil fuels and agro-chemicals, are highly self-maintaining, highly productive, and with strong positive impact on soil and water quality, and biodiversity.\n\nThere are multiple methods that are appropriate and reliable for reducing and monitoring deforestation. One method is the \"visual interpretation of aerial photos or satellite imagery that is labor-intensive but does not require high-level training in computer image processing or extensive computational resources\". Another method includes hot-spot analysis (that is, locations of rapid change) using expert opinion or coarse resolution satellite data to identify locations for detailed digital analysis with high resolution satellite images. Deforestation is typically assessed by quantifying the amount of area deforested, measured at the present time.\nFrom an environmental point of view, quantifying the damage and its possible consequences is a more important task, while conservation efforts are more focused on forested land protection and development of land-use alternatives to avoid continued deforestation. Deforestation rate and total area deforested, have been widely used for monitoring deforestation in many regions, including the Brazilian Amazon deforestation monitoring by INPE. A global satellite view is available.\n\nEfforts to stop or slow deforestation have been attempted for many centuries because it has long been known that deforestation can cause environmental damage sufficient in some cases to cause societies to collapse. In Tonga, paramount rulers developed policies designed to prevent conflicts between short-term gains from converting forest to farmland and long-term problems forest loss would cause, while during the 17th and 18th centuries in Tokugawa, Japan, the shōguns developed a highly sophisticated system of long-term planning to stop and even reverse deforestation of the preceding centuries through substituting timber by other products and more efficient use of land that had been farmed for many centuries. In 16th-century Germany, landowners also developed silviculture to deal with the problem of deforestation. However, these policies tend to be limited to environments with \"good rainfall\", \"no dry season\" and \"very young soils\" (through volcanism or glaciation). This is because on older and less fertile soils trees grow too slowly for silviculture to be economic, whilst in areas with a strong dry season there is always a risk of forest fires destroying a tree crop before it matures.\n\nIn the areas where \"slash-and-burn\" is practiced, switching to \"slash-and-char\" would prevent the rapid deforestation and subsequent degradation of soils. The biochar thus created, given back to the soil, is not only a durable carbon sequestration method, but it also is an extremely beneficial amendment to the soil. Mixed with biomass it brings the creation of terra preta, one of the richest soils on the planet and the only one known to regenerate itself.\n\nCertification, as provided by global certification systems such as Programme for the Endorsement of Forest Certification and Forest Stewardship Council, contributes to tackling deforestation by creating market demand for timber from sustainably managed forests. According to the United Nations Food and Agriculture Organization (FAO), \"A major condition for the adoption of sustainable forest management is a demand for products that are produced sustainably and consumer willingness to pay for the higher costs entailed. Certification represents a shift from regulatory approaches to market incentives to promote sustainable forest management. By promoting the positive attributes of forest products from sustainably managed forests, certification focuses on the demand side of environmental conservation.\" Rainforest Rescue argues that the standards of organizations like FSC are too closely connected to timber industry interests and therefore do not guarantee environmentally and socially responsible forest management. In reality, monitoring systems are inadequate and various cases of fraud have been documented worldwide.\n\nSome nations have taken steps to help increase the number of trees on Earth. In 1981, China created National Tree Planting Day Forest and forest coverage had now reached 16.55% of China's land mass, as against only 12% two decades ago.\n\nUsing fuel from bamboo rather than wood results in cleaner burning, and since bamboo matures much faster than wood, deforestation is reduced as supply can be replenished faster.\n\nIn many parts of the world, especially in East Asian countries, reforestation and afforestation are increasing the area of forested lands. The amount of woodland has increased in 22 of the world's 50 most forested nations. Asia as a whole gained 1 million hectares of forest between 2000 and 2005. Tropical forest in El Salvador expanded more than 20% between 1992 and 2001. Based on these trends, one study projects that global forestation will increase by 10%—an area the size of India—by 2050.\n\nIn the People's Republic of China, where large scale destruction of forests has occurred, the government has in the past required that every able-bodied citizen between the ages of 11 and 60 plant three to five trees per year or do the equivalent amount of work in other forest services. The government claims that at least 1 billion trees have been planted in China every year since 1982. This is no longer required today, but 12 March of every year in China is the Planting Holiday. Also, it has introduced the Green Wall of China project, which aims to halt the expansion of the Gobi desert through the planting of trees. However, due to the large percentage of trees dying off after planting (up to 75%), the project is not very successful. There has been a 47-million-hectare increase in forest area in China since the 1970s. The total number of trees amounted to be about 35 billion and 4.55% of China's land mass increased in forest coverage. The forest coverage was 12% two decades ago and now is 16.55%.\n\nAn ambitious proposal for China is the Aerially Delivered Re-forestation and Erosion Control System and the proposed Sahara Forest Project coupled with the Seawater Greenhouse.\n\nIn Western countries, increasing consumer demand for wood products that have been produced and harvested in a sustainable manner is causing forest landowners and forest industries to become increasingly accountable for their forest management and timber harvesting practices.\n\nThe Arbor Day Foundation's Rain Forest Rescue program is a charity that helps to prevent deforestation. The charity uses donated money to buy up and preserve rainforest land before the lumber companies can buy it. The Arbor Day Foundation then protects the land from deforestation. This also locks in the way of life of the primitive tribes living on the forest land. Organizations such as Community Forestry International, Cool Earth, The Nature Conservancy, World Wide Fund for Nature, Conservation International, African Conservation Foundation and Greenpeace also focus on preserving forest habitats. Greenpeace in particular has also mapped out the forests that are still intact and published this information on the internet. World Resources Institute in turn has made a simpler thematic map showing the amount of forests present just before the age of man (8000 years ago) and the current (reduced) levels of forest. These maps mark the amount of afforestation required to repair the damage caused by people.\n\nIn order to acquire the world's demand for wood, it is suggested that high yielding forest plantations are suitable according to forest writers Botkins and Sedjo. Plantations that yield 10 cubic meters per hectare a year would supply enough wood for trading of 5% of the world's existing forestland. By contrast, natural forests produce about 1–2 cubic meters per hectare; therefore, 5–10 times more forestland would be required to meet demand. Forester Chad Oliver has suggested a forest mosaic with high-yield forest lands interspersed with conservation land.\n\nGlobally, planted forests increased from 4.1% to 7.0% of the total forest area between 1990 and 2015. Plantation forests made up 280 million ha in 2015, an increase of about 40 million ha in the last ten years. Globally, planted forests consist of about 18% exotic or introduced species while the rest are species native to the country where they are planted. In South America, Oceania, and East and Southern Africa, planted forests are dominated by introduced species: 88%, 75% and 65%, respectively. In North America, West and Central Asia, and Europe the proportions of introduced species in plantations are much lower at 1%, 3% and 8% of the total area planted, respectively.\n\nIn the country of Senegal, on the western coast of Africa, a movement headed by youths has helped to plant over 6 million mangrove trees. The trees will protect local villages from storm damages and will provide a habitat for local wildlife. The project started in 2008, and already the Senegalese government has been asked to establish rules and regulations that would protect the new mangrove forests.\n\nWhile demands for agricultural and urban use for the human population cause the preponderance of deforestation, military causes can also intrude. One example of deliberate deforestation played out in the U.S. zone of occupation in Germany after World War II ended in 1945. Before the onset of the Cold War, defeated Germany was still considered a potential future threat rather than a potential future ally. To address this threat, the victorious Allies made attempts to lower German industrial potential, of which forests were deemed an element. Sources in the U.S. government admitted that the purpose of this was that the \"ultimate destruction of the war potential of German forests\". As a consequence of the practice of clear-felling, deforestation resulted which could \"be replaced only by long forestry development over perhaps a century\".\n\nOperations in war can also cause deforestation. For example, in the 1945 Battle of Okinawa, bombardment and other combat operations reduced a lush tropical landscape into \"a vast field of mud, lead, decay and maggots\".\n\nDeforestation can also result from the intentional tactics of military forces. Clearing forest became an element in the Russian Empire's successful conquest of the Caucasus in the mid-19th century.\nThe British (during the Malayan Emergency) and the United States (in the Korean War and in the Vietnam War) used defoliants (like Agent Orange or others).\n\nDeforestation eliminates a great number of species of plants and animals which also often results in an increase in disease. Loss of native species allows new species to come to dominance. Often the destruction of predatory species can result in an increase in rodent populations which can carry plague. Additionally, erosion can produce pools of stagnant water that are perfect breeding grounds for mosquitos, well known vectors of malaria, yellow fever, nipah virus, and more. Deforestation can also create a path for non-native species to flourish such as certain types of snails, which have been correlated with an increase in schistosomiasis cases.\n\nDeforestation is occurring all over the world and has been coupled with an increase in the occurrence of disease outbreaks. In Malaysia, thousands of acres of forest have been cleared for pig farms. This has resulted in an increase in the zoonosis the Nipah virus. In Kenya, deforestation has led to an increase in malaria cases which is now the leading cause of morbidity and mortality the country. A 2017 study in the \"American Economic Review\" found that deforestation substantially increased the incidence of malaria in Nigeria.\n\nAnother pathway through which deforestation affects disease is the relocation and dispersion of disease-carrying hosts. This disease emergence pathway can be called \"range expansion\", whereby the host's range (and thereby the range of pathogens) expands to new geographic areas. Through deforestation, hosts and reservoir species are forced into neighboring habitats. Accompanying the reservoir species are pathogens that have the ability to find new hosts in previously unexposed regions. As these pathogens and species come into closer contact with humans, they are infected both directly and indirectly.\n\nA catastrophic example of range expansion is the 1998 outbreak of Nipah virus in Malaysia. For a number of years, deforestation, drought, and subsequent fires led to a dramatic geographic shift and density of fruit bats, a reservoir for Nipah virus. Deforestation reduced the available fruiting trees in the bats’ habitat, and they encroached on surrounding orchards which also happened to be the location of a large number of pigsties. The bats, through proximity spread the Nipah to pigs. While the virus infected the pigs, mortality was much lower than among humans, making the pigs a virulent host leading to the transmission of the virus to humans. This resulted in 265 reported cases of encephalitis, of which 105 resulted in death. This example provides an important lesson for the impact deforestation can have on human health.\n\nAnother example of range expansion due to deforestation and other anthropogenic habitat impacts includes the Capybara rodent in Paraguay. This rodent is the host of a number of zoonotic diseases and, while there has not yet been a human-borne outbreak due to the movement of this rodent into new regions, it offers an example of how habitat destruction through deforestation and subsequent movements of species is occurring regularly.\n\nA now well-developed theory is that the spread of HIV it is at least partially due deforestation. Rising populations created a food demand and with deforestation opening up new areas of the forest the hunters harvested a great deal of primate bushmeat, which is believed to be the origin of HIV.\n\n\n\n\n\n\n\n\n"}
{"id": "8900", "url": "https://en.wikipedia.org/wiki?curid=8900", "title": "Discrimination", "text": "Discrimination\n\nIn human social behavior, discrimination is prejudiced treatment or consideration of, or making a distinction towards, a being based on the group, class, or category to which they are perceived to belong. These include age, caste, criminal record, height, disability, family status, gender identity, gender expression, generation, genetic characteristics, marital status, nationality, color, race and ethnicity, religion, sex and sex characteristics, sexual orientation, social class, species, as well as other categories. Discrimination consists of treatment of an individual or group, based on their actual or perceived membership in a certain group or social category, \"in a way that is worse than the way people are usually treated\". It involves the group's initial reaction or interaction going on to influence the individual's actual behavior towards the group leader or the group, restricting members of one group from opportunities or privileges that are available to another group, leading to the exclusion of the individual or entities based on illogical or irrational decision making.\n\nDiscriminatory traditions, policies, ideas, practices and laws exist in many countries and institutions in every part of the world, including in territories where discrimination is generally looked down upon. In some places, controversial attempts such as quotas have been used to benefit those who are believed to be current or past victims of discrimination—but they have sometimes been called reverse discrimination.\n\nThe term \"discriminate\" appeared in the early 17th century in the English language. It is from the Latin \"discriminat-\" 'distinguished between', from the verb \"discriminare\", from \"discrimen\" 'distinction', from the verb \"discernere\". Since the American Civil War the term \"discrimination\" generally evolved in American English usage as an understanding of prejudicial treatment of an individual based solely on their race, later generalized as membership in a certain socially undesirable group or social category. The word \"discrimination\" derives from Latin, where the verb \"discrimire\" means \"to separate, to distinguish, to make a distinction\".\n\nMoral philosophers have defined discrimination as \"disadvantageous\" treatment or consideration. This is a comparative definition. An individual need not be actually harmed in order to be discriminated against. They just need to be treated \"worse\" than others for some arbitrary reason. If someone decides to donate to help orphan children, but decides to donate less, say, to black children out of a racist attitude, then they would be acting in a discriminatory way despite the fact that the people they discriminate against actually benefit by receiving a donation. In addition to this discrimination develops into a source of oppression. It is similar to the action of recognizing someone as 'different' so much that they are treated inhumanly and degraded.\n\nBased on realistic-conflict theory and social-identity theory, Rubin and Hewstone have highlighted a distinction among three types of discrimination:\n\nThe United Nations stance on discrimination includes the statement: \"Discriminatory behaviors take many forms, but they all involve some form of exclusion or rejection.\" International bodies United Nations Human Rights Council work towards helping ending discrimination around the world.\n\nAgeism or age discrimination is discrimination and stereotyping based on the grounds of someone's age. It is a set of beliefs, norms, and values which used to justify discrimination or subordination based on a person's age. Ageism is most often directed towards old people, or adolescents and children.\n\nAge discrimination in hiring has been shown to exist in the United States. Joanna Lahey, professor at The Bush School of Government and Public Service at Texas A&M, found that firms are more than 40% more likely to interview a young adult job applicant than an older job applicant. In Europe, Stijn Baert, Jennifer Norga, Yannick Thuy and Marieke Van Hecke, researchers at Ghent University, measured comparable ratios in Belgium. They found that age discrimination is heterogeneous by the activity older candidates undertook during their additional post-educational years. In Belgium, they are only discriminated if they have more years of inactivity or irrelevant employment.\n\nIn a survey for the University of Kent, England, 29% of respondents stated that they had suffered from age discrimination. This is a higher proportion than for gender or racial discrimination. Dominic Abrams, social psychology professor at the university, concluded that ageism is the most pervasive form of prejudice experienced in the UK population.\n\nAccording to UNICEF and Human Rights Watch, caste discrimination affects an estimated 250 million people worldwide. Discrimination based on caste, as perceived by UNICEF, is mainly prevalent in parts of Asia, (India, Sri Lanka, Bangladesh, China, Pakistan, Nepal, Japan), Africa and others. , there were 200 million Dalits or Scheduled Castes (formerly known as \"untouchables\") in India.\n\nDiscrimination can be directed towards persons based on their cultures or subcultures. In 2013, the Greater Manchester Police in the United Kingdom began to classify attacks on subcultures such as goths, emos, punks and metallers as hate crimes, in the same way it records abuses against people because of their religion, race, disability, sexual orientation or transgender identity. The decision followed the murder of Sophie Lancaster and beating of her boyfriend in 2007, who were attacked because they were goths.\n\nDiscrimination against people with disabilities in favor of people who are not is called ableism or disablism. Disability discrimination, which treats non-disabled individuals as the standard of 'normal living', results in public and private places and services, educational settings, and social services that are built to serve 'standard' people, thereby excluding those with various disabilities. Studies have shown that disabled people not only need employment in order to be provided with the opportunity to earn a living but they also need employment in order to sustain their mental health and well-being. Work fulfils a number of basic needs for an individual such as collective purpose, social contact, status, and activity. A person with a disability is often found to be socially isolated and work is one way to reduce his or her isolation.\n\nIn the United States, the Americans with Disabilities Act mandates the provision of equality of access to both buildings and services and is paralleled by similar acts in other countries, such as the Equality Act 2010 in the UK.\n\nDiversity of language is protected and respected by most nations who value cultural diversity. However, people are sometimes subjected to different treatment because their preferred language is associated with a particular group, class or category. Notable examples are the Anti-French sentiment in the United States as well as the Anti-Quebec sentiment in Canada targeting people who speak the French language. Commonly, the preferred language is just another attribute of separate ethnic groups. Discrimination exists if there is prejudicial treatment against a person or a group of people who either do or do not speak a particular language or languages. An example of this is when thousands of Wayúu Native Colombians were given derisive names and the same birth date, by government officials, during a campaign to provide them with identification cards. The issue was not discovered until many years later.\n\nAnother noteworthy example of linguistic discrimination is the backdrop to the Bengali Language Movement in erstwhile Pakistan, a political campaign that played a key role in the creation of Bangladesh. In 1948, Mohammad Ali Jinnah declared Urdu as the national language of Pakistan and branded those supporting the use of Bengali, the most widely spoken language in the state, as enemies of the state.\n\nLanguage discrimination is suggested to be labeled linguicism or logocism. Anti-discriminatory and inclusive efforts to accommodate persons who speak different languages or cannot have fluency in the country's predominant or \"official\" language, is bilingualism such as official documents in two languages, and multiculturalism in more than two languages.\n\nDiscrimination based on a person's name may also occur, with researchers suggesting that this form of discrimination is present based on a name's meaning, its pronunciation, its uniqueness, its gender affiliation, and its racial affiliation. Research has further shown that real world recruiters spend an average of just six seconds reviewing each résumé before making their initial \"fit/no fit\" screen-out decision and that a person's name is one of the six things they focus on most. France has made it illegal to view a person's name on a résumé when screening for the initial list of most qualified candidates. Great Britain, Germany, Sweden, and the Netherlands have also experimented with name-blind summary processes. Some apparent discrimination may be explained by other factors such as name frequency. The effects of name discrimination based on a name's fluency is subtle, small and subject to significantly changing norms.\n\nDiscrimination on the basis of nationality is usually included in employment laws (see above section for employment discrimination specifically). It is sometimes referred to as bound together with racial discrimination although it can be separate. It may vary from laws that stop refusals of hiring based on nationality, asking questions regarding origin, to prohibitions of firing, forced retirement, compensation and pay, etc., based on nationality.\n\nDiscrimination on the basis of nationality may show as a \"level of acceptance\" in a sport or work team regarding new team members and employees who differ from the nationality of the majority of team members.\n\nIn the GCC states, in the workplace, preferential treatment is given to full citizens, even though many of them lack experience or motivation to do the job. State benefits are also generally available for citizens only. Westerners might also get paid more than other expatriates.\n\nRacial and ethnic discrimination differentiates individuals on the basis of real and perceived racial and ethnic differences and leads to various forms of the ethnic penalty. It has been official government policy in several countries, such as South Africa during the apartheid era. Discriminatory policies towards ethnic minorities include the race-based discrimination against ethnic Indians and Chinese in Malaysia After the Vietnam war, many Vietnamese refugees moved to the United States, where they face discrimination.\n\n, aboriginal people (First Nations, such as the Métis, and Inuit) comprise 4 percent of Canada's population, but they account for 23.2 percent of the country's total federal prison population. According to the Australian government's June 2006 publication of prison statistics, Aborigines account for 24% of the overall prison population in Australia.\n\nIn 2004, the Māori made up just 15% of the total population of New Zealand but they accounted for 49.5% of all prisoners in the country. Māori were entering prison at eight times the rate of non-Māori. A quarter of the people in England's prisons are members of ethnic minority groups. The Equality and Human Rights Commission found that in England and Wales , a black person was five times more likely to be imprisoned than a white person. The discrepancy was attributed to \"decades of racial prejudice in the criminal justice system\".\n\nIn the United States, the racial profiling of minorities by law-enforcement officials has been called racial discrimination. Within the criminal justice system of the United States, minorities are convicted and imprisoned disproportionately when compared to the majority. As early as 1866, the Civil Rights Act and Civil Rights Act of 1871 provided a remedy for intentional racism in employment by private employers and state and local public employers. The Civil Rights Act of 1991 expanded the damages available in Title VII cases and granted Title VII plaintiffs the right to a jury trial.\n\nRacial discrimination in hiring has been shown to exist in both the United States and Europe. Using a field experiment, Marianne Bertrand and Sendhil Mullainathan showed that applications from job candidates with white-sounding names received 50 percent more callbacks for interviews than those with African-American-sounding names in the United States at the start of this millennium. A 2009 study by Devah Pager, Bruce Western, and Bart Bonikowski found that black applicants to low-wage jobs were half as likely as identically qualified white applicants to receive callbacks or job offers. More recently, Stijn Baert, Bart Cockx, Niels Gheyle and Cora Vandamme replicated and extended their field experiment in Belgium, Europe. They found that racial discrimination in the labour market is heterogeneous by the labour market tightness in the occupation: compared to natives, candidates with a foreign-sounding name are equally often invited to a job interview in Belgium if they apply for occupations for which vacancies are difficult to fill, but they have to send twice as many applications for occupations for which labor market tightness is low.\n\nRegional or geographic discrimination is a form of discrimination that is based on the region in which a person lives or the region in which a person was born. It differs from national discrimination because it may not be based on national borders or the country in which the victim lives, instead, it is based on prejudices against a specific region of one or more countries. Examples include discrimination against Chinese people who were born in regions of the countryside that are far away from cities that are located within China, and discrimination against Americans who are from the southern or northern regions of the United States. It is often accompanied by discrimination that is based on accent, dialect, or cultural differences.\n\nReligious discrimination is valuing or treating people or groups differently because of what they do or do not believe in or because of their feelings towards a given religion. For instance, the indigenous Christian population of the Balkans, known as the \"rayah\" or the \"protected flock\", was subjected to discrimination under the Ottoman Kanun–i–Rayah. The word is sometimes translated as 'cattle' rather than 'flock' or 'subjects' in order to emphasize the Christian population's inferior status to that of the Muslim rayah.\n\nRestrictions on the types of occupations that Jewish people could hold were imposed by Christian authorities. Local rulers and church officials closed many professions to religious Jews, pushing them into marginal roles that were considered socially inferior, such as tax and rent collecting and moneylending, occupations that were only tolerated as a \"necessary evil\". The number of Jews who were permitted to reside in different places was limited; they were concentrated in ghettos and banned from owning land. In Saudi Arabia, non-Muslims are not allowed to publicly practice their religions and they cannot enter Mecca and Medina. Furthermore, private non-Muslim religious gatherings might be raided by the religious police.\n\nIn a 1979 consultation on the issue, the United States commission on civil rights defined religious discrimination in relation to the civil rights which are guaranteed by the Fourteenth Amendment. Whereas religious civil liberties, such as the right to hold or not to hold a religious belief, are essential for Freedom of Religion (in the United States as secured by the First Amendment), religious discrimination occurs when someone is denied \"equal protection under the law, equality of status under the law, equal treatment in the administration of justice, and equality of opportunity and access to employment, education, housing, public services and facilities, and public accommodation because of their exercise of their right to religious freedom\".\n\nThough gender discrimination and sexism refer to beliefs and attitudes in relation to the gender of a person, such beliefs and attitudes are of a social nature and do not, normally, carry any legal consequences. Sex discrimination, on the other hand, may have legal consequences. Though what constitutes sex discrimination varies between countries, the essence is that it is an adverse action taken by one person against another person that would not have occurred had the person been of another sex. Discrimination of that nature is considered a form of prejudice and in certain enumerated circumstances is illegal in many countries.\n\nSexual discrimination can arise in different contexts. For instance, an employee may be discriminated against by being asked discriminatory questions during a job interview, or by an employer not hiring or promoting, unequally paying, or wrongfully terminating, an employee based on their gender. Sexual discrimination can also arise when the dominant group holds a bias against the minority group. One such example is Wikipedia. In the Wikipedian community, around 13 percent of registered users are women. This creates gender imbalances, and leaves room for systemic bias. Women are not only more harshly scrutinized, but the representation of women authors are also overlooked. Relative to men, across all source lists, women have a 2.6 greater odds of omission in Wikipedia. In an educational setting, there could be claims that a student was excluded from an educational institution, program, opportunity, loan, student group, or scholarship because of their gender. In the housing setting, there could be claims that a person was refused negotiations on seeking a house, contracting/leasing a house or getting a loan based on their gender. Another setting where there have been claims of gender discrimination is banking; for example if one is refused credit or is offered unequal loan terms based on one's gender. As with other forms of unlawful discrimination, there are two types of sex discrimination – direct discrimination and indirect discrimination. Direct sex discrimination is fairly easy to spot – 'Barmaid wanted', but indirect sex discrimination, where an unnecessary requirement puts one sex at a disproportionate disadvantage compared to the opposite sex, is sometimes less easy to spot, although some are obvious – 'Bar person wanted – must look good in a mini skirt'. Another setting where there is usually gender discrimination is when one is refused to extend their credit, refused approval of credit/loan process, and if there is a burden of unequal loan terms based on one's gender.\n\nSocially, sexual differences have been used to justify different roles for men and women, in some cases giving rise to claims of primary and secondary roles. While there are alleged non-physical differences between men and women, major reviews of the academic literature on gender difference find only a tiny minority of characteristics where there are consistent psychological differences between men and women, and these relate directly to experiences grounded in biological difference. Unfair discrimination usually follows the gender stereotyping held by a society.\n\nThe United Nations had concluded that women often experience a \"glass ceiling\" and that there are no societies in which women enjoy the same opportunities as men. The term \"glass ceiling\" is used to describe a perceived barrier to advancement in employment based on discrimination, especially sex discrimination. In the United States in 1995, the Glass Ceiling Commission, a government-funded group, stated: \"Over half of all Master's degrees are now awarded to women, yet 95% of senior-level managers, of the top Fortune 1000 industrial and 500 service companies are men. Of them, 97% are white.\" In its report, it recommended affirmative action, which is the consideration of an employee's gender and race in hiring and promotion decisions, as a means to end this form of discrimination. , women accounted for 51% of workers in high-paying management, professional, and related occupations. They outnumbered men in such occupations as public relations managers, financial managers, and human resource managers.\n\nIn addition, women are found to experience a sticky floor. While a glass ceiling implies that women are less like to reach the top of the job ladder, a sticky floor is defined as the pattern that women are, compared to men, less likely to start to climb the job ladder. A sticky floor is related to gender differences at the bottom of the wage distribution. It might be explained by both employer discrimination and gender differences in career aspirations.\n\nIntersex persons experience discrimination due to innate, atypical sex characteristics. Multiple jurisdictions now protect individuals on grounds of \"intersex status\" or \"sex characteristics\". South Africa was the first country to explicitly add intersex to legislation, as part of the attribute of 'sex'. Australia was the first country to add an independent attribute, of 'intersex status'. Malta was the first to adopt a broader framework of 'sex characteristics', through legislation that also ended modifications to the sex characteristics of minors undertaken for social and cultural reasons.\n\nTransgender individuals, whether trans women, trans men, or non-binary, often experience transphobic problems that often lead to dismissals, underachievement, difficulty in finding a job, social isolation, and, occasionally, violent attacks against them.\n\nNevertheless, the problem of gender discrimination does not stop at transgender individuals or with women. Men are often the victim in certain areas of employment as men begin to seek work in office and childcare settings traditionally perceived as \"women's jobs\". One such situation seems to be evident in a recent case concerning alleged YMCA discrimination and a Federal Court Case in Texas. The case actually involves alleged discrimination against both men and black people in childcare, even when they pass the same strict background tests and other standards of employment. It is currently being contended in federal court, as of fall 2009.\n\nDiscrimination in slasher films is relevant. Gloria Cowan had a research group study on 57 different slasher films. Their results showed that the non-surviving females were more frequently sexual than the surviving females and the non-surviving males. Surviving as a female slasher victim was strongly associated with the absence of sexual behavior. In slasher films, the message appears to be that sexual women get killed and only the pure women survive, thus reinforcing the idea that female sexuality can be costly.\n\nOne's sexual orientation is a \"predilection for homosexuality, heterosexuality, or bisexuality\". Like most minority groups, homosexuals and bisexuals are vulnerable to prejudice and discrimination from the majority group. They may experience hatred from others because of their sexuality; a term for such hatred based upon one's sexual orientation is often called homophobia. Many continue to hold negative feelings towards those with non-heterosexual orientations and will discriminate against people who have them or are thought to have them. People of other uncommon sexual orientations also experience discrimination. One study found its sample of heterosexuals to be more prejudiced against asexual people than to homosexual or bisexual people.\n\nEmployment discrimination based on sexual orientation varies by country. Revealing a lesbian sexual orientation (by means of mentioning an engagement in a rainbow organisation or by mentioning one's partner name) lowers employment opportunities in Cyprus and Greece but overall, it has no negative effect in Sweden and Belgium. In the latter country, even a positive effect of revealing a lesbian sexual orientation is found for women at their fertile ages.\n\nBesides these academic studies, in 2009, ILGA published a report based on research carried out by Daniel Ottosson at Södertörn University College, Stockholm, Sweden. This research found that of the 80 countries around the world that continue to consider homosexuality illegal, five carry the death penalty for homosexual activity, and two do in some regions of the country. In the report, this is described as \"State sponsored homophobia\". This happens in Islamic states, or in two cases regions under Islamic authority. On February 5, 2005, the IRIN issued a reported titled \"Iraq: Male homosexuality still a taboo\". The article stated, among other things that honor killings by Iraqis against a gay family member are common and given some legal protection. In August 2009, Human Rights Watch published an extensive report detailing torture of men accused of being gay in Iraq, including the blocking of men's anuses with glue and then giving the men laxatives. Although gay marriage has been legal in South Africa since 2006, same-sex unions are often condemned as \"un-African\". Research conducted in 2009 shows 86% of black lesbians from the Western Cape live in fear of sexual assault.\nA number of countries, especially those in the Western world, have passed measures to alleviate discrimination against sexual minorities, including laws against anti-gay hate crimes and workplace discrimination. Some have also legalized same-sex marriage or civil unions in order to grant same-sex couples the same protections and benefits as opposite-sex couples. In 2011, the United Nations passed its first resolution recognizing LGBT rights.\n\nSpeciesism is intolerance or discrimination on the basis of species membership. It involves treating members of one species as morally more important than members of other species even when their moral interests are equivalent. It is thought that speciesism plays a role in inspiring or justifying cruelty in the forms of factory farming, the use of animals for entertainment such as in bullfighting and rodeos, the taking of animals' fur and skin, experimentation on animals, and the refusal to aid wild animals that suffer due to natural processes.\n\nDrug use discrimination is the unequal treatment people experience because of the drugs they use. People who use or have used illicit drugs may face discrimination in employment, welfare, housing, child custody, and travel, in addition to imprisonment, asset forfeiture, and in some cases forced labor, torture, and execution. Though often prejudicially stereotyped as deviants and misfits, most drug users are well-adjusted and productive members of society. Drug prohibitions may have been partly motivated by racism and other prejudice against minorities, and racial disparities have been found to exist in the enforcement and prosecution of drug laws. Discrimination due to illicit drug use was the most commonly reported type of discrimination among Blacks and Latinos in a 2003 study of minority drug users in New York City, double to triple that due to race. People who use legal drugs such as tobacco and prescription medications may also face discrimination.\n\nIdeas of self-ownership and cognitive liberty affirm rights to use drugs, whether for medicine recreation, or spiritual fulfilment. Those espousing such ideas question the legality of drug prohibition and cite the rights and freedoms enshrined in such documents as the U.S. Declaration of Independence, the U.S. Constitution and Bill of Rights, the European Convention on Human Rights, and the Universal Declaration of Human Rights, as protecting personal drug choices. They are inspired by and see themselves following in the tradition of those who have struggled against other forms of discrimination in the past.\n\nDrug policy reform organizations such as the Drug Policy Alliance, the Drug Equality Alliance, the Transform Drug Policy Foundation, and the Beckley Foundation have highlighted the issue of stigma and discrimination in drug policy. The Partnership for Drug-Free Kids also recognizes this issue and shares on its website stories that \"break through the stigma and discrimination that people with drug or drinking problems often face.\"\n\nA report issued by the Global Commission on Drug Policy, critical of the global war on drugs, states, under \"Undermining Human Rights, Fostering Discrimination\":\n\nPunitive approaches to drug policy are severely undermining human rights in every region of the world. They lead to the erosion of civil liberties and fair trial standards, the stigmatization of individuals and groups – particularly women, young people, and ethnic minorities – and the imposition of abusive and inhumane punishments.\n\nAlthough still illegal at the federal level, about half of U.S. states have legalized marijuana for medical use and several of those states have laws, or are considering legislation, specifically protecting medical marijuana patients from discrimination in such areas as education, employment, housing, child custody, and organ transplantation.\n\nIn the US, a government policy which is known as affirmative action was instituted in order to encourage employers and universities to seek out and accept groups such as African Americans and women, who have been subject to discrimination for a long time.\n\nSome attempts at antidiscrimination have been criticized as reverse discrimination. In particular, minority quota systems such as affirmative action) may discriminate against members of a dominant or majority group or members of other minority groups. In its opposition to race preferences, the American Civil Rights Institute's Ward Connerly stated, \"There is nothing positive, affirmative, or equal about 'affirmative action' programs that give preference to some groups based on race.\"\n\nThere are instances in which groups, victims of racialistic practices, have themselves applied doctrines with racist implications in their struggle for freedom. Such an attitude is a secondary phenomenon, a reaction stemming from one's search for an identity which prior racist theory and racialistic practices denied them. The new forms of racist ideology, resulting from this prior exploitation, are a product of a political struggle.\n\nAustralia\nCanada\nHong Kong\nIsrael\n\nNetherlands\n\nUnited Kingdom\nUnited States\n\nImportant UN documents addressing discrimination include:\n\nSocial theories such as egalitarianism assert that social equality should prevail. In some societies, including most developed countries, each individual's civil rights include the right to be free from government sponsored social discrimination. Due to a belief in the capacity to perceive pain or suffering shared by all animals, \"abolitionist\" or \"vegan\" egalitarianism maintains that the interests of every individual (regardless its species), warrant equal consideration with the interests of humans, and that not doing so is \"speciesist\".\n\nDiscrimination, in labeling theory, takes form as mental categorization of minorities and the use of stereotype. This theory describes difference as deviance from the norm, which results in internal devaluation and social stigma that may be seen as discrimination. It is started by describing a \"natural\" social order. It is distinguished between the fundamental principle of fascism and social democracy. The Nazis in 1930s-era Germany and the pre-1990 Apartheid government of South Africa used racially discriminatory agendas for their political ends. This practice continues with some present day governments.\n\nEconomist Yanis Varoufakis (2013) argues that \"discrimination based on utterly arbitrary characteristics evolves quickly and systematically in the experimental laboratory\", and that neither classical game theory nor neoclassical economics can explain this. Varoufakis and Shaun Hargreaves-Heap (2002) ran an experiment where volunteers played a computer-mediated, multiround hawk-dove game (HD game). At the start of each session, each participant was assigned a color at random, either red or blue. At each round, each player learned the color assigned to his or her opponent, but nothing else about the opponent. Hargreaves-Heap and Varoufakis found that the players' behavior within a session frequently developed a discriminatory convention, giving a Nash equilibrium where players of one color (the \"advantaged\" color) consistently played the aggressive \"hawk\" strategy against players of the other, \"disadvantaged\" color, who played the acquiescent \"dove\" strategy against the advantaged color. Players of both colors used a mixed strategy when playing against players assigned the same color as their own.\n\nThe experimenters then added a cooperation option to the game, and found that disadvantaged players usually cooperated with each other, while advantaged players usually did not. They state that while the equilibria reached in the original HD game are predicted by evolutionary game theory, game theory does not explain the emergence of cooperation in the disadvantaged group. Citing earlier psychological work of Matthew Rabin, they hypothesize that a norm of differing entitlements emerges across the two groups, and that this norm could define a \"fairness\" equilibrium within the disadvantaged group.\n\nIt is debated as to whether or not markets discourage discrimination brought about by the state. One argument is that since discrimination restricts access to customers and incurs additional expense, market logic will punish discrimination. Opposition by companies to \"Jim Crow\" segregation laws is an example of this. An alternative argument is that markets don't necessarily undermine discrimination, as it is argued that if discrimination is profitable by catering to the \"tastes\" of individuals (which is the point of the market), then the market will not punish discrimination. It is argued that micro economic analysis of discrimination uses unusual methods to determine its effects (using explicit treatment of production functions) and that the very existence of discrimination in employment (defined as wages which differ from marginal product of the discriminated employees) in the long run contradicts claims that the market will function well and punish discrimination. Furthermore, economic actors may have imperfect information and statistical discrimination may occur rationally and without prejudice.\n\n"}
{"id": "25613", "url": "https://en.wikipedia.org/wiki?curid=25613", "title": "Racism", "text": "Racism\n\nRacism is the belief that groups of humans possess different behavioral traits corresponding to physical appearance, and can be divided based on the superiority of one race over another. It may also mean prejudice, discrimination, or antagonism directed against other people because they are of a different race or ethnicity. Modern variants of racism are often based in social perceptions of biological differences between peoples. These views can take the form of social actions, practices or beliefs, or political systems in which different races are ranked as inherently superior or inferior to each other, based on presumed shared inheritable traits, abilities, or qualities.\n\nIn terms of political systems (e.g., apartheid) that support the expression of prejudice or aversion in discriminatory practices or laws, racist ideology may include associated social aspects such as nativism, xenophobia, otherness, segregation, hierarchical ranking, and supremacism.\n\nWhile the concepts of race and ethnicity are considered to be separate in contemporary social science, the two terms have a long history of equivalence in popular usage and older social science literature. \"Ethnicity\" is often used in a sense close to one traditionally attributed to \"race\": the division of human groups based on qualities assumed to be essential or innate to the group (e.g. shared ancestry or shared behavior). Therefore, \"racism\" and \"racial discrimination\" are often used to describe discrimination on an ethnic or cultural basis, independent of whether these differences are described as racial. According to a United Nations convention on racial discrimination, there is no distinction between the terms \"racial\" and \"ethnic\" discrimination. The UN Convention further concludes that superiority based on racial differentiation is scientifically false, morally condemnable, socially unjust and dangerous. The Convention also declared that there is no justification for racial discrimination, anywhere, in theory or in practice.\n\nRacism is a relatively modern concept, arising in the European age of imperialism, the subsequent growth of capitalism, and especially the Atlantic slave trade, of which it was a major driving force. It was also a major force behind racial segregation especially in the United States in the nineteenth and early twentieth centuries and South Africa under apartheid; 19th and 20th century racism in Western culture is particularly well documented and constitutes a reference point in studies and discourses about racism. Racism has played a role in genocides such as the Holocaust, and the Armenian genocide, and colonial projects like the European colonization of the Americas, Africa, and Asia. Indigenous peoples have been –and are– often subject to racist attitudes.\n\nIn the 19th century, many scientists subscribed to the belief that the human population can be divided into races. The term \"racism\" is a noun describing the state of being racist, i.e., subscribing to the belief that the human population can or should be classified into races with differential abilities and dispositions, which in turn may motivate a political ideology in which rights and privileges are differentially distributed based on racial categories. The origin of the root word \"race\" is not clear. Linguists generally agree that it came to the English language from Middle French, but there is no such agreement on how it generally came into Latin-based languages. A recent proposal is that it derives from the Arabic \"ra's\", which means \"head, beginning, origin\" or the Hebrew \"rosh\", which has a similar meaning. Early race theorists generally held the view that some races were inferior to others and they consequently believed that the differential treatment of races was fully justified. These early theories guided pseudo-scientific research assumptions; the collective endeavors to adequately define and form hypotheses about racial differences are generally termed scientific racism, though this term is a misnomer, due to the lack of any actual science backing the claims.\n\nToday, most biologists, anthropologists, and sociologists reject a taxonomy of races in favor of more specific and/or empirically verifiable criteria, such as geography, ethnicity, or a history of endogamy. To date, there is little evidence in human genome research which indicates that race can be defined in such a way as to be useful in determining a genetic classification of humans.\n\nAn entry in the \"Oxford English Dictionary\" (2008) defines racialism as \"[a]n earlier term than racism, but now largely superseded by it\", and cites the term \"racialism\" in a 1902 quote. The revised Oxford English Dictionary cites the shortened term \"racism\" in a quote from the following year, 1903. It was first defined by the Oxford English Dictionary (2nd edition, 1989) as \"[t]he theory that distinctive human characteristics and abilities are determined by race\"; the same dictionary termed \"racism\" a synonym of \"racialism\": \"belief in the superiority of a particular race\". By the end of World War II, \"racism\" had acquired the same supremacist connotations formerly associated with \"racialism\": \"racism\" now implied racial discrimination, racial supremacism, and a harmful intent. (The term \"race hatred\" had also been used by sociologist Frederick Hertz in the late 1920s.)\n\nAs its history indicates, the popular use of the word \"racism\" is relatively recent. The word came into widespread usage in the Western world in the 1930s, when it was used to describe the social and political ideology of Nazism, which treated \"race\" as a naturally given political unit. It is commonly agreed that racism existed before the coinage of the word, but there is not a wide agreement on a single definition of what racism is and what it is not. Today, some scholars of racism prefer to use the concept in the plural \"racisms\", in order to emphasize its many different forms that do not easily fall under a single definition. They also argue that different forms of racism have characterized different historical periods and geographical areas. Garner (2009: p. 11) summarizes different existing definitions of racism and identifies three common elements contained in those definitions of racism. First, a historical, hierarchical power relationship between groups; second, a set of ideas (an ideology) about racial differences; and, third, discriminatory actions (practices).\n\nThough many countries around the globe have passed laws related to race and discrimination, the first significant international human rights instrument developed by the United Nations (UN) was the Universal Declaration of Human Rights (UDHR), which was adopted by the United Nations General Assembly in 1948. The UDHR recognizes that if people are to be treated with dignity, they require economic rights, social rights including education, and the rights to cultural and political participation and civil liberty. It further states that everyone is entitled to these rights \"without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status\".\n\nThe UN does not define \"racism\"; however, it does define \"racial discrimination\". According to the 1965 UN International Convention on the Elimination of All Forms of Racial Discrimination,\n\nThe term \"racial discrimination\" shall mean any distinction, exclusion, restriction, or preference based on race, colour, descent, or national or ethnic origin that has the purpose or effect of nullifying or impairing the recognition, enjoyment or exercise, on an equal footing, of human rights and fundamental freedoms in the political, economic, social, cultural or any other field of public life.\nIn their 1978 United Nations Educational, Scientific, and Cultural Organization (UNESCO) Declaration on Race and Racial Prejudice (Article 1), the UN states, \"All human beings belong to a single species and are descended from a common stock. They are born equal in dignity and rights and all form an integral part of humanity.\"\n\nThe UN definition of racial discrimination does not make any distinction between discrimination based on ethnicity and race, in part because the distinction between the two has been a matter of debate among academics, including anthropologists. Similarly, in British law, the phrase \"racial group\" means \"any group of people who are defined by reference to their race, colour, nationality (including citizenship) or ethnic or national origin\".\n\nIn Norway, the word \"race\" has been removed from national laws concerning discrimination because the use of the phrase is considered problematic and unethical. The Norwegian Anti-Discrimination Act bans discrimination based on ethnicity, national origin, descent, and skin color.\n\nSociologists, in general, recognize \"race\" as a social construct. This means that, although the concepts of race and racism are based on observable biological characteristics, any conclusions drawn about race on the basis of those observations are heavily influenced by cultural ideologies. Racism, as an ideology, exists in a society at both the individual and institutional level.\n\nWhile much of the research and work on racism during the last half-century or so has concentrated on \"white racism\" in the Western world, historical accounts of race-based social practices can be found across the globe. Thus, racism can be broadly defined to encompass individual and group prejudices and acts of discrimination that result in material and cultural advantages conferred on a majority or a dominant social group. So-called \"white racism\" focuses on societies in which white populations are the majority or the dominant social group. In studies of these majority white societies, the aggregate of material and cultural advantages is usually termed \"white privilege\".\n\nRace and race relations are prominent areas of study in sociology and economics. Much of the sociological literature focuses on white racism. Some of the earliest sociological works on racism were penned by sociologist W. E. B. Du Bois, the first African American to earn a doctoral degree from Harvard University. Du Bois wrote, \"[t]he problem of the twentieth century is the problem of the color line.\" Wellman (1993) defines racism as \"culturally sanctioned beliefs, which, regardless of intentions involved, defend the advantages whites have because of the subordinated position of racial minorities\". In both sociology and economics, the outcomes of racist actions are often measured by the inequality in income, wealth, net worth, and access to other cultural resources (such as education), between racial groups.\n\nIn sociology and social psychology, racial identity and the acquisition of that identity, is often used as a variable in racism studies. Racial ideologies and racial identity affect individuals' perception of race and discrimination. Cazenave and Maddern (1999) define racism as \"a highly organized system of 'race'-based group privilege that operates at every level of society and is held together by a sophisticated ideology of color/'race' supremacy. Racial centrality (the extent to which a culture recognizes individuals' racial identity) appears to affect the degree of discrimination African American young adults perceive whereas racial ideology may buffer the detrimental emotional effects of that discrimination.\" Sellers and Shelton (2003) found that a relationship between racial discrimination and emotional distress was moderated by racial ideology and social beliefs.\n\nSome sociologists also argue that, particularly in the West, where racism is often negatively sanctioned in society, racism has changed from being a blatant to a more covert expression of racial prejudice. The \"newer\" (more hidden and less easily detectable) forms of racism – which can be considered embedded in social processes and structures – are more difficult to explore as well as challenge. It has been suggested that, while in many countries overt or explicit racism has become increasingly taboo, even among those who display egalitarian explicit attitudes, an implicit or aversive racism is still maintained subconsciously.\n\nThis process has been studied extensively in social psychology as implicit associations and implicit attitudes, a component of implicit cognition. Implicit attitudes are evaluations that occur without conscious awareness towards an attitude object or the self. These evaluations are generally either favorable or unfavorable. They come about from various influences in the individual experience. Implicit attitudes are not consciously identified (or they are inaccurately identified) traces of past experience that mediate favorable or unfavorable feelings, thoughts, or actions towards social objects. These feelings, thoughts, or actions have an influence on behavior of which the individual may not be aware.\n\nTherefore, subconscious racism can influence our visual processing and how our minds work when we are subliminally exposed to faces of different colors. In thinking about crime, for example, social psychologist Jennifer L. Eberhardt (2004) of Stanford University holds that, \"blackness is so associated with crime you're ready to pick out these crime objects.\" Such exposures influence our minds and they can cause subconscious racism in our behavior towards other people or even towards objects. Thus, racist thoughts and actions can arise from stereotypes and fears of which we are not aware.\n\nLanguage, linguistics, and discourse are active areas of study in the humanities, along with literature and the arts. Discourse analysis seeks to reveal the meaning of race and the actions of racists through careful study of the ways in which these factors of human society are described and discussed in various written and oral works. For example, Van Dijk (1992) examines the different ways in which descriptions of racism and racist actions are depicted by the perpetrators of such actions as well as by their victims. He notes that when descriptions of actions have negative implications for the majority, and especially for white elites, they are often seen as controversial and such controversial interpretations are typically marked with quotation marks or they are greeted with expressions of distance or doubt. The previously cited book, \"The Souls of Black Folk\" by W.E.B. Du Bois, represents early African-American literature that describes the author's experiences with racism when he was traveling in the South as an African American.\n\nMuch American fictional literature has focused on issues of racism and the black \"racial experience\" in the US, including works written by whites, such as \"Uncle Tom's Cabin\", \"To Kill a Mockingbird\", and \"Imitation of Life\", or even the non-fiction work \"Black Like Me\". These books, and others like them, feed into what has been called the \"white savior narrative in film\", in which the heroes and heroines are white even though the story is about things that happen to black characters. Textual analysis of such writings can contrast sharply with black authors' descriptions of African Americans and their experiences in US society. African American writers have sometimes been portrayed in African-American studies as retreating from racial issues when they write about \"whiteness\", while others identify this as an African American literary tradition called \"the literature of white estrangement\", part of a multi-pronged effort to challenge and dismantle white supremacy in the US.\n\nAccording to dictionaries, the word is commonly used to describe prejudice and discrimination based on race.\n\nRacism can also be said to describe a condition in society in which a dominant racial group benefits from the oppression of others, whether that group wants such benefits or not. Foucauldian scholar Ladelle McWhorter, in her 2009 book, \"Racism and Sexual Oppression in Anglo-America: A Genealogy\", posits modern racism similarly, focusing on the notion of a dominant group, usually whites, vying for racial purity and progress, rather than an overt or obvious ideology focused on the oppression of nonwhites.\n\nIn popular usage, as in some academic usage, little distinction is made between \"racism\" and \"ethnocentrism\". Often, the two are listed together as \"racial and ethnic\" in describing some action or outcome that is associated with prejudice within a majority or dominant group in society. Furthermore, the meaning of the term racism is often conflated with the terms prejudice, bigotry, and discrimination. Racism is a complex concept that can involve each of those; but it cannot be equated with, nor is it synonymous, with these other terms.\n\nThe term is often used in relation to what is seen as prejudice within a minority or subjugated group, as in the concept of reverse racism. \"Reverse racism\" is a concept often used to describe acts of discrimination or hostility against members of a dominant racial or ethnic group while favoring members of minority groups. This concept has been used especially in the United States in debates over color-conscious policies (such as affirmative action) intended to remedy racial inequalities. Those who campaign for the interests of ethnic minorities commonly reject the concept of reverse racism. Scholars also commonly define racism not only in terms of individual prejudice, but also in terms of a power structure that protects the interests of the dominant culture and actively discriminates against ethnic minorities. From this perspective, while members of ethnic minorities may be prejudiced against members of the dominant culture, they lack the political and economic power to actively oppress them, and they are therefore not practicing \"racism\".\n\nThe ideology underlying racism can manifest in many aspects of social life. Such aspects are described in this section, although the list is not exhaustive.\n\nAversive racism is a form of implicit racism, in which a person's unconscious negative evaluations of racial or ethnic minorities are realized by a persistent avoidance of interaction with other racial and ethnic groups. As opposed to traditional, overt racism, which is characterized by overt hatred for and explicit discrimination against racial/ethnic minorities, aversive racism is characterized by more complex, ambivalent expressions and attitudes. Aversive racism is similar in implications to the concept of symbolic or modern racism (described below), which is also a form of implicit, unconscious, or covert attitude which results in unconscious forms of discrimination.\n\nThe term was coined by Joel Kovel to describe the subtle racial behaviors of any ethnic or racial group who rationalize their aversion to a particular group by appeal to rules or stereotypes. People who behave in an aversively racial way may profess egalitarian beliefs, and will often deny their racially motivated behavior; nevertheless they change their behavior when dealing with a member of another race or ethnic group than the one they belong to. The motivation for the change is thought to be implicit or subconscious. Experiments have provided empirical support for the existence of aversive racism. Aversive racism has been shown to have potentially serious implications for decision making in employment, in legal decisions and in helping behavior.\n\nIn relation to racism, color blindness is the disregard of racial characteristics in social interaction, for example in the rejection of affirmative action, as a way to address the results of past patterns of discrimination. Critics of this attitude argue that by refusing to attend to racial disparities, racial color blindness in fact unconsciously perpetuates the patterns that produce racial inequality.\n\nEduardo Bonilla-Silva argues that color blind racism arises from an \"abstract liberalism, biologization of culture, naturalization of racial matters, and minimization of racism\". Color blind practices are \"subtle, institutional, and apparently nonracial\" because race is explicitly ignored in decision-making. If race is disregarded in predominantly white populations, for example, whiteness becomes the normative standard, whereas people of color are othered, and the racism these individuals experience may be minimized or erased. At an individual level, people with \"color blind prejudice\" reject racist ideology, but also reject systemic policies intended to fix institutional racism.\n\nCultural racism manifests as societal beliefs and customs that promote the assumption that the products of a given culture, including the language and traditions of that culture, are superior to those of other cultures. It shares a great deal with xenophobia, which is often characterised by fear of, or aggression toward, members of an outgroup by members of an ingroup.\nIn that sense it is also similar to communalism as used in South Asia.\n\nCultural racism exists when there is a widespread acceptance of stereotypes concerning different ethnic or population groups. Whereas racism can be characterised by the belief that one race is inherently superior to another, cultural racism can be characterised by the belief that one culture is inherently superior to another.\n\nHistorical economic or social disparity is alleged to be a form of discrimination caused by past racism and historical reasons, affecting the present generation through deficits in the formal education and kinds of preparation in previous generations, and through primarily unconscious racist attitudes and actions on members of the general population.\n\nIn 2011, Bank of America agreed to pay $335 million to settle a federal government claim that its mortgage division, Countrywide Financial, discriminated against black and Hispanic homebuyers.\n\nDuring the Spanish colonial period, Spaniards developed a complex caste system based on race, which was used for social control, and which also determined a person's importance in society. While many Latin American countries have long since rendered the system officially illegal through legislation, usually at the time of their independence, prejudice based on degrees of perceived racial distance from European ancestry combined with one's socioeconomic status remain, an echo of the colonial caste system.\n\nInstitutional racism (also known as structural racism, state racism or systemic racism) is racial discrimination by governments, corporations, religions, or educational institutions or other large organizations with the power to influence the lives of many individuals. Stokely Carmichael is credited for coining the phrase \"institutional racism\" in the late 1960s. He defined the term as \"the collective failure of an organization to provide an appropriate and professional service to people because of their colour, culture or ethnic origin\".\n\nMaulana Karenga argued that racism constituted the destruction of culture, language, religion, and human possibility and that the effects of racism were \"the morally monstrous destruction of human possibility involved redefining African humanity to the world, poisoning past, present and future relations with others who only know us through this stereotyping and thus damaging the truly human relations among peoples\".\n\nOthering is the term used by some to describe a system of discrimination whereby the characteristics of a group are used to distinguish them as separate from the norm.\n\nOthering plays a fundamental role in the history and continuation of racism. To objectify a culture as something different, exotic or underdeveloped is to generalize that it is not like 'normal' society. Europe's colonial attitude towards the Orientals exemplifies this as it was thought that the East was the opposite of the West; feminine where the West was masculine, weak where the West was strong and traditional where the West was progressive. By making these generalizations and othering the East, Europe was simultaneously defining herself as the norm, further entrenching the gap.\n\nMuch of the process of othering relies on imagined difference, or the expectation of difference. Spatial difference can be enough to conclude that \"we\" are \"here\" and the \"others\" are over \"there\". Imagined differences serve to categorize people into groups and assign them characteristics that suit the imaginer's expectations.\n\nRacial discrimination refers to discrimination against someone on the basis of their race.\n\nRacial segregation is the separation of humans into socially-constructed racial groups in daily life. It may apply to activities such as eating in a restaurant, drinking from a water fountain, using a bathroom, attending school, going to the movies, or in the rental or purchase of a home. Segregation is generally outlawed, but may exist through social norms, even when there is no strong individual preference for it, as suggested by Thomas Schelling's models of segregation and subsequent work.\n\nCenturies of European colonialism in the Americas, Africa and Asia were often justified by white supremacist attitudes. During the early 20th century, the phrase \"The White Man's Burden\" was widely used to justify an imperialist policy as a noble enterprise. A justification for the policy of conquest and subjugation of Native Americans emanated from the stereotyped perceptions of the indigenous people as \"merciless Indian savages\" (as described in the United States Declaration of Independence). In an 1890 article about colonial expansion onto Native American land, author L. Frank Baum wrote: \"The Whites, by law of conquest, by justice of civilization, are masters of the American continent, and the best safety of the frontier settlements will be secured by the total annihilation of the few remaining Indians.\" Attitudes of black supremacy, Arab supremacy, and East Asian supremacy also exist.\n\nSome scholars argue that in the US, earlier violent and aggressive forms of racism have evolved into a more subtle form of prejudice in the late 20th century. This new form of racism is sometimes referred to as \"modern racism\" and it is characterized by outwardly acting unprejudiced while inwardly maintaining prejudiced attitudes, displaying subtle prejudiced behaviors such as actions informed by attributing qualities to others based on racial stereotypes, and evaluating the same behavior differently based on the race of the person being evaluated. This view is based on studies of prejudice and discriminatory behavior, where some people will act ambivalently towards black people, with positive reactions in certain, more public contexts, but more negative views and expressions in more private contexts. This ambivalence may also be visible for example in hiring decisions where job candidates that are otherwise positively evaluated may be unconsciously disfavored by employers in the final decision because of their race. Some scholars consider modern racism to be characterized by an explicit rejection of stereotypes, combined with resistance to changing structures of discrimination for reasons that are ostensibly non-racial, an ideology that considers opportunity at a purely individual basis denying the relevance of race in determining individual opportunities and the exhibition of indirect forms of micro-aggression toward and/or avoidance of people of other races.\n\nRecent research has shown that individuals who consciously claim to reject racism may still exhibit race-based subconscious biases in their decision-making processes. While such \"subconscious racial biases\" do not fully fit the definition of racism, their impact can be similar, though typically less pronounced, not being explicit, conscious or deliberate.\n\nIn 1919, a proposal to include a racial equality provision in the Covenant of the League of Nations was supported by a majority, but not adopted in the Paris Peace Conference in 1919. In 1943, Japan and its allies declared work for the abolition of racial discrimination to be their aim at the Greater East Asia Conference. Article 1 of the 1945 UN Charter includes \"promoting and encouraging respect for human rights and for fundamental freedoms for all without distinction as to race\" as UN purpose.\n\nIn 1950, UNESCO suggested in \"The Race Question\" – a statement signed by 21 scholars such as Ashley Montagu, Claude Lévi-Strauss, Gunnar Myrdal, Julian Huxley, etc. – to \"drop the term \"race\" altogether and instead speak of ethnic groups\". The statement condemned scientific racism theories that had played a role in the Holocaust. It aimed both at debunking scientific racist theories, by popularizing modern knowledge concerning \"the race question\", and morally condemned racism as contrary to the philosophy of the Enlightenment and its assumption of equal rights for all. Along with Myrdal's \"\" (1944), \"The Race Question\" influenced the 1954 U.S. Supreme Court desegregation decision in \"Brown v. Board of Education.\" Also, in 1950, the European Convention on Human Rights was adopted, which was widely used on racial discrimination issues.\n\nThe United Nations use the definition of racial discrimination laid out in the \"International Convention on the Elimination of All Forms of Racial Discrimination\", adopted in 1966:\n\n... any distinction, exclusion, restriction or preference based on race, color, descent, or national or ethnic origin that has the purpose or effect of nullifying or impairing the recognition, enjoyment or exercise, on an equal footing, of human rights and fundamental freedoms in the political, economic, social, cultural or any other field of public life. (Part 1 of Article 1 of the U.N. International Convention on the Elimination of All Forms of Racial Discrimination)\nIn 2001, the European Union explicitly banned racism, along with many other forms of social discrimination, in the Charter of Fundamental Rights of the European Union, the legal effect of which, if any, would necessarily be limited to Institutions of the European Union: \"Article 21 of the charter prohibits discrimination on any ground such as race, color, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, disability, age or sexual orientation and also discrimination on the grounds of nationality.\"\n\nRacism existed during the 19th century as \"scientific racism\", which attempted to provide a racial classification of humanity. In 1775 Johann Blumenbach divided the world's population into five groups according to skin color (Caucasians, Mongols, etc.), positing the view that the non-Caucasians had arisen through a process of degeneration. Another early view in scientific racism was the polygenist view, which held that the different races had been separately created. Polygenist Christoph Meiners for example, split mankind into two divisions which he labeled the \"beautiful White race\" and the \"ugly Black race\". In Meiners' book, \"The Outline of History of Mankind\", he claimed that a main characteristic of race is either beauty or ugliness. He viewed only the white race as beautiful. He considered ugly races to be inferior, immoral and animal-like.\n\nAnders Retzius demonstrated that neither Europeans nor others are one \"pure race\", but of mixed origins. While discredited, derivations of Blumenbach's taxonomy are still widely used for the classification of the population in the United States. Hans Peder Steensby, while strongly emphasizing that all humans today are of mixed origins, in 1907 claimed that the origins of human differences must be traced extraordinarily far back in time, and conjectured that the \"purest race\" today would be the Australian Aboriginals.\n\nScientific racism fell strongly out of favor in the early 20th century, but the origins of fundamental human and societal differences are still researched within academia, in fields such as human genetics including paleogenetics, social anthropology, comparative politics, history of religions, history of ideas, prehistory, history, ethics, and psychiatry. There is widespread rejection of any methodology based on anything similar to Blumenbach's races. It is more unclear to which extent and when ethnic and national stereotypes are accepted.\n\nAlthough after World War II and the Holocaust, racist ideologies were discredited on ethical, political and scientific grounds, racism and racial discrimination have remained widespread around the world. \n\nDu Bois observed that it is not so much \"race\" that we think about, but culture: \"... a common history, common laws and religion, similar habits of thought and a conscious striving together for certain ideals of life\". Late 19th century nationalists were the first to embrace contemporary discourses on \"race\", ethnicity, and \"survival of the fittest\" to shape new nationalist doctrines. Ultimately, race came to represent not only the most important traits of the human body, but was also regarded as decisively shaping the character and personality of the nation. According to this view, culture is the physical manifestation created by ethnic groupings, as such fully determined by racial characteristics. Culture and race became considered intertwined and dependent upon each other, sometimes even to the extent of including nationality or language to the set of definition. Pureness of race tended to be related to rather superficial characteristics that were easily addressed and advertised, such as blondness. Racial qualities tended to be related to nationality and language rather than the actual geographic distribution of racial characteristics. In the case of Nordicism, the denomination \"Germanic\" was equivalent to superiority of race.\n\nBolstered by some nationalist and ethnocentric values and achievements of choice, this concept of racial superiority evolved to distinguish from other cultures that were considered inferior or impure. This emphasis on culture corresponds to the modern mainstream definition of racism: \"[r]acism does not originate from the existence of 'races'. It \"creates\" them through a process of social division into categories: anybody can be racialised, independently of their somatic, cultural, religious differences.\"\n\nThis definition explicitly ignores the biological concept of race, which is still subject to scientific debate. In the words of David C. Rowe, \"[a] racial concept, although sometimes in the guise of another name, will remain in use in biology and in other fields because scientists, as well as lay persons, are fascinated by human diversity, some of which is captured by race.\"\n\nRacial prejudice became subject to international legislation. For instance, the Declaration on the Elimination of All Forms of Racial Discrimination, adopted by the United Nations General Assembly on November 20, 1963, address racial prejudice explicitly next to discrimination for reasons of race, colour or ethnic origin (Article I).\n\nDebates over the origins of racism often suffer from a lack of clarity over the term. Many use the term \"racism\" to refer to more general phenomena, such as xenophobia and ethnocentrism, although scholars attempt to clearly distinguish those phenomena from racism as an ideology or from scientific racism, which has little to do with ordinary xenophobia. Others conflate recent forms of racism with earlier forms of ethnic and national conflict. In most cases, ethno-national conflict seems to owe itself to conflict over land and strategic resources. In some cases, ethnicity and nationalism were harnessed in order to rally combatants in wars between great religious empires (for example, the Muslim Turks and the Catholic Austro-Hungarians).\n\nNotions of race and racism have often played central roles in ethnic conflicts. Throughout history, when an adversary is identified as \"other\" based on notions of race or ethnicity (in particular when \"other\" is interpreted to mean \"inferior\"), the means employed by the self-presumed \"superior\" party to appropriate territory, human chattel, or material wealth often have been more ruthless, more brutal, and less constrained by moral or ethical considerations. According to historian Daniel Richter, Pontiac's Rebellion saw the emergence on both sides of the conflict of \"the novel idea that all Native people were 'Indians,' that all Euro-Americans were 'Whites,' and that all on one side must unite to destroy the other\". Basil Davidson states in his documentary, \"Africa: Different but Equal\", that racism, in fact, only just recently surfaced as late as the 19th century, due to the need for a justification for slavery in the Americas.\n\nHistorically, racism was a major driving force behind the Transatlantic slave trade. It was also a major force behind racial segregation, especially in the United States in the nineteenth and early twentieth centuries, and South Africa under apartheid; 19th and 20th century racism in the Western world is particularly well documented and constitutes a reference point in studies and discourses about racism. Racism has played a role in genocides such as the Armenian genocide, and the Holocaust, and colonial projects like the European colonization of the Americas, Africa, and Asia. Indigenous peoples have been – and are – often subject to racist attitudes. Practices and ideologies of racism are condemned by the United Nations in the Declaration of Human Rights.\n\nAfter the Napoleonic Wars, Europe was confronted with the new \"nationalities question\", leading to reconfigurations of the European map, on which the frontiers between the states had been delineated during the 1648 Peace of Westphalia. Nationalism had made its first appearance with the invention of the \"levée en masse\" by the French Revolutionaries, thus inventing mass conscription in order to be able to defend the newly founded Republic against the \"Ancien Régime\" order represented by the European monarchies. This led to the French Revolutionary Wars (1792–1802) and then to the conquests of Napoleon, and to the subsequent European-wide debates on the concepts and realities of nations, and in particular of nation-states. The Westphalia Treaty had divided Europe into various empires and kingdoms (such as the Ottoman Empire, the Holy Roman Empire, the Swedish Empire, the Kingdom of France, etc.), and for centuries wars were waged between princes (\"Kabinettskriege\" in German).\n\nModern nation-states appeared in the wake of the French Revolution, with the formation of patriotic sentiments for the first time in Spain during the Peninsula War (1808–1813, known in Spain as the Independence War). Despite the restoration of the previous order with the 1815 Congress of Vienna, the \"nationalities question\" became the main problem of Europe during the Industrial Era, leading in particular to the 1848 Revolutions, the Italian unification completed during the 1871 Franco-Prussian War, which itself culminated in the proclamation of the German Empire in the Hall of Mirrors in the Palace of Versailles, thus achieving the German unification.\n\nMeanwhile, the Ottoman Empire, the \"sick man of Europe\", was confronted with endless nationalist movements, which, along with the dissolving of the Austrian-Hungarian Empire, would lead to the creation, after World War I, of the various nation-states of the Balkans, with \"national minorities\" in their borders.\n\nEthnic nationalism, which advocated the belief in a hereditary membership of the nation, made its appearance in the historical context surrounding the creation of the modern nation-states.\n\nOne of its main influences was the Romantic nationalist movement at the turn of the 19th century, represented by figures such as Johann Herder (1744–1803), Johan Fichte (1762–1814) in the \"Addresses to the German Nation\" (1808), Friedrich Hegel (1770–1831), or also, in France, Jules Michelet (1798–1874). It was opposed to liberal nationalism, represented by authors such as Ernest Renan (1823–1892), who conceived of the nation as a community, which, instead of being based on the \"Volk\" ethnic group and on a specific, common language, was founded on the subjective will to live together (\"the nation is a daily plebiscite\", 1882) or also John Stuart Mill (1806–1873).\nEthnic nationalism blended with scientific racist discourses, as well as with \"continental imperialist\" (Hannah Arendt, 1951) discourses, for example in the pan-Germanism discourses, which postulated the racial superiority of the German \"Volk\" (people/folk). The Pan-German League (\"Alldeutscher Verband\"), created in 1891, promoted German imperialism and \"racial hygiene\", and was opposed to intermarriage with Jews. Another popular current, the \"Völkisch movement\", was also an important proponent of the German ethnic nationalist discourse, and it combined Pan-Germanism with modern racial antisemitism. Members of the Völkisch movement, in particular the Thule Society, would participate in the founding of the German Workers' Party (DAP) in Munich in 1918, the predecessor of the National Socialist German Workers' Party (NSDAP; commonly known in English as the Nazi party). Pan-Germanism played a decisive role in the interwar period of the 1920s–1930s.\n\nThese currents began to associate the idea of the nation with the biological concept of a \"master race\" (often the \"Aryan race\" or the \"Nordic race\") issued from the scientific racist discourse. They conflated nationalities with ethnic groups, called \"races\", in a radical distinction from previous racial discourses that posited the existence of a \"race struggle\" inside the nation and the state itself. Furthermore, they believed that political boundaries should mirror these alleged racial and ethnic groups, thus justifying ethnic cleansing, in order to achieve \"racial purity\" and also to achieve ethnic homogeneity in the nation-state.\n\nSuch racist discourses, combined with nationalism, were not, however, limited to pan-Germanism. In France, the transition from Republican liberal nationalism, to ethnic nationalism, which made nationalism a characteristic of far-right movements in France, took place during the Dreyfus Affair at the end of the 19th century. During several years, a nationwide crisis affected French society, concerning the alleged treason of Alfred Dreyfus, a French Jewish military officer. The country polarized itself into two opposite camps, one represented by Émile Zola, who wrote \"J'Accuse…!\" in defense of Alfred Dreyfus, and the other represented by the nationalist poet, Maurice Barrès (1862–1923), one of the founders of the ethnic nationalist discourse in France. At the same time, Charles Maurras (1868–1952), founder of the monarchist \"Action française\" movement, theorized the \"anti-France\", composed of the \"four confederate states of Protestants, Jews, Freemasons and foreigners\" (his actual word for the latter being the pejorative \"métèques\"). Indeed, to him the first three were all \"internal foreigners\", who threatened the ethnic unity of the French people.\n\nBernard Lewis has cited the Greek philosopher Aristotle who, in his discussion of slavery, stated that while Greeks are free by nature, \"barbarians\" (non-Greeks) are slaves by nature, in that it is in their nature to be more willing to submit to a despotic government. Though Aristotle does not specify any particular races, he argues that people from nations outside Greece are more prone to the burden of slavery than those from Greece. While Aristotle makes remarks about the most natural slaves being those with strong bodies and slave souls (unfit for rule, unintelligent) which would seem to imply a physical basis for discrimination, he also explicitly states that the right kind of souls and bodies don't always go together, implying that the greatest determinate for inferiority and natural slaves versus natural masters is the soul, not the body. This proto-racism is seen as an important precursor to modern racism by classicist Benjamin Isaac.\n\nSuch proto-racism and ethnocentrism must be looked at within context, because a modern understanding of racism based on hereditary inferiority (with modern racism based on eugenics and scientific racism) was not yet developed and it is unclear whether Aristotle believed the natural inferiority of Barbarians was caused by environment and climate (like many of his contemporaries) or by birth.\n\nHistorian Dante A. Puzzo, in his discussion of Aristotle, racism, and the ancient world writes that:\n\nRacism rests on two basic assumptions: that a correlation exists between physical characteristics and moral qualities; that mankind is divisible into superior and inferior stocks. Racism, thus defined, is a modern conception, for prior to the XVIth century there was virtually nothing in the life and thought of the West that can be described as racist. To prevent misunderstanding a clear distinction must be made between racism and ethnocentrism ... The Ancient Hebrews, in referring to all who were not Hebrews as Gentiles, were indulging in ethnocentrism, not in racism. ... So it was with the Hellenes who denominated all non-Hellenes—whether the wild Scythians or the Egyptians whom they acknowledged as their mentors in the arts of civilization—Barbarians, the term denoting that which was strange or foreign.\n\nBernard Lewis has also cited historians and geographers of the Middle East and North Africa region, including Al-Muqaddasi, Al-Jahiz, Al-Masudi, Abu Rayhan Biruni, Nasir al-Din al-Tusi, and Ibn Qutaybah. Though the Qur'an expresses no racial prejudice, Lewis argues that ethnocentric prejudice later developed among Arabs, for a variety of reasons: their extensive conquests and slave trade; the influence of Aristotelian ideas regarding slavery, which some Muslim philosophers directed towards Zanj (Bantu) and Turkic peoples; and the influence of Judeo-Christian ideas regarding divisions among humankind. The Afro-Arab author Al-Jahiz, himself having a Zanj grandfather, wrote a book entitled \"Superiority of the Blacks to the Whites\", and explained why the Zanj were black in terms of environmental determinism in the \"On the Zanj\" chapter of \"The Essays\". By the 14th century, a significant number of slaves came from sub-Saharan Africa; Lewis argues that this led to the likes of Egyptian historian Al-Abshibi (1388–1446) writing that \"[i]t is said that when the [black] slave is sated, he fornicates, when he is hungry, he steals.\" According to Lewis, the 14th-century Tunisian scholar Ibn Khaldun also wrote:\n\n...beyond [known peoples of black West Africa] to the south there is no civilization in the proper sense. There are only humans who are closer to dumb animals than to rational beings. They live in thickets and caves, and eat herbs and unprepared grain. They frequently eat each other. They cannot be considered human beings. Therefore, the Negro nations are, as a rule, submissive to slavery, because (Negroes) have little that is (essentially) human and possess attributes that are quite similar to those of dumb animals, as we have stated.\n\nHowever, according to Wesleyan University professor Abdelmajid Hannoum, such attitudes were not prevalent until the 18th and 19th centuries. He argues that some accounts of Arabic texts, such as those of Ibn Khaldun, were mistranslations by French Orientalists projecting racist and colonialist views of the 19th century into their translations of medieval Arabic writings. James E. Lindsay also argues that the concept of an Arab identity itself did not exist until modern times.\n\nWith the Umayyad Caliphate's conquest of Hispania, invading Muslim Berbers overthrew the previous Visigothic rulers and created Al-Andalus, which contributed to the Golden age of Jewish culture, and lasted for six centuries. It was followed by the centuries-long \"Reconquista\", terminated under the Catholic monarchs Ferdinand V and Isabella I. The legacy Catholic Spaniards then formulated the \"Cleanliness of blood\" doctrine. It was during this time in history that the Western concept of aristocratic \"blue blood\" emerged in a racialized, religious and feudal context, so as to stem the upward social mobility of the converted New Christians. Robert Lacey explains:\nIt was the Spaniards who gave the world the notion that an aristocrat's blood is not red but blue. The Spanish nobility started taking shape around the ninth century in classic military fashion, occupying land as warriors on horseback. They were to continue the process for more than five hundred years, clawing back sections of the peninsula from its Moorish occupiers, and a nobleman demonstrated his pedigree by holding up his sword arm to display the filigree of blue-blooded veins beneath his pale skin—proof that his birth had not been contaminated by the dark-skinned enemy. Sangre azul, blue blood, was thus a euphemism for being a white man—Spain's own particular reminder that the refined footsteps of the aristocracy through history carry the rather less refined spoor of racism.\nFollowing the expulsion of the Arabic Moors and most of the Sephardic Jews from the Iberian peninsula, the remaining Jews and Muslims were forced to convert to Roman Catholicism, becoming \"New Christians\", who were sometimes discriminated against by the \"Old Christians\" in some cities (including Toledo), despite condemnations by the Church and the State, which both welcomed the new flock. The Inquisition was carried out by members of the Dominican Order in order to weed out the converts who still practiced Judaism and Islam in secret. The system and ideology of the \"limpieza de sangre\" ostracized false Christian converts from society in order to protect it against treason. The remnants of such legislation persevered into the 19th century in military contexts.\n\nIn Portugal, the legal distinction between New and Old Christian was only ended through a legal decree issued by the Marquis of Pombal in 1772, almost three centuries after the implementation of the racist discrimination. The \"limpieza de sangre\" legislation was common also during the colonization of the Americas, where it led to the racial and feudal separation of peoples and social strata in the colonies. It was however often ignored in practice, as the new colonies needed skilled people.\n\nAt the end of the Renaissance, the Valladolid debate (1550–1551), concerning the treatment of the natives of the \"New World\" pitted the Dominican friar and Bishop of Chiapas, Bartolomé de Las Casas, to another Dominican and Humanist philosopher, Juan Ginés de Sepúlveda. The latter argued that the Indians practiced human sacrifice of innocents, cannibalism, and other such \"crimes against nature\"; they were unacceptable and should be suppressed by any means possible including war, thus reducing them to slavery or serfdom was in accordance with Catholic theology and natural law. To the contrary, Bartolomé de Las Casas argued that the Amerindians were free men in the natural order and deserved the same treatment as others, according to Catholic theology. It was one of the many controversies concerning racism, slavery, religion, and European morality that would arise in the following centuries and which resulted in the legislation protecting the natives. The marriage between Luisa de Abrego, a free black domestic servant from Seville and Miguel Rodríguez, a white segovian conquistador in 1565 in St. Augustine (Spanish Florida), is the first known and recorded Christian marriage anywhere in the continental United States.\n\nAlthough antisemitism has a long history, related to Christianity and native Egyptian or Greek religions (anti-Judaism), racism itself is sometimes described as a \"modern\" phenomenon. In the view of the French philosopher and historian Michel Foucault, the first formulation of racism emerged in the Early Modern period as the \"discourse of race struggle\", and a historical and political discourse, which Foucault opposed to the philosophical and juridical discourse of sovereignty. On the other hand, e.g. Chinese self-identification as a \"yellow race\" predated such European racial concepts.\n\nThis European analysis, which first appeared in Great Britain, was then carried on in France by such people as Boulainvilliers, Nicolas Fréret, and then, during the 1789 French Revolution, Sieyès, and afterwards, Augustin Thierry and Cournot. Boulainvilliers, who created the matrix of such racist discourse in medieval France, conceived of the \"race\" as being something closer to the sense of a \"nation\", that is, in his time, the \"race\" meant the \"people\".\n\nHe conceived of France as being divided between various nations – the unified nation-state is an anachronism here – which themselves formed different \"races\". Boulainvilliers opposed the absolute monarchy, which tried to bypass the aristocracy by establishing a direct relationship to the Third Estate. Thus, he developed the theory that the French aristocrats were the descendants of foreign invaders, whom he called the \"Franks\", while according to him, the Third Estate constituted the autochthonous, vanquished Gallo-Romans, who were dominated by the Frankish aristocracy as a consequence of the right of conquest. Early modern racism was opposed to nationalism and the nation-state: the Comte de Montlosier, in exile during the French Revolution, who borrowed Boulainvilliers' discourse on the \"Nordic race\" as being the French aristocracy that invaded the plebeian \"Gauls\", thus showed his contempt for the Third Estate, calling it \"this new people born of slaves ... mixture of all races and of all times\".\n\nWhile 19th-century racism became closely intertwined with nationalism, leading to the ethnic nationalist discourse that identified the \"race\" with the \"folk\", leading to such movements as pan-Germanism, pan-Turkism, pan-Arabism, and pan-Slavism, medieval racism precisely divided the nation into various non-biological \"races\", which were thought to be the consequence of historical conquests and social conflicts. Michel Foucault traced the genealogy of modern racism to this medieval \"historical and political discourse of race struggle\". According to him, it divided itself in the 19th century according to two rival lines: on one hand, it was incorporated by racists, biologists and eugenicists, who gave it the modern sense of \"race\", and they also transformed this popular discourse into a \"state racism\" (e.g., Nazism). On the other hand, Marxism also seized this discourse founded on the assumption of a political struggle that provided the real engine of history and continued to act underneath the apparent peace. Thus, Marxists transformed the essentialist notion of \"race\" into the historical notion of \"class struggle\", defined by socially structured positions: capitalist or proletarian. In \"The Will to Knowledge\" (1976), Foucault analyzed another opponent of the \"race struggle\" discourse: Sigmund Freud's psychoanalysis, which opposed the concept of \"blood heredity\", prevalent in the 19th century racist discourse.\n\nAuthors such as Hannah Arendt, in her 1951 book \"The Origins of Totalitarianism\", have said that the racist ideology (\"popular racism\") which developed at the end of the 19th century helped legitimize the imperialist conquests of foreign territories and the atrocities that sometimes accompanied them (such as the Herero and Namaqua Genocide of 1904–1907 or the Armenian Genocide of 1915–1917). Rudyard Kipling's poem, \"The White Man's Burden\" (1899), is one of the more famous illustrations of the belief in the inherent superiority of the European culture over the rest of the world, though it is also thought to be a satirical appraisal of such imperialism. Racist ideology thus helped legitimize the conquest and incorporation of foreign territories into an empire, which were regarded as a humanitarian obligation partially as a result of these racist beliefs.\n\nHowever, during the 19th century, Western European colonial powers were involved in the suppression of the Arab slave trade in Africa, as well as in the suppression of the slave trade in West Africa. Some Europeans during the time period objected to injustices that occurred in some colonies and lobbied on behalf of aboriginal peoples. Thus, when the Hottentot Venus was displayed in England in the beginning of the 19th century, the African Association publicly opposed itself to the exhibition. The same year that Kipling published his poem, Joseph Conrad published \"Heart of Darkness\" (1899), a clear criticism of the Congo Free State, which was owned by Leopold II of Belgium.\n\nExamples of racial theories used include the creation of the Hamitic ethno-linguistic group during the European exploration of Africa. It was then restricted by Karl Friedrich Lepsius (1810–1877) to non-Semitic Afro-Asiatic languages.\n\nThe term \"Hamite\" was applied to different populations within North Africa, mainly comprising Ethiopians, Eritreans, Somalis, Berbers, and the ancient Egyptians. Hamites were regarded as Caucasoid peoples who probably originated in either Arabia or Asia on the basis of their cultural, physical and linguistic similarities with the peoples of those areas. Europeans considered Hamites to be more civilized than Sub-Saharan Africans, and more akin to themselves and Semitic peoples. In the first two-thirds of the 20th century, the Hamitic race was, in fact, considered one of the branches of the Caucasian race, along with the Indo-Europeans, Semites, and the Mediterraneans.\n\nHowever, the Hamitic peoples themselves were often deemed to have failed as rulers, which was usually ascribed to interbreeding with Negroes. In the mid-20th century, the German scholar Carl Meinhof (1857–1944) claimed that the Bantu race was formed by a merger of Hamitic and Negro races. The Hottentots (Nama or Khoi) were formed by the merger of Hamitic and Bushmen (San) races—both being termed nowadays as Khoisan peoples.\n\nIn the United States in the early 19th century, the American Colonization Society was established as the primary vehicle for proposals to return black Americans to greater freedom and equality in Africa. The colonization effort resulted from a mixture of motives with its founder Henry Clay stating that \"unconquerable prejudice resulting from their color, they never could amalgamate with the free whites of this country. It was desirable, therefore, as it respected them, and the residue of the population of the country, to drain them off\". Racism spread throughout the New World in the late 19th century and early 20th century. Whitecapping, which started in Indiana in the late 19th century, soon spread throughout all of North America, causing many African laborers to flee from the land they worked on. In the US, during the 1860s, racist posters were used during election campaigns. In one of these racist posters (see above), a black man is depicted lounging idly in the foreground as one white man ploughs his field and another chops wood. Accompanying labels are: \"In the sweat of thy face shalt thou eat thy bread\", and \"The white man must work to keep his children and pay his taxes.\" The black man wonders, \"Whar is de use for me to work as long as dey make dese appropriations.\" Above in a cloud is an image of the \"Freedman's Bureau! Negro Estimate of Freedom!\" The bureau is pictured as a large domed building resembling the U.S. Capitol and is inscribed \"Freedom and No Work\". Its columns and walls are labeled, \"Candy\", \"Rum, Gin, Whiskey\", \"Sugar Plums\", \"Indolence\", \"White Women\", \"Apathy\", \"White Sugar\", \"Idleness\", and so on.\n\nOn June 5, 1873, Sir Francis Galton, distinguished English explorer and cousin of Charles Darwin, wrote in a letter to \"The Times\":\n\nThe Nazi party, which seized power in the 1933 German elections and maintained a dictatorship over much of Europe until the End of World War II on the European continent, deemed the Germans to be part of an Aryan \"master race\" (\"Herrenvolk\"), who therefore had the right to expand their territory and enslave or kill members of other races deemed inferior.\n\nThe racial ideology conceived by the Nazis graded humans on a scale of pure Aryan to non-Aryan, with the latter viewed as subhuman. At the top of the scale of pure Aryans were Germans and other Germanic peoples including the Dutch, Scandinavians, and the English as well as other peoples such as some northern Italians and the French, who were said to have a suitable admixture of Germanic blood. Nazi policies labeled Romani people, people of color, and Slavs (mainly Poles, Serbs, Russians, Belarusians, Ukrainians and Czechs) as inferior non-Aryan subhumans. Jews were at the bottom of the hierarchy, considered inhuman and thus unworthy of life. In accordance with Nazi racial ideology, approximately six million Jews were killed in the Holocaust. 2.5 million ethnic Poles, 0.5 million ethnic Serbs and 0.22–0.5 million Romani were killed by the regime and its collaborators.\n\nThe Nazis considered most Slavs to be non-Aryan \"Untermenschen\". The Nazi Party's chief racial theorist, Alfred Rosenberg, adopted the term from Klansman Lothrop Stoddard's 1922 book \"The Revolt Against Civilization: The Menace of the Under-man\". Slavic nations such as the Slovaks, Bulgarians, and Croats, who collaborated with Nazi Germany were perceived as ethnically superior to other Slavs, mostly due to pseudoscientific theories about these nations having a considerable admixture of Germanic blood. In the secret plan Generalplan Ost (\"Master Plan East\") the Nazis resolved to expel, enslave, or exterminate most Slavic people to provide \"living space\" for Germans, however Nazi policy towards Slavs changed during World War II due to manpower shortages which necessitated limited Slavic participation in the Waffen-SS. Significant war crimes were committed against Slavs, particularly Poles, and Soviet POWs had a far higher mortality rate than their American and British counterparts due to deliberate neglect and mistreatment. Between June 1941 and January 1942, the Nazis killed an estimated 2.8 million Red Army POWs, whom they viewed as \"subhuman\".\n\nGerman praise for America's institutional racism was continuous throughout the early 1930s, and Nazi lawyers were advocates of the use of American models. Race based U.S. citizenship laws and anti-miscegenation laws (no race mixing) directly inspired the Nazi's two principal Nuremberg racial laws – the Citizenship Law and the Blood Law. Hitler's 1925 memoir \"Mein Kampf\" was full of admiration for America's treatment of \"coloreds\". Nazi expansion eastward was accompanied with invocation of America's colonial expansion westward, with the accompanying actions toward the Native Americans. In 1928, Hitler praised Americans for having \"gunned down the millions of Redskins to a few hundred thousand, and now keeps the modest remnant under observation in a cage.\" On Nazi Germany's expansion eastward, in 1941 Hitler stated, \"Our Mississippi [the line beyond which Thomas Jefferson wanted all Indians expelled] must be the Volga.\"\n\nWhite supremacy was dominant in the U.S. up to the civil rights movement. On the U.S. immigration laws prior to 1965, sociologist Stephen Klineberg cited the law as clearly declaring \"that Northern Europeans are a superior subspecies of the white race.\" While anti-Asian racism was embedded in U.S. politics and culture in the early 20th century, Indians were also racialized for their anticolonialism, with U.S. officials, casting them as a \"Hindu\" menace, pushing for Western imperial expansion abroad. The Naturalization Act of 1790 limited U.S. citizenship to whites only, and in the 1923 case, \"United States v. Bhagat Singh Thind\", the Supreme Court ruled that high caste Hindus were not \"white persons\" and were therefore racially ineligible for naturalized citizenship. It was after the Luce–Celler Act of 1946 that a quota of 100 Indians per year could immigrate to the U.S. and become citizens. The Immigration and Nationality Act of 1965 dramatically opened entry to the U.S. to immigrants other than traditional Northern European and Germanic groups, and as a result would significantly alter the demographic mix in the U.S.\n\nSerious race riots in Durban between Indians and Zulus erupted in 1949. Ne Win's rise to power in Burma in 1962 and his relentless persecution of \"resident aliens\" led to an exodus of some 300,000 Burmese Indians. They migrated to escape racial discrimination and wholesale nationalisation of private enterprises a few years later, in 1964. The Zanzibar Revolution of January 12, 1964, put an end to the local Arab dynasty. Thousands of Arabs and Indians in Zanzibar were massacred in riots, and thousands more were detained or fled the island. In August 1972, Ugandan President Idi Amin started the expropriation of properties owned by Asians and Europeans. In the same year, Amin ethnically cleansed Uganda's Asians, giving them 90 days to leave the country.\nShortly after World War II, the South African National Party took control of the government in South Africa. Between 1948 and 1994, the apartheid regime took place. This regime based its ideology on the racial separation of whites and non-whites, including the unequal rights of non-whites. Several protests and violence occurred during the struggle against apartheid, the most famous of these include the Sharpeville Massacre in 1960, the Soweto uprising in 1976, the Church Street bombing of 1983, and the Cape Town peace march of 1989.\n\nDuring the Congo Civil War (1998–2003), Pygmy people were hunted down like game animals and eaten. Both sides in the war regarded them as \"subhuman\" and some say their flesh can confer magical powers. UN human rights activists reported in 2003 that rebels had carried out acts of cannibalism. Sinafasi Makelo, a representative of the Mbuti pygmies, has asked the UN Security Council to recognise cannibalism as both a crime against humanity and an act of genocide. A report released by the United Nations Committee on the Elimination of Racial Discrimination condemns Botswana's treatment of the 'Bushmen' as racist. In 2008, the tribunal of the 15-nation Southern African Development Community (SADC) accused Zimbabwean President Robert Mugabe of having a racist attitude towards white people.\n\nThe mass demonstrations and riots against African students in Nanjing, China, lasted from December 1988 to January 1989. Bar owners in central Beijing had been forced by the police \"not to serve black people or Mongolians\" during the 2008 Summer Olympics, as the police associated these ethnic groups with illegal prostitution and drug trafficking. In November 2009, British newspaper \"The Guardian\" reported that Lou Jing, of mixed Chinese and African parentage, had emerged as the most famous talent show contestant in China and has become the subject of intense debate because of her skin color. Her attention in the media opened serious debates about racism in China and racial prejudice.\n\nSome 70,000 black African Mauritanians were expelled from Mauritania in the late 1980s. In the Sudan, black African captives in the civil war were often enslaved, and female prisoners were often sexually abused. The Darfur conflict has been described by some as a racial matter. In October 2006, Niger announced that it would deport the approximately 150,000 Arabs living in the Diffa region of eastern Niger to Chad. While the government collected Arabs in preparation for the deportation, two girls died, reportedly after fleeing Government forces, and three women suffered miscarriages.\n\nThe Jakarta riots of May 1998 targeted many Chinese Indonesians. The anti-Chinese legislation was in the Indonesian constitution until 1998. Resentment against Chinese workers has led to violent confrontations in Africa and Oceania. Anti-Chinese rioting, involving tens of thousands of people, broke out in Papua New Guinea in May 2009. Indo-Fijians suffered violent attacks after the Fiji coup in 2000. Non-indigenous citizens of Fiji are subject to discrimination. Racial divisions also exist in Guyana, Malaysia, Trinidad and Tobago, Madagascar, and South Africa.\n\nPeter Bouckaert, the Human Rights Watch's emergencies director, said in an interview that \"racist hatred\" is the chief motivation behind the violence against Rohingya Muslims in Myanmar.\n\nWith the aim of preserving the demographic makeup of the Zionist state, elements within Israeli society have been accused of discriminatory behavior against the Arab population and of a darker complexion. These communities disproportionately occupy laborer positions with the workforce. Accusations of racism have also included birth control policies, education, and housing discrimination.\n\nOne form of racism in the United States was enforced racial segregation, which existed until the 1960s, when it was outlawed in the Civil Rights Act of 1964. It has been argued that this separation of races continues to exist \"de facto\" today in different forms, such as lack of access to loans and resources or discrimination by police and other government officials.\n\nThe 2016 Pew Research poll found that Italians, in particular, hold strong anti-Roma views, with 82% of Italians expressing negative opinions about Roma. In Greece, there are 67%, in Hungary, 64%, in France, 61%, in Spain, 49%, in Poland, 47%, in the UK, 45%, in Sweden, 42%, in Germany, 40%, and in the Netherlands, 37%, that have an unfavourable view of Roma.\n\nThe modern biological definition of race developed in the 19th century with scientific racist theories. The term \"scientific racism\" refers to the use of science to justify and support racist beliefs, which goes back to the early 18th century, though it gained most of its influence in the mid-19th century, during the New Imperialism period. Also known as academic racism, such theories first needed to overcome the Church's resistance to positivist accounts of history and its support of monogenism, the concept that all human beings were originated from the same ancestors, in accordance with creationist accounts of history.\n\nThese racist theories put forth on scientific hypothesis were combined with unilineal theories of social progress, which postulated the superiority of the European civilization over the rest of the world. Furthermore, they frequently made use of the idea of \"survival of the fittest\", a term coined by Herbert Spencer in 1864, associated with ideas of competition, which were named social Darwinism in the 1940s. Charles Darwin himself opposed the idea of rigid racial differences in \"The Descent of Man\" (1871), in which he argued that humans were all of one species, sharing common descent. He recognised racial differences as varieties of humanity, and emphasised the close similarities between people of all races in mental faculties, tastes, dispositions and habits, while still contrasting the culture of the \"lowest savages\" with European civilization.\n\nAt the end of the 19th century, proponents of scientific racism intertwined themselves with eugenics discourses of \"degeneration of the race\" and \"blood heredity\". Henceforth, scientific racist discourses could be defined as the combination of polygenism, unilinealism, social Darwinism, and eugenism. They found their scientific legitimacy on physical anthropology, anthropometry, craniometry, phrenology, physiognomy, and others now discredited disciplines in order to formulate racist prejudices.\n\nBefore being disqualified in the 20th century by the American school of cultural anthropology (Franz Boas, etc.), the British school of social anthropology (Bronisław Malinowski, Alfred Radcliffe-Brown, etc.), the French school of ethnology (Claude Lévi-Strauss, etc.), as well as the discovery of the neo-Darwinian synthesis, such sciences, in particular anthropometry, were used to deduce behaviours and psychological characteristics from outward, physical appearances.\n\nThe neo-Darwinian synthesis, first developed in the 1930s, eventually led to a gene-centered view of evolution in the 1960s. According to the Human Genome Project, the most complete mapping of human DNA to date indicates that there is no clear genetic basis to racial groups. While some genes are more common in certain populations, there are no genes that exist in all members of one population and no members of any other.\n\nThe first theory of eugenics was developed in 1869 by Francis Galton (1822–1911), who used the then-popular concept of \"degeneration\". He applied statistics to study human differences and the alleged \"inheritance of intelligence\", foreshadowing future uses of \"intelligence testing\" by the anthropometry school. Such theories were vividly described by the writer Émile Zola (1840–1902), who started publishing in 1871, a twenty-novel cycle, \"Les Rougon-Macquart\", where he linked heredity to behavior. Thus, Zola described the high-born Rougons as those involved in politics (\"Son Excellence Eugène Rougon\") and medicine (\"Le Docteur Pascal\") and the low-born Macquarts as those fatally falling into alcoholism (\"L'Assommoir\"), prostitution (\"Nana\"), and homicide (\"La Bête humaine\").\n\nDuring the rise of Nazism in Germany, some scientists in Western nations worked to debunk the regime's racial theories. A few argued against racist ideologies and discrimination, even if they believed in the alleged existence of biological races. However, in the fields of anthropology and biology, these were minority positions until the mid-20th century. According to the 1950 UNESCO statement, \"The Race Question\", an international project to debunk racist theories had been attempted in the mid-1930s. However, this project had been abandoned. Thus, in 1950, UNESCO declared that it had resumed:\n...up again, after a lapse of fifteen years, a project that the International Committee on Intellectual Cooperation has wished to carry through but that it had to abandon in deference to the appeasement policy of the pre-war period. The race question had become one of the pivots of Nazi ideology and policy. Masaryk and Beneš took the initiative of calling for a conference to re-establish in the minds and consciences of men everywhere the truth about race ... Nazi propaganda was able to continue its baleful work unopposed by the authority of an international organisation.\nThe Third Reich's racial policies, its eugenics programs and the extermination of Jews in the Holocaust, as well as the Romani people in the Porrajmos (the Romani Holocaust) and others minorities led to a change in opinions about scientific research into race after the war. Changes within scientific disciplines, such as the rise of the Boasian school of anthropology in the United States contributed to this shift. These theories were strongly denounced in the 1950 UNESCO statement, signed by internationally renowned scholars, and titled \"The Race Question\".\n\nWorks such as Arthur de Gobineau's \"An Essay on the Inequality of the Human Races\" (1853–1855) may be considered as one of the first theorizations of this new racism, founded on an essentialist notion of race, which opposed the former racial discourse, of Boulainvilliers for example, which saw in races a fundamentally historical reality, which changed over time. Gobineau, thus, attempted to frame racism within the terms of biological differences among humans, giving it the legitimacy of biology.\n\nGobineau's theories would be expanded in France by Georges Vacher de Lapouge (1854–1936)'s typology of races, who published in 1899 \"The Aryan and his Social Role\", in which he claimed that the white \"Aryan race\" \"dolichocephalic\", was opposed to the \"brachycephalic\" race, of whom the \"Jew\" was the archetype. Vacher de Lapouge thus created a hierarchical classification of races, in which he identified the \"\"Homo europaeus\" (Teutonic, Protestant, etc.), the \"\"Homo alpinus\" (Auvergnat, Turkish, etc.), and finally the \"Homo mediterraneus\"\" (Neapolitan, Andalus, etc.) He assimilated races and social classes, considering that the French upper class was a representation of the \"Homo europaeus\", while the lower class represented the \"Homo alpinus\". Applying Galton's eugenics to his theory of races, Vacher de Lapouge's \"selectionism\" aimed first at achieving the annihilation of trade unionists, considered to be a \"degenerate\"; second, creating types of man each destined to one end, in order to prevent any contestation of labour conditions. His \"anthroposociology\" thus aimed at blocking social conflict by establishing a fixed, hierarchical social order.\n\nThe same year, William Z. Ripley used identical racial classification in \"The Races of Europe\" (1899), which would have a great influence in the United States. Other scientific authors include H.S. Chamberlain at the end of the 19th century (a British citizen who naturalized himself as German because of his admiration for the \"Aryan race\") and Madison Grant, a eugenicist and author of \"The Passing of the Great Race\" (1916). Madison Grant provided statistics for the Immigration Act of 1924, which severely restricted immigration of Jews, Slavs, and southern Europeans, who were subsequently hindered in seeking to escape Nazi Germany.\n\nHuman zoos (called \"People Shows\"), were an important means of bolstering \"popular racism\" by connecting it to scientific racism: they were both objects of public curiosity and of anthropology and anthropometry. Joice Heth, an African American slave, was displayed by P.T. Barnum in 1836, a few years after the exhibition of Saartjie Baartman, the \"Hottentot Venus\", in England. Such exhibitions became common in the New Imperialism period, and remained so until World War II. Carl Hagenbeck, inventor of the modern zoos, exhibited animals beside humans who were considered \"savages\".\n\nCongolese pygmy Ota Benga was displayed in 1906 by eugenicist Madison Grant, head of the Bronx Zoo, as an attempt to illustrate the \"missing link\" between humans and orangutans: thus, racism was tied to Darwinism, creating a social Darwinist ideology that tried to ground itself in Darwin's scientific discoveries. The 1931 Paris Colonial Exhibition displayed Kanaks from New Caledonia. A \"Congolese village\" was on display as late as 1958 at the Brussels' World Fair.\n\nEvolutionary psychologists John Tooby and Leda Cosmides were puzzled by the fact that in the US, race is one of the three characteristics most often used in brief descriptions of individuals (the others are age and sex). They reasoned that natural selection would not have favoured the evolution of an instinct for using race as a classification, because for most of human history, humans almost never encountered members of other races. Tooby and Cosmides hypothesized that modern people use race as a proxy (rough-and-ready indicator) for coalition membership, since a better-than-random guess about \"which side\" another person is on will be helpful if one does not actually know in advance.\n\nTheir colleague Robert Kurzban designed an experiment whose results appeared to support this hypothesis. Using the Memory confusion protocol, they presented subjects with pictures of individuals and sentences, allegedly spoken by these individuals, which presented two sides of a debate. The errors that the subjects made in recalling who said what indicated that they sometimes mis-attributed a statement to a speaker of the same race as the \"correct\" speaker, although they also sometimes mis-attributed a statement to a speaker \"on the same side\" as the \"correct\" speaker. In a second run of the experiment, the team also distinguished the \"sides\" in the debate by clothing of similar colors; and in this case the effect of racial similarity in causing mistakes almost vanished, being replaced by the color of their clothing. In other words, the first group of subjects, with no clues from clothing, used race as a visual guide to guessing who was on which side of the debate; the second group of subjects used the clothing color as their main visual clue, and the effect of race became very small.\n\nSome research suggests that ethnocentric thinking may have actually contributed to the development of cooperation. Political scientists Ross Hammond and Robert Axelrod created a computer simulation wherein virtual individuals were randomly assigned one of a variety of skin colors, and then one of a variety of trading strategies: be color-blind, favor those of your own color, or favor those of other colors. They found that the ethnocentric individuals clustered together, then grew, until all the non-ethnocentric individuals were wiped out.\n\nIn \"The Selfish Gene\", evolutionary biologist Richard Dawkins writes that \"Blood-feuds and inter-clan warfare are easily interpretable in terms of Hamilton's genetic theory.\" Dawkins writes that racial prejudice, while not evolutionarily adaptive, \"could be interpreted as an irrational generalization of a kin-selected tendency to identify with individuals physically resembling oneself, and to be nasty to individuals different in appearance.\" Simulation-based experiments in evolutionary game theory have attempted to provide an explanation for the selection of ethnocentric-strategy phenotypes.\n\nDespite support for evolutionary theories relating to an innate origin of racism, various studies have suggested racism is associated with lower intelligence and less diverse peer groups during childhood. A neuroimaging study on amygdala activity during racial matching activities found increased activity to be associated with adolescent age as well as less racially diverse peer groups, which the author conclude suggest a learned aspect of racism. A meta analysis of neuroimaging studies found amygdala activity correlated to increased scores on implicit measures of racial bias. It was also argued amygdala activity in response to racial stimuli represents increased threat perception rather than the traditional theory of the amygdala activity represented ingroup-outgroup processing. Racism has also been associated with lower childhood IQ in an analysis of 15,000 people in the UK.\n\nState racism – that is, the institutions and practices of a nation-state that are grounded in racist ideology – has played a major role in all instances of settler colonialism, from the United States to Australia. It also played a prominent role in the Nazi German regime, in fascist regimes throughout Europe, and during the early years of Japan's Shōwa period. These governments advocated and implemented ideologies and policies that were racist, xenophobic, and, in the case of Nazism, genocidal.\n\nThe Nuremberg Race Laws of 1935 prohibited sexual relations between any Aryan and Jew, considering it \"Rassenschande\", \"racial pollution\". The Nuremberg Laws stripped all Jews, even quarter- and half-Jews (second and first degree \"Mischlings\"), of their German citizenship. This meant that they had no basic citizens' rights, e.g., the right to vote. In 1936, Jews were banned from all professional jobs, effectively preventing them from having any influence in education, politics, higher education, and industry. On 15 November 1938, Jewish children were banned from going to normal schools. By April 1939, nearly all Jewish companies had either collapsed under financial pressure and declining profits, or had been persuaded to sell out to the Nazi government. This further reduced their rights as human beings; they were in many ways officially separated from the German populace. Similar laws existed in Bulgaria – The Law for protection of the nation, Hungary, Romania, and Austria.\n\nLegislative state racism is known to have been enforced by the National Party of South Africa during its Apartheid regime between 1948 and 1994. Here, a series of Apartheid legislation was passed through the legal systems to make it legal for white South Africans to have rights which were superior to those of non-white South Africans. Non-white South Africans were not allowed involvement in any governing matters, including voting; access to quality healthcare; the provision of basic services, including clean water; electricity; as well as access to adequate schooling. Non-white South Africans were also prevented from accessing certain public areas, from using certain public transportation, and were required to live only in certain designated areas. Non-white South Africans were taxed differently than white South Africans and they were also required to carry on them at all times additional documentation, which later became known as \"dom passes\", to certify their non-white South African citizenship. All of these legislative racial laws were abolished through a series of equal human rights laws which were passed at the end of the Apartheid era in the early 1990s.\n\nAnti-racism includes beliefs, actions, movements, and policies which are adopted or developed in order to oppose racism. In general, it promotes an egalitarian society in which people are not discriminated against on the basis of race. Movements such as the civil rights movement and the Anti-Apartheid Movement were examples of anti-racist movements. Nonviolent resistance is sometimes embraced as an element of anti-racist movements, although this was not always the case. Hate crime laws, affirmative action, and bans on racist speech are also examples of government policy which is intended to suppress racism.\n\n"}
{"id": "27165", "url": "https://en.wikipedia.org/wiki?curid=27165", "title": "Sexism", "text": "Sexism\n\nSexism is prejudice or discrimination based on a person's sex or gender. Sexism can affect anyone, but it primarily affects women and girls. It has been linked to stereotypes and gender roles, and may include the belief that one sex or gender is intrinsically superior to another. Extreme sexism may foster sexual harassment, rape, and other forms of sexual violence. Gender discrimination may encompass sexism, and is discrimination toward people based on their gender identity or their gender or sex differences. Gender discrimination is especially defined in terms of workplace inequality. It may arise from social or cultural customs and norms.\n\nAccording to Fred R. Shapiro, the term \"sexism\" was most likely coined on November 18, 1965, by Pauline M. Leet during a \"Student-Faculty Forum\" at Franklin and Marshall College. Specifically, the word sexism appears in Leet's forum contribution \"Women and the Undergraduate\", and she defines it by comparing it to racism, stating in part (on page 3): \"When you argue ... that since fewer women write good poetry this justifies their total exclusion, you are taking a position analogous to that of the racist—I might call you in this case a 'sexist' ... Both the racist and the sexist are acting as if all that has happened had never happened, and both of them are making decisions and coming to conclusions about someone's value by referring to factors which are in both cases irrelevant.\"\n\nAlso according to Shapiro, the first time the term \"sexism\" appeared in print was in Caroline Bird's speech \"On Being Born Female\", which was published on November 15, 1968, in \"Vital Speeches of the Day\" (p. 6). In this speech she said in part: \"There is recognition abroad that we are in many ways a sexist country. Sexism is judging people by their sex when sex doesn't matter. Sexism is intended to rhyme with racism.\"\n\nSexism may be defined as an ideology based on the belief that one sex is superior to another. It is discrimination, prejudice, or stereotyping on the basis of gender, and is most often expressed toward girls and women. It has been characterized as the \"hatred of women\" and \"entrenched prejudice against women\".\n\nSociology has examined sexism as manifesting at both the individual and the institutional level. According to Schaefer, sexism is perpetuated by all major social institutions. Sociologists describe parallels among other ideological systems of oppression such as racism, which also operates at both the individual and institutional level. Early female sociologists Charlotte Perkins Gilman, Ida B. Wells, and Harriet Martineau described systems of gender inequality, but did not use the term \"sexism\", which was coined later. Sociologists who adopted the functionalist paradigm, e.g. Talcott Parsons, understood gender inequality as the natural outcome of a dimorphic model of gender.\n\nPsychologists Mary Crawford and Rhoda Unger define sexism as a form of prejudice held by individuals that encompasses \"negative attitudes and values about women as a group.\" Peter Glick and Susan Fiske coined the term \"ambivalent sexism\" to describe how stereotypes about women can be both positive and negative, and that individuals compartmentalize the stereotypes they hold into hostile sexism or benevolent sexism.\n\nFeminist author bell hooks defines sexism as a system of oppression that results in disadvantages for women. Feminist philosopher Marilyn Frye defines sexism as an \"attitudinal-conceptual-cognitive-orientational complex\" of male supremacy, male chauvinism, and misogyny.\n\nThe status of women in ancient Egypt depended on their fathers or husbands, but they had property rights and were allowed to attend court, including as plaintiffs. Women of the Anglo-Saxon era were commonly afforded equal status. Evidence, however, is lacking to support the idea that many pre-agricultural societies afforded women a higher status than women today. After the adoption of agriculture and sedentary cultures, the concept that one gender was inferior to the other was established; most often this was imposed upon women and girls. Examples of sexism in the ancient world include written laws preventing women from participating in the political process; women in ancient Rome could not vote or hold political office. Another example is scholarly texts that indoctrinate children in female inferiority; women in ancient China were taught the Confucian principles that a woman should obey her father in childhood, husband in marriage, and son in widowhood.\n\nSexism may have been the impetus that fueled the witch trials between the 15th and 18th centuries. In early modern Europe, and in the European colonies in North America, claims were made that witches were a threat to Christendom. The misogyny of that period played a role in the persecution of these women.\n\nIn \"Malleus Malificarum\", the book which played a major role in the witch hunts and trials, the authors argue that women are more likely to practice witchcraft than men, and write that:\n\nWitchcraft remains illegal in several countries, including Saudi Arabia, where it is punishable by death. In 2011, a woman was beheaded in that country for 'witchcraft and sorcery'. Murders of women after being accused of witchcraft remain common in some parts of the world; for example, in Tanzania, about 500 elderly women are murdered each year following such accusations.\n\nWhen women are targeted for accusations of witchcraft and subsequent violence, it is often the case that several forms of discrimination interact - for example, discrimination based on gender with discrimination based on caste, as is the case in India and Nepal, where such crimes are relatively common.\n\nUntil the 20th century, U.S. and English law observed the system of coverture, where \"by marriage, the husband and wife are one person in law; that is the very being or legal existence of the woman is suspended during the marriage\". U.S. women were not legally defined as \"persons\" until 1875 (\"Minor v. Happersett\", 88 U.S. 162). A similar legal doctrine, called marital power, existed under Roman Dutch law (and is still partially in force in present-day Eswatini).\n\nRestrictions on married women's rights were common in Western countries until a few decades ago: for instance, French married women obtained the right to work without their husband's permission in 1965, and in West Germany women obtained this right in 1977. During the Franco era, in Spain, a married woman required her husband's consent (called \"permiso marital\") for employment, ownership of property and traveling away from home; the \"permiso marital\" was abolished in 1975. In Australia, until 1983, the passport application of a married woman had to be authorized by her husband.\n\nWomen in parts of the world continue to lose their legal rights in marriage. For example, Yemeni marriage regulations state that a wife must obey her husband and must not leave home without his permission. In Iraq, the law allows husbands to legally \"punish\" their wives. In the Democratic Republic of Congo, the Family Code states that the husband is the head of the household; the wife owes her obedience to her husband; a wife has to live with her husband wherever he chooses to live; and wives must have their husbands' authorization to bring a case in court or to initiate other legal proceedings.\n\nAbuses and discriminatory practices against women in marriage are often rooted in financial payments such as dowry, bride price, and dower. These transactions often serve as legitimizing coercive control of the wife by her husband and in giving him authority over her; for instance Article 13 of the Code of Personal Status (Tunisia) states that \"The husband shall not, in default of payment of the dower, force the woman to consummate the marriage\", implying that, if the dower is paid, marital rape is permitted (in this regard, critics have questioned the alleged gains of women in Tunisia, and its image as a progressive country in the region, arguing that discrimination against women remains very strong in that country).\n\nThe OMCT has recognized the \"independence and ability to leave an abusive husband\" as crucial in stopping mistreatment of women. However, in some parts of the world, once married, women have very little chance of leaving a violent husband: obtaining a divorce is very difficult in many jurisdictions because of the need to prove fault in court; while attempting a \"de facto\" separation (moving away from the marital home) is also not possible due to laws preventing this. For instance, in Afghanistan, a wife who leaves her marital home risks being imprisoned for \"running away\". In addition, many former British colonies, including India, maintain the concept of restitution of conjugal rights, under which a wife may be ordered by court to return to her husband; if she fails to do so she may be held in contempt of court. Other problems have to do with the payment of the bride price: if the wife wants to leave, her husband may demand back the bride price that he had paid to the woman's family; and the woman's family often cannot or does not want to pay it back.\n\nLaws, regulations, and traditions related to marriage continue to discriminate against women in many parts of the world, and to contribute to the mistreatment of women, in particular in areas related to sexual violence and to self-determination in regard to sexuality, the violation of the latter now being acknowledged as a violation of women's rights; in 2012, Navi Pillay, then High Commissioner for Human Rights, has stated that:\n\nGender has been used, at times, as a tool for discrimination against women in the political sphere. Women's suffrage was not achieved until 1893, when New Zealand was the first country to grant women the right to vote. Saudi Arabia was the most recent country, as of August 2015, to extend the right to vote to women in 2011. Some Western countries allowed women the right to vote only relatively recently: Swiss women gained the right to vote in federal elections in 1971, and Appenzell Innerrhoden became the last canton to grant women the right to vote on local issues (in 1991, when it was forced to do so by the Federal Supreme Court of Switzerland). French women were granted the right to vote in 1944. In Greece, women obtained the right to vote in 1952. In Liechtenstein, women obtained the right to vote in 1984, through the women's suffrage referendum of 1984.\n\nWhile almost every woman today has the right to vote, there is still progress to be made for women in politics. Studies have shown that in several democracies including Australia, Canada, and the United States, women are still represented using gender stereotypes in the press. Multiple authors have shown that gender differences in the media are less evident today than they used to be in the 1980s, but are nonetheless still present. Certain issues (e.g., education) are likely to be linked with female candidates, while other issues (e.g., taxes) are likely to be linked with male candidates. In addition, there is more emphasis on female candidates' personal qualities, such as their appearance and their personality, as females are portrayed as emotional and dependent.\n\nSexism in politics can also be shown in the imbalance of lawmaking power between men and women. Lanyan Chen stated that men hold more political power than women, serving as the gatekeepers of policymaking. It is possible that this leads to women's needs not being properly represented. In this sense, the inequality of lawmaking power also causes gender discrimination in politics. The ratio of women to men in legislatures is used as a measure of gender equality in the UN created Gender Empowerment Measure and its newer incarnation the Gender Inequality Index.\n\nUntil the early 1980s, some high-end restaurants had two menus: a regular menu with the prices listed for men and a second menu for women, which did not have the prices listed (it was called the \"ladies' menu\"), so that the female diner would not know the prices of the items. In 1980, Kathleen Bick took a male business partner out to dinner at L'Orangerie in West Hollywood; after Bick got a women's menu without prices and her guest got the menu with prices, Bick hired lawyer Gloria Allred to file a discrimination lawsuit, on the grounds that the women's menu went against the California Civil Rights Act. Bick stated that getting a women's menu without prices left her feeling \"humiliated and incensed\". The owners of the restaurant defended the practice, saying it was done as a courtesy, like the way men would stand up when a woman enters the room. Even though the lawsuit was dropped, the restaurant ended its gender-based menu policy.\n\nGender stereotypes are widely held beliefs about the characteristics and behavior of women and men. Empirical studies have found widely shared cultural beliefs that men are more socially valued and more competent than women in a number of activities. Dustin B. Thoman and others (2008) hypothesize that \"[t]he socio-cultural salience of ability versus other components of the gender-math stereotype may impact women pursuing math\". Through the experiment comparing the math outcomes of women under two various gender-math stereotype components, which are the ability of math and the effort on math respectively, Thoman and others found that women’s math performance is more likely to be affected by the negative ability stereotype, which is influenced by sociocultural beliefs in the United States, rather than the effort component. As a result of this experiment and the sociocultural beliefs in the United States, Thoman and others concluded that individuals' academic outcomes can be affected by the gender-math stereotype component that is influenced by the sociocultural beliefs.\n\nSexism in language exists when language devalues members of a certain gender. Sexist language, in many instances, promotes male superiority. Sexism in language affects consciousness, perceptions of reality, encoding and transmitting cultural meanings and socialization. Researchers have pointed to the semantic rule in operation in language of the male-as-norm. This results in sexism as the male becomes the standard and those who are not male are relegated to the inferior. Sexism in language is considered a form of indirect sexism, in that it is not always overt.\n\nExamples include:\n\nVarious feminist movements in the 20th century, from liberal feminism and radical feminism to standpoint feminism, postmodern feminism and queer theory have all considered language in their theorizing. Most of these theories have maintained a critical stance on language that calls for a change in the way speakers use their language.\n\nOne of the most common calls is for gender-neutral language. Many have called attention, however, to the fact that the English language isn't inherently sexist in its linguistic system, but rather the way it is used becomes sexist and gender-neutral language could thus be employed. At the same time, other opposed critiques of sexism in language with explanations that language is a descriptive, rather than prescriptive, and attempts to control it can be fruitless.\n\nRomanic languages such as French and Spanish may be seen as reinforcing sexism, in that the masculine form is the default form. The word \"mademoiselle\", meaning \"miss\", was declared banished from French administrative forms in 2012 by Prime Minister François Fillon. Current pressure calls for the use of the masculine plural pronoun as the default in a mixed-sex group to change. As to Spanish, Mexico's Ministry of the Interior published a guide on how to reduce the use of sexist language.\n\nGerman speakers have also raised questions about how sexism intersects with grammar. The German language is heavily inflected for gender, number, and case; nearly all nouns denoting the occupations or statuses of human beings are gender-differentiated. For more gender-neutral constructions, gerund nouns are sometimes used instead, as this completely eliminates the grammatical gender distinction in the plural, and significantly reduces it in the singular. For example, instead of \"die Studenten\" (\"the men students\") or \"die Studentinnen\" (\"the women students\"), one writes \"die Studierenden\" (\"the [people who are] studying\"). However, this approach introduces an element of ambiguity, because gerund nouns more precisely denote one currently engaged in the activity, rather than one who routinely engages in it as their primary occupation.\n\nIn Chinese, some writers have pointed to sexism inherent in the structure of written characters. For example, the character for man is linked to those for positive qualities like courage and effect while the character for wife is composed of a female part and a broom, considered of low worth.\n\nGender-specific pejorative terms intimidate or harm another person because of their gender. Sexism can be expressed in language with negative gender-oriented implications, such as condescension. For example, one may refer to a female as a \"girl\" rather than a \"woman\", implying that they are subordinate or not fully mature. Other examples include obscene language. Some words are offensive to transgender people, including \"tranny\", \"she-male\", or \"he-she\". Intentional misgendering (assigning the wrong gender to someone) and the pronoun \"it\" are also considered pejorative.\n\nOccupational sexism refers to discriminatory practices, statements or actions, based on a person's sex, occurring in the workplace. One form of occupational sexism is wage discrimination. In 2008, the Organisation for Economic Co-operation and Development (OECD) found that while female employment rates have expanded and gender employment and wage gaps have narrowed nearly everywhere, on average women still have 20% less chance to have a job and are paid 17% less than men. The report stated: [In] many countries, labour market discrimination—i.e. the unequal treatment of equally productive individuals only because they belong to a specific group—is still a crucial factor inflating disparities in employment and the quality of job opportunities [...] Evidence presented in this edition of the \"Employment Outlook\" suggests that about 8 percent of the variation in gender employment gaps and 30 percent of the variation in gender wage gaps across OECD countries can be explained by discriminatory practices in the labor market.\n\nIt also found that despite the fact that almost all OECD countries, including the U.S., have established anti-discrimination laws, these laws are difficult to enforce.\n\nWomen who enter predominantly male work groups can experience the negative consequences of tokenism: performance pressures, social isolation, and role encapsulation. Tokenism could be used to camouflage sexism, to preserve male worker's advantage in the workplace. No link exists between the proportion of women working in an organization/company and the improvement of their working conditions. Ignoring sexist issues may exacerbate women’s occupational problems.\n\nIn the \"World Values Survey\" of 2005, responders were asked if they thought that wage work should be restricted only to men. In Iceland, the percentage that agreed was 3.6%, whereas in Egypt it was 94.9%.\n\nResearch has repeatedly shown that mothers in the United States are less likely to be hired than equally-qualified fathers and, if hired, receive a lower salary than male applicants with children.\n\nOne study found that female applicants were favored; however, its results have been met with skepticism from other researchers, since it contradicts most other studies on the issue. Joan C. Williams, a distinguished professor at the University of California's Hastings College of Law, raised issues with its methodology, pointing out that the fictional female candidates it used were unusually well-qualified. Studies using more moderately-qualified graduate students have found that male students are much more likely to be hired, offered better salaries, and offered mentorship.\n\nIn Europe, studies based on field experiments in the labour market, provide evidence for no severe levels of discrimination based on female gender. However, unequal treatment is still measured in particular situations, for instance when candidates apply for positions at a higher functional level in Belgium, when they apply at their fertiles ages in France, and when they apply for male-dominated occupations in Austria.\n\nStudies have concluded that on average women earn lower wages than men worldwide. Some people argue that this is the result of widespread gender discrimination in the workplace. Others argue that the wage gap is a result of different choices by men and women, such as women placing more value than men on having children, and men being more likely than women to choose careers in high paying fields such as business, engineering, and technology.\n\nEurostat found a persistent, average gender pay gap of 27.5% in the 27 EU member states in 2008. Similarly, the OECD found that female full-time employees earned 27% less than their male counterparts in OECD countries in 2009.\n\nIn the United States, the female-to-male earnings ratio was 0.77 in 2009; female full-time, year-round (FTYR) workers earned 77% as much as male FTYR workers. Women's earnings relative to men's fell from 1960 to 1980 (56.7–54.2%), rose rapidly from 1980 to 1990 (54.2–67.6%), leveled off from 1990 to 2000 (67.6–71.2%) and rose from 2000 to 2009 (71.2–77.0%). When the first Equal Pay Act was passed in 1963, female full-time workers earned 48.9% as much as male full-time workers.\n\nResearch conducted in the Czech and Slovak Republics shows that, even after the governments passed anti-discrimination legislation, two thirds of the gender gap in wages remained unexplained and segregation continued to \"represent a major source of the gap\".\n\nThe gender gap can also vary across-occupation and within occupation. In Taiwan, for example, studies show how the bulk of gender wage discrepancies occur within-occupation. In Russia, research shows that the gender wage gap is distributed unevenly across income levels, and that it mainly occurs at the lower end of income distribution. The research also found that \"wage arrears and payment in-kind attenuated wage discrimination, particularly amongst the lowest paid workers, suggesting that Russian enterprise managers assigned lowest importance to equity considerations when allocating these forms of payment\".\n\nThe gender pay gap has been attributed to differences in personal and workplace characteristics between men and women (such as education, hours worked and occupation), innate behavioral and biological differences between men and women and discrimination in the labor market (such as gender stereotypes and customer and employer bias). Women currently take significantly more time off to raise children than men. In certain countries such as South Korea, it has also been a long-established practice to lay-off female employees upon marriage. A study by professor Linda Babcock in her book \"Women Don't Ask\" shows that men are eight times more likely to ask for a pay raise, suggesting that pay inequality may be partly a result of behavioral differences between the sexes. However, studies generally find that a portion of the gender pay gap remains unexplained after accounting for factors assumed to influence earnings; the unexplained portion of the wage gap is attributed to gender discrimination.\n\nEstimates of the discriminatory component of the gender pay gap vary. The OECD estimated that approximately 30% of the gender pay gap across OECD countries is due to discrimination. Australian research shows that discrimination accounts for approximately 60% of the wage differential between men and women. Studies examining the gender pay gap in the United States show that a large portion of the wage differential remains unexplained, after controlling for factors affecting pay. One study of college graduates found that the portion of the pay gap unexplained after all other factors are taken into account is 5% one year after graduating and 12% a decade after graduation. A study by the American Association of University Women found that women graduates in the United States are paid less than men doing the same work and majoring in the same field.\n\nWage discrimination is theorized as contradicting the economic concept of supply and demand, which states that if a good or service (in this case, labor) is in demand and has value it will find its price in the market. If a worker offered equal value for less pay, supply and demand would indicate a greater demand for lower-paid workers. If a business hired lower-wage workers for the same work, it would lower its costs and enjoy a competitive advantage. According to supply and demand, if women offered equal value demand (and wages) should rise since they offer a better price (lower wages) for their service than men do.\n\nResearch at Cornell University and elsewhere indicates that mothers in the United States are less likely to be hired than equally-qualified fathers and, if hired, receive a lower salary than male applicants with children. The OECD found that \"a significant impact of children on women’s pay is generally found in the United Kingdom and the United States\". Fathers earn $7,500 more, on average, than men without children do.\n\nThere is research to suggest that the gender wage gap leads to big losses for the economy as a whole.\n\nAccording to Denise Venable at the National Center for Policy Analysis, the \"wage gap\" in the United States is not the result of discrimination but of differences in lifestyle choices. Venable's report found that women are less likely than men to sacrifice personal happiness for increases in income or to choose full-time work. She found that among American adults working between one and thirty-five hours a week and part-time workers who have never been married, women earn more than men. Venable also found that among people aged 27 to 33 who have never had a child, women's earnings approach 98% of men's and \"women who hold positions and have skills and experience similar to those of men face wage disparities of less than 10 percent, and many are within a couple of points\". Venable concluded that women and men with equal skills and opportunities in the same positions face little or no wage discrimination: \"Claims of unequal pay almost always involve comparing apples and oranges\".\n\nThere is considerable agreement that gender wage discrimination exists, however, when it comes to estimating its magnitude, significant discrepancies are visible. A meta-regression analysis concludes that \"the estimated gender gap has been steadily declining\" and that the wage rate calculation is proven to be crucial in estimating the wage gap. The analysis further notes that excluding experience and failing to correct for selection bias from analysis might also lead to incorrect conclusions.\n\n\"The popular notion of glass ceiling effects implies that gender (or other) disadvantages are stronger at the top of the hierarchy than at lower levels and that these disadvantages become worse later in a person's career.\"\n\nIn the United States, women account for 52% of the overall labor force, but only make up 3% of corporate CEOs and top executives. Some researchers see the root cause of this situation in the tacit discrimination based on gender, conducted by current top executives and corporate directors (primarily male), as well as \"the historic absence of women in top positions\", which \"may lead to hysteresis, preventing women from accessing powerful, male-dominated professional networks, or same-sex mentors\". The glass ceiling effect is noted as being especially persistent for women of color (according to a report, \"women of colour perceive a 'concrete ceiling' and not simply a glass ceiling\").\n\nIn the economics profession, it has been observed that women are more inclined than men to dedicate their time to teaching and service. Since continuous research work is crucial for promotion, \"the cumulative effect of small, contemporaneous differences in research orientation could generate the observed significant gender difference in promotion\". In the high-tech industry, research shows that, regardless of the intra-firm changes, \"extra-organizational pressures will likely contribute to continued gender stratification as firms upgrade, leading to the potential masculinization of skilled high-tech work\".\n\nThe United Nations asserts that \"progress in bringing women into leadership and decision making positions around the world remains far too slow\".\n\nResearch by David Matsa and Amalia Miller suggests that a possible remedy to the glass ceiling could be increasing the number of women on corporate boards, which could subsequently lead to increases in the number of women working in top management positions. The same research suggests that this could also result in a \"feedback cycle in which the presence of more female managers increases the qualified pool of potential female board members (for the companies they manage, as well as other companies), leading to greater female board membership and then further increases in female executives\".\n\nA 2009 study found that being overweight harms women's career advancement, but presents no barrier for men. Overweight women were significantly underrepresented among company bosses, making up between 5% and 22% of female CEOs. However, the proportion of overweight male CEOs was between 45% and 61%, over-representing overweight men. On the other hand, approximately 5% of CEOs were obese among both genders. The author of the study stated that the results suggest that \"the 'glass ceiling effect' on women's advancement may reflect not only general negative stereotypes about the competencies of women, but also weight bias that results in the application of stricter appearance standards to women\".\n\nTransgender people also experience significant workplace discrimination and harassment. Unlike sex-based discrimination, refusing to hire (or firing) a worker for their gender identity or expression is not explicitly illegal in most U.S. states.\n\nIn August 1995, Kimberly Nixon filed a complaint with the British Columbia Human Rights Tribunal against Vancouver Rape Relief & Women's Shelter. Nixon, a trans woman, had been interested in volunteering as a counselor with the shelter. When the shelter learned that she was transsexual, they told Nixon that she would not be allowed to volunteer with the organization. Nixon argued that this constituted illegal discrimination under Section 41 of the British Columbia Human Rights Code. Vancouver Rape Relief countered that individuals are shaped by the socialization and experiences of their formative years, and that Nixon had been socialized as a male growing up, and that, therefore, Nixon would not be able to provide sufficiently effective counseling to the female born women that the shelter served.\n\nIn social philosophy, objectification is the act of treating a person as an object or thing. Objectification plays a central role in feminist theory, especially sexual objectification. Feminist writer and gender equality activist Joy Goh-Mah argues that by being objectified, a person is denied agency. According to the philosopher Martha Nussbaum, a person might be objectified if one or more of the following properties are applied to them:\nRae Helen Langton, in \"Sexual Solipsism: Philosophical Essays on Pornography and Objectification\", proposed three more properties to be added to Nussbaum's list:\nAccording to objectification theory, objectification can have important repercussions on women, particularly young women, as it can negatively impact their psychological health and lead to the development of mental disorders, such as unipolar depression, sexual dysfunction, and eating disorders.\n\nWhile advertising used to portray women and men in obviously stereotypical roles (e.g., as a housewife, breadwinner), in modern advertisements, they are no longer solely confined to their traditional roles. However, advertising today nonetheless still stereotypes men and women, albeit in more subtle ways, including by sexually objectifying them. Women are most often targets of sexism in advertising. When in advertisements with men they are often shorter and put in the background of images, shown in more 'feminine' poses, and generally present a higher degree of 'body display'.\n\nToday, some countries (for example Norway and Denmark) have laws against sexual objectification in advertising. Nudity is not banned, and nude people can be used to advertise a product if they are relevant to the product advertised. Sol Olving, head of Norway's Kreativt Forum (an association of the country's top advertising agencies) explained, \"You could have a naked person advertising shower gel or a cream, but not a woman in a bikini draped across a car\".\n\nOther countries continue to ban nudity (on traditional obscenity grounds), but also make explicit reference to sexual objectification, such as Israel's ban of billboards that \"depicts sexual humiliation or abasement, or presents a\nhuman being as an object available for sexual use\".\n\nAnti-pornography feminist Catharine MacKinnon argues that pornography contributes to sexism by objectifying women and portraying them in submissive roles. MacKinnon, along with Andrea Dworkin, argues that pornography reduces women to mere tools, and is a form of sex discrimination. The two scholars highlight the link between objectification and pornography by stating:\n\n\"We define pornography as the graphic sexually explicit subordination of women through pictures and words that also includes (i) women are presented dehumanized as sexual objects, things, or commodities; or (ii) women are presented as sexual objects who enjoy humiliation or pain; or (iii) women are presented as sexual objects experiencing sexual pleasure in rape, incest or other sexual assault; or (iv) women are presented as sexual objects tied up, cut up or mutilated or bruised or physically hurt; or (v) women are presented in postures or positions of sexual submission, servility, or display; or (vi) women's body parts—including but not limited to vaginas, breasts, or buttocks—are exhibited such that women are reduced to those parts; or (vii) women are presented being penetrated by objects or animals; or (viii) women are presented in scenarios of degradation, humiliation, injury, torture, shown as filthy or inferior, bleeding, bruised, or hurt in a context that makes these conditions sexual.\"\n\nRobin Morgan and Catharine MacKinnon suggest that certain types of pornography also contribute to violence against women by eroticizing scenes in which women are dominated, coerced, humiliated or sexually assaulted.\n\nSome people opposed to pornography, including MacKinnon, charge that the production of pornography entails physical, psychological, and economic coercion of the women who perform and model in it. Opponents of pornography charge that it presents a distorted image of sexual relations and reinforces sexual myths; it shows women as continually available and willing to engage in sex at any time, with any person, on their terms, responding positively to any requests.\n\nMacKinnon writes:\nDefenders of pornography and anti-censorship activists (including sex-positive feminists) argue that pornography does not seriously impact a mentally healthy individual, since the viewer can distinguish between fantasy and reality. They contend that men and women are objectified in pornography (particularly sadistic or masochistic pornography, in which men are objectified and sexually used by women).\n\nProstitution is the business or practice of engaging in sexual relations in exchange for payment. Sex workers are often objectified and are seen as existing only to serve clients, thus calling their sense of agency into question. There is a prevailing notion that because they sell sex professionally, prostitutes automatically consent to all sexual contact. As a result, sex workers face higher rates of violence and sexual assault. This is often dismissed, ignored and not taken seriously by authorities.\n\nIn many countries, prostitution is dominated by brothels or pimps, who often claim ownership over sex workers. This sense of ownership furthers the concept that sex workers are void of agency. This is literally the case in instances of sexual slavery.\n\nVarious authors have argued that female prostitution is based on male sexism that condones the idea that unwanted sex with a woman is acceptable, that men's desires must be satisfied, and that women are coerced into and exist to serve men sexually. The European Women's Lobby condemned prostitution as \"an intolerable form of male violence\".\n\nCarole Pateman writes that:\n\nSome scholars believe that media portrayals of demographic groups can both maintain and disrupt attitudes and behaviors toward those groups. According to Susan Douglas: \"Since the early 1990s, much of the media have come to overrepresent women as having made it-completely-in the professions, as having gained sexual equality with men, and having achieved a level of financial success and comfort enjoyed primarily by Tiffany's-encrusted doyennes of Laguna Beach.\" These images may be harmful, particularly to women and racial and ethnic minority groups. For example, a study of African American women found they feel that media portrayals of African American women often reinforce stereotypes of this group as overly sexual and idealize images of lighter-skinned, thinner African American women (images African American women describe as objectifying). In a recent analysis of images of Haitian women in the Associated Press photo archive from 1994 to 2009, several themes emerged emphasizing the \"otherness\" of Haitian women and characterizing them as victims in need of rescue.\n\nIn an attempt to study the effect of media consumption on males, Samantha and Bridges found an effect on body shame, though not through self-objectification as it was found in comparable studies of women. The authors conclude that the current measures of objectification were designed for women and do not measure men accurately. Another study also found a negative effect on eating attitudes and body satisfaction of consumption of beauty and fitness magazines for women and men respectively, but again with different mechanisms, namely self-objectification for women and internalization for men.\n\nFrederick Attenborough argues that sexist jokes can be a form of sexual objectification, which reduce the butt of the joke to an object. They not only objectify women, but can also condone violence or prejudice against women. \"Sexist humor—the denigration of women through humor—for instance, trivializes sex discrimination under the veil of benign amusement, thus precluding challenges or opposition that nonhumorous sexist communication would likely incur.\" A study of 73 male undergraduate students by Ford found that \"sexist humor can promote the behavioral expression of prejudice against women amongst sexist men\". According to the study, when sexism is presented in a humorous manner it is viewed as tolerable and socially acceptable: \"Disparagement of women through humor 'freed' sexist participants from having to conform to the more general and more restrictive norms regarding discrimination against women.\"\n\nGender discrimination is discrimination on the basis of actual or perceived gender identity. Gender identity is \"the gender-related identity, appearance, or mannerisms or other gender-related characteristics of an individual, with or without regard to the individual's designated sex at birth\". Gender discrimination is theoretically different from sexism. Whereas sexism is prejudice based on biological sex, gender discrimination specifically addresses discrimination towards gender identities, including third gender, genderqueer, and other non-binary identified people. It is especially attributed to how people are treated in the workplace, and banning discrimination on the basis of gender identity and expression has emerged as a subject of contention in the American legal system.\n\nAccording to a recent report by the Congressional Research Service, \"although the majority of federal courts to consider the issue have concluded that discrimination on the basis of gender identity is not sex discrimination, there have been several courts that have reached the opposite conclusion\". Hurst states that \"[c]ourts often confuse sex, gender and sexual orientation, and confuse them in a way that results in denying the rights not only of gays and lesbians, but also of those who do not present themselves or act in a manner traditionally expected of their sex\".\n\nOppositional sexism is a term coined by transfeminist author Julia Serano, who defined oppositional sexism as \"the belief that male and female are rigid, mutually exclusive categories\". Oppositional sexism plays a vital role in a number of social norms, such as cissexism, heteronormativity, and traditional sexism.\n\nOppositional sexism normalizes masculine expression in males and feminine expression in females while simultaneously demonizing femininity in males and masculinity in females. This concept plays a crucial role in supporting cissexism, the social norm that views cisgender people as both natural and privileged as opposed to transgender people.\n\nThe idea of having two, totally opposite genders is tied to sexuality through what gender theorist Judith Butler calls a \"compulsory practice of heterosexuality\". Because oppositional sexism is tied to heteronormativity in this way, non-heterosexuals are seen as breaking gender norms.\n\nThe concept of opposite genders sets a \"dangerous precedent\", according to Serano, where \"if men are big then women must be small; and if men are strong then women must be weak\". The gender binary and oppositional norms work together to support \"traditional sexism\", the belief that femininity is inferior to and serves masculinity.\n\nSerano states that oppositional sexism works in tandem with \"traditional sexism\". This ensures that \"those who are masculine have power over those who are feminine, and that only those that are born male will be seen as authentically masculine\".\n\nTransgender discrimination is discrimination towards peoples whose gender identity differs from the social expectations of the biological sex they were born with. Forms of discrimination include but are not limited to identity documents not reflecting one's gender, sex-segregated public restrooms and other facilities, dress codes according to binary gender codes, and lack of access to and existence of appropriate health care services. In a recent adjudication, the Equal Employment Opportunity Commission (EEOC) concluded that discrimination against a transgender person is sex discrimination.\n\nThe 2008-09 National Transgender Discrimination Survey (NTDS)—a U.S. study by the National Center for Transgender Equality and the National Gay and Lesbian Task Force in collaboration with the National Black Justice Coalition that was, at its time, the most extensive survey of transgender discrimination—showed that Black transgender people in the United States suffer \"the combination of anti-transgender bias and persistent, structural and individual racism\" and that \"black transgender people live in extreme poverty that is more than twice the rate for transgender people of all races (15%), four times the general Black population rate 9% and over eight times the general US population rate (4%)\". Further discrimination is faced by gender nonconforming individuals, whether transitioning or not, due to displacement from societally acceptable gender binaries and visible stigmatization. According to the NTDS, transgender gender nonconforming (TGNC) individuals face between 8% and 15% high rates of self and social discrimination and violence than binary transgender individuals. Lisa R. Miller and Eric Anthony Grollman found in their 2015 study that \"gender nonconformity may heighten trans people's exposure to discrimination and health-harming behaviors. Gender nonconforming trans adults reported more events of major and everyday transphobic discrimination than their gender conforming counterparts.\"\n\nIn another study conducted in collaboration with the League of United Latin American Citizens, Latino/a transgender people who were non-citizens were most vulnerable to harassment, abuse and violence.\n\nAn updated version of the NTDS survey, called the 2015 U.S. Transgender Survey, was published in December 2016.\n\nAlthough the exact rates are widely disputed, there is a large body of cross-cultural evidence that women are subjected to domestic violence mostly committed by men. In addition, there is broad consensus that women are more often subjected to severe forms of abuse and are more likely to be injured by an abusive partner. The United Nations recognizes domestic violence as a form of gender-based violence, which it describes as a human rights violation, and the result of sexism.\n\nDomestic violence is tolerated and even legally accepted in many parts of the world. For instance, in 2010, the United Arab Emirates (UAE)'s Supreme Court ruled that a man has the right to physically discipline his wife and children if he does not leave visible marks. In 2015, Equality Now drew attention a section of the Penal Code of Northern Nigeria, titled \"Correction of Child, Pupil, Servant or Wife\" which reads: \"(1) Nothing is an offence which does not amount to the infliction of grievous hurt upon any persons which is done: (...) (d) by a husband for the purpose of correcting his wife, such husband and wife being subject to any native law or custom in which such correction is recognized as lawful.\"\n\nHonor killings are another form of domestic violence practiced in several parts of the world, and their victims are predominantly women. Honor killings can occur because of refusal to enter into an arranged marriage, maintaining a relationship relatives disapprove of, extramarital sex, becoming the victim of rape, dress seen as inappropriate, or homosexuality. The United Nations Office on Drugs and Crime states that, \"[h]onour crimes, including killing, are one of history's oldest forms of gender-based violence\".\n\nAccording to a report of the Special Rapporteur submitted to the 58th session of the United Nations Commission on Human Rights concerning cultural practices in the family that reflect violence against women:\n\nPractices such as honor killings and stoning continue to be supported by mainstream politicians and other officials in some countries. In Pakistan, after the 2008 Balochistan honour killings in which five women were killed by tribesmen of the Umrani Tribe of Balochistan, Pakistani Federal Minister for Postal Services Israr Ullah Zehri defended the practice: \"These are centuries-old traditions, and I will continue to defend them. Only those who indulge in immoral acts should be afraid.\" Following the 2006 case of Sakineh Mohammadi Ashtiani (which has placed Iran under international pressure for its stoning sentences), Mohammad-Javad Larijani (a senior envoy and chief of Iran’s Human Rights Council) defended the practice of stoning; he claimed it was a \"lesser punishment\" than execution, because it allowed those convicted a chance at survival.\n\nDowry deaths are the result of the killing women who are unable to pay the high dowry price for their marriage. According to Amnesty International, \"the ongoing reality of dowry-related violence is an example of what can happen when women are treated as property\".\n\nFemale infanticide is the killing of newborn female children, while female selective abortion is the terminating of a pregnancy based upon the female sex of the fetus. Gendercide is the systematic killing of members of a specific gender and it is an extreme form of gender-based violence. Female infanticide is more common than male infanticide, and is especially prevalent in South Asia, in countries such as China, India and Pakistan. Recent studies suggest that over 90 million girls and women are missing in China and India as a result of infanticide.\n\nSex-selective abortion involves terminating a pregnancy based upon the predicted sex of the baby. The abortion of female fetuses is most common in areas where the culture values male children over females, such as parts of East Asia and South Asia (China, India, Korea), the Caucasus (Azerbaijan, Armenia and Georgia), and Western Balkans (Albania, Macedonia, Montenegro, Kosovo).\n\nForced sterilization and forced abortion are also forms of gender-based violence. Forced sterilization was practiced during the first half of the 20th century by many Western countries and there are reports of this practice being currently employed in some countries, such as Uzbekistan and China.\n\nIn China, the one child policy interacting with the low status of women has been deemed responsible for many abuses, such female infanticide, sex-selective abortion, abandonment of baby girls, forced abortion, and forced sterilization.\n\nIn India the custom of dowry is strongly related to female infanticide, sex-selective abortion, abandonment and mistreatment of girls. Such practices are especially present in the northwestern part of the country (Jammu and Kashmir, Haryana, Punjab, Uttarakhand, Delhi); see Female foeticide in India and Female infanticide in India).\n\nFemale genital mutilation is defined by the World Health Organization (WHO) as \"all procedures that involve partial or total removal of the external female genitalia, or other injury to the female genital organs for non-medical reasons\". WHO further state that, \"the procedure has no health benefits for girls and women\" and \"[p]rocedures can cause severe bleeding and problems urinating, and later cysts, infections, infertility as well as complications in childbirth increased risk of newborn death,\" and \"is recognized internationally as a violation of the human rights of girls and women\" and \"constitutes an extreme form of discrimination against women\". The European Parliament stated in a resolution that the practice \"clearly goes against the European founding value of equality between women and men and maintains traditional values according to which women are seen as the objects and properties of men\".\n\nResearch by Lisak and Roth into factors motivating perpetrators of sexual assault, including rape, against women revealed a pattern of hatred towards women and pleasure in inflicting psychological and physical trauma, rather than sexual interest. Mary Odem and Peggy Reeves Sanday posit that rape is the result not of pathology but of systems of male dominance, cultural practices and beliefs.\n\nMary Odem, Jody Clay-Warner, and Susan Brownmiller argue that sexist attitudes are propagated by a series of myths about rape and rapists. They state that in contrast to those myths, rapists often plan a rape before they choose a victim and acquaintance rape (not assault by a stranger) is the most common form of rape. Odem also asserts that these rape myths propagate sexist attitudes about men, by perpetuating the belief that men cannot control their sexuality.\n\nSexism can promote the stigmatization of women and girls who have been raped and inhibit recovery. In many parts of the world, women who have been raped are ostracized, rejected by their families, subjected to violence, and—in extreme cases—may become victims of honor killings because they are deemed to have brought shame upon their families.\n\nThe criminalization of marital rape is very recent, having occurred during the past few decades; and in many countries it is still legal. Several countries in Eastern Europe and Scandinavia made spousal rape illegal before 1970; other European countries and some of the English-speaking countries outside Europe outlawed it later, mostly in the 1980s and 1990s; some countries outlawed it in the 2000s. The WHO wrote that: \"Marriage is often used to legitimize a range of forms of sexual violence against women. The custom of marrying off young children, particularly girls, is found in many parts of the world. This practice—legal in many countries—is a form of sexual violence, since the children involved are unable to give or withhold their consent\".\n\nIn countries where fornication or adultery are illegal, victims of rape can be charged criminally.\n\nSexism is manifested by the crime of rape targeting women civilians and soldiers, committed by soldiers, combatants or civilians during armed conflict, war or military occupation. This arises from the long tradition of women being seen as sexual booty and from the misogynistic culture of military training.\n\nThe United Nations Population Fund writes that \"Family planning is central to gender equality and women's empowerment\". Women in many countries around the world are denied medical and informational services related to reproductive health, including access to pregnancy care, family planning, and contraception. In countries with very strict abortion laws (particularly in Latin America) women who suffer miscarriages are often investigated by the police under suspicion of having deliberately provoked the miscarriage, and are sometimes jailed, a practice which Amnesty International called a \"ruthless campaign against women's rights\". Doctors may be reluctant to treat pregnant women who are very ill, because they are afraid the treatment may result in fetal loss. According to Amnesty Intentional, \"Discriminatory attitudes towards women and girls also means access to sex education and contraceptives are near impossible [in El Salvador]\". The organization has also criticized laws and policies which require the husband's consent for a woman to use reproductive health services as being discriminatory and dangerous to women's health and life: \"[F]or the woman who needs her husband's consent to get contraception, the consequences of discrimination can be serious – even fatal\".\n\nA child marriage is a marriage where one or both spouses are under 18, a practice that disproportionately affects women. Child marriages are most common in South Asia, the Middle East and Sub-Saharan Africa, but occur in other parts of the world, too. The practice of marrying young girls is rooted in patriarchal ideologies of control of female behavior, and is also sustained by traditional practices such as dowry and bride price. Child marriage is strongly connected with the protection of female virginity. UNICEF states that:\n\nConsequences of child marriage include restricted education and employment prospects, increased risk of domestic violence, child sexual abuse, pregnancy and birth complications, and social isolation. Early and forced marriage are defined as forms of modern-day slavery by the International Labour Organisation. In some cases a woman or girl who has been raped may be forced to marry her rapist, in order to restore the honor of her family; or marriage by abduction, a practice in which a man abducts the woman or girl whom he wishes to marry and rapes her, in order to force the marriage (common in Ethiopia).\n\nIn several OIC countries the legal testimony of a woman is worth legally half of that of a man (see Status of women's testimony in Islam). Such countries include: Algeria (in criminal cases), Bahrain (in Sharia courts), Egypt (in family courts), Iran (in most cases), Iraq (in some cases), Jordan (in Sharia courts), Kuwait (in family courts), Libya (in some cases), Morocco (in family cases), Palestine (in cases related to marriage, divorce and child custody), Qatar (in family law matters), Syria (in Sharia courts), United Arab Emirates (in some civil matters), Yemen (not allowed to testify at all in cases of adultery and retribution), and Saudi Arabia. Such laws have been criticized by Human Rights Watch and Equality Now as being discriminatory towards women.\n\nThe criminal justice system in many common law countries has also been accused of discriminating against women. Provocation is, in many common law countries, a partial defense to murder, which converts what would have been murder into manslaughter. It is meant to be applied when a person kills in the \"heat of passion\" upon being \"provoked\" by the behavior of the victim. This defense has been criticized as being gendered, favoring men, due to it being used disproportionately in cases of adultery, and other domestic disputes when women are killed by their partners. As a result of the defense exhibiting a strong gender bias, and being a form of legitimization of male violence against women and minimization of the harm caused by violence against women, it has been abolished or restricted in several jurisdictions.\n\nThe traditional leniently towards crimes of passion in Latin American countries has been deemed to have its origin in the view that women are property. In 2002, Widney Brown, advocacy director for Human Rights Watch, stated that, \"[S]o-called crimes of passion have a similar dynamic [to honor killings] in that the women are killed by male family members and the crimes are perceived as excusable or understandable.\" The OHCHR has called for \"the elimination of discriminatory provisions in the legislation, including mitigating factors for 'crimes of passion.\n\nIn the United States, some studies have shown that for identical crimes, men are given harsher sentences than women. Controlling for arrest offense, criminal history, and other pre-charge variables, sentences are over 60% heavier for men. Women are more likely to avoid charges entirely, and to avoid imprisonment if convicted. The gender disparity varies according to the nature of the case. For example, the gender gap is less pronounced in fraud cases than in drug trafficking and firearms. This disparity occurs in US federal courts, despite guidelines designed to avoid differential sentencing. The death penalty in may also suffer from gender bias. According to Shatz and Shatz, \"[t]he present study confirms what earlier studies have shown: that the death penalty is imposed on women relatively infrequently and that it is disproportionately imposed for the killing of women\".\n\nThere have been several reasons postulated for the gender criminal justice disparity in the United States. One of the most common is expectation that women are predominantly care-givers. Other possible reasons include the \"girlfriend theory\" (whereby women are seen as tools of their boyfriends), the theory that female defendants are more likely to cooperate with authorities, and that women are often successful at turning their violent crime into victimhood by citing defenses such as postpartum depression or battered wife syndrome. However, none of these theories account for the total disparity, and sexism has also been suggested as an underlying cause.\n\nGender discrimination also helps explain the differences between trial outcomes in which some female defendants are sentenced to death and other female defendants are sentenced to lesser punishments. Phillip Barron argues that female defendants are more likely to be sentenced to death for crimes that violate gender norms, such as killing children or killing strangers.\n\nTransgender people face widespread discrimination while incarcerated. They are generally housed according to their legal birth sex, rather than their gender identity. Studies have shown that transgender people are at an increased risk for harassment and sexual assault in this environment. They may also be denied access to medical procedures related to their reassignment.\n\nSome countries use stoning as a form of capital punishment. According to Amnesty International, the majority of those stoned are women and women are disproportionately affected by stoning because of sexism in the legal system.\n\nOne study found that \"on average, women receive lighter sentences in comparison with men... roughly 30% of the gender differences in incarceration cannot be explained by the observed criminal characteristics of offense and offender. We also find evidence of considerable heterogeneity across judges in their treatment of female and male offenders. There is little evidence, however, that tastes for gender discrimination are driving the mean gender disparity or the variance in treatment between judges.\"\n\nA 2017 study by Knepper found that \"female plaintiffs filing workplace sex discrimination claims are substantially more likely to settle and win compensation whenever a female judge is assigned to the case. Additionally, female judges are 15 percentage points less likely than male judges to grant motions filed by defendants, which suggests that final negotiations are shaped by the emergence of the bias.\"\n\nWomen have traditionally had limited access to higher education. In the past, when women were admitted to higher education, they were encouraged to major in less-scientific subjects; the study of English literature in American and British colleges and universities was instituted as a field considered suitable to women's \"lesser intellects\".\n\nEducational specialties in higher education produce and perpetuate inequality between men and women. Disparity persists particularly in computer and information science, where in the US women received only 21% of the undergraduate degrees, and in engineering, where women obtained only 19% of the degrees in 2008. Only one out of five of physics doctorates in the US are awarded to women, and only about half of those women are American. Of all the physics professors in the country, only 14% are women.\n\nWorld literacy is lower for females than for males. Data from \"The World Factbook\" shows that 79.7% of women are literate, compared to 88.6% of men (aged 15 and over). In some parts of the world, girls continue to be excluded from proper public or private education. In parts of Afghanistan, girls who go to school face serious violence from some local community members and religious groups.\nAccording to 2010 UN estimates, only Afghanistan, Pakistan and Yemen had less than 90 girls per 100 boys at school. Jayachandran and Lleras-Muney's study of Sri Lankan economic development has suggested that increases in the life expectancy for women encourages educational investment because a longer time horizon increases the value of investments that pay out over time.\n\nEducational opportunities and outcomes for women have greatly improved in the West. Since 1991, the proportion of women enrolled in college in the United States has exceeded the enrollment rate for men, and the gap has widened over time. , women made up the majority—54%—of the 10.8 million college students enrolled in the United States. However, research by Diane Halpern has indicated that boys receive more attention, praise, blame and punishment in the grammar-school classroom, and \"this pattern of more active teacher attention directed at male students continues at the postsecondary level\". Over time, female students speak less in a classroom setting.\n\nWriter Gerry Garibaldi has argued that the educational system has become \"feminized\", allowing girls more of a chance at success with a more \"girl-friendly\" environment in the classroom; this is seen to hinder boys by punishing \"masculine\" behavior and diagnosing boys with behavioral disorders. A recent study by the OECD in over 60 countries found that teachers give boys lower grades for the same work. The researchers attribute this to stereotypical ideas about boys and recommend teachers to be aware of this gender bias. One study found that students give female professors worse evaluation scores than male professors, even though the students appear to do as well under female professors as male professors.\n\nFeminists argue that clothing and footwear fashion has been oppressive to women, restricting their movements, increasing their vulnerability, and endangering their health. The use of thin models within the fashion industry has encouraged the development of bulimia and anorexia nervosa, as well as locking female consumers into false feminine identities.\n\nThe assignment of gender-specific baby clothes can instill in children a belief in negative gender stereotypes. One example is the assignment in some countries of the color pink to girls and blue to boys. The fashion is a recent one; at the beginning of the 20th century the trend was the opposite: blue for girls and pink for boys. In the early 1900s, \"The Women's Journal\" wrote that \"pink being a more decided and stronger colour, is more suitable for the boy, while blue, which is more delicate and dainty, is prettier for the girl\". \"DressMaker\" magazine also explained that \"[t]he preferred colour to dress young boys in is pink. Blue is reserved for girls as it is considered paler, and the more dainty of the two colours, and pink is thought to be stronger (akin to red)\". Today, in many countries, it is considered inappropriate for boys to wear dresses and skirts, but this is also a relatively recent view. From the mid-16th century until the late 19th or early 20th century, young boys in the Western world were unbreeched and wore gowns or dresses until an age that varied between two and eight.\n\nLaws that dictate how women must dress are seen by many international human rights organizations, such as Amnesty International, as a form of gender discrimination. In many countries, women are faced with violence for failing to adhere to certain dress codes, whether by the authorities (such as the religious police), family members, or the community. Amnesty International states:\n\nThe production process also faces criticism for sexist practices. In the garment industry, approximately 80 percent of workers are female. Much garment production is located in Asia because of the low labor cost. Women who work in these factories are sexually harassed by managers and male workers, paid low wages, and discriminated against when pregnant.\n\nConscription, or compulsory military service, has been criticized as sexist. Prior to the late 20th century, only men were subjected to conscription, and most countries still require only men to serve in the military.\n\nIn his book \"The Second Sexism: Discrimination Against Men and Boys\" (2012), philosopher David Benatar states that \"[t]he prevailing assumption is that where conscription is necessary, it is only men who should be conscripted and, similarly, that only males should be forced into combat\". This, he believes, \"is a sexist assumption\". Anthropologist Ayse Gül Altinay has commented that \"given equal suffrage rights, there is no other citizenship practice that differentiates as radically between men and women as compulsory male conscription\".\n\nCurrently, only nine countries conscript women into their armed forces: China, Eritrea, Israel, Libya, Malaysia, North Korea, Norway, Peru, and Taiwan. Other countries—such as Finland, Turkey, and Singapore—still use a system of conscription which requires military service from only men, although women are permitted to serve voluntarily. In 2014, Norway became the first NATO country to introduce obligatory military service for women as an act of gender equality and in 2015, the Dutch government started preparing a gender-neutral draft law. The gender selective draft has been challenged in the United States.\n\n\n"}
{"id": "9587", "url": "https://en.wikipedia.org/wiki?curid=9587", "title": "Euthanasia", "text": "Euthanasia\n\nEuthanasia (from ; \"good death\": εὖ, \"eu\"; \"well\" or \"good\" + θάνατος, \"thanatos\"; \"death\") is the practice of intentionally ending a life to relieve pain and suffering.\n\nDifferent countries have different euthanasia laws. The British House of Lords Select Committee on Medical Ethics defines euthanasia as \"a deliberate intervention undertaken with the express intention of ending a life, to relieve intractable suffering\". In the Netherlands and Belgium, euthanasia is understood as \"termination of life by a doctor at the request of a patient\". The Dutch law, however, does not use the term 'euthanasia' but includes the concept under the broader definition of \"assisted suicide and termination of life on request\".\n\nEuthanasia is categorized in different ways, which include voluntary, non-voluntary, or involuntary:\n\n\nIn some countries divisive public controversy occurs over the moral, ethical, and legal issues associated with euthanasia. Passive euthanasia (known as \"pulling the plug\") is legal under some circumstances in many countries. Active euthanasia, however, is legal or \"de facto\" legal in only a handful of countries (for example: Belgium, Canada and Switzerland), which limit it to specific circumstances and require the approval of counselors and doctors or other specialists. In some countries - such as Nigeria, Saudi Arabia and Pakistan - support for active euthanasia is almost non-existent.\n\nLike other terms borrowed from history, \"euthanasia\" has had different meanings depending on usage. The first apparent usage of the term \"euthanasia\" belongs to the historian Suetonius, who described how the Emperor Augustus, \"dying quickly and without suffering in the arms of his wife, Livia, experienced the 'euthanasia' he had wished for.\" The word \"euthanasia\" was first used in a medical context by Francis Bacon in the 17th century, to refer to an easy, painless, happy death, during which it was a \"physician's responsibility to alleviate the 'physical sufferings' of the body.\" Bacon referred to an \"outward euthanasia\"—the term \"outward\" he used to distinguish from a spiritual concept—the euthanasia \"which regards the preparation of the soul.\"\n\nIn current usage, euthanasia has been defined as the \"painless inducement of a quick death\". However, it is argued that this approach fails to properly define euthanasia, as it leaves open a number of possible actions which would meet the requirements of the definition, but would not be seen as euthanasia. In particular, these include situations where a person kills another, painlessly, but for no reason beyond that of personal gain; or accidental deaths that are quick and painless, but not intentional.\n\nAnother approach incorporates the notion of suffering into the definition. The definition offered by the Oxford English Dictionary incorporates suffering as a necessary condition, with \"the painless killing of a patient suffering from an incurable and painful disease or in an irreversible coma\", This approach is included in Marvin Khol and Paul Kurtz's definition of it as \"a mode or act of inducing or permitting death painlessly as a relief from suffering\". Counterexamples can be given: such definitions may encompass killing a person suffering from an incurable disease for personal gain (such as to claim an inheritance), and commentators such as Tom Beauchamp and Arnold Davidson have argued that doing so would constitute \"murder simpliciter\" rather than euthanasia.\n\nThe third element incorporated into many definitions is that of intentionality – the death must be intended, rather than being accidental, and the intent of the action must be a \"merciful death\". Michael Wreen argued that \"the principal thing that distinguishes euthanasia from intentional killing simpliciter is the agent's motive: it must be a good motive insofar as the good of the person killed is concerned.\" Likewise, James Field argued that euthanasia entails a sense of compassion towards the patient, in contrast to the diverse non-compassionate motives of serial killers who work in health care professions. Similarly, Heather Draper speaks to the importance of motive, arguing that \"the motive forms a crucial part of arguments for euthanasia, because it must be in the best interests of the person on the receiving end.\" Definitions such as that offered by the House of Lords Select committee on Medical Ethics take this path, where euthanasia is defined as \"a deliberate intervention undertaken with the express intention of ending a life, to relieve intractable suffering.\" Beauchamp and Davidson also highlight Baruch Brody's \"an act of euthanasia is one in which one person ... (A) kills another person (B) for the benefit of the second person, who actually does benefit from being killed\".\n\nDraper argued that any definition of euthanasia must incorporate four elements: an agent and a subject; an intention; a causal proximity, such that the actions of the agent lead to the outcome; and an outcome. Based on this, she offered a definition incorporating those elements, stating that euthanasia \"must be defined as death that results from the intention of one person to kill another person, using the most gentle and painless means possible, that is motivated solely by the best interests of the person who dies.\" Prior to Draper, Beauchamp and Davidson had also offered a definition that includes these elements. Their definition specifically discounts fetuses to distinguish between abortions and euthanasia:\n\nWreen, in part responding to Beauchamp and Davidson, offered a six-part definition:\n\nWreen also considered a seventh requirement: \"(7) The good specified in (6) is, or at least includes, the avoidance of evil\", although as Wreen noted in the paper, he was not convinced that the restriction was required.\n\nIn discussing his definition, Wreen noted the difficulty of justifying euthanasia when faced with the notion of the subject's \"right to life\". In response, Wreen argued that euthanasia has to be voluntary, and that \"involuntary euthanasia is, as such, a great wrong\". Other commentators incorporate consent more directly into their definitions. For example, in a discussion of euthanasia presented in 2003 by the European Association of Palliative Care (EPAC) Ethics Task Force, the authors offered: \"Medicalized killing of a person without the person's consent, whether nonvoluntary (where the person is unable to consent) or involuntary (against the person's will) is not euthanasia: it is murder. Hence, euthanasia can be voluntary only.\" Although the EPAC Ethics Task Force argued that both non-voluntary and involuntary euthanasia could not be included in the definition of euthanasia, there is discussion in the literature about excluding one but not the other.\n\nEuthanasia may be classified into three types, according to whether a person gives informed consent: voluntary, non-voluntary and involuntary.\n\nThere is a debate within the medical and bioethics literature about whether or not the non-voluntary (and by extension, involuntary) killing of patients can be regarded as euthanasia, irrespective of intent or the patient's circumstances. In the definitions offered by Beauchamp and Davidson and, later, by Wreen, consent on the part of the patient was not considered as one of their criteria, although it may have been required to justify euthanasia. However, others see consent as essential.\n\nVoluntary euthanasia is conducted with the consent of the patient. Active voluntary euthanasia is legal in Belgium, Luxembourg and the Netherlands. Passive voluntary euthanasia is legal throughout the US per \"Cruzan v. Director, Missouri Department of Health\". When the patient brings about their own death with the assistance of a physician, the term assisted suicide is often used instead. Assisted suicide is legal in Switzerland and the U.S. states of California, Oregon, Washington, Montana and Vermont.\n\nNon-voluntary euthanasia is conducted when the consent of the patient is unavailable. Examples include child euthanasia, which is illegal worldwide but decriminalised under certain specific circumstances in the Netherlands under the Groningen Protocol.\n\nInvoluntary euthanasia is conducted against the will of the patient.\n\nVoluntary, non-voluntary and involuntary types can be further divided into passive or active variants. Passive euthanasia entails the withholding treatment necessary for the continuance of life. Active euthanasia entails the use of lethal substances or forces (such as administering a lethal injection), and is the more controversial. While some authors consider these terms to be misleading and unhelpful, they are nonetheless commonly used. In some cases, such as the administration of increasingly necessary, but toxic doses of painkillers, there is a debate whether or not to regard the practice as active or passive.\n\nEuthanasia was practiced in Ancient Greece and Rome: for example, hemlock was employed as a means of hastening death on the island of Kea, a technique also employed in Marseilles. Euthanasia, in the sense of the deliberate hastening of a person's death, was supported by Socrates, Plato and Seneca the Elder in the ancient world, although Hippocrates appears to have spoken against the practice, writing \"I will not prescribe a deadly drug to please someone, nor give advice that may cause his death\" (noting there is some debate in the literature about whether or not this was intended to encompass euthanasia).\n\nThe term \"euthanasia\" in the earlier sense of supporting someone as they died, was used for the first time by Francis Bacon. In his work, \"Euthanasia medica\", he chose this ancient Greek word and, in doing so, distinguished between \"euthanasia interior\", the preparation of the soul for death, and \"euthanasia exterior\", which was intended to make the end of life easier and painless, in exceptional circumstances by shortening life. That the ancient meaning of an easy death came to the fore again in the early modern period can be seen from its definition in the 18th century \"Zedlers Universallexikon\":\n\nEuthanasia: a very gentle and quiet death, which happens without painful convulsions. The word comes from ευ, \"bene\", well, and θανατος, \"mors\", death.\n\nThe concept of euthanasia in the sense of alleviating the process of death goes back to the medical historian, Karl Friedrich Heinrich Marx, who drew on Bacon's philosophical ideas. According to Marx, a doctor had a moral duty to ease the suffering of death through encouragement, support and mitigation using medication. Such an \"alleviation of death\" reflected the contemporary \"zeitgeist\", but was brought into the medical canon of responsibility for the first time by Marx. Marx also stressed the distinction between the theological care of the soul of sick people from the physical care and medical treatment by doctors.\n\nEuthanasia in its modern sense has always been strongly opposed in the Judeo-Christian tradition. Thomas Aquinas opposed both and argued that the practice of euthanasia contradicted our natural human instincts of survival, as did Francois Ranchin (1565–1641), a French physician and professor of medicine, and Michael Boudewijns (1601–1681), a physician and teacher. Other voices argued for euthanasia, such as John Donne in 1624, and euthanasia continued to be practised. In 1678, the publication of Caspar Questel's \"De pulvinari morientibus non-subtrahend\", (\"On the pillow of which the dying should not be deprived\"), initiated debate on the topic. Questel described various customs which were employed at the time to hasten the death of the dying, (including the sudden removal of a pillow, which was believed to accelerate death), and argued against their use, as doing so was \"against the laws of God and Nature\". This view was shared by others who followed, including Philipp Jakob Spener, Veit Riedlin and Johann Georg Krünitz. Despite opposition, euthanasia continued to be practised, involving techniques such as bleeding, suffocation, and removing people from their beds to be placed on the cold ground.\n\nSuicide and euthanasia became more accepted during the Age of Enlightenment. Thomas More wrote of euthanasia in \"Utopia\", although it is not clear if More was intending to endorse the practice. Other cultures have taken different approaches: for example, in Japan suicide has not traditionally been viewed as a sin, as it is used in cases of honor, and accordingly, the perceptions of euthanasia are different from those in other parts of the world.\n\nIn the mid-1800s, the use of morphine to treat \"the pains of death\" emerged, with John Warren recommending its use in 1848. A similar use of chloroform was revealed by Joseph Bullar in 1866. However, in neither case was it recommended that the use should be to hasten death. In 1870 Samuel Williams, a schoolteacher, initiated the contemporary euthanasia debate through a speech given at the Birmingham Speculative Club in England, which was subsequently published in a one-off publication entitled \"Essays of the Birmingham Speculative Club\", the collected works of a number of members of an amateur philosophical society. Williams' proposal was to use chloroform to deliberately hasten the death of terminally ill patients:\n\nThe essay was favourably reviewed in \"The Saturday Review\", but an editorial against the essay appeared in \"The Spectator\". From there it proved to be influential, and other writers came out in support of such views: Lionel Tollemache wrote in favour of euthanasia, as did Annie Besant, the essayist and reformer who later became involved with the National Secular Society, considering it a duty to society to \"die voluntarily and painlessly\" when one reaches the point of becoming a 'burden'. \"Popular Science\" analyzed the issue in May 1873, assessing both sides of the argument. Kemp notes that at the time, medical doctors did not participate in the discussion; it was \"essentially a philosophical enterprise ... tied inextricably to a number of objections to the Christian doctrine of the sanctity of human life\".\n\nThe rise of the euthanasia movement in the United States coincided with the so-called Gilded Age, a time of social and technological change that encompassed an \"individualistic conservatism that praised laissez-faire economics, scientific method, and rationalism\", along with major depressions, industrialisation and conflict between corporations and labour unions. It was also the period in which the modern hospital system was developed, which has been seen as a factor in the emergence of the euthanasia debate.\n\nRobert Ingersoll argued for euthanasia, stating in 1894 that where someone is suffering from a terminal illness, such as terminal cancer, they should have a right to end their pain through suicide. Felix Adler offered a similar approach, although, unlike Ingersoll, Adler did not reject religion. In fact, he argued from an Ethical Culture framework. In 1891, Adler argued that those suffering from overwhelming pain should have the right to commit suicide, and, furthermore, that it should be permissible for a doctor to assist – thus making Adler the first \"prominent American\" to argue for suicide in cases where people were suffering from chronic illness. Both Ingersoll and Adler argued for voluntary euthanasia of adults suffering from terminal ailments. Dowbiggin argues that by breaking down prior moral objections to euthanasia and suicide, Ingersoll and Adler enabled others to stretch the definition of euthanasia.\n\nThe first attempt to legalise euthanasia took place in the United States, when Henry Hunt introduced legislation into the General Assembly of Ohio in 1906. Hunt did so at the behest of Anna Sophina Hall, a wealthy heiress who was a major figure in the euthanasia movement during the early 20th century in the United States. Hall had watched her mother die after an extended battle with liver cancer, and had dedicated herself to ensuring that others would not have to endure the same suffering. Towards this end she engaged in an extensive letter writing campaign, recruited Lurana Sheldon and Maud Ballington Booth, and organised a debate on euthanasia at the annual meeting of the American Humane Association in 1905 – described by Jacob Appel as the first significant public debate on the topic in the 20th century.\n\nHunt's bill called for the administration of an anesthetic to bring about a patient's death, so long as the person is of lawful age and sound mind, and was suffering from a fatal injury, an irrevocable illness, or great physical pain. It also required that the case be heard by a physician, required informed consent in front of three witnesses, and required the attendance of three physicians who had to agree that the patient's recovery was impossible. A motion to reject the bill outright was voted down, but the bill failed to pass, 79 to 23.\n\nAlong with the Ohio euthanasia proposal, in 1906 Assemblyman Ross Gregory introduced a proposal to permit euthanasia to the Iowa legislature. However, the Iowa legislation was broader in scope than that offered in Ohio. It allowed for the death of any person of at least ten years of age who suffered from an ailment that would prove fatal and cause extreme pain, should they be of sound mind and express a desire to artificially hasten their death. In addition, it allowed for infants to be euthanised if they were sufficiently deformed, and permitted guardians to request euthanasia on behalf of their wards. The proposed legislation also imposed penalties on physicians who refused to perform euthanasia when requested: a 6–12 month prison term and a fine of between $200 and $1,000. The proposal proved to be controversial. It engendered considerable debate and failed to pass, having been withdrawn from consideration after being passed to the Committee on Public Health.\n\nAfter 1906 the euthanasia debate reduced in intensity, resurfacing periodically, but not returning to the same level of debate until the 1930s in the United Kingdom.\n\nEuthanasia opponent Ian Dowbiggin argues that the early membership of the Euthanasia Society of America (ESA) reflected how many perceived euthanasia at the time, often seeing it as a eugenics matter rather than an issue concerning individual rights. Dowbiggin argues that not every eugenist joined the ESA \"solely for eugenic reasons\", but he postulates that there were clear ideological connections between the eugenics and euthanasia movements.\n\nThe Voluntary Euthanasia Legalisation Society was founded in 1935 by Charles Killick Millard (now called Dignity in Dying). The movement campaigned for the legalisation of euthanasia in Great Britain.\n\nIn January 1936, King George V was given a fatal dose of morphine and cocaine to hasten his death. At the time he was suffering from cardio-respiratory failure, and the decision to end his life was made by his physician, Lord Dawson. Although this event was kept a secret for over 50 years, the death of George V coincided with proposed legislation in the House of Lords to legalise euthanasia.\n\nA 24 July 1939 killing of a severely disabled infant in Nazi Germany was described in a BBC \"Genocide Under the Nazis Timeline\" as the first \"state-sponsored euthanasia\". Parties that consented to the killing included Hitler's office, the parents, and the Reich Committee for the Scientific Registration of Serious and Congenitally Based Illnesses. \"The Telegraph\" noted that the killing of the disabled infant—whose name was Gerhard Kretschmar, born blind, with missing limbs, subject to convulsions, and reportedly \"an idiot\"— provided \"the rationale for a secret Nazi decree that led to 'mercy killings' of almost 300,000 mentally and physically handicapped people\". While Kretchmar's killing received parental consent, most of the 5,000 to 8,000 children killed afterwards were forcibly taken from their parents.\n\nThe \"euthanasia campaign\" of mass murder gathered momentum on 14 January 1940 when the \"handicapped\" were killed with gas vans and killing centres, eventually leading to the deaths of 70,000 adult Germans. Professor Robert Jay Lifton, author of \"The Nazi Doctors\" and a leading authority on the T4 program, contrasts this program with what he considers to be a genuine euthanasia. He explains that the Nazi version of \"euthanasia\" was based on the work of Adolf Jost, who published \"The Right to Death\" (Das Recht auf den Tod) in 1895. Lifton writes: \n\nJost argued that control over the death of the individual must ultimately belong to the social organism, the state. This concept is in direct opposition to the Anglo-American concept of euthanasia, which emphasizes the \"individual's\" 'right to die' or 'right to death' or 'right to his or her own death,' as the ultimate human claim. In contrast, Jost was pointing to the state's right to kill. ... Ultimately the argument was biological: 'The rights to death [are] the key to the fitness of life.' The state must own death—must kill—in order to keep the social organism alive and healthy.\n\nIn modern terms, the use of \"euthanasia\" in the context of Action T4 is seen to be a euphemism to disguise a program of genocide, in which people were killed on the grounds of \"disabilities, religious beliefs, and discordant individual values\". Compared to the discussions of euthanasia that emerged post-war, the Nazi program may have been worded in terms that appear similar to the modern use of \"euthanasia\", but there was no \"mercy\" and the patients were not necessarily terminally ill. Despite these differences, historian and euthanasia opponent Ian Dowbiggin writes that \"the origins of Nazi euthanasia, like those of the American euthanasia movement, predate the Third Reich and were intertwined with the history of eugenics and Social Darwinism, and with efforts to discredit traditional morality and ethics.\"\n\nOn 6 January 1949, the Euthanasia Society of America presented to the New York State Legislature a petition to legalize euthanasia, signed by 379 leading Protestant and Jewish ministers, the largest group of religious leaders ever to have taken this stance. A similar petition had been sent to the New York Legislature in 1947, signed by approximately 1,000 New York physicians. Roman Catholic religious leaders criticized the petition, saying that such a bill would \"legalize a suicide-murder pact\" and a \"rationalization of the fifth commandment of God, 'Thou Shalt Not Kill.'\" The Right Reverend Robert E. McCormick stated that\n\nThe petition brought tensions between the American Euthanasia Society and the Catholic Church to a head that contributed to a climate of anti-Catholic sentiment generally, regarding issues such as birth control, eugenics, and population control. However, the petition did not result in any legal changes.\n\nHistorically, the euthanasia debate has tended to focus on a number of key concerns. According to euthanasia opponent Ezekiel Emanuel, proponents of euthanasia have presented four main arguments: a) that people have a right to self-determination, and thus should be allowed to choose their own fate; b) assisting a subject to die might be a better choice than requiring that they continue to suffer; c) the distinction between passive euthanasia, which is often permitted, and active euthanasia, which is not substantive (or that the underlying principle–the doctrine of double effect–is unreasonable or unsound); and d) permitting euthanasia will not necessarily lead to unacceptable consequences. Pro-euthanasia activists often point to countries like the Netherlands and Belgium, and states like Oregon, where euthanasia has been legalized, to argue that it is mostly unproblematic.\n\nSimilarly, Emanuel argues that there are four major arguments presented by opponents of euthanasia: a) not all deaths are painful; b) alternatives, such as cessation of active treatment, combined with the use of effective pain relief, are available; c) the distinction between active and passive euthanasia is morally significant; and d) legalising euthanasia will place society on a slippery slope, which will lead to unacceptable consequences. In fact, in Oregon, in 2013, pain wasn't one of the top five reasons people sought euthanasia. Top reasons were a loss of dignity, and a fear of burdening others.\n\nIn the United States in 2013, 47% nationwide supported doctor-assisted suicide. This included 32% of Latinos, 29% of African-Americans, and almost nobody with disabilities.\n\nA 2015 Populus poll in the United Kingdom found broad public support for assisted dying. 82% of people supported the introduction of assisted dying laws, including 86% of people with disabilities.\n\nOne concern is that euthanasia might undermine filial responsibility. In some countries, adult children of impoverished parents are legally entitled to support payments under filial responsibility laws. Thirty out of the fifty United States as well as France, Germany, Singapore, and Taiwan have filial responsibility laws.\n\nWest's \"Encyclopedia of American Law\" states that \"a 'mercy killing' or euthanasia is generally considered to be a criminal homicide\" and is normally used as a synonym of homicide committed at a request made by the patient.\n\nThe judicial sense of the term \"homicide\" includes any intervention undertaken with the express intention of ending a life, even to relieve intractable suffering. Not all homicide is unlawful. Two designations of homicide that carry no criminal punishment are justifiable and excusable homicide. In most countries this is not the status of euthanasia. The term \"euthanasia\" is usually confined to the active variety; the University of Washington website states that \"euthanasia generally means that the physician would act directly, for instance by giving a lethal injection, to end the patient's life\". Physician-assisted suicide is thus not classified as euthanasia by the US State of Oregon, where it is legal under the Oregon Death with Dignity Act, and despite its name, it is not legally classified as suicide either. Unlike physician-assisted suicide, withholding or withdrawing life-sustaining treatments with patient consent (voluntary) is almost unanimously considered, at least in the United States, to be legal. The use of pain medication to relieve suffering, even if it hastens death, has been held as legal in several court decisions.\n\nSome governments around the world have legalized voluntary euthanasia but most commonly it is still considered to be criminal homicide. In the Netherlands and Belgium, where euthanasia has been legalized, it still remains homicide although it is not prosecuted and not punishable if the perpetrator (the doctor) meets certain legal conditions.\n\nIn a historic judgment, the Supreme court of India legalized passive euthanasia. The apex court remarked in the judgment that the Constitution of India values liberty, dignity, autonomy, and privacy. A bench headed by Chief Justice Dipak Misra delivered a unanimous judgment.\n\nA 2010 survey in the United States of more than 10,000 physicians found that 16.3% of physicians would consider halting life-sustaining therapy because the family demanded it, even if they believed that it was premature. Approximately 54.5% would not, and the remaining 29.2% responded \"it depends\". The study also found that 45.8% of physicians agreed that physician-assisted suicide should be allowed in some cases; 40.7% did not, and the remaining 13.5% felt it depended.\n\nIn the United Kingdom, the assisted dying campaign group Dignity in Dying cites research in which 54% of General Practitioners support or are neutral towards a law change on assisted dying. Similarly, a 2017 Doctors.net.uk poll reported in the British Medical Journal stated that 55% of doctors believe assisted dying, in defined circumstances, should be legalised in the UK.\n\nOne concern among healthcare professionals is the possibility of being asked to participate in euthanasia in a situation where they personally believe it to be wrong. In a 1996 study of 852 nurses in adult ICUs, 19% admitted to participating in euthanasia. 30% of those who admitted to it also believed that euthanasia is unethical.\n\nThe Roman Catholic Church condemns euthanasia and assisted suicide as morally wrong. It states that, \"intentional euthanasia, whatever its forms or motives, is murder. It is gravely contrary to the dignity of the human person and to the respect due to the living God, his Creator\". Because of this, the practice is unacceptable within the Church. The Orthodox Church in America, along with other Eastern Orthodox Churches, also opposes euthanasia stating that it must be condemned as murder stating that, \"Euthanasia is the deliberate cessation to end human life.\"\n\nMany non-Catholic churches in the United States take a stance against euthanasia. Among Protestant denominations, the Episcopal Church passed a resolution in 1991 opposing euthanasia and assisted suicide stating that it is \"morally wrong and unacceptable to take a human life to relieve the suffering caused by incurable illnesses.\" Other Protestant churches which oppose euthanasia include:\nThe Church of England accepts passive euthanasia under some circumstances, but is strongly against active euthanasia, and has led opposition against recent attempt to legalise it. The United Church of Canada accepts passive euthanasia under some circumstances, but is in general against active euthanasia, with growing acceptance now that active euthanasia has been partly legalised in Canada..\n\nEuthanasia is a complex issue in Islamic theology; however, in general it is considered contrary to Islamic law and holy texts. Among interpretations of the Koran and Hadith, the early termination of life is a crime, be it by suicide or helping one commit suicide. The various positions on the cessation of medical treatment are mixed and considered a different class of action than direct termination of life, especially if the patient is suffering. Suicide and euthanasia are both crimes in almost all Muslim majority countries.\n\nThere is much debate on the topic of euthanasia in Judaic theology, ethics, and general opinion (especially in Israel and the United States). Passive euthanasia was declared legal by Israel's highest court under certain conditions and has reached some level of acceptance. Active euthanasia remains illegal, however the topic is actively under debate with no clear consensus through legal, ethical, theological and spiritual perspectives.\n\n\n"}
{"id": "51329", "url": "https://en.wikipedia.org/wiki?curid=51329", "title": "Famine", "text": "Famine\n\nA famine is a widespread scarcity of food, caused by several factors including war, inflation, crop failure, population imbalance, or government policies. This phenomenon is usually accompanied or followed by regional malnutrition, starvation, epidemic, and increased mortality. Every inhabited continent in the world has experienced a period of famine throughout history. In the 19th and 20th century, it was generally Southeast and South Asia, as well as Eastern and Central Europe that suffered the most deaths from famine. The numbers dying from famine began to fall sharply from the 2000s.\n\nSome countries, particularly in sub-Saharan Africa, continue to have extreme cases of famine. Since 2010, Africa has been the most affected continent in the world. As of 2017, the United Nations has warned over 20 million are at risk in South Sudan, Somalia, Nigeria and Yemen. The distribution of food has been affected by conflict. Most programmes now direct their aid towards Africa.\nAccording to the United Nations humanitarian criteria, even if there are food shortages with large numbers of people lacking nutrition, a famine is declared only when certain measures of mortality, malnutrition and hunger are met. The criteria are:\nThe declaration of a famine carries no binding obligations on the UN or member states, but serves to focus global attention on the problem.\n\nThe cyclical occurrence of famine has been a mainstay of societies engaged in subsistence agriculture since the dawn of agriculture itself. The frequency and intensity of famine has fluctuated throughout history, depending on changes in food demand, such as population growth, and supply-side shifts caused by changing climatic conditions. Famine was first eliminated in Holland and England during the 17th century, due to the commercialization of agriculture and the implementation of improved techniques to increase crop yields.\n\nIn the 16th and 17th century, the feudal system began to break down, and more prosperous farmers began to enclose their own land and improve their yields to sell the surplus crops for a profit. These capitalist landowners paid their labourers with money, thereby increasing the commercialization of rural society. In the emerging competitive labour market, better techniques for the improvement of labour productivity were increasingly valued and rewarded. It was in the farmer's interest to produce as much as possible on their land in order to sell it to areas that demanded that product. They produced guaranteed surpluses of their crop every year if they could.\n\nSubsistence peasants were also increasingly forced to commercialize their activities because of increasing taxes. Taxes that had to be paid to central governments in money forced the peasants to produce crops to sell. Sometimes they produced industrial crops, but they would find ways to increase their production in order to meet both their subsistence requirements as well as their tax obligations. Peasants also used the new money to purchase manufactured goods. The agricultural and social developments encouraging increased food production were gradually taking place throughout the 16th century, but took off in the early 17th century.\n\nBy the 1590s, these trends were sufficiently developed in the rich and commercialized province of Holland to allow its population to withstand a general outbreak of famine in Western Europe at that time. By that time, the Netherlands had one of the most commercialized agricultural systems in Europe. They grew many industrial crops such as flax, hemp and hops. Agriculture became increasingly specialized and efficient. The efficiency of Dutch agriculture allowed for much more rapid urbanization in the late sixteenth and early seventeenth centuries than anywhere else in Europe. As a result, productivity and wealth increased, allowing the Netherlands to maintain a steady food supply.\n\nBy 1650, English agriculture had also become commercialized on a much wider scale. The last peacetime famine in England was in 1623–24. There were still periods of hunger, as in the Netherlands, but no more famines ever occurred. Common areas for pasture were enclosed for private use and large scale, efficient farms were consolidated. Other technical developments included the draining of marshes, more efficient field use patterns, and the wider introduction of industrial crops. These agricultural developments led to wider prosperity in England and increasing urbanization. By the end of the 17th century, English agriculture was the most productive in Europe. In both England and the Netherlands, the population stabilized between 1650 and 1750, the same time period in which the sweeping changes to agriculture occurred. Famine still occurred in other parts of Europe, however. In Eastern Europe, famines occurred as late as the twentieth century.\n\nBecause of the severity of famine, it was a chief concern for governments and other authorities. In pre-industrial Europe, preventing famine, and ensuring timely food supplies, was one of the chief concerns of many governments, although they were severely limited in their options due to limited levels of external trade and an infrastructure and bureaucracy generally too rudimentary to effect real relief. Most governments were concerned by famine because it could lead to revolt and other forms of social disruption.\n\nBy the mid-19th century and the onset of the Industrial Revolution, it became possible for governments to alleviate the effects of famine through price controls, large scale importation of food products from foreign markets, stockpiling, rationing, regulation of production and charity. The Great Famine of 1845 in Ireland was one of the first famines to feature such intervention, although the government response was often lacklustre. The initial response of the British government to the early phase of the famine was \"prompt and relatively successful,\" according to F. S. L. Lyons. Confronted by widespread crop failure in the autumn of 1845, Prime Minister Sir Robert Peel purchased £100,000 worth of maize and cornmeal secretly from America. Baring Brothers & Co initially acted as purchasing agents for the Prime Minister. The government hoped that they would not \"stifle private enterprise\" and that their actions would not act as a disincentive to local relief efforts. Due to weather conditions, the first shipment did not arrive in Ireland until the beginning of February 1846. The maize corn was then re-sold for a penny a pound.\n\nIn 1846, Peel moved to repeal the Corn Laws, tariffs on grain which kept the price of bread artificially high. The famine situation worsened during 1846 and the repeal of the Corn Laws in that year did little to help the starving Irish; the measure split the Conservative Party, leading to the fall of Peel's ministry. In March, Peel set up a programme of public works in Ireland.\n\nDespite this promising start, the measures undertaken by Peel's successor, Lord John Russell, proved comparatively \"inadequate\" as the crisis deepened. Russell's ministry introduced public works projects, which by December 1846 employed some half million Irish and proved impossible to administer. The government was influenced by a laissez-faire belief that the market would provide the food needed. It halted government food and relief works, and turned to a mixture of \"indoor\" and \"outdoor\" direct relief; the former administered in workhouses through the Poor Law, the latter through soup kitchens.\n\nA systematic attempt at creating the necessary regulatory framework for dealing with famine was developed by the British Raj in the 1880s. In order to comprehensively address the issue of famine, the British created an Indian Famine commission to recommend steps that the government would be required to take in the event of a famine. The Famine Commission issued a series of government guidelines and regulations on how to respond to famines and food shortages called the Famine Code. The famine code was also one of the first attempts to scientifically predict famine in order to mitigate its effects. These were finally passed into law in 1883 under Lord Ripon.\n\nThe Code introduced the first famine scale: three levels of food insecurity were defined: near-scarcity, scarcity, and famine. \"Scarcity\" was defined as three successive years of crop failure, crop yields of one-third or one-half normal, and large populations in distress. \"Famine\" further included a rise in food prices above 140% of \"normal\", the movement of people in search of food, and widespread mortality. The Commission identified that the loss of wages from lack of employment of agricultural labourers and artisans were the cause of famines. The Famine Code applied a strategy of generating employment for these sections of the population and relied on open-ended public works to do so.\nDuring the 20th century, an estimated 70 to 100 million people died from famines across the world, of whom over half died in China, with an estimated 30 million dying during the famine of 1958–61, up to 10 million in the Chinese famine of 1928–30, and over two million in the Chinese famine of 1942–43, and millions more lost in famines in North and East China. The USSR lost approaching 20 million, between the 8 million claimed by the Soviet famine of 1932–33, over a million in both the Soviet famine of 1946–47 and Siege of Leningrad, the 5 million in the Russian famine of 1921–22, and others famines. Approximately 3 million died as a consequence of the Second Congo War, and Java suffered 2.5 million deaths under Japanese occupation. The other most notable famine of the century was the man-made Bengal famine of 1943, resulting more from the Japanese occupation of Burma, resulting in an influx of refugees, and blocking Burmese grain imports, a failure of the Bengali provincial Government to declare a famine, and fund relief, the imposition of grain and transport embargoes by the neighbouring provincial administrations, to prevent their own stocks being transferred to Bengal, the failure to implement India wide rationing by the central Delhi authority, hoarding and profiteering by merchants, medieval land management practices, an Axes powers denial program that confiscated boats once used to transport grain, programs of civil disobedience and sabotage to force an end of the British Raj, a Delhi administration that prioritised supplying, and offering medical treatment to the British Indian Army, War workers, and Civil servants, over the populous at large, incompetence and ignorance, and an Imperial War Cabinet initially leaving the issue to the Colonial administration to resolve, than to the original local crop failures, and blights.\n\nA few of the great famines of the late 20th century were: the Biafran famine in the 1960s, the Khmer Rouge-caused famine in Cambodia in the 1970s, the North Korean famine of the 1990s and the Ethiopian famine of 1983–85.\n\nThe latter event was reported on television reports around the world, carrying footage of starving Ethiopians whose plight was centered around a feeding station near the town of Korem. This stimulated the first mass movements to end famine across the world.\n\nBBC newsreader Michael Buerk gave moving commentary of the tragedy on 23 October 1984, which he described as a \"biblical famine\". This prompted the Band Aid single, which was organized by Bob Geldof and featured more than 20 pop stars. The Live Aid concerts in London and Philadelphia raised even more funds for the cause. Hundreds of thousands of people died within one year as a result of the famine, but the publicity Live Aid generated encouraged Western nations to make available enough surplus grain to end the immediate hunger crisis in Africa.\n\nSome of the famines of the 20th century served the geopolitical purposes of governments, including traumatizing and replacing distrusted ethnic populations in strategically important regions, rendering regions vulnerable to invasion difficult to govern by an enemy power and shifting the burden of food shortage onto regions where the distress of the population posed a lesser risk of catastrophic regime de-legitimation.\n\nUntil 2017, worldwide deaths from famine had been falling dramatically. The World Peace Foundation reported that from the 1870s to the 1970s, great famines killed an average of 928,000 people a year. Since 1980, annual deaths had dropped to an average of 75,000, less than 10% of what they had been until the 1970s. That reduction was achieved despite the approximately 150,000 lives lost in the 2011 Somalia famine. Yet in 2017, the UN officially declared famine had returned to Africa, with about 20 million people at risk of death from starvation in Nigeria, in South Sudan, in Yemen, and in Somalia.\n\nIn the mid-22nd century BC, a sudden and short-lived climatic change that caused reduced rainfall resulted in several decades of drought in Upper Egypt. The resulting famine and civil strife is believed to have been a major cause of the collapse of the Old Kingdom.\nAn account from the First Intermediate Period states, \"All of Upper Egypt was dying of hunger and people were eating their children.\" In 1680s, famine extended across the entire Sahel, and in 1738 half the population of Timbuktu died of famine. In Egypt, between 1687 and 1731, there were six famines. The famine that afflicted Egypt in 1784 cost it roughly one-sixth of its population. The Maghreb experienced famine and plague in the late 18th century and early 19th century. There was famine in Tripoli in 1784, and in Tunis in 1785.\n\nAccording to John Iliffe, \"Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys.\"\n\nThe first documentation of weather in West-Central Africa occurs around the mid-16th to 17th centuries in areas such as Luanda Kongo, however, not much data was recorded on the issues of weather and disease except for a few notable documents. The only records obtained are of violence between Portuguese and Africans during the Battle of Mbilwa in 1665. In these documents the Portuguese wrote of African raids on Portuguese merchants solely for food, giving clear signs of famine. Additionally, instances of cannibalism by the African Jaga were also more prevalent during this time frame, indicating an extreme deprivation of a primary food source.\n\n\nA notable period of famine occurred around the turn of the 20th century in the Congo Free State. In forming this state, Leopold used mass labor camps to finance his empire. This period resulted in the death of up to 10 million Congolese from brutality, disease and famine. Some colonial \"pacification\" efforts often caused severe famine, notably with the repression of the Maji Maji revolt in Tanganyika in 1906. The introduction of cash crops such as cotton, and forcible measures to impel farmers to grow these crops, sometimes impoverished the peasantry in many areas, such as northern Nigeria, contributing to greater vulnerability to famine when severe drought struck in 1913.\n\nA large-scale famine occurred in Ethiopia in 1888 and succeeding years, as the rinderpest epizootic, introduced into Eritrea by infected cattle, spread southwards reaching ultimately as far as South Africa. In Ethiopia it was estimated that as much as 90 percent of the national herd died, rendering rich farmers and herders destitute overnight. This coincided with drought associated with an el Nino oscillation, human epidemics of smallpox, and in several countries, intense war. The Ethiopian Great famine that afflicted Ethiopia from 1888 to 1892 cost it roughly one-third of its population. In Sudan the year 1888 is remembered as the worst famine in history, on account of these factors and also the exactions imposed by the Mahdist state.\n\nRecords compiled for the Himba recall two droughts from 1910 to 1917. They were recorded by the Himba through a method of oral tradition. From 1910 to 1911 the Himba described the drought as \"drought of the omutati seed\" also called \"omangowi\", which means the fruit of an unidentified vine that people ate during the time period. From 1914 to 1916 droughts brought \"katur' ombanda\" or \"kari' ombanda\" which means \"the time of eating clothing\".\n\n\nFor the middle part of the 20th century, agriculturalists, economists and geographers did not consider Africa to be especially famine prone. From 1870 to 2010, 87 per cent of deaths from famine occurred in Asia and Eastern Europe, with only 9.2 per cent in Africa. There were notable counter-examples, such as the famine in Rwanda during World War II and the Malawi famine of 1949, but most famines were localized and brief food shortages. Although the drought was brief the main cause of death in Rwanda was due to Belgian prerogatives to acquisition grain from their colony (Rwanda). The increased grain acquisition was related to WW2. This and the drought caused 300,000 Rwandans to perish.\n\nFrom 1967 to 1969 large scale famine occurred in Biafra and Nigeria due to a government blockade of the Breakaway territory. It is estimated that 1.5 million people died of starvation due to this famine. Additionally, drought and other government interference with the food supply caused 500 thousand Africans to perish in Central and West Africa.\n\nFamine recurred in the early 1970s, when Ethiopia and the west African Sahel suffered drought and famine. The Ethiopian famine of that time was closely linked to the crisis of feudalism in that country, and in due course helped to bring about the downfall of the Emperor Haile Selassie. The Sahelian famine was associated with the slowly growing crisis of pastoralism in Africa, which has seen livestock herding decline as a viable way of life over the last two generations.\nFamines occurred in Sudan in the late-1970s and again in 1990 and 1998. The 1980 famine in Karamoja, Uganda was, in terms of mortality rates, one of the worst in history. 21% of the population died, including 60% of the infants. In the 1980s, large scale multilayer drought occurred in the Sudan and Sahelian regions of Africa. This caused famine because even though the Sudanese Government believed there was a surplus of grain, there were local deficits across the region.\n\nIn October 1984, television reports describing the Ethiopian famine as \"biblical\", prompted the Live Aid concerts in London and Philadelphia, which raised large sums to alleviate the suffering. A primary cause of the famine (one of the largest seen in the country) is that Ethiopia (and the surrounding Horn) was still recovering from the droughts which occurred in the mid-late 1970s. Compounding this problem was the intermittent fighting due to civil war, the government's lack of organization in providing relief, and hoarding of supplies to control the population. Ultimately, over 1 million Ethiopians died and over 22 million people suffered due to the prolonged drought, which lasted roughly 2 years.\n\nIn 1992 Somalia became a war zone with no effective government, police, or basic services after the collapse of the dictatorship led by Siad Barre and the split of power between warlords. This coincided with a massive drought, causing over 300,000 Somalis to perish.\n\n\nSince the start of the 21st century, more effective early warning and humanitarian response actions have reduced the number of deaths by famine markedly. That said, many African countries are not self-sufficient in food production, relying on income from cash crops to import food. Agriculture in Africa is susceptible to climatic fluctuations, especially droughts which can reduce the amount of food produced locally. Other agricultural problems include soil infertility, land degradation and erosion, swarms of desert locusts, which can destroy whole crops, and livestock diseases. Desertification is increasingly problematic: the Sahara reportedly spreads up to per year. The most serious famines have been caused by a combination of drought, misguided economic policies, and conflict. The 1983–85 famine in Ethiopia, for example, was the outcome of all these three factors, made worse by the Communist government's censorship of the emerging crisis. In Capitalist Sudan at the same date, drought and economic crisis combined with denials of any food shortage by the then-government of President Gaafar Nimeiry, to create a crisis that killed perhaps 250,000 people—and helped bring about a popular uprising that overthrew Nimeiry.\n\nNumerous factors make the food security situation in Africa tenuous, including political instability, armed conflict and civil war, corruption and mismanagement in handling food supplies, and trade policies that harm African agriculture. An example of a famine created by human rights abuses is the 1998 Sudan famine. AIDS is also having long-term economic effects on agriculture by reducing the available workforce, and is creating new vulnerabilities to famine by overburdening poor households. On the other hand, in the modern history of Africa on quite a few occasions famines acted as a major source of acute political instability. In Africa, if current trends of population growth and soil degradation continue, the continent might\nbe able to feed just 25% of its population by 2025, according to United Nations University (UNU)'s Ghana-based Institute for Natural Resources in Africa.\nRecent famines in Africa include the 2005–06 Niger food crisis, the 2010 Sahel famine and the 2011 East Africa drought, where two consecutive missed rainy seasons precipitated the worst drought in East Africa in 60 years. An estimated 50,000 to 150,000 people are reported to have died during the period. In 2012, the Sahel drought put more than 10 million people in the western Sahel at risk of famine (according to a Methodist Relief & Development Fund (MRDF) aid expert), due to a month-long heat wave.<ref name=\"http://www.poverties.org Famine in Africa\"></ref>\n\nToday, famine is most widespread in Sub-Saharan Africa, but with exhaustion of food resources, overdrafting of groundwater, wars, internal struggles, and economic failure, famine continues to be a worldwide problem with hundreds of millions of people suffering. These famines cause widespread malnutrition and impoverishment. The famine in Ethiopia in the 1980s had an immense death toll, although Asian famines of the 20th century have also produced extensive death tolls. Modern African famines are characterized by widespread destitution and malnutrition, with heightened mortality confined to young children.\n\nAgainst a backdrop of conventional interventions through the state or markets, alternative initiatives have been pioneered to address the problem of food security. One pan-African example is the Great Green Wall. Another example is the \"Community Area-Based Development Approach\" to agricultural development (\"CABDA\"), an NGO programme with the objective of providing an alternative approach to increasing food security in Africa. CABDA proceeds through specific areas of intervention such as the introduction of drought-resistant crops and new methods of food production such as agro-forestry. Piloted in Ethiopia in the 1990s it has spread to Malawi, Uganda, Eritrea and Kenya. In an analysis of the programme by the Overseas Development Institute, CABDA's focus on individual and community capacity-building is highlighted. This enables farmers to influence and drive their own development through community-run institutions, bringing food security to their household and region.\n\nThe organization of African unity and its role in the African crisis has been interested in the political aspects of the continent, especially the liberation of the occupied parts of it and the elimination of racism. The organization has succeeded in this area but the economic field and development has not succeeded in these fields. African leaders have agreed to waive the role of their organization in the development to the United Nations through the Economic Commission for Africa \"ECA\".\n\nChinese scholars had kept count of 1,828 instances of famine from 108 BC to 1911 in one province or another—an average of close to one famine per year. From 1333 to 1337 a terrible famine killed 6 million Chinese. The four famines of 1810, 1811, 1846, and 1849 are said to have killed no fewer than 45 million people.\n\nJapan experienced more than 130 famines between 1603 and 1868.\n\nThe period from 1850 to 1873 saw, as a result of the Taiping Rebellion, drought, and famine, the population of China drop by over 30 million people. China's Qing Dynasty bureaucracy, which devoted extensive attention to minimizing famines, is credited with averting a series of famines following El Niño-Southern Oscillation-linked droughts and floods. These events are comparable, though somewhat smaller in scale, to the ecological trigger events of China's vast 19th-century famines. Qing China carried out its relief efforts, which included vast shipments of food, a requirement that the rich open their storehouses to the poor, and price regulation, as part of a state guarantee of subsistence to the peasantry (known as \"ming-sheng\").\n\nWhen a stressed monarchy shifted from state management and direct shipments of grain to monetary charity in the mid-19th century, the system broke down. Thus the 1867–68 famine under the Tongzhi Restoration was successfully relieved but the Great North China Famine of 1877–78, caused by drought across northern China, was a catastrophe. The province of Shanxi was substantially depopulated as grains ran out, and desperately starving people stripped forests, fields, and their very houses for food. Estimated mortality is 9.5 to 13 million people.\n\nThe largest famine of the 20th century, and almost certainly of all time, was the 1958–61 Great Leap Forward famine in China. The immediate causes of this famine lay in Mao Zedong's ill-fated attempt to transform China from an agricultural nation to an industrial power in one huge leap. Communist Party cadres across China insisted that peasants abandon their farms for collective farms, and begin to produce steel in small foundries, often melting down their farm instruments in the process. Collectivisation undermined incentives for the investment of labor and resources in agriculture; unrealistic plans for decentralized metal production sapped needed labor; unfavorable weather conditions; and communal dining halls encouraged overconsumption of available food. Such was the centralized control of information and the intense pressure on party cadres to report only good news—such as production quotas met or exceeded—that information about the escalating disaster was effectively suppressed. When the leadership did become aware of the scale of the famine, it did little to respond, and continued to ban any discussion of the cataclysm. This blanket suppression of news was so effective that very few Chinese citizens were aware of the scale of the famine, and the greatest peacetime demographic disaster of the 20th century only became widely known twenty years later, when the veil of censorship began to lift.\n\nThe exact number of famine deaths during 1958–61 is difficult to determine, and estimates range from 18 to at least 42 million people, with a further 30 million cancelled or delayed births. It was only when the famine had wrought its worst that Mao reversed agricultural collectivisation policies, which were effectively dismantled in 1978. China has not experienced a famine of the proportions of the Great Leap Forward since 1961.\n\n\nIn 1975, the Khmer Rouge took control of Cambodia. The new government was led by Pol Pot, who desired to turn Cambodia into a communist, agrarian utopia. His regime emptied the cities, abolished currency and private property, and forced Cambodia's population into slavery on communal farms. In less than four years, the Khmer Rouge had executed nearly 1.4 million people, mostly those believed to be a threat to the new ideology.\n\nDue to the failure of the Khmer Rouge's agrarian reform policies, Cambodia experienced widespread famine. As many as one million more died from starvation, disease, and exhaustion resulting from these policies. In 1979 Vietnam invaded Cambodia and removed the Khmer Rouge from power. By that time about one quarter of Cambodia's population had been killed.\n\nFamine struck North Korea in the mid-1990s, set off by unprecedented floods. This autarkic urban, industrial state depended on massive inputs of subsidised goods, including fossil fuels, primarily from the Soviet Union and the People's Republic of China. When the Soviet collapse and China's marketization switched trade to a hard currency, full-price basis, North Korea's economy collapsed. The vulnerable agricultural sector experienced a massive failure in 1995–96, expanding to full-fledged famine by 1996–99.\n\nEstimates based on the North Korean census suggest that 240,000 to 420,000 people died as a result of the famine and that there were 600,000 to 850,000 unnatural deaths in North Korea from 1993 to 2008. North Korea has not yet regained food self-sufficiency and relies on external food aid from China, Japan, South Korea, Russia and the United States. While Woo-Cumings have focused on the FAD side of the famine, Moon argues that FAD shifted the incentive structure of the authoritarian regime to react in a way that forced millions of disenfranchised people to starve to death.\n\nAccording to the UN's Food and Agriculture Organisation (FAO), North Korea is facing a serious cereal shortfall in 2017 after the country's crop harvest was diminished as a result of severe drought. The FAO estimated that early-season production fell by over 30 percent compared to agricultural output from the previous year, leading to the country's worst famine since 2001.\n\nVarious famines have occurred in Vietnam. Japanese occupation during World War II caused the Vietnamese Famine of 1945, which caused 2 million deaths, or 10% of the population then. Following the unification of the country after the Vietnam War, Vietnam experienced a food shortage in the 1980s, which prompted many people to flee the country.\n\nOwing to its almost entire dependence upon the monsoon rains, India is vulnerable to crop failures, which upon occasion deepen into famine. There were 14 famines in India between the 11th and 17th centuries (Bhatia, 1985). For example, during the 1022–1033 Great famines in India entire provinces were depopulated. Famine in Deccan killed at least two million people in 1702–1704. B.M. Bhatia believes that the earlier famines were localised, and it was only after 1860, during the British rule, that famine came to signify general shortage of foodgrains in the country. There were approximately 25 major famines spread through states such as Tamil Nadu in the south, and Bihar and Bengal in the east during the latter half of the 19th century.\nRomesh Chunder Dutt argued as early as 1900, and present-day scholars such as Amartya Sen agree, that some historic famines were a product of both uneven rainfall and British economic and administrative policies, which since 1857 had led to the seizure and conversion of local farmland to foreign-owned plantations, restrictions on internal trade, heavy taxation of Indian citizens to support British expeditions in Afghanistan (see The Second Anglo-Afghan War), inflationary measures that increased the price of food, and substantial exports of staple crops from India to Britain. (Dutt, 1900 and 1902; Srivastava, 1968; Sen, 1982; Bhatia, 1985.)\n\nSome British citizens, such as William Digby, agitated for policy reforms and famine relief, but Lord Lytton, the governing British viceroy in India, opposed such changes in the belief that they would stimulate shirking by Indian workers. The first, the Bengal famine of 1770, is estimated to have taken around 10 million lives—one-third of Bengal's population at the time. Other notable famines include the Great Famine of 1876–78, in which 6.1 million to 10.3 million people died and the Indian famine of 1899–1900, in which 1.25 to 10 million people died. The famines were ended by the 20th century with the exception of the Bengal famine of 1943 killing an estimated 2.1 million Bengalis during World War II.\n\nThe observations of the Famine Commission of 1880 support the notion that food distribution is more to blame for famines than food scarcity. They observed that each province in British India, including Burma, had a surplus of foodgrains, and the annual surplus was 5.16 million tons (Bhatia, 1970). At that time, annual export of rice and other grains from India was approximately one million tons.\n\nThe Maharashtra drought in which there were zero deaths and one which is known for the successful employment of famine prevention policies, unlike during British rule.\n\nThe Great Persian famine of 1870–1872 is believed to have caused the death of 1.5 million persons (20–25 percent of the population) in Persia (present-day Iran).\n\nIn the early 20th century an Ottoman blockade of food being exported to Lebanon caused a famine which killed up to 450,000 Lebanese (about one-third of the population). The famine killed more people than the Lebanese Civil War. The blockade was caused by uprisings in the Syrian region of the Empire including one which occurred in the 1860s which lead to the massacre of thousands of Lebanese and Syrian by Ottoman Turks and local Druze.\n\nThe Great Famine of 1315–1317 (or to 1322) was the first major food crisis to strike Europe in the 14th century. Millions in northern Europe died over an extended number of years, marking a clear end to the earlier period of growth and prosperity during the 11th and 12th centuries. An unusually cold and wet spring of 1315 led to widespread crop failures, which lasted until at least the summer of 1317; some regions in Europe did not fully recover until 1322. Most nobles, cities, and states were slow to respond to the crisis and when they realized its severity, they had little success in securing food for their people. In 1315, in Norfolk England, the price of grain soared from 5 shillings/quarter to 20 shillings/quarter. It was a period marked by extreme levels of criminal activity, disease and mass death, infanticide, and cannibalism. It had consequences for Church, State, European society and future calamities to follow in the 14th century. There were 95 famines in medieval Britain, and 75 or more in medieval France. More than 10% of England's population, or at least 500,000 people, may have died during the famine of 1315–1316.\n\nFamine was a very destabilizing and devastating occurrence. The prospect of starvation led people to take desperate measures. When scarcity of food became apparent to peasants, they would sacrifice long-term prosperity for short-term survival. They would kill their draught animals, leading to lowered production in subsequent years. They would eat their seed corn, sacrificing next year's crop in the hope that more seed could be found. Once those means had been exhausted, they would take to the road in search of food. They migrated to the cities where merchants from other areas would be more likely to sell their food, as cities had a stronger purchasing power than did rural areas. Cities also administered relief programs and bought grain for their populations so that they could keep order. With the confusion and desperation of the migrants, crime would often follow them. Many peasants resorted to banditry in order to acquire enough to eat.\n\nOne famine would often lead to difficulties in the following years because of lack of seed stock or disruption of routine, or perhaps because of less-available labour. Famines were often interpreted as signs of God's displeasure. They were seen as the removal, by God, of His gifts to the people of the Earth. Elaborate religious processions and rituals were made to prevent God's wrath in the form of famine.\n\nDuring the 15th century to the 18th century, famines in Europe became more frequent due to the Little Ice Age. The colder climate resulted in harvest failures and shortfalls that led to a rise in conspiracy theories concerning the causes behind these famines, such as the Pacte de Famine in France.\n\nThe 1590s saw the worst famines in centuries across all of Europe. Famine had been relatively rare during the 16th century. The economy and population had grown steadily as subsistence populations tend to when there is an extended period of relative peace (most of the time). Although peasants in areas of high population density, such as northern Italy, had learned to increase the yields of their lands through techniques such as promiscuous culture, they were still quite vulnerable to famines, forcing them to work their land even more intensively.\n\nThe great famine of the 1590s began a period of famine and decline in the 17th century. The price of grain, all over Europe was high, as was the population. Various types of people were vulnerable to the succession of bad harvests that occurred throughout the 1590s in different regions. The increasing number of wage labourers in the countryside were vulnerable because they had no food of their own, and their meager living was not enough to purchase the expensive grain of a bad-crop year. Town labourers were also at risk because their wages would be insufficient to cover the cost of grain, and, to make matters worse, they often received less money in bad-crop years since the disposable income of the wealthy was spent on grain. Often, unemployment would be the result of the increase in grain prices, leading to ever-increasing numbers of urban poor.\n\nAll areas of Europe were badly affected by the famine in these periods, especially rural areas. The Netherlands was able to escape most of the damaging effects of the famine, though the 1590s were still difficult years there. Amsterdam's grain trade with the Baltic guaranteed a food supply.\n\nThe years around 1620 saw another period of famine sweep across Europe. These famines were generally less severe than the famines of twenty-five years earlier, but they were nonetheless quite serious in many areas. Perhaps the worst famine since 1600, the great famine in Finland in 1696, killed one-third of the population.\n\nDevastating harvest failures afflicted the northern Italian economy from 1618 to 1621, and it did not recover fully for centuries. There were serious famines in the late-1640s and less severe ones in the 1670s throughout northern Italy.\n\nOver two million people died in two famines in France between 1693 and 1710. Both famines were made worse by ongoing wars.\nAs late as the 1690s, Scotland experienced famine which reduced the population of parts of Scotland by at least 15%.\n\nThe Great Famine of 1695–1697 may have killed a third of the Finnish population. and roughly 10% of Norway's population. Death rates rose in Scandinavia between 1740 and 1800 as the result of a series of crop failures. For instance, the Finnish famine of 1866–1868 killed 15% of the population.\n\nThe period of 1740–43 saw frigid winters and summer droughts, which led to famine across Europe and a major spike in mortality. The winter 1740–41 was unusually cold, possibly because of volcanic activity.\n\nAccording to Scott and Duncan (2002), \"Eastern Europe experienced more than 150 recorded famines between AD 1500 and 1700 and there were 100 hunger years and 121 famine years in Russia between AD 971 and 1974.\"\n\nThe Great Famine, which lasted from 1770 until 1771, killed about one tenth of Czech lands' population, or 250,000 inhabitants, and radicalised countrysides leading to peasant uprisings.\n\nThere were sixteen good harvests and 111 famine years in northern Italy from 1451 to 1767. According to Stephen L. Dyson and Robert J. Rowland, \"The Jesuits of Cagliari [in Sardinia] recorded years during the late 1500s \"of such hunger and so sterile that the majority of the people could sustain life only with wild ferns and other weeds\"... During the terrible famine of 1680, some 80,000 persons, out of a total population of 250,000, are said to have died, and entire villages were devastated...\"\n\nAccording to Bryson (1974), there were thirty-seven famine years in Iceland between 1500 and 1804. In 1783 the volcano Laki in south-central Iceland erupted. The lava caused little direct damage, but ash and sulphur dioxide spewed out over most of the country, causing three-quarters of the island's livestock to perish. In the following famine, around ten thousand people died, one-fifth of the population of Iceland. [Asimov, 1984, 152–53]\n\nOther areas of Europe have known famines much more recently. France saw famines as recently as the 19th century. The Great Famine in Ireland, 1846–1851, caused by the failure of the potato crop over a few years, resulted in 1,000,000 dead and another 2,000,000 refugees fleeing to Britain, Australia and the United States.\n\nFamine still occurred in Eastern Europe during the 20th century. Droughts and famines in Imperial Russia are known to have happened every 10 to 13 years, with average droughts happening every 5 to 7 years. Russia experienced eleven major famines between 1845 and 1922, one of the worst being the famine of 1891–92. The Russian famine of 1921–22 killed an estimated 5 million.\nFamines continued in the Soviet era, the most notorious being the \"Holodomor\" in various parts of the country, especially the Volga, and the Ukrainian and northern Kazakh SSR's during the winter of 1932–1933. The Soviet famine of 1932–1933 is nowadays reckoned to have cost an estimated 6 million lives. The last major famine in the USSR happened in 1947 due to the severe drought and the mismanagement of grain reserves by the Soviet government.\n\nThe Hunger Plan, i.e. the Nazi plan to starve large sections of the Soviet population, caused the deaths of many. The Russian Academy of Sciences in 1995 reported civilian victims in the USSR at German hands, including Jews, totalled 13.7 million dead, 20% of the 68 million persons in the occupied USSR. This included 4.1 million famine and disease deaths in occupied territory. There were an additional estimated 3 million famine deaths in areas of the USSR not under German occupation.\n\nThe 872 days of the Siege of Leningrad (1941–1944) caused unparalleled famine in the Leningrad region through disruption of utilities, water, energy and food supplies. This resulted in the deaths of about one million people.\n\nFamine even struck in Western Europe during the Second World War. In the Netherlands, the \"Hongerwinter\" of 1944 killed approximately 30,000 people. Some other areas of Europe also experienced famine at the same time.\n\nThe pre-Columbian Americans often dealt with severe food shortages and famines. The persistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico.\n\nBrazil's 1877–78 \"Grande Seca\" (Great Drought), the worst in Brazil's history, caused approximately half a million deaths. The one from 1915 was devastating too.\n\nIn 2019, \"The New York Times\" reported children dying of hunger in Venezuela, caused by government policies.\n\nEaster Island was hit by a great famine between the 15th and 18th centuries. Hunger and subsequent cannibalism was caused by overpopulation and depletion of natural resources as a result of deforestation, partly because work on megalithic monuments required a lot of wood.\n\nThere are other documented episodes of famine in various islands of Polynesia, such as occurred in Kau, Hawaii in 1868.\n\nAccording to Daniel Lord Smail, \"'Famine cannibalism' was until recently a regular feature of life in the islands of the Massim near New Guinea and of some other societies of Southeast Asia and the Pacific.\"\n\n\"The Guardian\" reports that, as of 2007, approximately 40% of the world's agricultural land is seriously degraded. If current trends of soil degradation continue in Africa, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa. As of late 2007, increased farming for use in biofuels, along with world oil prices at nearly $100 a barrel, has pushed up the price of grain used to feed poultry and dairy cows and other cattle, causing higher prices of wheat (up 58%), soybean (up 32%), and maize (up 11%) over the year. In 2007 Food riots have taken place in many countries across the world. An epidemic of stem rust, which is destructive to wheat and is caused by race Ug99, has in 2007 spread across Africa and into Asia.\n\nBeginning in the 20th century, nitrogen fertilizers, new pesticides, desert farming, and other agricultural technologies began to be used to increase food production, in part to combat famine. Between 1950 and 1984, as the Green Revolution influenced agriculture] world grain production increased by 250%. Developed nations have shared these technologies with developing nations with a famine problem. However, as early as 1995, there were signs that these new developments may contribute to the decline of arable land (e.g. persistence of pesticides leading to soil contamination, salt accumulation due to irrigation, erosion).\n\nIn 1994, David Pimentel, professor of ecology and agriculture at Cornell University, and Mario Giampietro, senior researcher at the National Research Institute on Food and Nutrition (INRAN), estimated the maximum U.S. population for a sustainable economy at 200 million.\n\nAccording to geologist Dale Allen Pfeiffer, coming decades could see rising food prices without relief and massive starvation on a global level. Water deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China or India. The water tables are falling in many countries (including Northern China, the US, and India) due to widespread overconsumption. Other countries affected include Pakistan, Iran, and Mexico. This will eventually lead to water scarcity and cutbacks in grain harvest. Even while overexploiting its aquifers, China has developed a grain deficit, contributing to the upward pressure on grain prices. Most of the three billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages.\n\nAfter China and India, there is a second tier of smaller countries with large water deficits – Algeria, Egypt, Iran, Mexico, and Pakistan. Four of these already import a large share of their grain. Only Pakistan remains marginally self-sufficient. But with a population expanding by 4 million a year, it will also soon turn to the world market for grain. According to a UN climate report, the Himalayan glaciers that are the principal dry-season water sources of Asia's biggest rivers – Ganges, Indus, Brahmaputra, Yangtze, Mekong, Salween and Yellow – could disappear by 2350 as temperatures rise and human demand rises. Approximately 2.4 billion people live in the drainage basin of the Himalayan rivers. India, China, Pakistan, Afghanistan, Bangladesh, Nepal and Myanmar could experience floods followed by severe droughts in coming decades. In India alone, the Ganges provides water for drinking and farming for more than 500 million people.\n\nEvan Fraser, a geographer at the University of Guelph in Ontario, Canada, explores the ways in which climate change may affect future famines. To do this, he draws on a range of historic cases where relatively small environmental problems triggered famines as a way of creating theoretical links between climate and famine in the future. Drawing on situations as diverse as the Great Irish Potato Famine, a series of weather induced famines in Asia during the late 19th century, and famines in Ethiopia during the 1980s, he concludes there are three \"lines of defense\" that protect a community's food security from environmental change. The first line of defense is the agro-ecosystem on which food is produced: diverse ecosystems with well managed soils high in organic matter tend to be more resilient. The second line of defense is the wealth and skills of individual households: If those households affected by bad weather such as drought have savings or skills they may be able to do all right despite the bad weather. The final line of defense is created by the formal institutions present in a society. Governments, churches, or NGOs must be willing and able to mount effective relief efforts. Pulling this together, Evan Fraser argues that if an ecosystem is resilient enough, it may be able to withstand weather-related shocks. But if these shocks overwhelm the ecosystem's line of defense, it is necessary for the household to adapt using its skills and savings. If a problem is too big for the family or household, then people must rely on the third line of defense, which is whether or not the formal institutions present in a society are able to provide help. Evan Fraser concludes that in almost every situation where an environmental problem triggered a famine you see a failure in each of these three lines of defense. Hence, understanding how climate change may cause famines in the future requires combining both an assessment of local socio-economic and environmental factors along with climate models that predict where bad weather may occur in the future.\n\nDefinitions of famines are based on three different categories—these include food supply-based, food consumption-based and mortality-based definitions. Some definitions of famines are:\n\nFood shortages in a population are caused either by a lack of food or by difficulties in food distribution; it may be worsened by natural climate fluctuations and by extreme political conditions related to oppressive government or warfare. The conventional explanation until 1981 for the cause of famines was the Food availability decline (FAD) hypothesis. The assumption was that the central cause of all famines was a decline in food availability. However, FAD could not explain why only a certain section of the population such as the agricultural laborer was affected by famines while others were insulated from famines. Based on the studies of some recent famines, the decisive role of FAD has been questioned and it has been suggested that the causal mechanism for precipitating starvation includes many variables other than just decline of food availability. According to this view, famines are a result of entitlements, the theory being proposed is called the \"failure of exchange entitlements\" or FEE. A person may own various commodities that can be exchanged in a market economy for the other commodities he or she needs. The exchange can happen via trading or production or through a combination of the two. These entitlements are called trade-based or production-based entitlements. Per this proposed view, famines are precipitated due to a breakdown in the ability of the person to exchange his entitlements. An example of famines due to FEE is the inability of an agricultural laborer to exchange his primary entitlement, i.e., labor for rice when his employment became erratic or was completely eliminated.\n\nAccording to the Physicians for Social Responsibility (PSR), global climate change is additionally challenging the Earth's ability to produce food, potentially leading to famine.\n\nSome elements make a particular region more vulnerable to famine. These include poverty, population growth, an inappropriate social infrastructure, a suppressive political regime, and a weak or under-prepared government.\n\nAccording to a FEWSNET report, \"Famines are not natural phenomena, they are catastrophic political failures.\"\n\nThomas Malthus's \"Essay on the Principle of Population\" has made popular the theory that many famines are caused by imbalance of food production compared to the large populations of countries whose population exceeds the regional carrying capacity. However, Professor Alex de Waal, Executive Director of the World Peace Foundation, refutes the Malthus theory, looking instead to political factors as major causes of recent (over the last 150 years) famines. Historically, famines have occurred from agricultural problems such as drought, crop failure, or pestilence. Changing weather patterns, the ineffectiveness of medieval governments in dealing with crises, wars, and epidemic diseases such as the Black Death helped to cause hundreds of famines in Europe during the Middle Ages, including 95 in Britain and 75 in France. In France, the Hundred Years' War, crop failures and epidemics reduced the population by two-thirds.\n\nThe failure of a harvest or change in conditions, such as drought, can create a situation whereby large numbers of people continue to live where the carrying capacity of the land has temporarily dropped radically. Famine is often associated with subsistence agriculture. The total absence of agriculture in an economically strong area does not cause famine; Arizona and other wealthy regions import the vast majority of their food, since such regions produce sufficient economic goods for trade.\n\nFamines have also been caused by volcanism. The 1815 eruption of the Mount Tambora volcano in Indonesia caused crop failures and famines worldwide and caused the worst famine of the 19th century. The current consensus of the scientific community is that the aerosols and dust released into the upper atmosphere causes cooler temperatures by preventing the sun's energy from reaching the ground. The same mechanism is theorized to be caused by very large meteorite impacts to the extent of causing mass extinctions.\n\nIn certain cases, such as the Great Leap Forward in China (which produced the largest famine in absolute numbers), North Korea in the mid-1990s, or Zimbabwe in the early-2000s, famine can occur because of government policy.\nIn 1932, under the rule of the USSR, Ukraine experienced one of its largest famines when between 2.4 and 7.5 million peasants died as a result of a state sponsored famine. It was termed the Holodomor, suggesting that it was a deliberate campaign of repression designed to eliminate resistance to collectivization. Forced grain quotas imposed upon the rural peasants and a brutal reign of terror contributed to the widespread famine. The Soviet government continued to deny the problem and it did not provide aid to the victims nor did it accept foreign aid. Several contemporary scholars dispute the notion that the famine was deliberately inflicted by the Soviet government.\n\nIn 1958 in China, Mao Zedong's Communist Government launched the Great Leap Forward campaign, aimed at rapidly industrializing the country. The government forcibly took control of agriculture. Barely enough grain was left for the peasants, and starvation occurred in many rural areas. Exportation of grain continued despite the famine and the government attempted to conceal it. While the famine is attributed to unintended consequences, it is believed that the government refused to acknowledge the problem, thereby further contributing to the deaths. In many instances, peasants were persecuted. Between 20 and 45 million people perished in this famine, making it one of the deadliest famines to date.\n\nMalawi ended its famine by subsidizing farmers despite the strictures imposed by the World Bank. During the 1973 Wollo Famine in Ethiopia, food was shipped out of Wollo to the capital city of Addis Ababa, where it could command higher prices. In the late-1970s and early-1980s, residents of the dictatorships of Ethiopia and Sudan suffered massive famines, but the democracy of Botswana avoided them, despite also suffering a severe drop in national food production. In Somalia, famine occurred because of a failed state.\n\nThe famine in Yemen was a direct result of the Saudi Arabian-led intervention in Yemen and the blockade imposed by Saudi Arabia and its allies, including the United States. According to the UN, 130 children under 5 years of age were dying from starvation and starvation related diseases every day by the end of 2017, with 50,000 dead for the year. As of October 2018, half the population is at risk of famine.\n\nAccording to Amartya Sen (1999), \"there has never been a famine in a functioning multiparty democracy\". Hasell and Roser have demonstrated that while there have been a few minor exceptions, famines rarely occur in democratic systems but are strongly correlated with autocratic and colonial systems.\n\nRelief technologies, including immunization, improved public health infrastructure, general food rations and supplementary feeding for vulnerable children, has provided temporary mitigation to the mortality impact of famines, while leaving their economic consequences unchanged, and not solving the underlying issue of too large a regional population relative to food production capability. Humanitarian crises may also arise from genocide campaigns, civil wars, refugee flows and episodes of extreme violence and state collapse, creating famine conditions among the affected populations.\n\nDespite repeated stated intentions by the world's leaders to end hunger and famine, famine remains a chronic threat in much of Africa, Eastern Europe, the Southeast, South Asia, and the Middle East. In July 2005, the Famine Early Warning Systems Network labelled Niger with emergency status, as well as Chad, Ethiopia, South Sudan, Somalia and Zimbabwe. In January 2006, the United Nations Food and Agriculture Organization warned that 11 million people in Somalia,[Kenya, Djibouti and Ethiopia were in danger of starvation due to the combination of severe drought and military conflicts. In 2006, the most serious humanitarian crisis in Africa was in Sudan's region Darfur.\n\nFrances Moore Lappé, later co-founder of the Institute for Food and Development Policy (Food First) argued in \"Diet for a Small Planet\" (1971) that vegetarian diets can provide food for larger populations, with the same resources, compared to omnivorous diets.\n\nNoting that modern famines are sometimes aggravated by misguided economic policies, political design to impoverish or marginalize certain populations, or acts of war, political economists have investigated the political conditions under which famine is prevented. Economist Amartya Sen states that the liberal institutions that exist in India, including competitive elections and a free press, have played a major role in preventing famine in that country since independence. Alex de Waal has developed this theory to focus on the \"political contract\" between rulers and people that ensures famine prevention, noting the rarity of such political contracts in Africa, and the danger that international relief agencies will undermine such contracts through removing the locus of accountability for famines from national governments.\nThe demographic impacts of famine are sharp. Mortality is concentrated among children and the elderly. A consistent demographic fact is that in all recorded famines, male mortality exceeds female, even in those populations (such as northern India and Pakistan) where there is a male longevity advantage during normal times. Reasons for this may include greater female resilience under the pressure of malnutrition, and possibly female's naturally higher percentage of body fat. Famine is also accompanied by lower fertility. Famines therefore leave the reproductive core of a population—adult women—lesser affected compared to other population categories, and post-famine periods are often characterized a \"rebound\" with increased births.\n\nEven though the theories of Thomas Malthus would predict that famines reduce the size of the population commensurate with available food resources, in fact even the most severe famines have rarely dented population growth for more than a few years. The mortality in China in 1958–61, Bengal in 1943, and Ethiopia in 1983–85 was all made up by a growing population over just a few years. Of greater long-term demographic impact is emigration: Ireland was chiefly depopulated after the 1840s famines by waves of emigration.\n\nLong term measures to improve food security, include investment in modern agriculture techniques, such as fertilizers and irrigation, but can also include strategic national food storage.\n\nWorld Bank strictures restrict government subsidies for farmers, and increasing use of fertilizers is opposed by some environmental groups because of its unintended consequences: adverse effects on water supplies and habitat.\nThe effort to bring modern agricultural techniques found in the Western world, such as nitrogen fertilizers and pesticides, to the Indian Sub-continent, called the Green Revolution, resulted in decreases in malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available. Supporting farmers in areas of food insecurity, through such measures as free or subsidized fertilizers and seeds, increases food harvest and reduces food prices.\n\nThe World Bank and some rich nations press nations that depend on them for aid to cut back or eliminate subsidized agricultural inputs such as fertilizer, in the name of privatization even as the United States and Europe extensively subsidized their own farmers.\n\nThere is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable. The United Nations' World Food Program (WFP), the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a \"revolution\" in food aid. The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.\n\nHowever, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help. Fred Cuny stated that \"the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died.\" US Law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport. Fred Cuny further pointed out \"studies of every recent famine have shown that food was available in-country—though not always in the immediate food deficit area\" and \"even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad.\"\n\nDeficient micronutrients can be provided through fortifying foods. Fortifying foods such as peanut butter sachets (see Plumpy'Nut) have revolutionized emergency feeding in humanitarian emergencies because they can be eaten directly from the packet, do not require refrigeration or mixing with scarce clean water, can be stored for years and, vitally, can be absorbed by extremely ill children.\n\nWHO and other sources recommend that malnourished children—and adults who also have diarrhea—drink rehydration solution, and continue to eat, in addition to antibiotics, and zinc supplements. There is a special oral rehydration solution called ReSoMal which has less sodium and more potassium than standard solution. However, if the diarrhea is severe, the standard solution is preferable as the person needs the extra sodium. Obviously, this is a judgment call best made by a physician, and using either solution is better than doing nothing. Zinc supplements often can help reduce the duration and severity of diarrhea, and Vitamin A can also be helpful. The World Health Organization underlines the importance of a person with diarrhea continuing to eat, with a 2005 publication for physicians stating: \"Food should \"never\" be withheld and the child's usual foods should \"not\" be diluted. Breastfeeding should \"always\" be continued.\"\n\nEthiopia has been pioneering a program that has now become part of the World Bank's prescribed recipe for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food.\n\nThe Green Revolution was widely viewed as an answer to famine in the 1970s and 1980s. Between 1950 and 1984, hybrid strains of high-yielding crops transformed agriculture around the globe and world grain production increased by 250%. Some criticize the process, stating that these new high-yielding crops require more chemical fertilizers and pesticides, which can harm the environment. Although these high-yielding crops make it technically possible to feed more people, there are indications that regional food production has peaked in many world sectors, due to certain strategies associated with intensive agriculture such as groundwater overdrafting and overuse of pesticides and other agricultural chemicals.\n\nIn modern times, local and political governments and non-governmental organizations that deliver famine relief have limited resources with which to address the multiple situations of food insecurity that are occurring simultaneously. Various methods of categorizing the gradations of food security have thus been used in order to most efficiently allocate food relief. One of the earliest were the Indian Famine Codes devised by the British in the 1880s. The Codes listed three stages of food insecurity: near-scarcity, scarcity and famine, and were highly influential in the creation of subsequent famine warning or measurement systems. The early warning system developed to monitor the region inhabited by the Turkana people in northern Kenya also has three levels, but links each stage to a pre-planned response to mitigate the crisis and prevent its deterioration\n\nThe experiences of famine relief organizations throughout the world over the 1980s and 1990s resulted in at least two major developments: the \"livelihoods approach\" and the increased use of nutrition indicators to determine the severity of a crisis. Individuals and groups in food stressful situations will attempt to cope by rationing consumption, finding alternative means to supplement income, etc., before taking desperate measures, such as selling off plots of agricultural land. When all means of self-support are exhausted, the affected population begins to migrate in search of food or fall victim to outright mass starvation. Famine may thus be viewed partially as a social phenomenon, involving markets, the price of food, and social support structures. A second lesson drawn was the increased use of rapid nutrition assessments, in particular of children, to give a quantitative measure of the famine's severity.\n\nSince 2003, many of the most important organizations in famine relief, such as the World Food Programme and the U.S. Agency for International Development, have adopted a five-level scale measuring intensity and magnitude. The intensity scale uses both livelihoods' measures and measurements of mortality and child malnutrition to categorize a situation as food secure, food insecure, food crisis, famine, severe famine, and extreme famine. The number of deaths determines the magnitude designation, with under 1000 fatalities defining a \"minor famine\" and a \"catastrophic famine\" resulting in over 1,000,000 deaths.\n\nFamine personified as an allegory is found in some cultures, e.g. one of the Four Horsemen of the Apocalypse in Christian tradition, the fear gorta of Irish folklore, or the Wendigo of Algonquian tradition.\n\n\n"}
{"id": "46313", "url": "https://en.wikipedia.org/wiki?curid=46313", "title": "Globalization", "text": "Globalization\n\nGlobalization or globalisation is the process of interaction and integration among people, companies, and governments worldwide. As a complex and multifaceted phenomenon, globalization is considered by some as a form of capitalist expansion which entails the integration of local and national economies into a global, unregulated market economy. Globalization has grown due to advances in transportation and communication technology. With the increased global interactions comes the growth of international trade, ideas, and culture. Globalization is primarily an economic process of interaction and integration that's associated with social and cultural aspects. However, conflicts and diplomacy are also large parts of the history of globalization, and modern globalization.\n\nEconomically, globalization involves goods, services, the economic resources of capital, technology, and data. Also, the expansions of global markets liberalize the economic activities of the exchange of goods and funds. Removal of cross-border trade barriers has made formation of global markets more feasible. The steam locomotive, steamship, jet engine, and container ships are some of the advances in the means of transport while the rise of the telegraph and its modern offspring, the Internet and mobile phones show development in telecommunications infrastructure. All of these improvements have been major factors in globalization and have generated further interdependence of economic and cultural activities around the globe.\n\nThough many scholars place the origins of globalization in modern times, others trace its history long before the European Age of Discovery and voyages to the New World, some even to the third millennium BC. Large-scale globalization began in the 1820s. In the late 19th century and early 20th century, the connectivity of the world's economies and cultures grew very quickly. The term \"globalization\" is recent, only establishing its current meaning in the 1970s.\n\nIn 2000, the International Monetary Fund (IMF) identified four basic aspects of globalization: trade and transactions, capital and investment movements, migration and movement of people, and the dissemination of knowledge. Further, environmental challenges such as global warming, cross-boundary water, air pollution, and over-fishing of the ocean are linked with globalization. Globalizing processes affect and are affected by business and work organization, economics, socio-cultural resources, and the natural environment. Academic literature commonly subdivides globalization into three major areas: economic globalization, cultural globalization, and political globalization.\nThe term \"globalization\" became popular in social science in the 1990s. It derives from the word \"globalize\", which refers to the emergence of an international network of economic systems. The term 'globalization' had been used in its economic sense at least as early as 1981, and in other senses since at least as early as 1944. Theodore Levitt is credited with popularizing the term and bringing it into the mainstream business audience in the later half of the 1980s. Since its inception, the concept of globalization has inspired competing definitions and interpretations. Its antecedents date back to the great movements of trade and empire across Asia and the Indian Ocean from the 15th century onward.\nDue to the complexity of the concept, various research projects, articles, and discussions often stay focused on a single aspect of globalization.\n\nSociologists Martin Albrow and Elizabeth King define globalization as \"all those processes by which the people of the world are incorporated into a single world society.\" In \"The Consequences of Modernity\", Anthony Giddens writes: \"Globalization can thus be defined as the intensification of worldwide social relations which link distant localities in such a way that local happenings are shaped by events occurring many miles away and vice versa.\" In 1992, Roland Robertson, professor of sociology at the University of Aberdeen and an early writer in the field, described globalization as \"the compression of the world and the intensification of the consciousness of the world as a whole.\"\n\nIn \"Global Transformations\", David Held and his co-writers state:\nHeld and his co-writers' definition of globalization in that same book as \"transformation in the spatial organization of social relations and transactions—assessed in terms of their extensity, intensity, velocity and impact—generating transcontinental or inter-regional flows\" was called \"probably the most widely-cited definition\" in the 2014 DHL Global Connectiveness Index.\n\nSwedish journalist Thomas Larsson, in his book \"The Race to the Top: The Real Story of Globalization\", states that globalization:\nPaul James defines globalization with a more direct and historically contextualized emphasis:\n\nGlobalization is the extension of social relations across world-space, defining that world-space in terms of the historically variable ways that it has been practiced and socially understood through changing world-time.\n\nManfred Steger, professor of global studies and research leader in the Global Cities Institute at RMIT University, identifies four main empirical dimensions of globalization: economic, political, cultural, and ecological. A fifth dimension—the ideological—cutting across the other four. The ideological dimension, according to Steger, is filled with a range of norms, claims, beliefs, and narratives about the phenomenon itself.\n\nJames and Steger stated that the concept of globalization \"emerged from the intersection of four interrelated sets of 'communities of practice' (Wenger, 1998): academics, journalists, publishers/editors, and librarians.\" They note the term was used \"in education to describe the global life of the mind\"; in international relations to describe the extension of the European Common Market; and in journalism to describe how the \"American Negro and his problem are taking on a global significance\". They have also argued that four different forms of globalization can be distinguished that complement and cut across the solely empirical dimensions. According to James, the oldest dominant form of globalization is embodied globalization, the movement of people. A second form is agency-extended globalization, the circulation of agents of different institutions, organizations, and polities, including imperial agents. Object-extended globalization, a third form, is the movement of commodities and other objects of exchange. He calls the transmission of ideas, images, knowledge, and information across world-space disembodied globalization, maintaining that it is currently the dominant form of globalization. James holds that this series of distinctions allows for an understanding of how, today, the most embodied forms of globalization such as the movement of refugees and migrants are increasingly restricted, while the most disembodied forms such as the circulation of financial instruments and codes are the most deregulated.\n\nThe journalist Thomas L. Friedman popularized the term \"flat world\", arguing that globalized trade, outsourcing, supply-chaining, and political forces had permanently changed the world, for better and worse. He asserted that the pace of globalization was quickening and that its impact on business organization and practice would continue to grow.\n\nEconomist Takis Fotopoulos defined \"economic globalization\" as the opening and deregulation of commodity, capital, and labor markets that led toward present neoliberal globalization. He used \"political globalization\" to refer to the emergence of a transnational élite and a phasing out of the nation-state. Meanwhile, he used \"cultural globalization\" to reference the worldwide homogenization of culture. Other of his usages included \"ideological globalization\", \"technological globalization\", and \"social globalization\".\n\nLechner and Boli (2012) define globalization as more people across large distances becoming connected in more and different ways.\n\n\"Globophobia\" is used to refer to the fear of globalization, though it can also mean the fear of balloons.\n\nThere are both distal and proximate causes which can be traced in the historical factors affecting globalization. Large-scale globalization began in the 19th century.\n\nArchaic globalization conventionally refers to a phase in the history of globalization including globalizing events and developments from the time of the earliest civilizations until roughly the 1600s. This term is used to describe the relationships between communities and states and how they were created by the geographical spread of ideas and social norms at both local and regional levels.\n\nIn this schema, three main prerequisites are posited for globalization to occur. The first is the idea of Eastern Origins, which shows how Western states have adapted and implemented learned principles from the East. Without the spread of traditional ideas from the East, Western globalization would not have emerged the way it did. The second is distance. The interactions of states were not on a global scale and most often were confined to Asia, North Africa, the Middle East, and certain parts of Europe. With early globalization, it was difficult for states to interact with others that were not within a close proximity. Eventually, technological advances allowed states to learn of others' existence and thus another phase of globalization can occur. The third has to do with inter-dependency, stability, and regularity. If a state is not dependent on another, then there is no way for either state to be mutually affected by the other. This is one of the driving forces behind global connections and trade; without either, globalization would not have emerged the way it did and states would still be dependent on their own production and resources to work. This is one of the arguments surrounding the idea of early globalization. It is argued that archaic globalization did not function in a similar manner to modern globalization because states were not as interdependent on others as they are today.\n\nAlso posited is a \"multi-polar\" nature to archaic globalization, which involved the active participation of non-Europeans. Because it predated the Great Divergence in the nineteenth century, where Western Europe pulled ahead of the rest of the world in terms of industrial production and economic output, archaic globalization was a phenomenon that was driven not only by Europe but also by other economically developed Old World centers such as Gujarat, Bengal, coastal China, and Japan.\nThe German historical economist and sociologist Andre Gunder Frank argues that a form of globalization began with the rise of trade links between Sumer and the Indus Valley Civilization in the third millennium BCE. This archaic globalization existed during the Hellenistic Age, when commercialized urban centers enveloped the axis of Greek culture that reached from India to Spain, including Alexandria and the other Alexandrine cities. Early on, the geographic position of Greece and the necessity of importing wheat forced the Greeks to engage in maritime trade. Trade in ancient Greece was largely unrestricted: the state controlled only the supply of grain.\n\nTrade on the Silk Road was a significant factor in the development of civilizations from China, Indian subcontinent, Persia, Europe, and Arabia, opening long-distance political and economic interactions between them. Though silk was certainly the major trade item from China, common goods such as salt and sugar were traded as well; and religions, syncretic philosophies, and various technologies, as well as diseases, also traveled along the Silk Routes. In addition to economic trade, the Silk Road served as a means of carrying out cultural trade among the civilizations along its network. The movement of people, such as refugees, artists, craftsmen, missionaries, robbers, and envoys, resulted in the exchange of religions, art, languages, and new technologies.\n\n\"Early modern-\" or \"proto-globalization\" covers a period of the history of globalization roughly spanning the years between 1600 and 1800. The concept of \"proto-globalization\" was first introduced by historians A. G. Hopkins and Christopher Bayly. The term describes the phase of increasing trade links and cultural exchange that characterized the period immediately preceding the advent of high \"modern globalization\" in the late 19th century. This phase of globalization was characterized by the rise of maritime European empires, in the 15th and 17th centuries, first the Portuguese Empire (1415) followed by the Spanish Empire (1492), and later the Dutch and British Empires. In the 17th century, world trade developed further when chartered companies like the British East India Company (founded in 1600) and the Dutch East India Company (founded in 1602, often described as the first multinational corporation in which stock was offered) were established.\n\nEarly modern globalization is distinguished from modern globalization on the basis of expansionism, the method of managing global trade, and the level of information exchange. The period is marked by such trade arrangements as the East India Company, the shift of hegemony to Western Europe, the rise of larger-scale conflicts between powerful nations such as the Thirty Years' War, and the rise of newfound commodities—most particularly slave trade. The Triangular Trade made it possible for Europe to take advantage of resources within the Western Hemisphere. The transfer of animal stocks, plant crops, and epidemic diseases associated with Alfred W. Crosby's concept of the Columbian Exchange also played a central role in this process. European, Muslim, Indian, Southeast Asian, and Chinese merchants were all involved in early modern trade and communications, particularly in the Indian Ocean region.\n\nAccording to economic historians Kevin H. O'Rourke, Leandro Prados de la Escosura, and Guillaume Daudin, several factors promoted globalization in the period 1815–1870:\nDuring the 19th century, globalization approached its form as a direct result of the Industrial Revolution. Industrialization allowed standardized production of household items using economies of scale while rapid population growth created sustained demand for commodities. In the 19th century, steamships reduced the cost of international transport significantly and railroads made inland transportation cheaper. The transport revolution occurred some time between 1820 and 1850. More nations embraced international trade. Globalization in this period was decisively shaped by nineteenth-century imperialism such as in Africa and Asia. The invention of shipping containers in 1956 helped advance the globalization of commerce.\n\nAfter World War II, work by politicians led to the agreements of the Bretton Woods Conference, in which major governments laid down the framework for international monetary policy, commerce, and finance, and the founding of several international institutions intended to facilitate economic growth by lowering trade barriers. Initially, the General Agreement on Tariffs and Trade (GATT) led to a series of agreements to remove trade restrictions. GATT's successor was the World Trade Organization (WTO), which provided a framework for negotiating and formalizing trade agreements and a dispute resolution process. Exports nearly doubled from 8.5% of total gross world product in 1970 to 16.2% in 2001. The approach of using global agreements to advance trade stumbled with the failure of the Doha Development Round of trade negotiation. Many countries then shifted to bilateral or smaller multilateral agreements, such as the 2011 South Korea–United States Free Trade Agreement.\n\nSince the 1970s, aviation has become increasingly affordable to middle classes in developed countries. Open skies policies and low-cost carriers have helped to bring competition to the market. In the 1990s, the growth of low-cost communication networks cut the cost of communicating between different countries. More work can be performed using a computer without regard to location. This included accounting, software development, and engineering design.\n\nStudent exchange programs became popular after World War II, and are intended to increase the participants' understanding and tolerance of other cultures, as well as improving their language skills and broadening their social horizons. Between 1963 and 2006 the number of students studying in a foreign country increased 9 times.\n\nSince the 1980s, modern globalization has spread rapidly through the expansion of capitalism and neoliberal ideologies. The implementation of neoliberal policies has allowed for the privatization of public industry, deregulation of laws or policies that interfered with the free flow of the market, as well as cut-backs to governmental social services. These neoliberal policies were introduced to many developing countries in the form of structural adjustment programs (SAPs) that were implemented by the World Bank and the International Monetary Fund (IMF). These programs required that the country receiving monetary aid would open its markets to capitalism, privatize public industry, allow free trade, cut social services like healthcare and education and allow the free movement of giant multinational corporations. These programs allowed the World Bank and the IMF to become global financial market regulators that would promote neoliberalism and the creation of free markets for multinational corporations on a global scale.\n\nIn the late 19th and early 20th century, the connectedness of the world's economies and cultures grew very quickly. This slowed down from the 1910s onward due to the World Wars and the Cold War, but picked up again in the 1980s and 1990s. The revolutions of 1989 and subsequent liberalization in many parts of the world resulted in a significant expansion of global interconnectedness. The migration and movement of people can also be highlighted as a prominent feature of the globalization process. In the period between 1965 and 1990, the proportion of the labor force migrating approximately doubled. Most migration occurred between the developing countries and least developed countries (LDCs). As economic integration intensified workers moved to areas with higher wages and most of the developing world oriented toward the international market economy. The collapse of the Soviet Union not only ended the Cold War's division of the world – it also left the United States its sole policeman and an unfettered advocate of free market. It also resulted in the growing prominence of attention focused on the movement of diseases, the proliferation of popular culture and consumer values, the growing prominence of international institutions like the UN, and concerted international action on such issues as the environment and human rights. Other developments as dramatic were the Internet's becoming influential in connecting people across the world; , more than 2.4 billion people—over a third of the world's human population—have used the services of the Internet. Growth of globalization has never been smooth. One influential event was the late 2000s recession, which was associated with lower growth (in areas such as cross-border phone calls and Skype usage) or even temporarily negative growth (in areas such as trade) of global interconnectedness.\n\nGlobalized society offers a complex web of forces and factors that bring people, cultures, markets, beliefs, and practices into increasingly greater proximity to one another.\n\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital. Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market. Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from different locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes competition, technology, and corporations and industries.\n\nCurrent globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration.\n\nInternational standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced transport of its costs, supported the post-war boom in international trade, and was a major element in globalization. International Organization for Standardization is an international standard-setting body composed of representatives from various national standards organizations.\n\nA multinational corporation or worldwide enterprise is an organization that owns or controls production of goods or services in one or more countries other than their home country. It can also be referred as an international corporation, a transnational corporation, or a stateless corporation.\n\nA free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers import quotas and tariffs and to increase trade of goods and services with each other.\nIf people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border.\nArguably the most significant free-trade area in the world is the European Union, a politico-economic union of member states that are primarily located in Europe. The EU has developed European Single Market through a standardized system of laws that apply in all member states. EU policies aim to ensure the free movement of people, goods, services, and capital within the internal market,\n\nTrade facilitation looks at how procedures and controls governing the movement of goods across national borders can be improved to reduce associated cost burdens and maximize efficiency while safeguarding legitimate regulatory objectives.\n\nGlobal trade in services is also significant. For example, in India, business process outsourcing has been described as the \"primary engine of the country's development over the next few decades, contributing broadly to GDP growth, employment growth, and poverty alleviation\".\n\nWilliam I. Robinson's theoretical approach to globalization is a critique of Wallerstein's World Systems Theory. He believes that the global capital experienced today is due to a new and distinct form of globalization which began in the 1980s. Robinson argues not only are economic activities expanded across national boundaries but also there is a transnational fragmentation of these activities. One important aspect of Robinson's globalization theory is that production of goods are increasingly global. This means that one pair of shoes can be produced by six different countries, each contributing to a part of the production process.\n\nCultural globalization refers to the transmission of ideas, meanings, and values around the world in such a way as to extend and intensify social relations. This process is marked by the common consumption of cultures that have been diffused by the Internet, popular culture media, and international travel. This has added to processes of commodity exchange and colonization which have a longer history of carrying cultural meaning around the globe. The circulation of cultures enables individuals to partake in extended social relations that cross national and regional borders. The creation and expansion of such social relations is not merely observed on a material level. Cultural globalization involves the formation of shared norms and knowledge with which people associate their individual and collective cultural identities. It brings increasing interconnectedness among different populations and cultures.\n\nCross-cultural communication is a field of study that looks at how people from differing cultural backgrounds communicate, in similar and different ways among themselves, and how they endeavour to communicate across cultures. Intercultural communication is a related field of study.\n\nCultural diffusion is the spread of cultural items—such as ideas, styles, religions, technologies, languages etc.\nCultural globalization has increased cross-cultural contacts, but may be accompanied by a decrease in the uniqueness of once-isolated communities. For example, sushi is available in Germany as well as Japan, but Euro-Disney outdraws the city of Paris, potentially reducing demand for \"authentic\" French pastry. Globalization's contribution to the alienation of individuals from their traditions may be modest compared to the impact of modernity itself, as alleged by existentialists such as Jean-Paul Sartre and Albert Camus. Globalization has expanded recreational opportunities by spreading pop culture, particularly via the Internet and satellite television.\n\nReligions were among the earliest cultural elements to globalize, being spread by force, migration, evangelists, imperialists, and traders. Christianity, Islam, Buddhism, and more recently sects such as Mormonism are among those religions which have taken root and influenced endemic cultures in places far from their origins.\nGlobalization has strongly influenced sports. For example, the modern Olympic Games has athletes from more than 200 nations participating in a variety of competitions. The FIFA World Cup is the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; a ninth of the entire population of the planet watched the 2006 FIFA World Cup Final.\n\nThe term globalization implies transformation. Cultural practices including traditional music can be lost or turned into a fusion of traditions. Globalization can trigger a state of emergency for the preservation of musical heritage. Archivists may attempt to collect, record, or transcribe repertoires before melodies are assimilated or modified, while local musicians may struggle for authenticity and to preserve local musical traditions. Globalization can lead performers to discard traditional instruments. Fusion genres can become interesting fields of analysis.\n\nMusic has an important role in economic and cultural development during globalization. Music genres such as jazz and reggae began locally and later became international phenomena. Globalization gave support to the world music phenomenon by allowing music from developing countries to reach broader audiences. Though the term \"World Music\" was originally intended for ethnic-specific music, globalization is now expanding its scope such that the term often includes hybrid subgenres such as \"world fusion\", \"global fusion\", \"ethnic fusion\", and worldbeat.\nBourdieu claimed that the perception of consumption can be seen as self-identification and the formation of identity. Musically, this translates into each individual having their own musical identity based on likes and tastes. These likes and tastes are greatly influenced by culture, as this is the most basic cause for a person's wants and behavior. The concept of one's own culture is now in a period of change due to globalization. Also, globalization has increased the interdependency of political, personal, cultural, and economic factors.\n\nA 2005 UNESCO report showed that cultural exchange is becoming more frequent from Eastern Asia, but that Western countries are still the main exporters of cultural goods. In 2002, China was the third largest exporter of cultural goods, after the UK and US. Between 1994 and 2002, both North America's and the European Union's shares of cultural exports declined while Asia's cultural exports grew to surpass North America. Related factors are the fact that Asia's population and area are several times that of North America. Americanization is related to a period of high political American clout and of significant growth of America's shops, markets and objects being brought into other countries.\n\nSome critics of globalization argue that it harms the diversity of cultures. As a dominating country's culture is introduced into a receiving country through globalization, it can become a threat to the diversity of local culture. Some argue that globalization may ultimately lead to Westernization or Americanization of culture, where the dominating cultural concepts of economically and politically powerful Western countries spread and cause harm to local cultures.\n\nGlobalization is a diverse phenomenon which relates to a multilateral political world and to the increase of cultural objects and markets between countries. The Indian experience particularly reveals the plurality of the impact of cultural globalization.\n\nTransculturalism is defined as \"seeing oneself in the other\". Transcultural is in turn described as \"extending through all human cultures\" or \"involving, encompassing, or combining elements of more than one culture\".\n\nPolitical globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene.\nWilliam R. Thompson has defined it as \"the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed\". \nPolitical globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.\n\nIntergovernmentalism is a term in political science with two meanings. The first refers to a theory of regional integration originally proposed by Stanley Hoffmann; the second treats states and the national government as the primary factors for integration.\nMulti-level governance is an approach in political science and public administration theory that originated from studies on European integration. Multi-level governance gives expression to the idea that there are many interacting authority structures at work in the emergent global political economy. It illuminates the intimate entanglement between the domestic and international levels of authority.\n\nSome people are citizens of multiple nation-states. Multiple citizenship, also called dual citizenship or multiple nationality or dual nationality, is a person's citizenship status, in which a person is concurrently regarded as a citizen of more than one state under the laws of those states.\nIncreasingly, non-governmental organizations influence public policy across national boundaries, including humanitarian aid and developmental efforts. Philanthropic organizations with global missions are also coming to the forefront of humanitarian efforts; charities such as the Bill and Melinda Gates Foundation, Accion International, the Acumen Fund (now Acumen) and the Echoing Green have combined the business model with philanthropy, giving rise to business organizations such as the Global Philanthropy Group and new associations of philanthropists such as the Global Philanthropy Forum. The Bill and Melinda Gates Foundation projects include a current multibillion-dollar commitment to funding immunizations in some of the world's more impoverished but rapidly growing countries. The Hudson Institute estimates total private philanthropic flows to developing countries at US$59 billion in 2010.\n\nAs a response to globalization, some countries have embraced isolationist policies. For example, the North Korean government makes it very difficult for foreigners to enter the country and strictly monitors their activities when they do. Aid workers are subject to considerable scrutiny and excluded from places and regions the government does not wish them to enter. Citizens cannot freely leave the country.\n\nGlobalization has been a gendered process where giant multinational corporations have outsourced jobs to low-wage, low skilled, quota free economies like the ready made garment industry in Bangladesh where poor women make up the majority of labor force. Despite a large proportion of women workers in the garment industry, women are still heavily underemployed compared to men. Most women that are employed in the garment industry come from the countryside of Bangladesh triggering migration of women in search of garment work. It is still unclear as to whether or not access to paid work for women where it didn't exist before has empowered them. The answers varied depending on whether it is the employers perspective or the workers and how they view their choices. Women workers did not see the garment industry as economically sustainable for them in the long run due to long hours standing and poor working conditions. Although women workers did show significant autonomy over their personal lives including their ability to negotiate with family, more choice in marriage, and being valued as a wage earner in the family. This did not translate into workers being able to collectively organize themselves in order to negotiate a better deal for themselves at work.\n\nAnother example of outsourcing in manufacturing includes the maquiladora industry in Ciudad Juarez, Mexico where poor women make up the majority of the labor force. Women in the maquiladora industry have produced high levels of turnover not staying long enough to be trained compared to men. A gendered two tiered system within the maquiladora industry has been created that focuses on training and worker loyalty. Women are seen as being untrainable, placed in un-skilled, low wage jobs, while men are seen as more trainable with less turnover rates, and placed in more high skilled technical jobs. The idea of training has become a tool used against women to blame them for their high turnover rates which also benefit the industry keeping women as temporary workers.\n\nScholars also occasionally discuss other, less common dimensions of globalization, such as environmental globalization (the internationally coordinated practices and regulations, often in the form of international treaties, regarding environmental protection) or military globalization (growth in global extent and scope of security relationships). Those dimensions, however, receive much less attention the three described above, as academic literature commonly subdivides globalization into three major areas: economic globalization, cultural globalization and political globalization.\n\nAn essential aspect of globalization is movement of people, and state-boundary limits on that movement have changed across history. The movement of tourists and business people opened up over the last century. As transportation technology improved, travel time and costs decreased dramatically between the 18th and early 20th century. For example, travel across the Atlantic ocean used to take up to 5 weeks in the 18th century, but around the time of the 20th century it took a mere 8 days. Today, modern aviation has made long-distance transportation quick and affordable.\n\nTourism is travel for pleasure. The developments in technology and transport infrastructure, such as jumbo jets, low-cost airlines, and more accessible airports have made many types of tourism more affordable. International tourist arrivals surpassed the milestone of 1 billion tourists globally for the first time in 2012.\nA visa is a conditional authorization granted by a country to a foreigner, allowing them to enter and temporarily remain within, or to leave that country. Some countries – such as those in the Schengen Area – have agreements with other countries allowing each other's citizens to travel between them without visas. The World Tourism Organization announced that the number of tourists who require a visa before traveling was at its lowest level ever in 2015.\n\nImmigration is the international movement of people into a destination country of which they are not natives or where they do not possess citizenship in order to settle or reside there, especially as permanent residents or naturalized citizens, or to take-up employment as a migrant worker or temporarily as a foreign worker.\nAccording to the International Labour Organization, there were an estimated 232 million international migrants in the world (defined as persons outside their country of origin for 12 months or more) and approximately half of them were estimated to be economically active (i.e. being employed or seeking employment). International movement of labor is often seen as important to economic development. For example, freedom of movement for workers in the European Union means that people can move freely between member states to live, work, study or retire in another country.\nGlobalization is associated with a dramatic rise in international education. More and more students are seeking higher education in foreign countries and many international students now consider overseas study a stepping-stone to permanent residency within a country. The contributions that foreign students make to host nation economies, both culturally and financially has encouraged major players to implement further initiatives to facilitate the arrival and integration of overseas students, including substantial amendments to immigration and visa policies and procedures.\n\nA transnational marriage is a marriage between two people from different countries. A variety of special issues arise in marriages between people from different countries, including those related to citizenship and culture, which add complexity and challenges to these kinds of relationships.\nIn an age of increasing globalization, where a growing number of people have ties to networks of people and places across the globe, rather than to a current geographic location, people are increasingly marrying across national boundaries. Transnational marriage is a by-product of the movement and migration of people.\n\nBefore electronic communications, long-distance communications relied on mail. Speed of global communications was limited by the maximum speed of courier services (especially horses and ships) until the mid-19th century. The electric telegraph was the first method of instant long-distance communication. For example, before the first transatlantic cable, communications between Europe and the Americas took weeks because ships had to carry mail across the ocean. The first transatlantic cable reduced communication time considerably, allowing a message and a response in the same day. Lasting transatlantic telegraph connections were achieved in the 1865–1866. The first wireless telegraphy transmitters were developed in 1895.\n\nThe Internet has been instrumental in connecting people across geographical boundaries. For example, Facebook is a social networking service which has more than 1.65 billion monthly active users .\n\nGlobalization can be spread by Global journalism which provides massive information and relies on the internet to interact, \"makes it into an everyday routine to investigate how people and their actions, practices, problems, life conditions etc. in different parts of the world are interrelated. possible to assume that global threats such as climate change precipitate the further establishment of global journalism.\"\n\nOne index of globalization is the \"KOF Index of Globalization\", which measures three important dimensions of globalization: economic, social, and political. Another is the A.T. Kearney / Foreign Policy Magazine Globalization Index.\nMeasurements of economic globalization typically focus on variables such as trade, Foreign Direct Investment (FDI), Gross Domestic Product (GDP), portfolio investment, and income. However, newer indices attempt to measure globalization in more general terms, including variables related to political, social, cultural, and even environmental aspects of globalization.\n\nThe DHL Global Connectedness Index studies four main types of cross-border flow: trade (in both goods and services), information, people (including tourists, students, and migrants), and capital. It shows that the depth of global integration fell by about one-tenth after 2008, but by 2013 had recovered well above its pre-crash peak. The report also found a shift of economic activity to emerging economies.\n\nReactions to processes contributing to globalization have varied widely with a history as long as extraterritorial contact and trade. Philosophical differences regarding the costs and benefits of such processes give rise to a broad-range of ideologies and social movements. Proponents of economic growth, expansion and development, in general, view globalizing processes as desirable or necessary to the well-being of human society.\n\nAntagonists view one or more globalizing processes as detrimental to social well-being on a global or local scale; this includes those who focus on social or natural sustainability of long-term and continuous economic expansion, the social structural inequality caused by these processes, and the colonial, imperialistic, or hegemonic ethnocentrism, cultural assimilation and cultural appropriation that underlie such processes.\n\nGlobalization tends to bring people into contact with foreign people and cultures. Xenophobia is the fear of that which is perceived to be foreign or strange. Xenophobia can manifest itself in many ways involving the relations and perceptions of an ingroup towards an outgroup, including a fear of losing identity, suspicion of its activities, aggression, and desire to eliminate its presence to secure a presumed purity.\n\nCritiques of globalization generally stem from discussions surrounding the impact of such processes on the planet as well as the human costs. They challenge directly traditional metrics, such as GDP, and look to other measures, such as the Gini coefficient or the Happy Planet Index, and point to a \"multitude of interconnected fatal consequences–social disintegration, a breakdown of democracy, more rapid and extensive deterioration of the environment, the spread of new diseases, increasing poverty and alienation\" which they claim are the unintended consequences of globalization. Others point out that, while the forces of globalization have led to the spread of western-style democracy, this has been accompanied by an increase in inter-ethnic tension and violence as free market economic policies combine with democratic processes of universal suffrage as well as an escalation in militarization to impose democratic principles and as a means to conflict resolution.\n\nOn August 9, 2019, Pope Francis denounced isolationism and hinted that the Catholic Church will embrace globalization at the October 2019 Amazonia Synod, stating “the whole is greater than the parts. Globalization and unity should not be conceived as a sphere, but as a polyhedron: each people retains its identity in unity with others\"\n\nA 2005 study by Peer Fis and Paul Hirsch found a large increase in articles negative towards globalization in the years prior. In 1998, negative articles outpaced positive articles by two to one. The number of newspaper articles showing negative framing rose from about 10% of the total in 1991 to 55% of the total in 1999. This increase occurred during a period when the total number of articles concerning globalization nearly doubled.\n\nA number of international polls have shown that residents of Africa and Asia tend to view globalization more favorably than residents of Europe or North America. In Africa, a Gallup poll found that 70% of the population views globalization favorably. The BBC found that 50% of people believed that economic globalization was proceeding too rapidly, while 35% believed it was proceeding too slowly.\n\nIn 2004, Philip Gordon stated that \"a clear majority of Europeans believe that globalization can enrich their lives, while believing the European Union can help them take advantage of globalization's benefits while shielding them from its negative effects.\" The main opposition consisted of socialists, environmental groups, and nationalists. Residents of the EU did not appear to feel threatened by globalization in 2004. The EU job market was more stable and workers were less likely to accept wage/benefit cuts. Social spending was much higher than in the US. In a Danish poll in 2007, 76% responded that globalization is a good thing.\n\nFiss, \"et al.\", surveyed US opinion in 1993. Their survey showed that, in 1993, more than 40% of respondents were unfamiliar with the concept of globalization. When the survey was repeated in 1998, 89% of the respondents had a polarized view of globalization as being either good or bad. At the same time, discourse on globalization, which began in the financial community before shifting to a heated debate between proponents and disenchanted students and workers. Polarization increased dramatically after the establishment of the WTO in 1995; this event and subsequent protests led to a large-scale anti-globalization movement.\nInitially, college educated workers were likely to support globalization. Less educated workers, who were more likely to compete with immigrants and workers in developing countries, tended to be opponents. The situation changed after the financial crisis of 2007. According to a 1997 poll 58% of college graduates said globalization had been good for the US. By 2008 only 33% thought it was good. Respondents with high school education also became more opposed.\n\nAccording to Takenaka Heizo and Chida Ryokichi, there was a perception in Japan that the economy was \"Small and Frail\". However, Japan was resource-poor and used exports to pay for its raw materials. Anxiety over their position caused terms such as \"internationalization\" and \"globalization\" to enter everyday language. However, Japanese tradition was to be as self-sufficient as possible, particularly in agriculture.\n\nMany in developing countries see globalization as a positive force that lifts them out of poverty. Those opposing globalization typically combine environmental concerns with nationalism. Opponents consider governments as agents of neo-colonialism that are subservient to multinational corporations. Much of this criticism comes from the middle class; the Brookings Institution suggested this was because the middle class perceived upwardly mobile low-income groups as threatening to their economic security.\n\nThe literature analyzing the economics of free trade is extremely rich with extensive work having been done on the theoretical and empirical effects. Though it creates winners and losers, the broad consensus among economists is that free trade is a large and unambiguous net gain for society. In a 2006 survey of 83 American economists, \"87.5% agree that the U.S. should eliminate remaining tariffs and other barriers to trade\" and \"90.1% disagree with the suggestion that the U.S. should restrict employers from outsourcing work to foreign countries.\"\n\nQuoting Harvard economics professor N. Gregory Mankiw, \"Few propositions command as much consensus among professional economists as that open world trade increases economic growth and raises living standards.\" In a survey of leading economists, none disagreed with the notion that \"freer trade improves productive efficiency and offers consumers better choices, and in the long run these gains are much larger than any effects on employment.\" Most economists would agree that although increasing returns to scale might mean that certain industry could settle in a geographical area without any strong economic reason derived from comparative advantage, this is not a reason to argue against free trade because the absolute level of output enjoyed by both \"winner\" and \"loser\" will increase with the \"winner\" gaining more than the \"loser\" but both gaining more than before in an absolute level.\n\nIn the book \"The End of Poverty\", Jeffrey Sachs discusses how many factors can affect a country's ability to enter the world market, including government corruption; legal and social disparities based on gender, ethnicity, or caste; diseases such as AIDS and malaria; lack of infrastructure (including transportation, communications, health, and trade); unstable political landscapes; protectionism; and geographic barriers. Jagdish Bhagwati, a former adviser to the U.N. on globalization, holds that, although there are obvious problems with overly rapid development, globalization is a very positive force that lifts countries out of poverty by causing a virtuous economic cycle associated with faster economic growth. However, economic growth does not necessarily mean a reduction in poverty; in fact, the two can coexist. Economic growth is conventionally measured using indicators such as GDP and GNI that do not accurately reflect the growing disparities in wealth. Additionally, Oxfam International argues that poor people are often excluded from globalization-induced opportunities \"by a lack of productive assets, weak infrastructure, poor education and ill-health;\" effectively leaving these marginalized groups in a poverty trap. Economist Paul Krugman is another staunch supporter of globalization and free trade with a record of disagreeing with many critics of globalization. He argues that many of them lack a basic understanding of comparative advantage and its importance in today's world.\nThe flow of migrants to advanced economies has been claimed to provide a means through which global wages converge. An IMF study noted a potential for skills to be transferred back to developing countries as wages in those a countries rise. Lastly, the dissemination of knowledge has been an integral aspect of globalization. Technological innovations (or technological transfer) are conjectured to benefit most developing and least developing countries (LDCs), as for example in the adoption of mobile phones.\n\nThere has been a rapid economic growth in Asia after embracing market orientation-based economic policies that encourage private property rights, free enterprise and competition. In particular, in East Asian developing countries, GDP per head rose at 5.9% a year from 1975 to 2001 (according to 2003 Human Development Report of UNDP). Like this, the British economic journalist Martin Wolf says that incomes of poor developing countries, with more than half the world's population, grew substantially faster than those of the world's richest countries that remained relatively stable in its growth, leading to reduced international inequality and the incidence of poverty.\nCertain demographic changes in the developing world after active economic liberalization and international integration resulted in rising general welfare and, hence, reduced inequality. According to Wolf, in the developing world as a whole, life expectancy rose by four months each year after 1970 and infant mortality rate declined from 107 per thousand in 1970 to 58 in 2000 due to improvements in standards of living and health conditions. Also, adult literacy in developing countries rose from 53% in 1970 to 74% in 1998 and much lower illiteracy rate among the young guarantees that rates will continue to fall as time passes. Furthermore, the reduction in fertility rate in the developing world as a whole from 4.1 births per woman in 1980 to 2.8 in 2000 indicates improved education level of women on fertility, and control of fewer children with more parental attention and investment. Consequently, more prosperous and educated parents with fewer children have chosen to withdraw their children from the labor force to give them opportunities to be educated at school improving the issue of child labor. Thus, despite seemingly unequal distribution of income within these developing countries, their economic growth and development have brought about improved standards of living and welfare for the population as a whole.\n\nPer capita gross domestic product (GDP) growth among post-1980 globalizing countries accelerated from 1.4 percent a year in the 1960s and 2.9 percent a year in the 1970s to 3.5 percent in the 1980s and 5.0 percent in the 1990s. This acceleration in growth seems even more remarkable given that the rich countries saw steady declines in growth from a high of 4.7 percent in the 1960s to 2.2 percent in the 1990s. Also, the non-globalizing developing countries seem to fare worse than the globalizers, with the former's annual growth rates falling from highs of 3.3 percent during the 1970s to only 1.4 percent during the 1990s. This rapid growth among the globalizers is not simply due to the strong performances of China and India in the 1980s and 1990s—18 out of the 24 globalizers experienced increases in growth, many of them quite substantial.\nThe globalization of the late 20th and early 21st centuries has led to the resurfacing of the idea that the growth of economic interdependence promotes peace. This idea had been very powerful during the globalization of the late 19th and early 20th centuries, and was a central doctrine of classical liberals of that era, such as the young John Maynard Keynes (1883–1946).\n\nSome opponents of globalization see the phenomenon as a promotion of corporate interests. They also claim that the increasing autonomy and strength of corporate entities shapes the political policy of countries. They advocate global institutions and policies that they believe better address the moral claims of poor and working classes as well as environmental concerns. Economic arguments by fair trade theorists claim that unrestricted free trade benefits those with more financial leverage (i.e. the rich) at the expense of the poor.\n\nGlobalization allows corporations to outsource manufacturing and service jobs from high cost locations, creating economic opportunities with the most competitive wages and worker benefits. Critics of globalization say that it disadvantages poorer countries. While it is true that free trade encourages globalization among countries, some countries try to protect their domestic suppliers. The main export of poorer countries is usually agricultural productions. Larger countries often subsidize their farmers (e.g., the EU's Common Agricultural Policy), which lowers the market price for foreign crops.\n\nDemocratic globalization is a movement towards an institutional system of global democracy that would give world citizens a say in political organizations. This would, in their view, bypass nation-states, corporate oligopolies, ideological non-governmental organizations (NGO), political cults and mafias. One of its most prolific proponents is the British political thinker David Held. Advocates of democratic globalization argue that economic expansion and development should be the first phase of democratic globalization, which is to be followed by a phase of building global political institutions. Dr. Francesco Stipo, Director of the United States Association of the Club of Rome, advocates unifying nations under a world government, suggesting that it \"should reflect the political and economic balances of world nations. A world confederation would not supersede the authority of the State governments but rather complement it, as both the States and the world authority would have power within their sphere of competence\". Former Canadian Senator Douglas Roche, O.C., viewed globalization as inevitable and advocated creating institutions such as a directly elected United Nations Parliamentary Assembly to exercise oversight over unelected international bodies.\n\nGlobal civics suggests that civics can be understood, in a global sense, as a social contract between global citizens in the age of interdependence and interaction. The disseminators of the concept define it as the notion that we have certain rights and responsibilities towards each other by the mere fact of being human on Earth. World citizen has a variety of similar meanings, often referring to a person who disapproves of traditional geopolitical divisions derived from national citizenship. An early incarnation of this sentiment can be found in Socrates, whom Plutarch quoted as saying: \"I am not an Athenian, or a Greek, but a citizen of the world.\" In an increasingly interdependent world, world citizens need a compass to frame their mindsets and create a shared consciousness and sense of global responsibility in world issues such as environmental problems and nuclear proliferation.\n\nBaha'i-inspired author Meyjes, while favoring the single world community and emergent global consciousness, warns of globalization as a cloak for an expeditious economic, social, and cultural Anglo-dominance that is insufficiently inclusive to inform the emergence of an optimal world civilization. He proposes a process of \"universalization\" as an alternative.\n\nCosmopolitanism is the proposal that all human ethnic groups belong to a single community based on a shared morality. A person who adheres to the idea of cosmopolitanism in any of its forms is called a cosmopolitan or cosmopolite. A cosmopolitan community might be based on an inclusive morality, a shared economic relationship, or a political structure that encompasses different nations. The cosmopolitan community is one in which individuals from different places (e.g. nation-states) form relationships based on mutual respect. For instance, Kwame Anthony Appiah suggests the possibility of a cosmopolitan community in which individuals from varying locations (physical, economic, etc.) enter relationships of mutual respect despite their differing beliefs (religious, political, etc.).\n\nCanadian philosopher Marshall McLuhan popularized the term \"Global Village\" beginning in 1962. His view suggested that globalization would lead to a world where people from all countries will become more integrated and aware of common interests and shared humanity.\n\nMilitary cooperation – Past examples of international cooperation exist. One example is the security cooperation between the United States and the former Soviet Union after the end of the Cold War, which astonished international society. Arms control and disarmament agreements, including the Strategic Arms Reduction Treaty (see START I, START II, START III, and New START) and the establishment of NATO's Partnership for Peace, the Russia NATO Council, and the G8 Global Partnership against the Spread of Weapons and Materials of Mass Destruction, constitute concrete initiatives of arms control and de-nuclearization. The US–Russian cooperation was further strengthened by anti-terrorism agreements enacted in the wake of 9/11.\n\nEnvironmental cooperation – One of the biggest successes of environmental cooperation has been the agreement to reduce chlorofluorocarbon (CFC) emissions, as specified in the Montreal Protocol, in order to stop ozone depletion. The most recent debate around nuclear energy and the non-alternative coal-burning power plants constitutes one more consensus on what not to do. Thirdly, significant achievements in IC can be observed through development studies.\n\nEconomic cooperation - One of the biggest challenges in 2019 with globalization is that many believe the progress made in the past decades are now back tracking. The back tracking of globalization has coined the term \"Slobalization.\" Slobalization is a new, slower pattern of globalization.\n\nAnti-globalization, or counter-globalization, consists of a number of criticisms of globalization but, in general, is critical of the globalization of corporate capitalism. The movement is also commonly referred to as the alter-globalization movement, anti-globalist movement, anti-corporate globalization movement, or movement against neoliberal globalization. Opponents of globalization argue that there is unequal power and respect in terms of international trade between the developed and underdeveloped countries of the world. The diverse subgroups that make up this movement include some of the following: trade unionists, environmentalists, anarchists, land rights and indigenous rights activists, organizations promoting human rights and sustainable development, opponents of privatization, and anti-sweatshop campaigners.\n\nIn \"The Revolt of the Elites and the Betrayal of Democracy\", Christopher Lasch analyzes the widening gap between the top and bottom of the social composition in the United States. For him, our epoch is determined by a social phenomenon: the revolt of the elites, in reference to \"The revolt of the masses\" (1929) of the Spanish philosopher José Ortega y Gasset. According to Lasch, the new elites, i.e. those who are in the top 20% in terms of income, through globalization which allows total mobility of capital, no longer live in the same world as their fellow-citizens. In this, they oppose the old bourgeoisie of the nineteenth and twentieth centuries, which was constrained by its spatial stability to a minimum of rooting and civic obligations. Globalization, according to the sociologist, has turned elites into tourists in their own countries. The denationalization of business enterprise tends to produce a class who see themselves as \"world citizens, but without accepting ... any of the obligations that citizenship in a polity normally implies\". Their ties to an international culture of work, leisure, information – make many of them deeply indifferent to the prospect of national decline. Instead of financing public services and the public treasury, new elites are investing their money in improving their voluntary ghettos: private schools in their residential neighborhoods, private police, garbage collection systems. They have \"withdrawn from common life\".\nComposed of those who control the international flows of capital and information, who preside over philanthropic foundations and institutions of higher education, manage the instruments of cultural production and thus fix the terms of public debate. So, the political debate is limited mainly to the dominant classes and political ideologies lose all contact with the concerns of the ordinary citizen. The result of this is that no one has a likely solution to these problems and that there are furious ideological battles on related issues.\nHowever, they remain protected from the problems affecting the working classes: the decline of industrial activity, the resulting loss of employment, the decline of the middle class, increasing the number of the poor, the rising crime rate, growing drug trafficking, the urban crisis.\n\nD.A. Snow et al. contend that the anti-globalization movement is an example of a new social movement, which uses tactics that are unique and use different resources than previously used before in other social movements.\n\nOne of the most infamous tactics of the movement is the Battle of Seattle in 1999, where there were protests against the World Trade Organization's Third Ministerial Meeting. All over the world, the movement has held protests outside meetings of institutions such as the WTO, the International Monetary Fund (IMF), the World Bank, the World Economic Forum, and the Group of Eight (G8). Within the Seattle demonstrations the protesters that participated used both creative and violent tactics to gain the attention towards the issue of globalization.\n\nCapital markets have to do with raising and investing money in various human enterprises. Increasing integration of these financial markets between countries leads to the emergence of a global capital marketplace or a single world market. In the long run, increased movement of capital between countries tends to favor owners of capital more than any other group; in the short run, owners and workers in specific sectors in capital-exporting countries bear much of the burden of adjusting to increased movement of capital.\n\nThose opposed to capital market integration on the basis of human rights issues are especially disturbed by the various abuses which they think are perpetuated by global and international institutions that, they say, promote neoliberalism without regard to ethical standards. Common targets include the World Bank (WB), International Monetary Fund (IMF), the Organisation for Economic Co-operation and Development (OECD) and the World Trade Organization (WTO) and free trade treaties like the North American Free Trade Agreement (NAFTA), Free Trade Area of the Americas (FTAA), the Multilateral Agreement on Investment (MAI) and the General Agreement on Trade in Services (GATS). In light of the economic gap between rich and poor countries, movement adherents claim free trade without measures in place to protect the under-capitalized will contribute only to the strengthening the power of industrialized nations (often termed the \"North\" in opposition to the developing world's \"South\").\n\nCorporatist ideology, which privileges the rights of corporations (artificial or juridical persons) over those of natural persons, is an underlying factor in the recent rapid expansion of global commerce. In recent years, there have been an increasing number of books (Naomi Klein's 2000 \"No Logo\", for example) and films (\"e.g. The Corporation\" & \"Surplus\") popularizing an anti-corporate ideology to the public.\n\nA related contemporary ideology, consumerism, which encourages the personal acquisition of goods and services, also drives globalization. Anti-consumerism is a social movement against equating personal happiness with consumption and the purchase of material possessions. Concern over the treatment of consumers by large corporations has spawned substantial activism, and the incorporation of consumer education into school curricula. Social activists hold materialism is connected to global retail merchandizing and supplier convergence, war, greed, anomie, crime, environmental degradation, and general social malaise and discontent. One variation on this topic is activism by \"postconsumers\", with the strategic emphasis on moving \"beyond\" addictive consumerism.\n\nThe global justice movement is the loose collection of individuals and groups—often referred to as a \"movement of movements\"—who advocate fair trade rules and perceive current institutions of global economic integration as problems. The movement is often labeled an anti-globalization movement by the mainstream media. Those involved, however, frequently deny that they are anti-globalization, insisting that they support the globalization of communication and people and oppose only the global expansion of corporate power. The movement is based in the idea of social justice, desiring the creation of a society or institution based on the principles of equality and solidarity, the values of human rights, and the dignity of every human being. Social inequality within and between nations, including a growing global digital divide, is a focal point of the movement. Many nongovernmental organizations have now arisen to fight these inequalities that many in Latin America, Africa and Asia face. A few very popular and well known non-governmental organizations (NGOs) include: War Child, Red Cross, Free The Children and CARE International. They often create partnerships where they work towards improving the lives of those who live in developing countries by building schools, fixing infrastructure, cleaning water supplies, purchasing equipment and supplies for hospitals, and other aid efforts.\n\nThe economies of the world have developed unevenly, historically, such that entire geographical regions were left mired in poverty and disease while others began to reduce poverty and disease on a wholesale basis. From around 1980 through at least 2011, the GDP gap, while still wide, appeared to be closing and, in some more rapidly developing countries, life expectancies began to rise. If we look at the Gini coefficient for world income, since the late 1980s, the gap between some regions has markedly narrowed—between Asia and the advanced economies of the West, for example—but huge gaps remain globally. Overall equality across humanity, considered as individuals, has improved very little. Within the decade between 2003 and 2013, income inequality grew even in traditionally egalitarian countries like Germany, Sweden and Denmark. With a few exceptions—France, Japan, Spain—the top 10 percent of earners in most advanced economies raced ahead, while the bottom 10 percent fell further behind. By 2013, 85 multibillionaires had amassed wealth equivalent to all the wealth owned by the poorest half (3.5 billion) of the world's total population of 7 billion.\n\nCritics of globalization argue that globalization results in weak labor unions: the surplus in cheap labor coupled with an ever-growing number of companies in transition weakened labor unions in high-cost areas. Unions become less effective and workers their enthusiasm for unions when membership begins to decline. They also cite an increase in the exploitation of child labor: countries with weak protections for children are vulnerable to infestation by rogue companies and criminal gangs who exploit them. Examples include quarrying, salvage, and farm work as well as trafficking, bondage, forced labor, prostitution and pornography.\n\nWomen often participate in the workforce in precarious work, including export-oriented employment. Evidence suggests that while globalization has expanded women's access to employment, the long-term goal of transforming gender inequalities remains unmet and appears unattainable without regulation of capital and a reorientation and expansion of the state's role in funding public goods and providing a social safety net. Furthermore, the intersectionality of gender, race, class, and more remain overlooked when assessing the impact of globalization.\n\nIn 2016, a study published by the IMF posited that neoliberalism, the ideological backbone of contemporary globalized capitalism, has been \"oversold\", with the benefits of neoliberal policies being \"fairly difficult to establish when looking at a broad group of countries\" and the costs, most significantly higher income inequality within nations, \"hurt the level and sustainability of growth.\"\n\nBeginning in the 1930s, opposition arose to the idea of a world government, as advocated by organizations such as the World Federalist Movement (WFM). Those who oppose global governance typically do so on objections that the idea is unfeasible, inevitably oppressive, or simply unnecessary. In general, these opponents are wary of the concentration of power or wealth that such governance might represent. Such reasoning dates back to the founding of the League of Nations and, later, the United Nations.\n\nEnvironmentalism is a broad philosophy, ideology and social movement regarding concerns for environmental conservation and improvement of the health of the environment. Environmentalist concerns with globalization include issues such as global warming, climate change, global water supply and water crises, inequity in energy consumption and energy conservation, transnational air pollution and pollution of the world ocean, overpopulation, world habitat sustainability, deforestation, biodiversity loss and species extinction.\n\nOne critique of globalization is that natural resources of the poor have been systematically taken over by the rich and the pollution promulgated by the rich is systematically dumped on the poor. Some argue that Northern corporations are increasingly exploiting resources of less wealthy countries for their global activities while it is the South that is disproportionately bearing the environmental burden of the globalized economy. Globalization is thus leading to a type of\" environmental apartheid\".\n\nHelena Norberg-Hodge, the director and founder of Local Futures/International Society for Ecology and Culture, criticizes globalization in many ways. In her book \"\", Norberg-Hodge claims that \"centuries of ecological balance and social harmony are under threat from the pressures of development and globalization.\" She also criticizes the standardization and rationalization of globalization, as it does not always yield the expected growth outcomes. Although globalization takes similar steps in most countries, scholars such as Hodge claim that it might not be effective to certain countries and that globalization has actually moved some countries backward instead of developing them.\n\nA related area of concern is the pollution haven hypothesis, which posits that, when large industrialized nations seek to set up factories or offices abroad, they will often look for the cheapest option in terms of resources and labor that offers the land and material access they require (see Race to the bottom). This often comes at the cost of environmentally sound practices. Developing countries with cheap resources and labor tend to have less stringent environmental regulations, and conversely, nations with stricter environmental regulations become more expensive for companies as a result of the costs associated with meeting these standards. Thus, companies that choose to physically invest in foreign countries tend to (re)locate to the countries with the lowest environmental standards or weakest enforcement.\n\nThe European Union–Mercosur Free Trade Agreement, which would form one of the world's largest free trade areas, has been denounced by environmental activists and indigenous rights campaigners. The fear is that the deal could lead to more deforestation of the Amazon rainforest as it expands market access to Brazilian beef.\n\nGlobalization is associated with a more efficient system of food production. This is because crops are grown in countries with optimum growing conditions. This improvement causes an increase in the world's food supply which encourages improved food security.\n\nNorway's limited crop range advocates globalization of food production and availability. The northern-most country in Europe requires trade with other countries to ensure population food demands are met. The degree of self-sufficiency in food production is around 50% in Norway.\n\n\n"}
